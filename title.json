{"Title": "Network A/B Testing: From Sampling to Estimation", "Abstract": "A/B testing, also known as bucket testing, split testing, or\ncontrolled experiment, is a standard way to evaluate user\nengagement or satisfaction from a new service, feature, or\nproduct. It is widely used in online websites, including social\nnetwork sites such as Facebook, LinkedIn, and Twitter to\nmake data-driven decisions. The goal of A/B testing is to es-\ntimate the treatment effects of a new change, which becomes\nintricate when users are interacting, i.e., the treatment ef-\nfects of a user may spill over to other users via underlying\nsocial connections. When conducting these online controlled\nexperiments, it is a common practice to make the Stable\nUnit Treatment Value Assumption (SUTVA) that each in-\ndividual\u2019s response is affected by their own treatment only.\nThough this assumption simplifies the estimation of treat-\nment effects, it does not hold when network interference is\npresent, and may even lead to wrong conclusion.\nIn this paper, we study the problem of network A/B test-\ning in real networks, which have substantially different char-\nacteristics from the simulated random networks studied in\nprevious works. We first examine the existence of network\neffects in a recent online experiment conducted at LinkedIn;\nSecondly, we propose an efficient and effective estimator for\nAverage Treatment Effect (ATE) considering the interfer-\nence between users in real online experiments; Finally, we\napply our method in both simulations and a real world online\nexperiment. The simulation results show that our estimator\nachieves better performance with respect to both bias and\nvariance reduction. The real world online experiment not\nonly demonstrates that large-scale network A/B test is fea-\nsible but also further validates many of our observations in\nthe simulation studies."}
{"Title": "Always Valid Inference: Continuous Monitoring of\nA/B Tests", "Abstract": "A/B tests are typically analyzed via frequentist p-values and confidence intervals; but these inferences are\nwholly unreliable if users endogenously choose samples sizes by continuously monitoring their tests. We\ndefine always valid p-values and confidence intervals that let users try to take advantage of data as fast as\nit becomes available, providing valid statistical inference whenever they make their decision. Always valid\ninference can be interpreted as a natural interface for a sequential hypothesis test, which empowers users to\nimplement a modified test tailored to them. In particular, we show in an appropriate sense that the measures\nwe develop tradeoff sample size and power efficiently, despite a lack of prior knowledge of the user\u2019s relative\npreference between these two goals. We also use always valid p-values to obtain multiple hypothesis testing\ncontrol in the sequential context. Our methodology has been implemented in a large scale commercial A/B\ntesting platform to analyze hundreds of thousands of experiments to date."}
{"Title": "Continuous Monitoring of A/B Tests without Pain: Optional\nStopping in Bayesian Testing", "Abstract": "A/B testing is one of the most successful applications of\nstatistical theory in modern Internet age. One problem of\nNull Hypothesis Statistical Testing (NHST), the backbone\nof A/B testing methodology, is that experimenters are not\nallowed to continuously monitor the result and make de-\ncision in real time. Many people see this restriction as\na setback against the trend in the technology toward real\ntime data analytics. Recently, Bayesian Hypothesis Test-\ning, which intuitively is more suitable for real time deci-\nsion making, attracted growing interest as an alternative to\nNHST. While corrections of NHST for the continuous mon-\nitoring setting are well established in the existing literature\nand known in A/B testing community, the debate over the\nissue of whether continuous monitoring is a proper practice\nin Bayesian testing exists among both academic researchers\nand general practitioners. In this paper, we formally prove\nthe validity of Bayesian testing with continuous monitoring\nwhen proper stopping rules are used, and illustrate the the-\noretical results with concrete simulation illustrations. We\npoint out common bad practices where stopping rules are\nnot proper and also compare our methodology to NHST\ncorrections. General guidelines for researchers and practi-\ntioners are also provided."}
{"Title": "Offline A/B testing for Recommender Systems", "Abstract": "Online A/B testing evaluates the impact of a new technology by\nrunning it in a real production environment and testing its perfor-\nmance on a subset of the users of the platform. It is a well-known\npractice to run a preliminary offline evaluation on historical data\nto iterate faster on new ideas, and to detect poor policies in order\nto avoid losing money or breaking the system. For such offline\nevaluations, we are interested in methods that can compute offline\nan estimate of the potential uplift of performance generated by a\nnew technology. Offline performance can be measured using esti-\nmators knownas counterfactual oroff-policy estimators.Traditional\ncounterfactual estimators, such as capped importance sampling or\nnormalised importance sampling, exhibit unsatisfying bias-variance\ncompromises when experimenting on personalized product rec-\nommendation systems. To overcome this issue, we model the bias\nincurred by these estimators rather than bound it in the worst\ncase, which leads us to propose a new counterfactual estimator.\nWe provide a benchmark of the different estimators showing their\ncorrelation with business metrics observed by running online A/B\ntests on a large-scale commercial recommender system."}
{"Title": "Online Controlled Experiments and\nA/B Testing", "Abstract": "The Internet connectivity of client software (e.g.,\napps running on phones and PCs), websites, and\nonline services provide an unprecedented oppor-\ntunity to evaluate ideas quickly using controlled\nexperiments, also called A/B tests, split tests,\nrandomized experiments, control/treatment tests,\nand online field experiments. Unlike most data\nmining techniques for finding correlational pat-\nterns, controlled experiments allow establishing a\ncausal relationship with high probability. Experi-\nmenters can utilize the scientific method to form\na hypothesis of the form \u201cIf a specific change\nis introduced, will it improve key metrics?\u201d and\nevaluate it with real users."}
{"Title": "From Infrastructure to Culture: A/B Testing Challenges in\nLarge Scale Social Networks", "Abstract": "A/B testing, also known as bucket testing, split testing, or\ncontrolled experiment, is a standard way to evaluate user\nengagement or satisfaction from a new service, feature, or\nproduct. It is widely used among online websites, including social\nnetwork sites such as Facebook, LinkedIn, and Twitter to make\ndata-driven decisions. At LinkedIn, we have seen tremendous\ngrowth of controlled experiments over time, with now over 400\nconcurrent experiments running per day. General A/B testing\nframeworks and methodologies, including challenges and pitfalls,\nhave been discussed extensively in several previous KDD work\n[7, 8, 9, 10]. In this paper, we describe in depth the\nexperimentation platform we have built at LinkedIn and the\nchallenges that arise particularly when running A/B tests at large\nscale in a social network setting. We start with an introduction of\nthe experimentation platform and how it is built to handle each\nstep of the A/B testing process at LinkedIn, from designing and\ndeploying experiments to analyzing them. It is then followed by\ndiscussions on several more sophisticated A/B testing scenarios,\nsuch as running offline experiments and addressing the network\neffect, where one user\u2019s action can influence that of another.\nLastly, we talk about features and processes that are crucial for\nbuilding a strong experimentation culture."}
{"Title": "On the Complexity of A/B Testing", "Abstract": "A/B testing refers to the task of determining the best option among two alternatives that yield\nrandom outcomes. We provide distribution-dependent lower bounds for the performance of A/B\ntesting that improve over the results currently available both in the fixed-confidence (or \u03b4-PAC) and\nfixed-budget settings. When the distribution of the outcomes are Gaussian, we prove that the com-\nplexity of the fixed-confidence and fixed-budget settings are equivalent, and that uniform sampling\nof both alternatives is optimal only in the case of equal variances. In the common variance case,\nwe also provide a stopping rule that terminates faster than existing fixed-confidence algorithms. In\nthe case of Bernoulli distributions, we show that the complexity of fixed-budget setting is smaller\nthan that of fixed-confidence setting and that uniform sampling of both alternatives\u2014though not\noptimal\u2014is advisable in practice when combined with an appropriate stopping criterion."}
{"Title": "Research Landscape of Business Intelligence and Big Data analytics: A\nbibliometrics study", "Abstract": "Business Intelligence that applies data analytics to generate key information to support business deci-\nsion making, has been an important area for more than two decades. In the last five years, the trend of\n\u201cBig Data\u201d has emerged and become a core element of Business Intelligence research. In this article, we\nreview academic literature associated with \u201cBig Data\u201d and \u201cBusiness Intelligence\u201d to explore the devel-\nopment and research trends. We use bibliometric methods to analyze publications from 1990 to 2017 in\njournals indexed in Science Citation Index Expanded (SCIE), Social Science Citation Index (SSCI) and Arts\n& Humanities Citation Index (AHCI). We map the time trend, disciplinary distribution, high-frequency\nkeywords to show emerging topics. The findings indicate that Computer Science and management in-\nformation systems are two core disciplines that drive research associated with Big Data and Business\nIntelligence. \u201cData mining\u201d, \u201csocial media\u201d and \u201cinformation system\u201d are high frequency keywords, but\n\u201ccloud computing\u201d, \u201cdata warehouse\u201d and \u201cknowledge management\u201d are more emphasized after 2016."}
{"Title": "Clustering retail products based on customer behaviour", "Abstract": "The categorization\nof retail products is essential for the business decision-making process. It is a common\npractice to classify products based on their quantitative and qualitative characteristics. In this paper,\nwe\nuse a purely data-driven approach. Our clustering of products is based exclusively on the customer\nbehaviour. We propose a method for clustering retail products using market basket data. Our model\nis\nformulated as an optimization problem which is solved by a genetic algorithm. It is demonstrated on\nsimulated data how our method behaves in different settings. The application using real data from a Czech\ndrugstore\ncompany shows that our method leads to similar results in comparison with the classification\nby\nexperts. The number of clusters is a parameter of our algorithm. We demonstrate that if more clusters\nare\nallowed than the original number of categories is, the method yields additional information about\nthe\nstructure of the product categorization."}
{"Title": "A product network analysis for extending the market basket analysis", "Abstract": "In this study, we propose a product network analysis, a network-based analysis to analyze a network-\nleveled relation among all products. Compared to market basket analysis, which focuses on the transac-\ntion-leveled relation between products, the suggested product network analysis focuses on extended\nnetwork-leveled point of view of the relation between all products. For such a purpose, we suggest\ntwo kinds of product networks, market basket networks and co-purchased product networks. Two net-\nworks are comparatively evaluated to analyze the topological characteristics and the structure of those\nnetworks. The extended use of market basket analysis, network-leveled analysis are expected to be more\neffectively and efficiently used in personalized services, such as cross selling, up selling, and personalized\nproduct display utilizing the deep relation between products."}
{"Title": "A user similarity-based Top- N recommendation approach for mobile\nin-application advertising", "Abstract": "Ensuring scalability of recommender systems without sacrificing the quality of the recommendations pro-\nduced, presents significant challenges, especially in the large-scale, real-world setting of mobile ad tar-\ngeting. In this paper, we propose MobRec, a novel two-stage user similarity based approach to recom-\nmendation which combines information provided by slowly-changing features of the mobile context and\nimplicit user feedback indicative of user preferences. MobRec uses the contextual features to cluster, dur-\ning an off-line stage, users that share similar patterns of mobile behavior. In the online stage, MobRec\nfocuses on the cluster consisting of users that are most similar to the target user in terms of their con-\ntextual features as well as implicit feedback. MobRec also employs a novel strategy for robust estimation\nof user preferences from noisy clicks. Results of experiments using a large-scale real-world mobile adver-\ntising dataset demonstrate that MobRec outperforms the state-of-the-art neighborhood-based as well as\nlatent factor-based recommender systems, in terms of both scalability and the quality of the recommen-\ndations."}
{"Title": "An experimental comparison of classification techniques in debt\nrecoveries scoring: Evidence from South Africa\u2019s unsecured lending\nmarket", "Abstract": "In South Africa, almost 50% of the people who take loans cannot afford it. Previously, lenders were able to\nmake deductions from a borrower\u2019s payslip but this practice is no longer allowed. Consequently, lenders\nare now far more vulnerable to default particularly if these loans are no longer being backed by any form\nof meaningful collateral. The aim of this study is to investigate the predictive power of some of the more\npopular classification techniques currently in use with specific attention to predicting the propensity for\na borrower who is 90 days or more in arrears on an unsecured loan to pay over a fixed window period\nat least 30% of the total amount due. Results show that these classification techniques perform best for\npredicting payment patterns over a future horizon period between 3 and 12 months. It is also found\nthat generalized additive models (especially using a generalized extreme value link function), which have\nnot been extensively explored within the credit scoring literature, outperformed all the other classifiers\nconsidered in this study."}
{"Title": "Benefit-based consumer segmentation and performance evaluation of\nclustering approaches: An evidence of data-driven decision-making", "Abstract": "This study evaluates the performance of different data clustering approaches for searching the profitable\nconsumer segments in the UK hospitality industry. The paper focuses on three aspects of datasets includ-\ning the ordinal nature of data, high dimensionality and outliers. Data collected from 513 sample points\nare analysed in this paper using four clustering approaches: Hierarchical clustering, K-Medoids, fuzzy\nclustering, and Self-Organising Maps (SOM). The findings suggest that Fuzzy and SOM based clustering\ntechniques are comparatively more efficient than traditional approaches in revealing the hidden struc-\nture in the data set. The segments derived from SOM has more capability to provide interesting insights\nfor data-driven decision making in practice. This study makes a significant contribution to literature by\ncomparing different clustering approaches and addressing misconceptions of using these for market seg-mentation to support data-driven decision making in business practices."}
{"Title": "Do Retailers Benefit from Deploying Customer Analytics?", "Abstract": "Prior\nresearch has documented a general positive relationship between the deployment of customer analytics and firm performance. In this\nresearch we focus\non the retailing industry, an industry characterized by tight margins that lead to careful scrutiny of all business investments.\nUsing survey data from 418 top managers based in the Americas, Europe Middle East and Africa (EMEA) and Asia, we show that of the eight\nindustries in\nthe study,\nfirms in the retail industry have the most to gain from deploying customer analytics. However, we also find that not only\ndo many retailers not perceive this potential gain, they do not invest in customer analytics at an economically appropriate level. Thus we identify\na gap between perception and reality concerning the potential for customer analytics in the retail industry that has both theoretical and practical\nimplications."}
{"Title": "Extending market basket analysis with graph mining\ntechniques: A real case", "Abstract": "A common problem for many companies, like retail stores, it is to find sets of products that are sold\ntogether. The only source of information available is the history of sales transactional data. Common\ntechniques of market basket analysis fail when processing huge amounts of scattered data, finding\nmeaningless relationships. We developed a novel approach for market basket analysis based on graph\nmining techniques, able to process millions of scattered transactions. We demonstrate the effectiveness\nof our approach in a wholesale supermarket chain and a retail supermarket chain, processing around\n238,000,000 and 128,000,000 transactions respectively compared to classical approach."}
{"Title": "Making sense of organization dynamics using text analysis", "Abstract": "Being able to understand the implicit power structures and dynamics among members plays a crucial\nrole for the management of the organization. This paper introduces a novel and comprehensive approach\nto analyzing organizational discourse data. The proposed approach provides a holistic view of the power\nstructure implied by the communications. The paper contributes to the domain of text mining by inte-\ngrating various text-mining techniques to demonstrating different aspects of a power structure within an\norganization. It also contributes to the domain of supply chain management by using the conventional\ncommunication discourse method as the guideline for the development of the tool. We applied the pro-\nposed approach to a seven-year collection of meeting minutes from a co-op and our findings were largely\nconfirmed by members of the organization. We provide a roadmap of using the multi-aspect approach to\nanalyzing organizational discourse data in supply networks."}
{"Title": "Method to interactively visualize and navigate related information", "Abstract": "In this paper, a formal method is presented to: (1) establish a system to visualize huge quantities of\nrelated information (2) graphically show the degree of similarity between these related documents (3)\nnavigate between related documents so that the views are reconfigured in a dynamic and gradual way.\nThis method uses, as input, a set of documents that have been related using any existing algorithm (re-\nlationships based on contents, on votes to contents, on social relations, on demographic information,\netc.). The proposed method includes two different visualization and navigational versions related to: (a)\nvisualization by explicit graph and (b) visualization by implicit graph. As a summary, the following char-\nacteristics are presented: (1) this method maximizes the information that is displayed at each moment,\nso that a balance is found between: (a) the number of relations shown, (b) the number of documents\nviewed, (c) the amount of information presented by each document and (d) how to show the degree\nof similarity between documents, (2) the method includes specific rules of interaction with the user to\nallow navigation in a simple, gradual and intuitive way."}
{"Title": "Retail business analytics: Customer visit segmentation using market basket data", "Abstract": "Basket analytics is a powerful tool in the retail context for acquiring knowledge about consumer shopping\nhabits and preferences. In this paper, we propose a business analytics approach that mines customer visit\nsegments from basket sales data. We characterize a customer visit by the purchased product categories\nin the basket and identify the shopping intention or mission behind the visit e.g. a \u2018breakfast\u2019 visit to\npurchase cereal, milk, bread, cheese etc. We also suggest a semi-supervised feature selection approach\nthat uses the product taxonomy as input and suggests customized categories as output. This approach is\nutilized to balance the product taxonomy tree that has a significant effect on the data mining results. We\ndemonstrate the utility of our approach by applying it to a real case of a major European fast-moving\nconsumer goods (FMCG) retailer. Apart from its theoretical contribution, the proposed approach extracts\nknowledge that may support several decisions ranging from marketing campaigns per customer segment,\nredesign of a store\u2019s layout to product recommendations."}
{"Title": "Social media big data integration: A new approach based on\ncalibration", "Abstract": "In recent years, the growing availability of huge amounts of information, generated in every sector at high\nspeed and in a wide variety of forms and formats, is unprecedented. The ability to harness big data is an\nopportunity to obtain more accurate analyses and to improve decision-making in industry, government\nand many other organizations. However, handling big data may be challenging and proper data integra-\ntion is a key dimension in achieving high information quality. In this paper, we propose a novel approach\nto data integration that calibrates online generated big data with interview based customer survey data.\nA common issue of customer surveys is that responses are often overly positive, making it difficult to\nidentify areas of weaknesses in organizations. On the other hand, online reviews are often overly nega-\ntive, hampering an accurate evaluation of areas of excellence. The proposed methodology calibrates the\nlevels of unbalanced responses in different data sources via resampling and performs data integration\nusing Bayesian Networks to propagate the new re-balanced information. In this paper we show, with a\ncase study example, how the novel data integration approach allows businesses and organizations to get\na bias corrected appraisal of the level of satisfaction of their customers. The application is based on the\nintegration of online data of review blogs and customer satisfaction surveys from the San Francisco air-\nport. We illustrate how this integration enhances the information quality of the data analytic work in\nfour of InfoQ dimensions, namely, Data Structure, Data Integration, Temporal Relevance and Chronology\nof Data and Goal."}
{"Title": "Structural correlation between communities and core-periphery\nstructures in social networks: Evidence from Twitter data", "Abstract": "Social media, such as Twitter and Facebook, have become important venues for business and individu-\nals. Social interactions between actors result in the formation of meso\u2013scale subgroup structures, such as\ncommunities. Community detection is a classic task of social network analysis. Identification of another\nmeso\u2013scale structure, named core-periphery, arises recently. Much existing research discriminated com-\nmunities from core-periphery structures, and performed the two tasks individually by completely differ-\nent methodologies. The two meso\u2013scale structures can both attribute to unequal influence and asymmet-\nric interactions of actors in social networks. This research tries to unify communities and core-periphery\nstructures by regarding the two subgroup structures as the same thing, by which community structure\ncharacterizes the boundary of a subgroup and core-periphery structure characterizes its internal struc-\nture. Experiments are conducted on one-month twitter data, and results can provide empirical evidences\nthat social communities always have core-periphery structures."}
{"Title": "Towards a big data framework for analyzing social media content", "Abstract": "Modern companies generate value by digitalizing their services and products. Knowing what customers are\nsaying about the firm through reviews in social media content constitutes a key factor to succeed in the big data\nera. However, social media data analysis is a complex discipline due to the subjectivity in text review and the\nadditional features in raw data. Some frameworks proposed in the existing literature involve many steps that\nthereby increase their complexity. A two-stage framework to tackle this problem is proposed: the first stage is\nfocused on data preparation and finding an optimal machine learning model for this data; the second stage relies\non established layers of big data architectures focused on getting an outcome of data by taking most of the\nmachine learning model of stage one. Thus, a first stage is proposed to analyze big and small datasets in a non-\nbig data environment, whereas the second stage analyzes big datasets by applying the first stage machine\nlearning model of. Then, a study case is presented for the first stage of the framework to analyze reviews of hotel-\nrelated businesses. Several machine learning algorithms were trained for two, three and five classes, with the\nbest results being found for binary classification."}
{"Title": "An Insight Extraction System on BioMedical Literature\nwith Deep Neural Networks", "Abstract": "Mining biomedical text offers an oppor-\ntunity to automatically discover important\nfacts and infer associations among them.\nAs new scientific findings appear across\na large collection of biomedical publica-\ntions, our aim is to tap into this literature\nto automate biomedical knowledge extrac-\ntion and identify important insights from\nthem. Towards that goal, we develop a\nsystem with novel deep neural networks\nto extract insights on biomedical litera-\nture. Evaluation shows our system is able\nto provide insights with competitive accu-\nracy of human acceptance and its relation\nextraction component outperforms previ-\nous work."}
{"Title": "Author-aware Aspect Topic Sentiment Model to Retrieve Supporting\nOpinions from Reviews", "Abstract": "User generated content about products and\nservices in the form of reviews are of-\nten diverse and even contradictory. This\nmakes it difficult for users to know if an\nopinion in a review is prevalent or bi-\nased. We study the problem of search-\ning for supporting opinions in the con-\ntext of reviews. We propose a framework\ncalled SURF, that first identifies opinions\nexpressed in a review, and then finds sim-\nilar opinions from other reviews. We de-\nsign a novel probabilistic graphical model\nthat captures opinions as a combination\nof aspect, topic and sentiment dimensions,\ntakes into account the preferences of indi-\nvidual authors, as well as the quality of the\nentity under review, and encodes the flow\nof thoughts in a review by constraining\nthe aspect distribution dynamically among\nsuccessive review segments. We derive a\nsimilarity measure that considers both lex-\nical and semantic similarity to find sup-\nporting opinions. Experiments on TripAd-\nvisor hotel reviews and Yelp restaurant re-\nviews show that our model outperforms\nexisting methods for modeling opinions,\nand the proposed framework is effective in\nfinding supporting opinions."}
{"Title": "Capturing User and Product Information for Document Level Sentiment\nAnalysis with Deep Memory Network", "Abstract": "Document-level sentiment classification is\na fundamental problem which aims to pre-\ndict a user\u2019s overall sentiment about a\nproduct in a document. Several methods\nhave been proposed to tackle the problem\nwhereas most of them fail to consider the\ninfluence of users who express the senti-\nmentandproductswhichareevaluated. To\naddress the issue, we propose a deep mem-\nory network for document-level sentiment\nclassification which could capture the user\nand product information at the same time.\nTo prove the effectiveness of our algo-\nrithm, we conduct experiments on IMDB\nand Yelp datasets and the results indicate\nthat our model can achieve better perfor-\nmance than several existing methods."}
{"Title": "Context-Aware Representations\nfor Knowledge Base Relation Extraction", "Abstract": "We demonstrate that for sentence-level relation extraction it is beneficial to consider\nother relations in the sentential context\nwhile predicting the target relation. Our\narchitecture uses an LSTM-based encoder\nto jointly learn representations for all rela-\ntions in a single sentence. We combine the\ncontext representations with an attention\nmechanism to make the final prediction.\nWe use the Wikidata knowledge base to\nconstruct a dataset of multiple relations\nper sentence and to evaluate our approach.\nComparedtoabaselinesystem, ourmethod\nresults in an average error reduction of 24%\non a held-out set of relations.\nThe code and the dataset to replicate\nthe experiments are made available at\nhttps://github.com/ukplab."}
{"Title": "Detecting Perspectives in Political Debates", "Abstract": "We explore how to detect people\u2019s per-\nspectives that occupy a certain proposi-\ntion. We propose a Bayesian modelling\napproach where topics (or propositions)\nand their associated perspectives (or view-\npoints) are modeled as latent variables.\nWords associated with topics or perspec-\ntives follow different generative routes.\nBased on the extracted perspectives, we\ncan extract the top associated sentences\nfrom text to generate a succinct summary\nwhich allows a quick glimpse of the main\nviewpoints in a document. The model is\nevaluated on debates from the House of\nCommons of the UK Parliament, reveal-\ning perspectives from the debates without\nthe use of labelled data and obtaining bet-\nter results than previous related solutions\nunder a variety of evaluations."}
{"Title": "Dimensions of Interpersonal Relationships: Corpus and Experiments", "Abstract": "This paper presents a corpus and exper-\niments to determine dimensions of inter-\npersonal relationships. We define a set of\ndimensions heavily inspired byworkinso-\ncial science. Wecreate a corpus by retriev-\ning pairs of people, and then annotating\ndimensions for their relationships. A cor-\npus analysis shows that dimensions can be\nannotated reliably. Experimental results\nshow that given a pair of people, values to\ndimensions can be assigned automatically."}
{"Title": "Document-Level Multi-Aspect Sentiment Classification as\nMachine Comprehension", "Abstract": "Document-level multi-aspect sentiment\nclassification is an important task for cus-\ntomer relation management. In this paper,\nwe model the task as a machine compre-\nhension problem where pseudo question-\nanswer pairs are constructed by a small\nnumberofaspect-relatedkeywordsandas-\npect ratings. A hierarchical iterative atten-\ntion model is introduced to build aspect-\nspecific representations by frequent and\nrepeated interactions between documents\nand aspect questions. We adopt a hi-\nerarchical architecture to represent both\nword level and sentence level informa-\ntion, and use the attention operations for\naspect questions and documents alterna-\ntively with the multiple hop mechanism.\nExperimental results on the TripAdvisor\nand BeerAdvocate datasets show that "}
{"Title": "A Framework for the Experimental Comparison\nof Graph-based Document Exploration Techniques", "Abstract": "Graphs have long been proposed as a tool\nto browse and navigate in a collection of\ndocuments in order to support exploratory\nsearch. Many techniques to automatically\nextract different types of graphs, showing\nforexampleentitiesorconceptsanddiffer-\nent relationships between them, have been\nsuggested. While experimental evidence\nthat they are indeed helpful exists for some\nof them, it is largely unknown which type\nof graph is most helpful for a specific ex-\nploratory task. However, carrying out ex-\nperimental comparisons with human sub-\njects is challenging and time-consuming.\nTowards this end, we present the Graph-\nDocExplore framework. It provides an in-\ntuitive web interface for graph-based doc-\nument exploration that is optimized for ex-\nperimentaluserstudies. Throughageneric\ngraph interface, different methods to ex-\ntract graphs from text can be plugged into\nthe system. Hence, they can be compared\nat minimal implementation effort in an en-\nvironment that ensures controlled compar-\nisons. The system is publicly available un-\nder an open-source license. 1"}
{"Title": "Learning Fine-grained Relations from Chinese User Generated Categories", "Abstract": "User generated categories (UGCs) are\nshort texts that reflect how people describe\nand organize entities, expressing rich se-\nmantic relations implicitly. While most\nmethods on UGC relation extraction are\nbased on pattern matching in English cir-\ncumstances, learning relations from Chi-\nnese UGCs poses different challenges due\nto the flexibility of expressions. In this pa-\nper, we present a weakly supervised learn-\ning framework to harvest relations from\nChinese UGCs. We identify is-a relations\nvia word embedding based projection and\ninference, extract non-taxonomic relations\nand their category patterns by graph min-\ning. We conduct experiments on Chinese\nWikipedia and achieve high accuracy, out-\nperforming state-of-the-art methods."}
{"Title": "Scientific Information Extraction with Semi-supervised Neural Tagging", "Abstract": "This paper addresses the problem of ex-\ntracting keyphrases from scientific articles\nand categorizing them as corresponding to\na task, process, or material. We cast the\nproblem as sequence tagging and intro-\nduce semi-supervised methods to a neu-\nral tagging model, which builds on re-\ncent advances in named entity recognition.\nSince annotated training data is scarce in\nthis domain, we introduce a graph-based\nsemi-supervised algorithm together with a\ndata selection scheme to leverage unanno-\ntated articles. Both inductive and trans-\nductive semi-supervised learning strate-\ngies outperform state-of-the-art informa-\ntion extraction performance on the 2017\nSemEval Task 10 ScienceIE task."}
{"Title": "Entity Linking meets Word Sense Disambiguation: a Unified Approach", "Abstract": "Entity Linking (EL) and Word Sense Disam-\nbiguation (WSD) both address the lexical am-\nbiguity of language. But while the two tasks\nare pretty similar, they differ in a fundamen-\ntal respect: in EL the textual mention can be\nlinkedtoanamedentitywhichmayormaynot\ncontaintheexactmention, whileinWSDthere\nis a perfect match between the word form (bet-\nter, its lemma) and a suitable word sense.\nIn this paper we present Babelfy, a unified\ngraph-based approach to EL and WSD based\non a loose identification of candidate mean-\nings coupled with a densest subgraph heuris-\ntic which selects high-coherence semantic in-\nterpretations. Our experiments show state-of-\nthe-art performances on both tasks on 6 differ-\nent datasets, including a multilingual setting.\nBabelfy is online at http://babelfy.org"}
{"Title": "Deep Feature Synthesis:\nTowards Automating Data Science Endeavors", "Abstract": "Abstract\u2014In this paper, we develop the Data Science Ma-\nchine, which is able to derive predictive models from raw data\nautomatically. To achieve this automation, we first propose and\ndevelop the Deep Feature Synthesis algorithm for automatically\ngenerating features for relational datasets. The algorithm follows\nrelationships in the data to a base field, and then sequentially\napplies mathematical functions along that path to create the final\nfeature. Second, we implement a generalizable machine learning\npipeline and tune it using a novel Gaussian Copula process based\napproach. We entered the Data Science Machine in 3 data science\ncompetitions that featured 906 other data science teams. Our\napproach beats 615 teams in these data science competitions. In\n2 of the 3 competitions we beat a majority of competitors, and\nin the third, we achieved 94% of the best competitor\u2019s score. In\nthe best case, with an ongoing competition, we beat 85.6% of the\nteams and achieved 95.7% of the top submissions score."}
{"Title": "One button machine for automating feature\nengineering in relational databases", "Abstract": "Abstract\u2014Feature engineering is one of the most important\nand time consuming tasks in predictive analytics projects. It\ninvolves understanding domain knowledge and data exploration\nto discover relevant hand-crafted features from raw data. In this\npaper, we introduce a system called One Button Machine, or\nOneBM for short, which automates feature discovery in relational\ndatabases. OneBM automatically performs a key activity of\ndata scientists, namely, joining of database tables and applying\nadvanced data transformations to extract useful features from\ndata. We validated OneBM in Kaggle competitions in which\nOneBM achieved performance as good as top 16% to 24%\ndata scientists in three Kaggle competitions. More importantly,\nOneBM outperformed the state-of-the-art system in a Kaggle\ncompetition in terms of prediction accuracy and ranking on\nKaggle leaderboard. The results show that OneBM can be useful\nfor both data scientists and non-experts. It helps data scientists\nreduce data exploration time allowing them to try and error many\nideas in short time. On the other hand, it enables non-experts,\nwho are not familiar with data science, to quickly extract value\nfrom their data with a little effort, time and cost."}
{"Title": "ExploreKit: Automatic Feature Generation and\nSelection", "Abstract": "Abstract\u2014Feature generation is one of the challenging aspects\nof machine learning. We present ExploreKit, a framework for\nautomated feature generation. ExploreKit generates a large set\nof candidate features by combining information in the original\nfeatures, with the aim of maximizing predictive performance\naccording to user-selected criteria. To overcome the exponential\ngrowth of the feature space, ExploreKit uses a novel ma-\nchine learning-based feature selection approach to predict the\nusefulness of new candidate features. This approach enables\nefficient identification of the new features and produces superior\nresults compared to existing feature selection solutions. We\ndemonstrate the effectiveness and robustness of our approach\nby conducting an extensive evaluation on 25 datasets and 3\ndifferent classification algorithms. We show that ExploreKit can\nachieve classification-error reduction of 20% overall. Our code\nis available at https://github.com/giladkatz/ExploreKit."}
{"Title": "Cognito: Automated Feature Engineering for\nSupervised Learning", "Abstract": "Abstract\u2014Feature engineering involves constructing novel fea-\ntures from given data with the goal of improving predictive\nlearning performance. Feature engineering is predominantly a\nhuman-intensive and time consuming step that is central to the\ndata science workflow. In this paper, we present a novel system\ncalled Cognito, that performs automatic feature engineering on\na given dataset for supervised learning. The system explores\nvarious feature construction choices in a hierarchical and non-\nexhaustive manner, while progressively maximizing the accuracy\nof the model through a greedy exploration strategy. Additionally,\nthe system allows users to specify domain or data specific choices\nto prioritize the exploration. Cognito is capable of handling large\ndatasets through sampling and built-inparallelism, and integrates\nwell with a state-of-the-art model selection strategy. We present\nthe design and operation of Cognito, along with experimental\nresults on eight real datasets to demonstrate its efficacy."}
{"Title": "AppUsage2Vec: Modeling Smartphone App Usage\nfor Prediction", "Abstract": "Abstract\u2014App usage prediction, i.e. which apps will be used\nnext, is very useful for smartphone system optimization, such\nas operating system resource management, battery energy con-\nsumption optimization, and user experience improvement as well.\nHowever, it is still challenging to achieve usage prediction of high\naccuracy. In this paper, we propose a novel framework for app\nusage prediction, called AppUsage2Vec, inspired by Doc2Vec. It\nmodels app usage records by considering the contribution of\ndifferent apps, user personalized characteristics, and temporal\ncontext. We measure the contribution of each app to the target\napp by introducing an app-attention mechanism. The user\npersonalized characteristics in app usage are learned by a module\nof dual-DNN. Furthermore, we encode the top-k supervised\ninformation in loss function for training the model to predict\nthe app most likely to be used next. The AppUsage2Vec was\nevaluated on a dataset of 10,360 users and 46,434,380 records\nin three months. The results demonstrate the state-of-the-art\nperformance."}
{"Title": "Accurate Product Attribute Extraction on the Field", "Abstract": "Abstract\u2014In this paper we present a bootstrapping approach\nfor attribute value extraction that minimizes the need for human\nintervention.\nOur approach automatically extracts attribute names and val-\nues from semi-structured text, generates a small labelled dataset,\nand bootstraps it by extracting new values from unstructured\ntext. It is domain/language-independent, relying only on existing\nsemi-structured text to create the initial labeled dataset.\nWe assess the impact of different machine learning approaches\nto increase precision of the core approach without compromising\ncoverage.\nWe perform an extensive evaluation using e-commerce product\ndata across different categories in two languages and hundreds of\nthousands of product pages. We show that our approach provides\nhigh precision and good coverage. In addition, we study the\nimpact of different methods that address specific sources of error.\nWith error analysis we highlight how these methods complement\neach other, obtaining insights about the individual methods and\nthe ensamble as a whole"}
{"Title": "Empirical characterization of random forest\nvariable importance measures", "Abstract": "Microarray studies yield data sets consisting of a large number of candidate predictors (genes) on a small number of observations\n(samples). When interest lies in predicting phenotypic class using gene expression data, often the goals are both to produce an\naccurate classifier and to uncover the predictive structure of the problem. Most machine learning methods, such as k-nearest\nneighbors, support vector machines, and neural networks, are useful for classification. However, these methods provide no insight\nregarding the covariates that best contribute to the predictive structure. Other methods, such as linear discriminant analysis, require\nthe predictor space be substantially reduced prior to deriving the classifier.A recently developed method, random forests (RF), does\nnot require reduction of the predictor space prior to classification. Additionally, RF yield variable importance measures for each\ncandidate predictor. This study examined the effectiveness of RF variable importance measures in identifying the true predictor\namong a large number of candidate predictors. An extensive simulation study was conducted using 20 levels of correlation among\nthe predictor variables and 7 levels of association between the true predictor and the dichotomous response. We conclude that the\nRF methodology is attractive for use in classification problems when the goals of the study are to produce an accurate classifier and\nto provide insight regarding the discriminative ability of individual predictor variables. Such goals are common among microarray\nstudies, and therefore application of the RF methodology for the purpose of obtaining variable importance measures is demonstrated\non a microarray data set."}
{"Title": "Correlation and variable importance in random forests", "Abstract": "This paper is about variable selection with the random forests algorithm in presence of\ncorrelated predictors. In high-dimensional regression or classification frameworks, variable\nselection is a difficult task, that becomes even more challenging in the presence of highly\ncorrelated predictors. Firstly we provide a theoretical study of the permutation importance\nmeasure for an additive regression model. This allows us to describe how the correlation\nbetween predictors impacts the permutation importance. Our results motivate the use of\nthe Recursive Feature Elimination (RFE) algorithm for variable selection in this context.\nThis algorithm recursively eliminates the variables using permutation importance measure\nas a ranking criterion. Next various simulation experiments illustrate the efficiency of the\nRFE algorithm for selecting a small number of variables together with a good prediction\nerror. Finally, this selection algorithm is tested on the Landsat Satellite data from the UCI\nMachine Learning Repository."}
{"Title": "Variable importance in regression models", "Abstract": "Regression analysis is one of the most-used statistical methods. Often part of the research question is\nthe identification of the most important regressors or an importance ranking of the regressors. Most\nregression models are not specifically suited for answering the variable importance question, so that\nmany different proposals have been made. This article reviews in detail the various variable\nimportance metrics for the linear model, particularly emphasizing variance decomposition metrics. All\nlinear model metrics are illustrated by an example analysis. For non-linear parametric models, several\nprinciples from linear models have been adapted, and machine-learning methods have their own set of\nvariable importance methods. These are also briefly covered. Although there are many variable\nimportance metrics, there is still no convincing theoretical basis for them, and they all have a heuristic\ntouch. Nevertheless, some metrics are considered useful for a crude assessment in the absence of a\ngood subject matter theory."}
{"Title": "Variable importance analysis: A comprehensive review", "Abstract": "Abstract: Measuring variable importance for computational models or measured data is an important task in\nmany applications. It has drawn our attention that the variable importance analysis (VIA) techniques were\ndeveloped independently in many disciplines. We are strongly aware of the necessity to aggregate all the good\npractices in each discipline, and compare the relative merits of each method, so as to instruct the practitioners to\nchoose the optimal methods to meet different analysis purposes, and to guide current research on VIA. To this\nend, all the good practices, including eight groups of methods, i.e., the difference-based variable importance\nmeasures (VIMs), parametric regression and related VIMs, nonparametric regression techniques, random forest\nbased VIMs, hypothesis test techniques, variance-based VIMs, moment-independent VIMs and graphic VIMs,\nare reviewed and compared with a numerical test example set in two situations (independent and dependent\ncases). For ease of use, the recommendations are provided for different types of applications, and packages as\nwell as software for implementing these VIA techniques are collected. Prospects for future study of VIA\ntechniques are also proposed."}
{"Title": "Variable Importance Assessment in Regression:\nLinear Regression versus Random Forest", "Abstract": "Relative importance of regressor variables is an old topic\nthat still awaits a satisfactory solution. When interest is in at-\ntributing importance in linear regression, averaging over order-\nings methods for decomposing R 2 are among the state-of-the-\nart methods, although the mechanism behind their behavior is\nnot (yet) completely understood. Random forests\u2014a machine-\nlearning tool for classification and regression proposed a few\nyears ago\u2014have an inherent procedure of producing variable\nimportances. This article compares the two approaches (linear\nmodel on the one hand and two versions of random forests on\nthe other hand) and finds both striking similarities and differ-\nences, some of which can be explained whereas others remain\na challenge. The investigation improves understanding of the\nnature of variable importance in random forests. This article\nhas supplementary material online."}
{"Title": "Reinforcement Learning for Online Information\nSeeking", "Abstract": "Information seeking techniques, satisfying users\u2019 information needs by suggesting users person-\nalized objects (information or services) at the appropriate time and place, play a crucial role in\nmitigating the information overload problem on the Web. With recent great advances in Rein-\nforcement Learning (RL), there have been increasing interests in developing RL based information\nseeking techniques. These RL based techniques have two key advantages \u2013 (1) they are able to\ncontinuously update information seeking strategies according to users\u2019 real-time feedback, and (2)\nthey can maximize the expected cumulative long-term reward from users where reward has differ-\nent definitions according to information seeking applications such as click-through rate, revenue,\nuser satisfaction and engagement. In this survey, we give an overview about reinforcement learn-\ning for information seeking on the web from methodologies to applications, review representative\nalgorithms, and discuss some appealing research directions."}
{"Title": "Aspect extraction for opinion mining with a deep convolutional neural\nnetwork", "Abstract": "In this paper, we present the first deep learning approach to aspect extraction in opinion mining. Aspect\nextraction is a subtask of sentiment analysis that consists in identifying opinion targets in opinionated\ntext, i.e., in detecting the specific aspects of a product or service the opinion holder is either praising or\ncomplaining about. We used a 7-layer deep convolutional neural network to tag each word in opinionated\nsentences as either aspect or non-aspect word. We also developed a set of linguistic patterns for the\nsame purpose and combined them with the neural network. The resulting ensemble classifier, coupled\nwith a word-embedding model for sentiment analysis, allowed our approach to obtain significantly better\naccuracy than state-of-the-art methods."}
{"Title": "Learning word dependencies in text by means of a deep recurrent\nbelief network", "Abstract": "We propose a deep recurrent belief network with distributed time delays for learning multivariate Gaus-\nsians. Learning long time delays in deep belief networks is difficult due to the problem of vanishing\nor exploding gradients with increase in delay. To mitigate this problem and improve the transparency\nof learning time-delays, we introduce the use of Gaussian networks with time-delays to initialize the\nweights of each hidden neuron. From our knowledge of time delays, it is possible to learn the long de-\nlays from short delays in a hierarchical manner. In contrast to previous works, here dynamic Gaussian\nBayesian networks over training samples are evolved using Markov Chain Monte Carlo to determine the\ninitial weights of each hidden layer of neurons. In this way, the time-delayed network motifs of increasing\nMarkov order across layers can be modeled hierarchically using a deep model. To validate the proposed\nVariable-order Belief Network (VBN) framework, it is applied for modeling word dependencies in text.\nTo explore the generality of VBN, it is further considered for a real-world scenario where the dynamic\nmovements of basketball players are modeled. Experimental results obtained showed that the proposed\nVBN could achieve over 30% improvement in accuracy on real-world scenarios compared to the state-of-\nthe-art baselines."}
{"Title": "Learning Phrase Representations using RNN Encoder\u2013Decoder\nfor Statistical Machine Translation", "Abstract": "In this paper, we propose a novel neu-\nral network model called RNN Encoder\u2013\nDecoder that consists of two recurrent\nneural networks (RNN). One RNN en-\ncodes a sequence of symbols into a fixed-\nlength vector representation, and the other\ndecodes the representation into another se-\nquence of symbols. The encoder and de-\ncoder of the proposed model are jointly\ntrained to maximize the conditional prob-\nability of a target sequence given a source\nsequence. The performance of a statisti-\ncal machine translation system is empiri-\ncally found to improve by using the con-\nditional probabilities of phrase pairs com-\nputed by the RNN Encoder\u2013Decoder as an\nadditional feature in the existing log-linear\nmodel. Qualitatively, we show that the\nproposed model learns a semantically and\nsyntactically meaningful representation of\nlinguistic phrases."}
{"Title": "N EURAL M ACHINE T RANSLATION\nBY J OINTLY L EARNING TO A LIGN AND T RANSLATE", "Abstract": "Neural machine translation is a recently proposed approach to machine transla-\ntion. Unlike the traditional statistical machine translation, the neural machine\ntranslation aims at building a single neural network that can be jointly tuned to\nmaximize the translation performance. The models proposed recently for neu-\nral machine translation often belong to a family of encoder\u2013decoders and encode\na source sentence into a fixed-length vector from which a decoder generates a\ntranslation. In this paper, we conjecture that the use of a fixed-length vector is a\nbottleneck in improving the performance of this basic encoder\u2013decoder architec-\nture, and propose to extend this by allowing a model to automatically (soft-)search\nfor parts of a source sentence that are relevant to predicting a target word, without\nhaving to form these parts as a hard segment explicitly. With this new approach,\nwe achieve a translation performance comparable to the existing state-of-the-art\nphrase-based system on the task of English-to-French translation. Furthermore,\nqualitative analysis reveals that the (soft-)alignments found by the model agree\nwell with our intuition."}
{"Title": "Strategies for Training Large Vocabulary Neural Language Models", "Abstract": "Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such\nas Kneser-Ney. At the same time, neural\nlanguage models are gaining popularity for\nmany applications such as speech recogni-\ntion and machine translation whose success\ndepends on scalability. We present a sys-\ntematic comparison of strategies to represent\nand train large vocabularies, including soft-\nmax, hierarchical softmax, target sampling,\nnoise contrastive estimation and self normal-\nization. We further extend self normalization\nto be a proper estimator of likelihood and in-\ntroduce an efficient variant of softmax. We\nevaluate each method on three popular bench-\nmarks, examining performance on rare words,\nthe speed/accuracy trade-off and complemen-\ntarity to Kneser-Ney."}
{"Title": "Massive Exploration of Neural Machine Translation\nArchitectures", "Abstract": "Neural Machine Translation (NMT) has\nshown remarkable progress over the past\nfew years with production systems now\nbeing deployed to end-users. One major\ndrawback of current architectures is that\nthey are expensive to train, typically re-\nquiring days to weeks of GPU time to\nconverge. This makes exhaustive hyper-\nparameter search, as is commonly done\nwith other neural network architectures,\nprohibitively expensive. In this work,\nwe present the first large-scale analy-\nsis of NMT architecture hyperparameters.\nWe report empirical results and variance\nnumbers for several hundred experimental\nruns, corresponding to over 250,000 GPU\nhours on the standard WMT English to\nGerman translation task. Our experiments\nlead to novel insights and practical advice\nfor building and extending NMT architec-\ntures. As part of this contribution, we\nrelease an open-source NMT framework 1\nthat enables researchers to easily experi-\nment with novel techniques and reproduce\nstate of the art results."}
{"Title": "Aspect-Based Sentiment Analysis Using a\nTwo-Step Neural Network Architecture", "Abstract": "The World Wide Web holds a wealth of information in the\nform of unstructured texts such as customer reviews for products, events\nand more. By extracting and analyzing the expressed opinions in cus-\ntomer reviews in a fine-grained way, valuable opportunities and insights\nfor customers and businesses can be gained.\nWe propose a neural network based system to address the task of Aspect-\nBased Sentiment Analysis to compete in Task 2 of the ESWC-2016 Chal-\nlenge on Semantic Sentiment Analysis. Our proposed architecture divides\nthe task in two subtasks: aspect term extraction and aspect-specific sen-\ntiment extraction. This approach is flexible in that it allows to address\neach subtask independently. As a first step, a recurrent neural network is\nused to extract aspects from a text by framing the problem as a sequence\nlabeling task. In a second step, a recurrent network processes each ex-\ntracted aspect with respect to its context and predicts a sentiment label.\nThe system uses pretrained semantic word embedding features which we\nexperimentally enhance with semantic knowledge extracted from Word-\nNet. Further features extracted from SenticNet prove to be beneficial for\nthe extraction of sentiment labels. As the best performing system in its\ncategory, our proposed system proves to be an effective approach for the\nAspect-Based Sentiment Analysis."}
{"Title": "A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional\nNeural Networks for Sentence Classification", "Abstract": "Convolutional Neural Networks (CNNs)\nhave recently achieved remarkably strong\nperformance on the practically impor-\ntant task of sentence classification (Kim,\n2014; Kalchbrenner et al., 2014; Johnson\nand Zhang, 2014). However, these mod-\nels require practitioners to specify an ex-\nact model architecture and set accompa-\nnying hyperparameters, including the fil-\nter region size, regularization parameters,\nand so on. It is currently unknown how\nsensitive model performance is to changes\nin these configurations for the task of sen-\ntence classification. We thus conduct a\nsensitivity analysis of one-layer CNNs to\nexplore the effect of architecture com-\nponents on model performance; our aim\nis to distinguish between important and\ncomparatively inconsequential design de-\ncisions for sentence classification. We\nfocus on one-layer CNNs (to the exclu-\nsion of more complex models) due to their\ncomparative simplicity and strong empiri-\ncal performance, which makes it a modern\nstandard baseline method akin to Support\nVector Machine (SVMs) and logistic re-\ngression. We derive practical advice from\nour extensive empirical results for those\ninterested in getting the most out of CNNs\nfor sentence classification in real world\nsettings."}
{"Title": "A S TRUCTURED S ELF - ATTENTIVE\nS ENTENCE E MBEDDING", "Abstract": "This paper proposes a new model for extracting an interpretable sentence embed-\nding by introducing self-attention. Instead of using a vector, we use a 2-D matrix\nto represent the embedding, with each row of the matrix attending on a different\npart of the sentence. We also propose a self-attention mechanism and a special\nregularization term for the model. As a side effect, the embedding comes with an\neasy way of visualizing what specific parts of the sentence are encoded into the\nembedding. We evaluate our model on 3 different tasks: author profiling, senti-\nment classification and textual entailment. Results show that our model yields a\nsignificant performance gain compared to other sentence embedding methods in\nall of the 3 tasks."}
{"Title": "ADADELTA: AN ADAPTIVE LEARNING RATE METHOD", "Abstract": "We present a novel per-dimension learning rate method for\ngradient descent called ADADELTA. The method dynami-\ncally adapts over time using only first order information and\nhas minimal computational overhead beyond vanilla stochas-\nticgradientdescent. Themethodrequiresnomanualtuningof\na learning rate and appears robust to noisy gradient informa-\ntion, differentmodelarchitecturechoices, variousdatamodal-\nities and selection of hyperparameters. We show promising\nresults compared to other methods on the MNIST digit clas-\nsification task using a single machine and on a large scale\nvoice dataset in a distributed cluster environment."}
{"Title": "Adaptive Subgradient Methods for\nOnline Learning and Stochastic Optimization ", "Abstract": "We present a new family of subgradient methods that dynamically incorporate knowledge of the\ngeometry of the data observed in earlier iterations to perform more informative gradient-based\nlearning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very\npredictive but rarely seen features. Our paradigm stems from recent advances in stochastic op-\ntimization and online learning which employ proximal functions to control the gradient steps of\nthe algorithm. We describe and analyze an apparatus for adaptively modifying the proximal func-\ntion, which significantly simplifies setting a learning rate and results in regret guarantees that are\nprovably as good as the best proximal function that can be chosen in hindsight. We give several\nefficient algorithms for empirical risk minimization problems with common and important regu-\nlarization functions and domain constraints. We experimentally study our theoretical analysis and\nshow that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient\nalgorithms."}
{"Title": "Stochastic Dropout: Activation-level Dropout to\nLearn Better Neural Language Models", "Abstract": "Recurrent NeuralNetworks are verypowerful computational toolsthat are capable\nof learning many tasks across different domains. However, it is prone to overfit-\nting and can be very difficult to regularize. Inspired by Recurrent Dropout [1]\nand Skip-connections [2], we describe a new and simple regularization scheme:\nStochastic Dropout. It resembles the structure of recurrent dropout, but offers\nskip-connection over the recurrent depth. We reason the theoretical construct\nof such method and compare its regularization effectiveness with feedforward\ndropout and recurrent dropout. We demonstrate that Stochastic Dropout not only\noffers improvement when applied to vanilla RNN models, but also outperforms\nfeedforward Dropout on word-level language modeling. At last, we show that the\nmodel can achieve even better result if stochastic dropout and feedforward dropout\nare combined."}
{"Title": "An Introductory Survey on Attention Mechanisms in NLP Problems", "Abstract": "First derived from human intuition, later adapted to machine\ntranslation for automatic token alignment, attention mecha-\nnism, a simple method that can be used for encoding se-\nquence data based on the importance score each element is\nassigned, has been widely applied to and attained significant\nimprovement in various tasks in natural language process-\ning, including sentiment classification, text summarization,\nquestion answering, dependency parsing, etc. In this paper,\nwe survey through recent works and conduct an introductory\nsummary of the attention mechanism in different NLP prob-\nlems, aiming to provide our readers with basic knowledge\non this widely used method, discuss its different variants for\ndifferent tasks, explore its association with other techniques\nin machine learning, and examine methods for evaluating its\nperformance."}
{"Title": "Attention Is All You Need", "Abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature."}
{"Title": "Convolutional Neural Networks for Sentence Classification", "Abstract": "We report on a series of experiments with\nconvolutional neural networks (CNN)\ntrained on top of pre-trained word vec-\ntors for sentence-level classification tasks.\nWe show that a simple CNN with lit-\ntle hyperparameter tuning and static vec-\ntors achieves excellent results on multi-\nple benchmarks. Learning task-specific\nvectors through fine-tuning offers further\ngains in performance. We additionally\npropose a simple modification to the ar-\nchitecture to allow for the use of both\ntask-specific and static vectors. The CNN\nmodels discussed herein improve upon the\nstate of the art on 4 out of 7 tasks, which\ninclude sentiment analysis and question\nclassification."}
{"Title": "JLM - Fast RNN Language Model with Large Vocabulary", "Abstract": "The language model is a key to many tasks like\nmachine translation, speech recognition, and input\nmethod. While neural network language model shows\nbetter accuracy and scalability to a wider context, the\ncost to get the probability of next word is non-trivial.\nMoreover, eastern Asian languages like Japanese and\nChinese can easily have a large vocabulary over 100K\nwords to cover the most frequently appeared tokens.\nIn this paper, a fast RNN model named JLM 1 with a\nhybrid optimization is proposed. The experiment on\nBCCWJ Japanese corpus shows a 50x speed up during\ninference with decoder and up to 90% model size\nreduced without significant perplexity change."}
{"Title": "Effective Approaches to Attention-based Neural Machine Translation", "Abstract": "An attentional mechanism has lately been\nused to improve neural machine transla-\ntion (NMT) by selectively focusing on\nparts of the source sentence during trans-\nlation. However, there has been little\nwork exploring useful architectures for\nattention-based NMT. This paper exam-\nines two simple and effective classes of at-\ntentional mechanism: a global approach\nwhich always attends to all source words\nand a local one that only looks at a subset\nof source words at a time. We demonstrate\nthe effectiveness of both approaches on the\nWMT translation tasks between English\nand German in both directions. With local\nattention, we achieve a significant gain of\n5.0 BLEU points over non-attentional sys-\ntems that already incorporate known tech-\nniques such as dropout. Our ensemble\nmodel using different attention architec-\ntures yields a new state-of-the-art result in\nthe WMT\u201915 English to German transla-\ntion task with 25.9 BLEU points, an im-\nprovement of 1.0 BLEU points over the\nexisting best system backed by NMT and\nan n-gram reranker. "}
{"Title": "Deep Keyphrase Generation", "Abstract": "Keyphrase provides highly-summative\ninformation that can be effectively used\nfor understanding, organizing and retriev-\ning text content. Though previous studies\nhave provided many workable solutions\nfor automated keyphrase extraction, they\ncommonly divided the to-be-summarized\ncontent into multiple text chunks, then\nranked and selected the most meaningful\nones. These approaches could neither\nidentify keyphrases that do not appear\nin the text, nor capture the real semantic\nmeaning behind the text. We propose a\ngenerative model for keyphrase prediction\nwith an encoder-decoder framework,\nwhich can effectively overcome the above\ndrawbacks. We name it as deep keyphrase\ngeneration since it attempts to capture the\ndeep semantic meaning of the content with\na deep learning method. Empirical analy-\nsis on six datasets demonstrates that our\nproposed model not only achieves a sig-\nnificant performance boost on extracting\nkeyphrases that appear in the source text,\nbut also can generate absent keyphrases\nbased on the semantic meaning of the\ntext. Code and dataset are available\nat https://github.com/memray/seq2seq-\nkeyphrase."}
{"Title": "Effective Approaches to Attention-based Neural Machine Translation", "Abstract": "An attentional mechanism has lately been\nused to improve neural machine transla-\ntion (NMT) by selectively focusing on\nparts of the source sentence during trans-\nlation. However, there has been little\nwork exploring useful architectures for\nattention-based NMT. This paper exam-\nines two simple and effective classes of at-\ntentional mechanism: a global approach\nwhich always attends to all source words\nand a local one that only looks at a subset\nof source words at a time. We demonstrate\nthe effectiveness of both approaches on the\nWMT translation tasks between English\nand German in both directions. With local\nattention, we achieve a significant gain of\n5.0 BLEU points over non-attentional sys-\ntems that already incorporate known tech-\nniques such as dropout. Our ensemble\nmodel using different attention architec-\ntures yields a new state-of-the-art result in\nthe WMT\u201915 English to German transla-\ntion task with 25.9 BLEU points, an im-\nprovement of 1.0 BLEU points over the\nexisting best system backed by NMT and\nan n-gram reranker. 1"}
{"Title": "Improving Deep Neural Networks Using Softplus\nUnits", "Abstract": "Abstract\u2014Recently, DNNs have achieved great improvement\nfor acoustic modeling in speech recognition tasks. However, it is\ndifficult to train the models well when the depth grows. One main\nreason is that when training DNNs with traditional sigmoid units,\nthe derivatives damp sharply while back-propagating between\nlayers, which restrict the depth of model especially with insuffi-\ncient training data. To deal with this problem, some unbounded\nactivation functions have been proposed to preserve sufficient\ngradients, including ReLU and softplus. Compared with ReLU,\nthe smoothing and nonzero properties of the in gradient makes\nsoftplus-based DNNs perform better in both stabilization and\nperformance. However, softplus-based DNNs have been rarely\nexploited for the phoneme recognition task. In this paper, we\nexplore the use of softplus units for DNNs in acoustic modeling\nfor context-independent phoneme recognition tasks.The revised\nRBM pre-training and dropout strategy are also applied to\nimprove the performance of softplus units. Experiments show\nthat, the DNNs with softplus units get significantly performance\nimprovement and uses less epochs to get convergence compared\nto the DNNs trained with standard sigmoid units and ReLUs."}
{"Title": "Keyphrase Extraction Using Deep Recurrent Neural Networks on Twitter", "Abstract": "Keyphrases can provide highly condensed\nand valuable information that allows users to\nquickly acquire the main ideas. The task of\nautomatically extracting them have received\nconsiderable attention in recent decades.\nDifferent from previous studies, which are\nusually focused on automatically extracting\nkeyphrases from documents or articles, in\nthis study, we considered the problem of\nautomatically extracting keyphrases from\ntweets. Because of the length limitations\nof Twitter-like sites, the performances of\nexisting methods usually drop sharply. We\nproposed a novel deep recurrent neural\nnetwork (RNN) model to combine keywords\nand context information to perform this\nproblem. To evaluate the proposed method,\nwe also constructed a large-scale dataset\ncollected from Twitter. The experimental\nresults showed that the proposed method\nperforms significantly better than previous\nmethods."}
{"Title": "An Alternative View: When Does SGD Escape Local Minima?", "Abstract": "Stochastic gradient descent (SGD) is widely used\nin machine learning. Although being commonly\nviewed as a fast but not accurate version of gradi-\nent descent (GD), it always finds better solutions\nthan GD for modern neural networks. In order to\nunderstand this phenomenon, we take an alterna-\ntive view that SGD is working on the convolved\n(thus smoothed) version of the loss function. We\nshow that, even if the function f has many bad\nlocal minima or saddle points, as long as for every\npoint x , the weighted average of the gradients of\nitsneighborhoodsisonepointconvexwithrespect\nto the desired solution x \u2217 , SGD will get close to,\nand then stay around x \u2217 with constant probability.\nOur result identifies a set of functions that SGD\nprovably works, which is much larger than the set\nof convex functions. Empirically, we observe that\nthe loss surface of neural networks enjoys nice\none point convexity properties locally, therefore\nour theorem helps explain why SGD works so\nwell for neural networks."}
{"Title": "Layer Normalization", "Abstract": "Training state-of-the-art, deep neural networks is computationally expensive. One\nway to reduce the training time is to normalize the activities of the neurons. A\nrecently introduced technique called batch normalization uses the distribution of\nthe summed input to a neuron over a mini-batch of training cases to compute a\nmean and variance which are then used to normalize the summed input to that\nneuron on each training case. This significantly reduces the training time in feed-\nforward neural networks. However, the effect of batch normalization is dependent\non the mini-batch size and it is not obvious how to apply it to recurrent neural net-\nworks. In this paper, we transpose batch normalization into layer normalization by\ncomputing the mean and variance used for normalization from all of the summed\ninputs to the neurons in a layer on a single training case. Like batch normalization,\nwe also give each neuron its own adaptive bias and gain which are applied after\nthe normalization but before the non-linearity. Unlike batch normalization, layer\nnormalization performs exactly the same computation at training and test times.\nIt is also straightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is very\neffective at stabilizing the hidden state dynamics in recurrent networks. Empiri-\ncally, we show that layer normalization can substantially reduce the training time\ncompared with previously published techniques."}
{"Title": "Learning Distributed Representations of Sentences from Unlabelled Data", "Abstract": "Unsupervisedmethodsforlearningdistributed\nrepresentations of words are ubiquitous in to-\nday\u2019s NLP research, but far less is known\nabout the best ways to learn distributed phrase\nor sentence representations from unlabelled\ndata. This paper is a systematic comparison\nof models that learn such representations. We\nfind that the optimal approach depends crit-\nically on the intended application. Deeper,\nmore complex models are preferable for rep-\nresentations to be used in supervised sys-\ntems, but shallow log-linear models work best\nfor building representation spaces that can\nbe decoded with simple spatial distance met-\nrics. We also propose two new unsupervised\nrepresentation-learningobjectives designed to\noptimise the trade-off between training time,\ndomain portability and performance."}
{"Title": "Learning Phrase Representations using RNN Encoder\u2013Decoder\nfor Statistical Machine Translation", "Abstract": "In this paper, we propose a novel neu-\nral network model called RNN Encoder\u2013\nDecoder that consists of two recurrent\nneural networks (RNN). One RNN en-\ncodes a sequence of symbols into a fixed-\nlength vector representation, and the other\ndecodes the representation into another se-\nquence of symbols. The encoder and de-\ncoder of the proposed model are jointly\ntrained to maximize the conditional prob-\nability of a target sequence given a source\nsequence. The performance of a statisti-\ncal machine translation system is empiri-\ncally found to improve by using the con-\nditional probabilities of phrase pairs com-\nputed by the RNN Encoder\u2013Decoder as an\nadditional feature in the existing log-linear\nmodel. Qualitatively, we show that the\nproposed model learns a semantically and\nsyntactically meaningful representation of\nlinguistic phrases."}
{"Title": "Long Short-Term Memory-Networks for Machine Reading", "Abstract": "In this paper we address the question of how\nto render sequence-level networks better at\nhandling structured input. We propose a ma-\nchine reading simulator which processes text\nincrementally from left to right and performs\nshallow reasoning with memory and atten-\ntion. The reader extends the Long Short-Term\nMemory architecture with a memory network\nin place of a single memory cell. This en-\nables adaptive memory usage during recur-\nrence with neural attention, offering a way to\nweakly induce relations among tokens. The\nsystem is initially designed to process a single\nsequence but we also demonstrate how to inte-\ngrate it with an encoder-decoder architecture.\nExperimentsonlanguagemodeling, sentiment\nanalysis, and natural language inference show\nthat our model matches or outperforms the\nstate of the art."}
{"Title": "Neural information retrieval: at the end of the early\nyears", "Abstract": " A recent \u2018\u2018third wave\u2019\u2019 of neural network (NN) approaches now delivers state-of-\nthe-art performance in many machine learning tasks, spanning speech recognition, com-\nputer vision, and natural language processing. Because these modern NNs often comprise\nmultiple interconnected layers, work in this area is often referred to as deep learning.\nRecent years have witnessed an explosive growth of research into NN-based approaches to\ninformation retrieval (IR). A significant body of work has now been created. In this paper,"}
{"Title": "N EURAL M ACHINE T RANSLATION\nBY J OINTLY L EARNING TO A LIGN AND T RANSLATE", "Abstract": "Neural machine translation is a recently proposed approach to machine transla-\ntion. Unlike the traditional statistical machine translation, the neural machine\ntranslation aims at building a single neural network that can be jointly tuned to\nmaximize the translation performance. The models proposed recently for neu-\nral machine translation often belong to a family of encoder\u2013decoders and encode\na source sentence into a fixed-length vector from which a decoder generates a\ntranslation. In this paper, we conjecture that the use of a fixed-length vector is a\nbottleneck in improving the performance of this basic encoder\u2013decoder architec-\nture, and propose to extend this by allowing a model to automatically (soft-)search\nfor parts of a source sentence that are relevant to predicting a target word, without\nhaving to form these parts as a hard segment explicitly. With this new approach,\nwe achieve a translation performance comparable to the existing state-of-the-art\nphrase-based system on the task of English-to-French translation. Furthermore,\nqualitative analysis reveals that the (soft-)alignments found by the model agree\nwell with our intuition."}
{"Title": "Question Answering over Freebase via Attentive RNN\nwith Similarity Matrix based CNN", "Abstract": "Abstract. With the rapid growth of knowledge bases (KBs), question answering\nover knowledge base, a.k.a. KBQA has drawn huge attention in recent years.\nMost of the existing KBQA methods follow so called encoder-compare\nframework. They map the question and the KB facts to a common embedding\nspace, in which the similarity between the question vector and the fact vectors\ncan be conveniently computed. This, however, inevitably loses original words\ninteraction information. To preserve more original information, we propose an\nattentive recurrent neural network with similarity matrix based convolutional\nneural network (AR-SMCNN) model, which is able to capture comprehensive\nhierarchical information utilizing the advantages of both RNN and CNN. We use\nRNN to capture semantic-level correlation by its sequential modeling nature, and\nuse an attention mechanism to keep track of the entities and relations\nsimultaneously. Meanwhile, we use a similarity matrix based CNN with\ntwo-directions pooling to extract literal-level words interaction matching\nutilizing CNN\u2019s strength of modeling spatial correlation among data. Moreover,\nwe have developed a new heuristic extension method for entity detection, which\nsignificantly decreases the effect of noise. Our method has outperformed the\nstate-of-the-arts on SimpleQuestion benchmark in both accuracy and efficiency."}
{"Title": "Question Answering over Knowledge Base with Neural Attention Combining\nGlobal Knowledge Information", "Abstract": "With the rapid growth of knowledge bases (KBs)\non the web, how to take full advantage of them\nbecomes increasingly important. Knowledge base-\nbased question answering (KB-QA) is one of the\nmost promising approaches to access the substan-\ntial knowledge. Meantime, as the neural network-\nbased (NN-based) methods develop, NN-based\nKB-QA has already achieved impressive results.\nHowever, previous work did not put emphasis on\nquestion representation, and the question is con-\nverted into a fixed vector regardless of its candidate\nanswers. This simple representation strategy is un-\nable to express the proper information of the ques-\ntion. Hence, we present a neural attention-based\nmodel to represent the questions dynamically ac-\ncording to the different focuses of various candi-\ndate answer aspects. In addition, we leverage the\nglobal knowledge inside the underlying KB, aim-\ning at integrating the rich KB information into the\nrepresentation of the answers. And it also alleviates\nthe out of vocabulary (OOV) problem, which helps\nthe attention model to represent the question more\nprecisely. TheexperimentalresultsonWEBQUES-\nTIONS demonstrate the effectiveness of the pro-\nposed approach."}
{"Title": "R ECURRENT N EURAL N ETWORK R EGULARIZATION", "Abstract": "We present a simple regularization technique for Recurrent Neural Networks\n(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most suc-\ncessful techniquefor regularizingneuralnetworks, does notwork well with RNNs\nand LSTMs. In this paper, we show how to correctly apply dropout to LSTMs,\nand show that it substantially reduces overfittingon a variety of tasks. These tasks\ninclude language modeling, speech recognition, image caption generation, and\nmachine translation."}
{"Title": "A review of novelty detection", "Abstract": "Novelty detection is the task of classifying test data that differ in some respect from the\ndata that are available during training. This may be seen as \u201cone-class classification\u201d, in\nwhich a model is constructed to describe \u201cnormal\u201d training data. The novelty detection\napproach is typically used when the quantity of available \u201cabnormal\u201d data is insufficient\nto construct explicit models for non-normal classes. Application includes inference in\ndatasets from critical systems, where the quantity of available normal data is very large,\nsuch that \u201cnormality\u201d may be accurately modelled. In this review we aim to provide an\nupdated and structured investigation of novelty detection research papers that have\nappeared in the machine learning literature during the last decade."}
{"Title": "Unsupervised Anomaly Detection\nwith Generative Adversarial Netw", "Abstract": " Obtaining models that capture imaging markers relevant\nfor disease progression and treatment monitoring is challenging. Mod-\nels are typically based on large amounts of data with annotated exam-\nples of known markers aiming at automating detection. High annotation\neffort and the limitation to a vocabulary of known markers limit the\npower of such approaches. Here, we perform unsupervised learning to\nidentify anomalies in imaging data as candidates for markers. We pro-\npose AnoGAN, a deep convolutional generative adversarial network to\nlearn a manifold of normal anatomical variability, accompanying a novel\nanomaly scoring scheme based on the mapping from image space to a\nlatent space. Applied to new data, the model labels anomalies, and scores\nimage patches indicating their fit into the learned distribution. Results\non optical coherence tomography images of the retina demonstrate that\nthe approach correctly identifies anomalous images, such as images con-\ntaining retinal fluid or hyperreflective foci."}
{"Title": "An Unsupervised Approach for Aspect Category\nDetection Using Soft Cosine Similarity Measure", "Abstract": " Aspect category detection is one of the important and chal-\nlenging subtasks of aspect-based sentiment analysis. Given a set of pre-\ndefined categories, this task aims to detect categories which are indicated\nimplicitly or explicitly in a given review sentence. Supervised machine\nlearning approaches perform well to accomplish this subtask. Note that,\nthe performance of these methods depends on the availability of labeled\ntrain data, which is often difficult and costly to obtain. Besides, most of\nthese supervised methods require feature engineering to perform well. In\nthis paper, we propose an unsupervised method to address aspect cat-\negory detection task without the need for any feature engineering. Our\nmethod utilizes clusters of unlabeled reviews and soft cosine similarity\nmeasure to accomplish aspect category detection task. Experimental re-\nsults on SemEval-2014 restaurant dataset shows that proposed unsuper-\nvised approach outperforms several baselines by a substantial margin."}
{"Title": "A hybrid unsupervised method for aspect term and opinion target\nextraction", "Abstract": "Aspect term extraction (ATE) and opinion target extraction (OTE) are two important tasks in fine-grained\nsentiment analysis field. Existing approaches to ATE and OTE are mainly based on rules or machine learn-\ning methods. Rule-based methods are usually unsupervised, but they can\u2019t make use of high level fea-\ntures. Although supervised learning approaches usually outperform the rule-based ones, they need a large\nnumber of labeled samples to train their models, which are expensive and time-consuming to annotate.\nIn this paper, we propose a hybrid unsupervised method which can combine rules and machine learn-\ning methods to address ATE and OTE tasks. First, we use chunk-level linguistic rules to extract nominal\nphrase chunks and regard them as candidate opinion targets and aspects. Then we propose to filter irrele-\nvant candidates based on domain correlation. Finally, we use these texts with extracted chunks as pseudo\nlabeled data to train a deep gated recurrent unit (GRU) network for aspect term extraction and opinion\ntarget extraction. The experiments on benchmark datasets validate the effectiveness of our approach in\nextracting opinion targets and aspects with minimal manual annotation."}
{"Title": "Semi-supervised Learning with\nDeep Generative Models", "Abstract": "The ever-increasing size of modern data sets combined with the difficulty of ob-\ntaining label information has made semi-supervised learning one of the problems\nof significant practical importance in modern data analysis. We revisit the ap-\nproach to semi-supervised learning with generative models and develop new mod-\nels that allow for effective generalisation from small labelled data sets to large\nunlabelled ones. Generative approaches have thus far been either inflexible, in-\nefficient or non-scalable. We show that deep generative models and approximate\nBayesian inference exploiting recent advances in variational methods can be used\nto provide significant improvements, making generative approaches highly com-\npetitive for semi-supervised learning."}
{"Title": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions", "Abstract": "An approach to semi-supervised learning is pro-\nposed that is based on a Gaussian random field\nmodel. Labeled and unlabeled data are rep-\nresented as vertices in a weighted graph, with\nedge weights encodingthe similarity between in-\nstances. The learningproblem is then formulated\nintermsofaGaussianrandomfieldonthisgraph,\nwhere the mean of the field is characterized in\nterms of harmonic functions, and is efficiently\nobtained using matrix methods or belief propa-\ngation. The resulting learning algorithms have\nintimate connections with random walks, elec-\ntric networks, and spectral graph theory. We dis-\ncuss methods to incorporate class priors and the\npredictions of classifiers obtained by supervised\nlearning. We also proposea methodof parameter\nlearning by entropy minimization, and show the\nalgorithm\u2019s ability to perform feature selection.\nPromising experimental results are presented for\nsynthetic data, digit classification, and text clas-\nsification tasks."}
{"Title": "Revisiting Semi-Supervised Learning with Graph Embeddings", "Abstract": "We present a semi-supervised learning frame-\nwork based on graph embeddings. Given a graph\nbetween instances, we train an embedding for\neach instance to jointly predict the class label and\nthe neighborhood context in the graph. We de-\nvelop both transductive and inductive variants of\nour method. In the transductive variant of our\nmethod, the class labels are determined by both\nthe learned embeddings and input feature vec-\ntors, while in the inductive variant, the embed-\ndings are defined as a parametric function of the\nfeature vectors, so predictions can be made on in-\nstances not seen during training. On a large and\ndiverse set of benchmark tasks, including text\nclassification, distantly supervised entity extrac-\ntion, and entity classification, we show improved\nperformance over many of the existing models."}
{"Title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis", "Abstract": "Sentiment analysis is increasingly viewed\nas a vital task both from an academic and\na commercial standpoint. The majority of\ncurrent approaches, however, attempt to\ndetect the overall polarity of a sentence,\nparagraph, or text span, irrespective of the\nentities mentioned (e.g., laptops) and their\naspects (e.g., battery, screen). SemEval-\n2014 Task 4 aimed to foster research in the\nfield of aspect-based sentiment analysis,\nwhere the goal is to identify the aspects\nof given target entities and the sentiment\nexpressed for each aspect. The task pro-\nvided datasets containing manually anno-\ntated reviews of restaurants and laptops, as\nwell as a common evaluation procedure. It\nattracted 163 submissions from 32 teams."}
{"Title": "NRC-Canada-2014: Detecting Aspects and Sentiment\nin Customer Reviews", "Abstract": "Reviews depict sentiments of customers\ntowards various aspects of a product or\nservice. Some of these aspects can be\ngrouped into coarser aspect categories.\nSemEval-2014 had a shared task (Task 4)\non aspect-level sentiment analysis, with\nover 30 teams participated. In this pa-\nper, we describe our submissions, which\nstood first in detecting aspect categories,\nfirst in detecting sentiment towards aspect\ncategories, third in detecting aspect terms,\nand first and second in detecting senti-\nment towards aspect terms in the laptop\nand restaurant domains, respectively."}
{"Title": "Supervised and Unsupervised Aspect Category Detection for\nSentiment Analysis with Co-occurrence Data", "Abstract": "Using on-line consumer reviews as Electronic Word of Mouth to assist purchase-decision making has become increasingly popular.\nThe Web provides an extensive source of consumer reviews, but one can hardly read all reviews to obtain a fair evaluation of\na product or service. A text processing framework that can summarize reviews, would therefore be desirable. A sub-task to be\nperformed by such a framework would be to find the general aspect categories addressed in review sentences, for which this work\npresents two methods. In contrast to most existing approaches, the first method presented is an unsupervised method that applies\nassociation rule mining on co-occurrence frequency data obtained from a corpus to find these aspect categories. While not on\npar with state-of-the-art supervised methods, the proposed unsupervised method performs better than several simple baselines, a\nsimilar but supervised method, and a supervised baseline, with an F 1 -score of 67%. The second method is a supervised variant that\noutperforms existing methods with an F 1 -score of 84%"}
{"Title": "Multi-Aspect Opinion Polling from Textual Reviews", "Abstract": "This paper presents an unsupervised approach to aspect-based\nopinion polling from raw textual reviews without explicit ratings.\nThe key contribution of this paper is three-fold. First, a multi-\naspect bootstrapping algorithm is proposed to learn from\nunlabeled data aspect-related terms of each aspect to be used for\naspect identification. Second, an unsupervised segmentation\nmodel is proposed to address the challenge of identifying multiple\nsingle-aspect units in a multi-aspect sentence. Finally, an aspect-\nbased opinion polling algorithm is presented. Experiments on real\nChinese restaurant reviews show that our opinion polling method\ncan achieve 75.5% precision performance."}
{"Title": "A Knowledge-based Semantic Kernel for Text\nClassification", "Abstract": " Typically, in textual document classification the documents\nare represented in the vector space using the \u201cBag of Words\u201d (BOW)\napproach. Despite its ease of use, BOW representation cannot handle\nword synonymy and polysemy problems and does not consider semantic\nrelatedness between words. In this paper, we overcome the shortages of\nthe BOW approach by embedding a known WordNet-based semantic\nrelatedness measure for pairs of words, namely Omiotis, into a seman-\ntic kernel. The suggested measure incorporates the TF-IDF weighting\nscheme, thus creating a semantic kernel which combines both seman-\ntic and statistical information from text. Empirical evaluation with real\ndata sets demonstrates that our approach successfully achieves improved\nclassification accuracy with respect to the standard BOW representation,\nwhen Omiotis is embedded in four different classifiers."}
{"Title": "A Latent Semantic Model with Convolutional-Pooling\nStructure for Information Retrieval", "Abstract": "In this paper, we propose a new latent semantic model that\nincorporates a convolutional-pooling structure over word\nsequences  to  learn  low-dimensional,  semantic  vector\nrepresentations for search queries and Web documents. In order to\ncapture the rich contextual structures in a query or a document, we\nstart with each word within a temporal context window in a word\nsequence to directly capture contextual features at the word n-\ngram level. Next, the salient word n-gram features in the word\nsequence are discovered by the model and are then aggregated to\nform a sentence-level feature vector. Finally, a non-linear\ntransformation is applied to extract high-level semantic\ninformation to generate a continuous vector representation for the\nfull text string. The proposed convolutional latent semantic model\n(CLSM) is trained on clickthrough data and is evaluated on a Web\ndocument ranking task using a large-scale, real-world data set.\nResults show that the proposed model effectively captures salient\nsemantic information in queries and documents for the task while\nsignificantly outperforming previous state-of-the-art semantic\nmodels."}
{"Title": "Building Semantic Kernels for Text Classification using\nWikipedia", "Abstract": "Document classification presents difficult challenges due to\nthe sparsity and the high dimensionality of text data, and to\nthe complex semantics of the natural language. The tradi-\ntional document representation is a word-based vector (Bag\nof Words, or BOW), where each dimension is associated with\na term of the dictionary containing all the words that ap-\npear in the corpus. Although simple and commonly used,\nthis representation has several limitations. It is essential to\nembed semantic information and conceptual patterns in or-\nder to enhance the prediction capabilities of classification\nalgorithms. In this paper, we overcome the shortages of the\nBOW approach by embedding background knowledge de-\nrived from Wikipedia into a semantic kernel, which is then\nused to enrich the representation of documents. Our empir-\nical evaluation with real data sets demonstrates that our ap-\nproach successfully achieves improved classification accuracy\nwith respect to the BOW technique, and to other recently\ndeveloped methods."}
{"Title": "Collective Entity Linking in Web Text:\nA Graph-Based Method", "Abstract": "Entity Linking (EL) is the task of linking name mentions in Web\ntext with their referent entities in a knowledge base. Traditional\nEL methods usually link name mentions in a document by\nassuming them to be independent. However, there is often\nadditional interdependence between different EL decisions, i.e.,\nthe entities in the same document should be semantically related\nto each other. In these cases, Collective Entity Linking, in which\nthe name mentions in the same document are linked jointly by\nexploiting the interdependence between them, can improve the\nentity linking accuracy.\nThis paper proposes a graph-based collective EL method, which\ncan model and exploit the global interdependence between\ndifferent EL decisions. Specifically, we first propose a graph-\nbased representation, called Referent Graph, which can model the\nglobal interdependence between different EL decisions. Then we\npropose a collective inference algorithm, which can jointly infer\nthe referent entities of all name mentions by exploiting the\ninterdependence captured in Referent Graph. The key benefit of\nour method comes from: 1) The global interdependence model of\nEL decisions; 2) The purely collective nature of the inference\nalgorithm, in which evidence for related EL decisions can be\nreinforced into high-probability decisions. Experimental results\nshow that our method can achieve significant performance\nimprovement over the traditional EL methods."}
{"Title": "Deep Learning in Semantic Kernel Spaces", "Abstract": "Kernel methods enable the direct usage of\nstructured representations of textual data\nduring language learning and inference\ntasks. Expressive kernels, such as Tree\nKernels, achieve excellent performance in\nNLP. On the other side, deep neural net-\nworks have been demonstrated effective in\nautomatically learning feature representa-\ntions during training. However, their in-\nput is tensor data, i.e., they cannot man-\nage rich structured information. In this pa-\nper, we show that expressive kernels and\ndeep neural networks can be combined in\na common framework in order to (i) ex-\nplicitly model structured information and\n(ii) learn non-linear decision functions.\nWe show that the input layer of a deep ar-\nchitecture can be pre-trained through the\napplication of the Nystr\u00f6m low-rank ap-\nproximation of kernel spaces. The result-\ning \u201ckernelized\u201d neural network achieves\nstate-of-the-art accuracy in three different\ntasks."}
{"Title": "Entity Linking with a Knowledge Base:\nIssues, Techniques, and Solutions", "Abstract": "Abstract\u2014The large number of potential applications from bridging Web data with knowledge bases have led to an increase in\nthe entity linking research. Entity linking is the task to link entity mentions in text with their corresponding entities in a knowledge\nbase. Potential applications include information extraction, information retrieval, and knowledge base population. However, this\ntask is challenging due to name variations and entity ambiguity. In this survey, we present a thorough overview and analysis\nof the main approaches to entity linking, and discuss various applications, the evaluation of entity linking systems, and future\ndirections."}
{"Title": "Learning Deep Structured Semantic Models\nfor Web Search using Clickthrough Data", "Abstract": "Latent semantic models, such as LSA, intend to map a query to its\nrelevant documents at the semantic level where keyword-based\nmatching often fails. In this study we strive to develop a series of\nnew latent semantic models with a deep structure that project\nqueries and documents into a common low-dimensional space\nwhere the relevance of a document given a query is readily\ncomputed as the distance between them. The proposed deep\nstructured semantic models are discriminatively trained by\nmaximizing the conditional likelihood of the clicked documents\ngiven a query using the clickthrough data. To make our models\napplicable to large-scale Web search applications, we also use a\ntechnique called word hashing, which is shown to effectively\nscale up our semantic models to handle large vocabularies which\nare common in such tasks. The new models are evaluated on a\nWeb document ranking task using a real-world data set. Results\nshow that our best model significantly outperforms other latent\nsemantic models, which were considered state-of-the-art in the\nperformance prior to the work presented in this paper."}
{"Title": "Mining Domain-Specific Thesauri from Wikipedia: A case study", "Abstract": "Domain-specific  thesauri  are  high-cost,  high-\nmaintenance, high-value knowledge structures. We show\nhow the classic thesaurus structure of terms and links can\nbe mined automatically from Wikipedia. In a comparison\nwith a professional thesaurus for agriculture we find that\nWikipedia contains a substantial proportion of its\nconcepts and semantic relations; furthermore it has\nimpressive coverage of contemporary documents in the\ndomain. Thesauri derived using our techniques capitalize\non existing public efforts and tend to reflect contemporary\nlanguage usage better than their costly, painstakingly-\nconstructed manual counterparts."}
{"Title": "Semantic Web in Data Mining and Knowledge Discovery:\nA Comprehensive Survey", "Abstract": "Data Mining and Knowledge Discovery in Databases (KDD) is a research field concerned with deriving higher-level\ninsights from data. The tasks performed in that field are knowledge intensive and can often benefit from using\nadditional knowledge from various sources. Therefore, many approaches have been proposed in this area that combine\nSemantic Web data with the data mining and knowledge discovery process. This survey article gives a comprehensive\noverview of those approaches in di ff erent stages of the knowledge discovery process. As an example, we show how\nLinked Open Data can be used at various stages for building content-based recommender systems. The survey shows\nthat, while there are numerous interesting research works performed, the full potential of the Semantic Web and Linked\nOpen Data for data mining and KDD is still to be unlocked."}
{"Title": "Text Relatedness Based on a Word Thesaurus", "Abstract": "Thecomputation ofrelatedness between twofragments oftext inanautomated manner requires\ntaking into account a wide range of factors pertaining to the meaning the two fragments convey,\nand the pairwise relations between their words. Without doubt, a measure of relatedness between\ntext segments must take into account both the lexical and the semantic relatedness between words.\nSuch a measure that captures well both aspects of text relatedness may help in many tasks, such as\ntext retrieval, classification and clustering. In this paper we present a new approach for measuring\nthe semantic relatedness between words based on their implicit semantic links. The approach ex-\nploits only a word thesaurus in order to devise implicit semantic links between words. Based on\nthis approach, we introduce Omiotis, a new measure of semantic relatedness between texts which\ncapitalizes on the word-to-word semantic relatedness measure (SR) and extends it to measure the\nrelatedness between texts. We gradually validate our method: we first evaluate the performance\nof the semantic relatedness measure between individual words, covering word-to-word similar-\nity and relatedness, synonym identification and word analogy; then, we proceed with evaluating\nthe performance of our method in measuring text-to-text semantic relatedness in two tasks, namely\nsentence-to-sentence similarity and paraphrase recognition. Experimental evaluation shows that the\nproposed method outperforms every lexicon-based method of semantic relatedness in the selected\ntasks and the used data sets, and competes well against corpus-based and hybrid approaches."}
{"Title": "Towards Beter Text Understanding and Retrieval through\nKernel Entity Salience Modeling", "Abstract": "Tis paper presents a Kernel Entity Salience Model ( KESM ) that\nimproves text understanding and retrieval by beter estimating en-\ntity salience (importance) in documents. KESM represents entities\nby knowledge enriched distributed representations, models the in-\nteractions between entities and words by kernels, and combines\nthe kernel scores to estimate entity salience. Te whole model is\nlearned end-to-end using entity salience labels. Te salience model\nalso improves ad hoc search accuracy, providing efective ranking\nfeatures by modeling the salience of query entities in candidate doc-\numents. Our experiments on two entity salience corpora and two\nTREC ad hoc search datasets demonstrate the efectiveness of KESM\nover frequency-based and feature-based methods. We also provide\nexamples showing how KESM conveys its text understanding ability\nlearned from entity salience to search."}
{"Title": "A semantic enhanced hybrid recommendation approach: A", "Abstract": "Recommender systems are effectively used as a personalized information filtering technology to automatically\npredictandidentifyasetofinterestingitemsonbehalfofusersaccordingtotheirpersonalneedsandpreferences.\nCollaborative Filtering (CF) approach is commonly used in the context of recommender systems; however,\nobtaining better prediction accuracy and overcoming the main limitations of the standard CF recommendation\nalgorithms, such as sparsity and cold-start item problems, remain a significant challenge. Recent developments\nin personalization and recommendation techniques support the use of semantic enhanced hybrid recommender\nsystems, which incorporate ontology-based semantic similarity measure with other recommendation ap-\nproaches to improve the quality of recommendations. Consequently, this paper presents the effectiveness of uti-\nlizing semantic knowledge of items to enhance the recommendation quality. It proposes a new Inferential\nOntology-basedSemantic Similarity (IOBSS) measure to evaluate semantic similarity betweenitems ina specific\ndomain of interest by taking into account their explicit hierarchical relationships, shared attributes and implicit\nrelationships. The paper further proposes a hybrid semantic enhanced recommendation approach by combining\nthe new IOBSS measure and the standard item-based CF approach. A set of experiments with promising results\nvalidates the effectiveness of the proposed hybrid approach, using a case study of the Australian e-Government\ntourism services."}
{"Title": "A WordNet-based semantic similarity measurement combining\nedge-counting and information content theory", "Abstract": "Semantic similarity measuring between words can be applied to many applications, such as Artificial\nIntelligence, Information Processing, Medical Care and Linguistics. In this paper, we present a new\napproach for semantic similarity measuring which is based on edge-counting and information content\ntheory. Specifically, the proposed measure nonlinearly transforms the weighted shortest path length\nbetween the compared concepts to achieve the semantic similarity results, and the relation between\nparameters and the correlation value is discussed in detail. Experimental results show that the proposed\napproach not only achieves high correlation value against human ratings but also has better distribution\ncharacteristics of the correlation coefficient compared with several related works in the literature. In\naddition, the proposed method is computationally efficient due to the simplified ways of weighting the\nshortest path length between the concept pairs."}
{"Title": "Ontology-based semantic similarity: A new feature-based approach", "Abstract": "Estimation of the semantic likeness between words is of great importance in many applications dealing\nwith textual data such as natural language processing, knowledge acquisition and information retrieval.\nSemantic similarity measures exploit knowledge sources as the base to perform the estimations. In recent\nyears, ontologies have grown in interest thanks to global initiatives such as the Semantic Web, offering an\nstructured knowledge representation. Thanks to the possibilities that ontologies enable regarding seman-\ntic interpretation of terms many ontology-based similarity measures have been developed. According to\nthe principle in which those measures base the similarity assessment and the way in which ontologies\nare exploited or complemented with other sources several families of measures can be identified. In this\npaper, we survey and classify most of the ontology-based approaches developed in order to evaluate their\nadvantages and limitations and compare their expected performance both from theoretical and practical\npoints of view. We also present a new ontology-based measure relying on the exploitation of taxonomical\nfeatures. The evaluation and comparison of our approach\u2019s results against those reported by related\nworks under a common framework suggest that our measure provides a high accuracy without some\nof the limitations observed in other works."}
{"Title": "Heterogeneous Graph Attention Networks for Semi-supervised\nShort Text Classification", "Abstract": "Short text classification has found rich and\ncritical applications in news and tweet tagging\nto help users find relevant information. Due to\nlack of labeled training data in many practical\nuse cases, there is a pressing need for study-\ning semi-supervised short text classification.\nMost existing studies focus on long texts and\nachieve unsatisfactory performance on short\ntexts due to the sparsity and limited labeled\ndata. In this paper, we propose a novel hetero-\ngeneous graph neural network based method\nfor semi-supervised short text classification,\nleveraging full advantage of few labeled data\nand large unlabeled data through information\npropagation along the graph. In particular,\nwe first present a flexible HIN (heterogeneous\ninformation network) framework for model-\ning the short texts, which can integrate any\ntype of additional information as well as cap-\nture their relations to address the semantic\nsparsity. Then, we propose Heterogeneous\nGraph ATtention networks (HGAT) to embed\nthe HIN for short text classification based on\na dual-level attention mechanism, including\nnode-level and type-level attentions. The at-\ntention mechanism can learn the importance of\ndifferent neighboring nodes as well as the im-\nportance of different node (information) types\nto a current node. Extensive experimental\nresults have demonstrated that our proposed\nmodel outperforms state-of-the-art methods\nacross six benchmark datasets significantly."}
{"Title": "Omiotis: A Thesaurus-Based Measure\nof Text Relatedness", "Abstract": "In this paper we present a new approach for measuring the relatedness\nbetween text segments, based on implicit semantic links between their words, as\noffered by a word thesaurus, namely WordNet. The approach does not require\nany type of training, since it exploits only WordNet to devise the implicit se-\nmantic links between text words. The paper presents a prototype on-line demo of\nthe measure, that can provide word-to-word relatedness values, even for words\nof different part of speech. In addition the demo allows for the computation of\nrelatedness between text segments."}
{"Title": "An Association Network for Computing Semantic Relatedness", "Abstract": "To judge how much a pair of words (or texts) are semanti-\ncally related is a cognitive process. However, previous algo-\nrithms for computing semantic relatedness are largely based\non co-occurrences within textual windows, and do not ac-\ntively leverage cognitive human perceptions of relatedness.\nTo bridge this perceptional gap, we propose to utilize free as-\nsociation as signals to capture such human perceptions. How-\never, free association, being manually evaluated, has limited\nlexical coverage and is inherently sparse. We propose to ex-\npand lexical coverage and overcome sparseness by construct-\ning an association network of terms and concepts that com-\nbinessignalsfromfreeassociationnormsandfivetypesofco-\noccurrences extracted from the rich structures of Wikipedia.\nOur evaluation results validate that simple algorithms on this\nnetwork give competitive results in computing semantic re-\nlatedness between words and between short texts."}
{"Title": "Text Relatedness Based on a Word Thesaurus", "Abstract": "Thecomputation ofrelatedness between twofragments oftextinanautomated manner requires\ntaking into account a wide range of factors pertaining to the meaning the two fragments convey,\nand the pairwise relations between their words. Without doubt, a measure of relatedness between\ntext segments must take into account both the lexical and the semantic relatedness between words.\nSuch a measure that captures well both aspects of text relatedness may help in many tasks, such as\ntext retrieval, classification and clustering. In this paper we present a new approach for measuring\nthe semantic relatedness between words based on their implicit semantic links. The approach ex-\nploits only a word thesaurus in order to devise implicit semantic links between words. Based on\nthis approach, we introduce Omiotis, a new measure of semantic relatedness between texts which\ncapitalizes on the word-to-word semantic relatedness measure (SR) and extends it to measure the\nrelatedness between texts. We gradually validate our method: we first evaluate the performance\nof the semantic relatedness measure between individual words, covering word-to-word similar-\nity and relatedness, synonym identification and word analogy; then, we proceed with evaluating\nthe performance of our method in measuring text-to-text semantic relatedness in two tasks, namely\nsentence-to-sentence similarity and paraphrase recognition. Experimental evaluation shows that the\nproposed method outperforms every lexicon-based method of semantic relatedness in the selected\ntasks and the used data sets, and competes well against corpus-based and hybrid approaches."}
{"Title": "A Tutorial on Energy-Based Learning", "Abstract": "Energy-Based Models (EBMs) capture dependencies between variables by as-\nsociating a scalar energy to each configuration of the variables. Inference consists\nin clamping the value of observed variables and finding configurations of the re-\nmaining variables that minimize the energy. Learning consists in finding an energy\nfunction in which observed configurations of the variables are given lower energies\nthan unobserved ones. The EBM approach provides a common theoretical frame-\nwork for many learning models, including traditional discriminative and genera-\ntive approaches, as well as graph-transformer networks, conditional random fields,\nmaximum margin Markov networks, and several manifold learning methods.\nProbabilistic models must be properly normalized, which sometimes requires\nevaluating intractable integrals over the space of all possible variable configura-\ntions. Since EBMs have no requirement for proper normalization, this problem is\nnaturally circumvented. EBMs can be viewed as a form of non-probabilistic factor\ngraphs, and they provide considerably more flexibility in the design of architec-\ntures and training criteria than probabilistic approaches."}
{"Title": "A Word Embedding based Generalized Language Model for\nInformation Retrieval", "Abstract": "Word2vec, a word embedding technique, has gained significant in-\nterest among researchers in natural language processing (NLP) in\nrecent years. The embedding of the word vectors helps to identify a\nlist of words that are used in similar contexts with respect to a given\nword. In this paper, we focus on the use of word embeddings for\nenhancing retrieval effectiveness. In particular, we construct a gen-\neralized language model, where the mutual independence between\na pair of words (say t and t 0 ) no longer holds. Instead, we make use\nof the vector embeddings of the words to derive the transformation\nprobabilities between words. Specifically, the event of observing a\nterm t in the query from a document d is modeled by two distinct\nevents, that of generating a different term t 0 , either from the docu-\nment itself or from the collection, respectively, and then eventually\ntransforming it to the observed query term t. The first event of gen-\nerating an intermediate term from the document intends to capture\nhow well a term fits contextually within a document, whereas the\nsecond one of generating it from the collection aims to address the\nvocabulary mismatch problem by taking into account other related\nterms in the collection. Our experiments, conducted on the stan-\ndard TREC 6-8 ad hoc and Robust tasks, show that our proposed\nmethodyields significantimprovements overlanguagemodel (LM)\nand LDA-smoothed LM baselines."}
{"Title": "Concept Mining via Embedding", "Abstract": "Abstract\u2014In this work, we study the problem of concept\nmining, which serves as the first step in transforming unstruc-\ntured text into structured information, and supports downstream\nanalytical tasks such as information extraction, organization,\nrecommendation and search. Previous work mainly relies on\nstatistical signals, existing knowledge bases, or predefined lin-\nguistic patterns. In this work, we propose a novel approach\nthat mines concepts based on their occurrence contexts, by\nlearning embedding vector representations that summarize the\ncontext information for each possible candidates, and use these\nembeddings to evaluate the concept\u2019s global quality and their\nfitness to each local context. Experiments over several real-\nworld corpora demonstrate the superior performance of our\nmethod. A publicly available implementation is provided at\nhttps://github.com/kleeeeea/ECON."}
{"Title": "GNEG: Graph-Based Negative Sampling for word2vec", "Abstract": "Negative sampling is an important com-\nponent in word2vec for distributed word\nrepresentation learning. We hypothesize\nthat taking into account global, corpus-\nlevel information and generating a differ-\nent noise distribution for each target word\nbetter satisfies the requirements of nega-\ntive examples for each training word than\nthe original frequency-based distribution.\nIn this purpose we pre-compute word co-\noccurrence statistics from the corpus and\napply to it network algorithms such as ran-\ndom walk. We test this hypothesis through\na set of experiments whose results show\nthat our approach boosts the word anal-\nogy task by about 5% and improves the\nperformance on word similarity tasks by\nabout 1% compared to the skip-gram neg-\native sampling baseline."}
{"Title": "Record2Vec: Unsupervised Representation Learning for Structured Records", "Abstract": "Abstract\u2014Structured records \u2013 data with a fixed number of\ndescriptive fields (or attributes) \u2013 are often represented by one-\nhot encoded or term frequency-inverse document frequency\n(TF-IDF) weighted vectors. These vectors are typically sparse\nand long, and are inefficient in representing structured records.\nHere, we introduce Record2Vec, a framework for generating\ndense embeddings of structured records by training associ-\nations between attributes within record instances. We build\nour embedding from a simple premise that structured records\nhave attributes that are associated, and therefore we can\ntrain the embedding of an attribute based on other attributes\n(or context), much like how we train embeddings for words\nbased on their surrounding context. Because this embedding\ntechnique is general and does not assume the availability of\nany labeled data, it is extendable across different domains and\nfields. We demonstrate its utility in the context of clustering,\nrecord matching, movie rating and movie genre prediction."}
{"Title": "Supervised and Unsupervised Aspect Category Detection for\nSentiment Analysis with Co-occurrence Data", "Abstract": "Using on-line consumer reviews as Electronic Word of Mouth to assist purchase-decision making has become increasingly popular.\nThe Web provides an extensive source of consumer reviews, but one can hardly read all reviews to obtain a fair evaluation of\na product or service. A text processing framework that can summarize reviews, would therefore be desirable. A sub-task to be\nperformed by such a framework would be to find the general aspect categories addressed in review sentences, for which this work\npresents two methods. In contrast to most existing approaches, the first method presented is an unsupervised method that applies\nassociation rule mining on co-occurrence frequency data obtained from a corpus to find these aspect categories. While not on\npar with state-of-the-art supervised methods, the proposed unsupervised method performs better than several simple baselines, a\nsimilar but supervised method, and a supervised baseline, with an F 1 -score of 67%. The second method is a supervised variant that\noutperforms existing methods with an F 1 -score of 84%."}
{"Title": "Survey on\nAspect-Level Sentiment Analysis", "Abstract": "Abstract\u2014The field of sentiment analysis, in which sentiment is gathered, analyzed, and aggregated from text, has seen a lot\nof attention in the last few years. The corresponding growth of the field has resulted in the emergence of various subareas, each\naddressing a different level of analysis or research question. This survey focuses on aspect-level sentiment analysis, where the\ngoal is to find and aggregate sentiment on entities mentioned within documents or aspects of them. An in-depth overview of the\ncurrent state-of-the-art is given, showing the tremendous progress that has already been made in finding both the target, which\ncan be an entity as such, or some aspect of it, and the corresponding sentiment. Aspect-level sentiment analysis yields very fine-\ngrained sentiment information which can be useful for applications in various domains. Current solutions are categorized based\non whether they provide a method for aspect detection, sentiment analysis, or both. Furthermore, a breakdown based on the\ntype of algorithm used is provided. For each discussed study, the reported performance is included. To facilitate the quantitative\nevaluation of the various proposed methods, a call is made for the standardization of the evaluation methodology that includes\nthe use of shared data sets. Semantically-rich concept-centric aspect-level sentiment analysis is discussed and identified as one\nof the most promising future research direction."}
{"Title": "WordNet Embeddings", "Abstract": "Semantic networks and semantic spaces\nhave been two prominent approaches to\nrepresent lexical semantics. While a uni-\nfied account of the lexical meaning re-\nlies on one being able to convert between\nthese representations, in both directions,\nthe conversion direction from semantic\nnetworks into semantic spaces started to\nattract more attention recently. In this pa-\nper we present a methodology for this con-\nversion and assess it with a case study.\nWhen it is applied over WordNet, the per-\nformance of the resulting embeddings in\na mainstream semantic similarity task is\nvery good, substantially superior to the\nperformance of word embeddings based\non very large collections of texts like\nword2vec."}
{"Title": "Preserving Privacy in Social Networks Against\nNeighborhood Attacks", "Abstract": "Abstract\u2014Recently, as more and more social network data\nhas been published in one way or another, preserving privacy\nin publishing social network data becomes an important con-\ncern. With some local knowledge about individuals in a social\nnetwork, an adversary may attack the privacy of some victims\neasily. Unfortunately, most of the previous studies on privacy\npreservation can deal with relational data only, and cannot be\napplied to social network data. In this paper, we take an initiative\ntowards preserving privacy in social network data. We identify\nan essential type of privacy attacks: neighborhood attacks. If an\nadversary has some knowledge about the neighbors of a target\nvictim and the relationship among the neighbors, the victim may\nbe re-identified from a social network even if the victim\u2019s identity\nis preserved using the conventional anonymization techniques. We\nshow that the problem is challenging, and present a practical\nsolution to battle neighborhood attacks. The empirical study\nindicates that anonymized social networks generated by our\nmethod can still be used to answer aggregate network queries\nwith high accuracy."}
{"Title": "A Non-parametric Semi-supervised Discretization Method", "Abstract": "Semi-supervised classification methods aim to exploit\nlabelled and unlabelled examples to train a predictive\nmodel. Most of these approaches make assumptions on\nthe distribution of classes. This article first proposes a\nnew semi-supervised discretization method which adopts\nvery low informative prior on data. This method dis-\ncretizes the numerical domain of a continuous input vari-\nable, while keeping the information relative to the predic-\ntion of classes. Then, an in-depth comparison of this semi-\nsupervised method with the original supervised MODL\napproach is presented. We demonstrate that the semi-\nsupervised approach is asymptotically equivalent to the su-\npervisedapproach,improvedwithapost-optimizationofthe\nintervals bounds location."}
{"Title": "Generalized Framework for Syntax-based Relation Mining", "Abstract": "Supervised approaches to Data Mining are particularly\nappealing as they allow for the extraction of complex rela-\ntions from data objects. In order to facilitate their applica-\ntion in different areas, ranging from protein to protein in-\nteraction in bioinformatics to text mining in computational\nlinguistics research, a modular and general mining frame-\nwork is needed. The major constraint to the generalization\nprocess concerns the feature design for the description of\nrelational data.\nIn this paper, we present a machine learning framework\nfor the automatic mining of relations, where the target ob-\njects are structurally organized in a tree. Object types are\ngeneralized by means of the use of roles, whereas the re-\nlation properties are described by means of the underlying\ntree structure. The latter is encoded in the learning algo-\nrithm thanks to kernel methods for structured data, which\nrepresent structures in terms of their all possible subparts.\nThis approach can be applied to any kind of data disregard-\ning their very nature.\nExperiments with Support Vector Machines on two text\nmining datasets for relation extraction, i.e. the PropBank\nandFrameNetcorpora, showboththatourapproachisgen-\neral, and that it reaches state-of-the-art accuracy."}
{"Title": "Text Mining in Radiology Reports", "Abstract": "Medical text mining has gained increasing interest in re-\ncent years. Radiology reports contain rich information de-\nscribing radiologist\u2019s observations on the patient\u2019s medical\nconditions in the associated medical images. However, as\nmost reports are in free text format, the valuable informa-\ntion contained in those reports cannot be easily accessed\nand used, unless proper text mining has been applied. In\nthis paper, we propose a text mining system to extract and\nuse the information in radiology reports. The system con-\nsists of three main modules: a medical finding extractor, a\nreport and image retriever, and a text-assisted image fea-\nture extractor. In evaluation, the overall precision and re-\ncall for medical finding extraction are 95.5% and 87.9%\nrespectively, and for all modifiers of the medical findings\n88.2% and 82.8% respectively. The overall result of report\nand image retrieval module and text-assisted image feature\nextraction module is satisfactory to radiologists."}
{"Title": "The Structure of Comment Networks", "Abstract": "Blogs form an important on-line social network as they\nare maintained periodically, and are easily accessible. An\nimportant aspect of any blogspace is the relationship of\nbloggers based on comments. This important aspect is,\nhowever, largely ignored in previous works.\nIn this work, we study the evolution of this specific type\nof social network, blogosphere comment graph. We look at\nthedensificationofthecommentnetwork, andstudyitslocal\npatterns. We observe how the comment network evolves as\ndifferent bloggers place comments on each other. We inves-\ntigate the densification of this specific network and observe\na high correlation between the number of comments placed\nand received. Finally, we propose a growth model that best\ndescribes the behavior of users who place comments."}
{"Title": "Collective Latent Dirichlet Allocation", "Abstract": "In this paper, we propose a new variant of Latent Dirich-\nlet Allocation(LDA): Collective LDA (C-LDA), for multi-\nple corpora modeling. C-LDA combines multiple corpora\nduring learning such that it can transfer knowledge from\none corpus to another; meanwhile it keeps a discrimina-\ntive node which represents the corpus ID to constrain the\nlearned topics in each corpus. Compared with LDA locally\napplied to the target corpus, C-LDA results in refined topic-\nword distribution, while compared with applying LDA glob-\nally and straightforwardly to the combined corpus, C-LDA\nkeeps each topic only for one corpus. We demonstrate that\nC-LDAhasimprovedperformancewiththeseadvantages by\nexperiments on several benchmark document data sets ."}
{"Title": "A Non-parametric Approach to Pair-wise Dynamic Topic Correlation Detection", "Abstract": "We introduce dynamic correlated topic models (DCTM)\nfor analyzingdiscrete dataover time. This modelis inspired\nby the hierarchical Gaussian process latent variable mod-\nels (GP-LVM). DCTM is essentially a non-linear dimen-\nsion reduction technique which is capable of (1) detecting\ntopic evolution within a document corpus, (2) discovering\ntopic correlations between document corpora, (3) monitor-\ning topic and correlation trends dynamically. Unlike gener-\native aspect models such like LDA, DCTM demonstrates a\nmuch faster converging rate with better model fitting to the\ndata. We empirically assess our approach using 268,231\nscientific documents, from the year 1988 to 2005. Posterior\ninferences suggest that DCTM is useful for capturing topic\nandcorrelationdynamics,as well aspredictingtheirtrends."}
{"Title": "A Topic Modeling Approach and its Integration into the Random Walk\nFramework for Academic Search", "Abstract": "In this paper, we propose a unified topic modeling ap-\nproach and its integration into the random walk framework\nfor academic search. Specifically, we present a topic model\nfor simultaneously modeling papers, authors, and publica-\ntion venues. We combine the proposed topic model into the\nrandom walk framework. Experimental results show that\nour proposed approach for academic search significantly\noutperforms the baseline methods of using BM25 and lan-\nguage model, and those of using the existing topic models\n(including pLSI, LDA, and the AT model)."}
{"Title": "Evolutionary Clustering by Hierarchical Dirichlet Process with Hidden Markov\nState", "Abstract": "This paper studies evolutionary clustering, which is a re-\ncently hot topic with many important applications, notice-\nably in social network analysis. In this paper, based on the\nrecent literature on Hierarchical Dirichlet Process (HDP)\nand Hidden Markov Model (HMM), we have developed a\nstatistical model HDP-HTM that combines HDP with a Hi-\nerarchical Transition Matrix (HTM) based on the proposed\nInfinite Hierarchical Hidden Markov State model (iH 2 MS)\nas an effective solution to this problem. The HDP-HTM\nmodel substantially advances the literature on evolution-\nary clustering in the sense that not only it performs bet-\nter than the existing literature, but more importantly it is\ncapable of automatically learning the cluster numbers and\nstructures and at the same time explicitly addresses the cor-\nrespondence issue during the evolution. Extensive evalu-\nations have demonstrated the effectiveness and promise of\nthis solution against the state-of-the-art literature."}
{"Title": "Unsupervised Face Annotation by Mining the Web\n", "Abstract": "Searching for images of people is an essential task for\nimage and video search engines. However, current search\nengineshavelimited capabilitiesfor this task since theyrely\non text associated with images and video, and such text\nis likely to return many irrelevant results. We propose a\nmethod for retrieving relevant faces of one person by learn-\ningthe visual consistencyamongresults retrieved from text-\ncorrelation-based search engines. The method consists of\ntwo steps. In the first step, each candidate face obtained\nfrom a text-based search engine is ranked with a score that\nmeasures the distribution of visual similarities among the\nfaces. Faces thatare possiblyvery relevantor irrelevantare\nrankedat the top or bottom of the list, respectively. The sec-\nond step improves this rankingby treating this problem as a\nclassification problem in which input faces are classified as\n\u2019person-X\u2019or \u2019non-person-X\u2019;andthefaces are re-ranked\naccording to their relevant score inferred from the classi-\nfier\u2019s probability output. To train this classifier, we use a\nbagging-based framework to combine results from multiple\nweak classifiers trainedusing differentsubsets. These train-\ning subsets are extracted and labeled automatically from\nthe rank list produced from the classifier trained from the\nprevious step. In this way, the accuracy of the ranked list\nincreases after a number of iterations. Experimental results\non various face sets retrieved from captions of news photos\nshow that the retrieval performance improved after each it-\neration, with the final performance being higher than those\nof the existing algorithms."}
{"Title": "Temporal-Relational Classifiers for Prediction in Evolving Domains", "Abstract": "Many relational domains contain temporal information\nand dynamics that are important to model (e.g., social net-\nworks, protein networks). However, past work in relational\nlearning has focused primarily on modeling static \u201csnap-\nshots\u201d of the data and has largely ignored the temporal di-\nmension of these data. In this work, we extend relational\ntechniques to temporally-evolving domains and outline a\nrepresentational framework that is capable of modeling\nboth temporal and relational dependencies in the data. We\ndevelop efficient learning and inference techniques within\nthe framework by considering a restricted set of temporal-\nrelational dependencies and using parameter-tying meth-\nods to generalize across relationships and entities. More\nspecifically, we model dynamic relational data with a two-\nphase process, first summarizing the temporal-relational in-\nformation with kernel smoothing, and then moderating at-\ntribute dependencies with the summarized relational infor-\nmation. We develop a number of novel temporal-relational\nmodels using the framework and then show that the current\napproaches to modeling static relational data are special\ncases within the framework. We compare the new models to\nthe competing static relational methods on three real-world\ndatasets and show that the temporal-relational models con-\nsistently outperform the relational models that ignore tem-\nporal information\u2014achieving significant reductions in er-\nror ranging from 15% to 70%."}
{"Title": "Fast Counting of Triangles in Large Real Networks:\nAlgorithms and Laws", "Abstract": "How can we quickly find the number of triangles in\na large graph, without actually counting them? Trian-\ngles are important for real world social networks, ly-\ning at the heart of the clustering coefficient and of the\ntransitivity ratio. However, straight-forward and even\napproximate counting algorithms can be slow, trying\nto execute or approximate the equivalent of a 3-way\ndatabase join.\nIn this paper, we provide two algorithms, the Eigen-\nTriangle for counting the total number of triangles\nin a graph, and the EigenTriangleLocal algorithm\nthat gives the count of triangles that contain a desired\nnode. Additional contributions include the following:\n(a) We show that both algorithms achieve excellent ac-\ncuracy, with up to \u2248 1000x faster execution time, on\nseveral, real graphs and (b) we discover two new power\nlaws (Degree-Triangle and TriangleParticipa-\ntion laws) with surprising properties."}
{"Title": "A Generative Probabilistic Model for Multi-Label Classification", "Abstract": "Traditional discriminative classification method makes\nlittle attempt to reveal the probabilistic structure and the\ncorrelation within both input and output spaces. In the sce-\nnario of multi-label classification, most of the classifiers\nsimply assume the predefined classes are independently dis-\ntributed, which would definitely hinder the classification\nperformance when there are intrinsic correlations between\nthe classes. In this article, we propose a generative proba-\nbilistic model, the Correlated Labeling Model (CoL Model),\nto formulate the correlation between different classes. The\nCoL model is presented to capture the correlation between\nclasses and the underlying structures via the latent random\nvariables in a supervised manner. We develop a varia-\ntional procedure to approximate the posterior distribution\nand employ the EM algorithm for the empirical Bayes pa-\nrameter estimation. In our evaluations, the proposed model\nachieved promising results on various data sets."}
{"Title": "Explore/Exploit Schemes for Web Content\nOptimization", "Abstract": "Abstract\u2014 We propose novel multi-armed bandit (ex-\nplore/exploit) schemes to maximize total clicks on a content\nmodule published regularly on Yahoo! Intuitively, one can \u201cex-\nplore\u201d each candidate item by displaying it to a small fraction\nof user visits to estimate the item\u2019s click-through rate (CTR),\nand then \u201cexploit\u201d high CTR items in order to maximize clicks.\nWhile bandit methods that seek to find the optimal trade-off\nbetween explore and exploit have been studied for decades,\nexisting solutions are not satisfactory for web content publishing\napplications where dynamic set of items with short lifetimes,\ndelayed feedback and non-stationary reward (CTR) distributions\nare typical. In this paper, we develop a Bayesian solution and\nextend several existing schemes to our setting. Through extensive\nevaluation with nine bandit schemes, we show that our Bayesian\nsolution is uniformly better in several scenarios. We also study\nthe empirical characteristics of our schemes and provide useful\ninsights on the strengths and weaknesses of each. Finally, we\nvalidate our results with a \u201cside-by-side\u201d comparison of schemes\nthrough live experiments conducted on a random sample of real\nuser visits to Yahoo!"}
{"Title": "Connecting Sparsely Distributed Similar Bloggers", "Abstract": "Abstract\u2014The nature of the Blogosphere determines that\nthe majority of bloggers are only connected with a small\nnumber of fellow bloggers, and similar bloggers can be largely\ndisconnected from each other. Aggregating them allows for\ncost-effective personalized services, targeted marketing, and\nexploration of new business opportunities. As most bloggers\nhave only a small number of adjacent bloggers, the problem of\naggregating similar bloggers presents challenges that demand\nnovel algorithms of connecting the non-adjacent due to the\nfragmented distributions of bloggers. In this work, we define\nthe problem, delineate its challenges, and present an approach\nthat uses innovative ways to employ contextual information and\ncollective wisdom to aggregate similar bloggers. A real-world\nblog directory is used for experiments. We demonstrate the\nefficacy of our approach, report findings, and discuss related\nissues and future work."}
{"Title": "Rule Ensembles for Multi-Target Regression", "Abstract": "Abstract\u2014Methods for learning decision rules are being\nsuccessfully applied to many problem domains, especially\nwhere understanding and interpretation of the learned model\nis necessary. In many real life problems, we would like to\npredict multiple related (nominal or numeric) target attributes\nsimultaneously. Methods for learning rules that predict multi-\nple targets at once already exist, but are unfortunately based\non the covering algorithm, which is not very well suited for\nregression problems. A better solution for regression problems\nmay be a rule ensemble approach that transcribes an ensemble\nof decision trees into a large collection of rules. An optimization\nprocedure is then used for selecting the best (and much smaller)\nsubset of these rules, and to determine their weights.\nUsing the rule ensembles approach we have developed a new\nsystem for learning rule ensembles for multi-target regression\nproblems. The newly developed method was extensively eval-\nuated and the results show that the accuracy of multi-target\nregression rule ensembles is better than the accuracy of multi-\ntarget regression trees, but somewhat worse than the accuracy\nof multi-target random forests. The rules are significantly more\nconcise than random forests, and it is also possible to create\nvery small rule sets that are still comparable in accuracy to\nsingle regression trees."}
{"Title": "A Local Scalable Distributed Expectation Maximization Algorithm for Large\nPeer-to-Peer Networks", "Abstract": "Abstract\u2014This paper describes a local and distributed ex-\npectation maximization algorithm for learning parameters of\nGaussian mixture models (GMM) in large peer-to-peer (P2P)\nenvironments. The algorithm can be used for a variety of\nwell-known data mining tasks in distributed environments\nsuch as clustering, anomaly detection, target tracking, and\ndensity estimation to name a few, necessary for many emerging\nP2P applications in bioinformatics, webmining and sensor\nnetworks. Centralizing all or some of the data to build global\nmodels is impractical in such P2P environments because of\nthe large number of data sources, the asynchronous nature of\nthe P2P networks, and dynamic nature of the data/network.\nThe proposed algorithm takes a two-step approach. In the\nmonitoring phase, the algorithm checks if the model \u2018quality\u2019\nis acceptable by using an efficient local algorithm. This is then\nused as a feedback loop to sample data from the network and\nrebuild the GMM when it is outdated. We present thorough\nexperimental results to verify our theoretical claims."}
{"Title": "Cross-Guided Clustering:\nTransfer of Relevant Supervision across Domains for Improved Clustering", "Abstract": "Abstract\u2014Lack of supervision in clustering algorithms often\nleads to clusters that are not useful or interesting to human\nreviewers. We investigate if supervision can be automatically\ntransferred to a clustering task in a target domain, by providing\na relevant supervised partitioning of a dataset from a different\nsource domain. The target clustering is made more meaningful\nfor the human user by trading off intrinsic clustering goodness\non the target dataset for alignment with relevant supervised\npartitions in the source dataset, wherever possible. We propose\na cross-guided clustering algorithm that builds on traditional\nk-means by aligning the target clusters with source partitions.\nThe alignment process makes use of a cross-domain similarity\nmeasure that discovers hidden relationships across domains\nwith potentially different vocabularies. Using multiple real-\nworld datasets, we show that our approach improves clustering\naccuracy significantly over traditional k-means."}
{"Title": "Audio Classification of Bird Species: a Statistical Manifold Approach", "Abstract": "Our goal is to automatically identify which species of\nbird is present in an audio recording using supervised\nlearning. Devising effective algorithms for bird species\nclassification is a preliminary step toward extracting useful\necological data from recordings collected in the field. We\npropose a probabilistic model for audio features within a\nshort interval of time, then derive its Bayes risk-minimizing\nclassifier, and show that it is closely approximated by a\nnearest-neighbor classifier using Kullback-Leibler diver-\ngence to compare histograms of features. We note that fea-\nture histograms can be viewed as points on a statistical\nmanifold, and KL divergence approximates geodesic dis-\ntances defined by the Fisher information metric on such\nmanifolds. Motivated by this fact, we propose the use\nof another approximation to the Fisher information met-\nric, namely the Hellinger metric. The proposed classifiers\nachieve over 90% accuracy on a data set containing six\nspecies of bird, and outperform support vector machines."}
{"Title": "Finding Associations and Computing Similarity via Biased Pair Sampling", "Abstract": "Abstract\u2014Sampling-based methods have previously been\nproposed for the problem of finding interesting associations in\ndata, even for low-support items. While these methods do not\nguarantee precise results, they can be vastly more efficient than\napproaches that rely on exact counting. However, for many sim-\nilarity measures no such methods have been known. In this pa-\nper we show how a wide variety of measures can be supported\nby a simple biased sampling method. The method also extends\nto find high-confidence association rules. We demonstrate\ntheoretically that our method is superior to exact methods when\nthe threshold for \u201cinteresting similarity/confidence\u201d is above\nthe average pairwise similarity/confidence, and the average\nsupport is not too low. Our method is particularly good when\ntransactions contain many items. We confirm in experiments\non standard association mining benchmarks that this gives a\nsignificant speedup on real data sets (sometimes much larger\nthan the theoretical guarantees). Reductions in computation\ntime of over an order of magnitude, and significant savings in\nspace, are observed."}
{"Title": "Beyond Banditron: A Conservative and Efficient Reduction for Online Multiclass\nPrediction with Bandit Setting Model", "Abstract": "Abstract\u2014In this paper, we consider a recently proposed\nsupervised learning problem, called online multiclass prediction\nwith bandit setting model. Aiming at learning from partial\nfeedback of online classification results, i.e. \u201ctrue\u201d when the\npredicting label is right or \u201cfalse\u201d when the predicting label is\nwrong, this new kind of problems arouses much of researchers\u2019\ninterest due to its close relations to real world internet applica-\ntions and human cognitive procedure. While some algorithms\nhave been brought forward, we propose a novel algorithm\nto deal with such problems. First, we reduce the multiclass\nprediction problem to binary based on Conservative one-versus-\nall others Reduction scheme; Then Online Passive-Aggressive\nAlgorithm is embedded as binary learning algorithm to solve\nthe reduced problem. Also we derive a pleasing cumulative\nmistake bound for our algorithm and a time complexity bound\nlinear to the sample size. Further experimental evaluation on\nseveral real world multiclass datasets including RCV1, MNIST,\n20 Newsgroups and USPS shows that our method outperforms\nthe existing algorithms with a great improvement."}
{"Title": "Probabilistic Similarity Query on Dimension Incomplete Data", "Abstract": "Retrieving similar data has drawn many research ef-\nforts in the literature due to its importance in data mining,\ndatabase and information retrieval. This problem is chal-\nlenging when the data is incomplete. In previous research,\ndata incompleteness refers to the fact that data values for\nsome dimensions are unknown. However, in many practical\napplications (e.g., data collection by sensor network under\nbad environment), not only data values but even data di-\nmension information may also be missing, which will make\nmost similarity query algorithms infeasible. In this work,\nwe propose the novel similarity query problem on dimen-\nsion incomplete data and adopt a probabilistic framework\nto model this problem. For this problem, users can give\na distance threshold and a probability threshold to specify\ntheir retrieval requirements. The distance threshold is used\nto specify the allowed distance between query and data ob-\njects and the probability threshold is used to require that\nthe retrieval results satisfy the distance condition at least\nwith the given probability. Instead of enumerating all pos-\nsible cases to recover the missed dimensions, we propose\nan efficient approach to speed up the retrieval process by\nleveragingthe inherent relations between query and dimen-\nsion incomplete data objects. During the query process,\nwe estimate the lower/upper bounds of the probability that\nthe query is satisfied by a given data object, and utilize\ntheseboundsto filter irrelevantdataobjectsefficiently. Fur-\nthermore, a probability triangle inequality is proposed to\nfurther speed up query processing. According to our ex-\nperiments on real data sets, the proposed similarity query\nmethod is verified to be effective and efficient on dimension\nincomplete data."}
{"Title": "flowNet: Flow-based Approach for Efficient Analysis of Complex Biological\nNetworks", "Abstract": "Abstract\u2014Biological networks having complex connectivity\nhave been widely studied recently. By characterizing their\ninherent and structural behaviors in a topological perspective,\nthese studies have attempted to discover hidden knowledge in\nthe systems. However, even though various algorithms with\ngraph-theoretical modeling have provided fundamentals in the\nnetwork analysis, the availability of practical approaches to\nefficiently handle the complexity has been limited. In this paper,\nwe present a novel flow-based approach, called flowNet, to ef-\nficiently analyze large-sized, complex networks. Our approach\nis based on the functional influence model that quantifies the\ninfluence of a biological component on another. We introduce a\ndynamic flow simulation algorithm to generate a flow pattern\nwhich is a unique characteristic for each component. The\nset of patterns can be used in identifying functional modules\n(i.e., clustering). The proposed flow simulation algorithm runs\nvery efficiently in sparse networks. Since our approach uses\na weighted network as an input, we also discuss supervised\nand unsupervised weighting schemes for unweighted biological\nnetworks. As experimental results in real applications to the\nyeast protein interaction network, we demonstrate that our\napproach outperforms previous graph clustering methods with\nrespect to accuracy."}
{"Title": "\u03bd -Anomica: A Fast Support Vector based Novelty Detection Technique", "Abstract": "Abstract\u2014In this paper we propose \u03bd-Anomica, a novel\nanomaly detection technique that can be trained on huge\ndata sets with much reduced running time compared to the\nbenchmark one-class Support Vector Machines algorithm.\nIn \u03bd-Anomica, the idea is to train the machine such that\nit can provide a close approximation to the exact decision\nplane using fewer training points and without losing\nmuch of the generalization performance of the classical\napproach. We have tested the proposed algorithm on a\nvariety of continuous data sets under different conditions.\nWe show that under all test conditions the developed\nprocedure closely preserves the accuracy of standard one-\nclass Support Vector Machines while reducing both the\ntraining time and the test time by 5 \u2212 20 times."}
{"Title": "Temporal Neighborhood Discovery using Markov Models", "Abstract": "Temporal data, which is a sequence of data tuples mea-\nsured at successive time instances, is typically very large.\nHence instead of mining the entire data, we are interested\nin dividing the huge data into several smaller intervals of\ninterest which we call temporal neighborhoods. In this paper\nwe propose an approach to generate temporal neighborhoods\nthrough unequal depth discretization.\nWe describe two novel algorithms (a) Similarity based\nMerging (SMerg) and, (b) Stationary distribution based\nMerging (StMerg). These algorithms are based on the\nrobust framework of Markov models and the Markov Sta-\ntionary distribution respectively. We identify temporal neigh-\nborhoods with distinct demarcations based on unequal depth\ndiscretization of the data. We discuss detailed experimental\nresults in both synthetic and real world data. Specifically we\nshow (i) the efficacy of our approach through precision and\nrecall of labeled bins, (ii) the ground truth validation in real\nworld datasets and, (iii) knowledge discovery in the temporal\nneighborhoods such as global anomalies. Our results indicate\nthat we are able to identify valuable knowledge based on our\nground truth validation from real world traffic data."}
{"Title": "Active Learning with Generalized Queries", "Abstract": "Abstract\u2014Active learning can actively select or construct\nexamples to label to reduce the number of labeled examples\nneeded for building accurate classifiers. However, previous\nworks of active learning can only ask specific queries. For\nexample, to predict osteoarthritis from a patient dataset with\n30 attributes, specific queries always contain values of all these\n30 attributes, many of which may be irrelevant. A more natural\nway is to ask \u201cgeneralized queries\u201d with don\u2019t-care attributes,\nsuch as \u201care people over 50 with knee pain likely to have\nosteoarthritis?\u201d (with only two attributes: age and type of pain).\nWe assume that the oracle (and human experts) can readily\nanswer those generalized queries by returning probabilistic\nlabels. The power of such generalized queries is that one\ngeneralized query may be equivalent to many specific ones.\nHowever, overly general queries may receive highly uncertain\nlabels from the oracle, and this makes learning difficult. In this\npaper, we propose a novel active learning algorithm that asks\ngeneralized queries. We demonstrate experimentally that our\nnew method asks significantly fewer queries compared with the\nprevious works of active learning. Our method can be readily\ndeployed in real-world tasks where obtaining labeled examples\nis costly."}
{"Title": "Conditional Models for Non-smooth Ranking Loss Functions", "Abstract": "Learning to rank is an important area at the interface of machine\nlearning, information retrieval and Web search. The central challenge in\noptimizing various measures of ranking loss is that the objectives tend\nto be non-convex and discontinuous. To make such functions amenable to\ngradient based optimization procedures one needs to design clever bounds.\nIn recent years, boosting, neural networks, support vector machines, and\nmany other techniques have been applied. However, there is little work\non directly modeling a conditional probability Pr(y|x q ) where y is a\npermutation of the documents to be ranked and x q represents their feature\nvectors with respect to a query q. A major reason is that the space of y\nis huge: n! if n documents must be ranked. We first propose an intuitive\nand appealing expected loss minimization objective, and give an efficient\nshortcut to evaluate it despite the huge space of ys. Unfortunately, the\noptimization is non-convex, so we propose a convex approximation. We give\na new, efficient Monte Carlo sampling method to compute the objective\nand gradient of this approximation, which can then be used in a quasi-\nNewton optimizer like LBFGS. Extensive experiments with the widely-used\nLETOR dataset show large ranking accuracy improvements beyond recent\nand competitive algorithms."}
{"Title": "Unsupervised Class Separation of Multivariate Data\nthrough Cumulative Variance-based Ranking", "Abstract": "Abstract\u2014This paper introduces a new extension of outlier\ndetection approaches and a new concept, class separation\nthrough variance. We show that accumulating information\nabout the outlierness of points in multiple subspaces leads to a\nranking in which classes with differing variance naturally tend\nto separate. Exploiting this leads to a highly effective and effi-\ncient unsupervised class separation approach, especially useful\nin the difficult case of heavily overlapping distributions. Unlike\ntypical outlier detection algorithms, this method can be applied\nbeyond the \u2018rare classes\u2019 case with great success. Two novel\nalgorithms that implement this approach are provided. Ad-\nditionally, experiments show that the novel methods typically\noutperform other state-of-the-art outlier detection methods on\nhigh dimensional data such as Feature Bagging, SOE1, LOF,\nORCA and Robust Mahalanobis Distance and competes even\nwith the leading supervised classification methods."}
{"Title": "Execution Anomaly Detection in Distributed Systems\nthrough Unstructured Log Analysis", "Abstract": "Abstract -- Detection of execution anomalies is very im-\nportant for the maintenance, development, and perfor-\nmance refinement of large scale distributed systems. Ex-\necution anomalies include both work flow errors and low\nperformance problems. People often use system logs pro-\nduced by distributed systems for troubleshooting and\nproblem diagnosis. However, manually inspecting system\nlogs to detect anomalies is unfeasible due to the increasing\nscale and complexity of distributed systems. Therefore,\nthere is a great demand for automatic anomaly detection\ntechniques based on log analysis. In this paper, we pro-\npose an unstructured log analysis technique for anomaly\ndetection. In the technique, we propose a novel algorithm\nto convert free form text messages in log files to log keys\nwithout heavily relying on application specific knowledge.\nThe log keys correspond to the log-print statements in the\nsource code which can provide cues of system execution\nbehavior. After converting log messages to log keys, we\nlearn a Finite State Automaton (FSA) from training log\nsequences to present the normal work flow for each sys-\ntem component. At the same time, a performance mea-\nsurement model is learned to characterize the normal\nexecution performance based on the log messages\u2019 timing\ninformation. With these learned models, we can automat-\nically detect anomalies in newly input log files. Experi-\nments on Hadoop and SILK show that the technique can\neffectively detect running anomalies."}
{"Title": "Learning the Shared Subspace for Multi-Task Clustering and Transductive Transfer\nClassification", "Abstract": "Abstract\u2014There are many clustering tasks which are closely\nrelated in the real world, e.g. clustering the web pages of\ndifferent universities. However, existing clustering approaches\nneglect the underlying relation and treat these clustering\ntasks either individually or simply together. In this paper,\nwe will study a novel clustering paradigm, namely multi-task\nclustering, which performs multiple related clustering tasks\ntogether and utilizes the relation of these tasks to enhance the\nclustering performance. We aim to learn a subspace shared by\nall the tasks, through which the knowledge of the tasks can\nbe transferred to each other. The objective of our approach\nconsists of two parts: (1) Within-task clustering: clustering the\ndata of each task in its input space individually; and (2) Cross-\ntask clustering: simultaneous learning the shared subspace and\nclustering the data of all the tasks together. We will show that it\ncan be solved by alternating minimization, and its convergence\nis theoretically guaranteed. Furthermore, we will show that\ngiven the labels of one task, our multi-task clustering method\ncan be extended to transductive transfer classification (a.k.a.\ncross-domain classification, domain adaption). Experiments\non several cross-domain text data sets demonstrate that the\nproposed multi-task clustering outperforms traditional single-\ntask clustering methods greatly. And the transductive transfer\nclassification method is comparable to or even better than\nseveral existing transductive transfer classification approaches."}
{"Title": "Accurate Estimation of the Degree Distribution of Private Networks", "Abstract": "Abstract\u2014We describe an efficient algorithm for releasing\na provably private estimate of the degree distribution of\na network. The algorithm satisfies a rigorous property of\ndifferential privacy, and is also extremely efficient, running on\nnetworks of 100 million nodes in a few seconds. Theoretical\nanalysis shows that the error scales linearly with the number of\nunique degrees, whereas the error of conventional techniques\nscales linearly with the number of nodes. We complement the\ntheoretical analysis with a thorough empirical analysis on real\nand synthetic graphs, showing that the algorithm\u2019s variance\nand bias is low, that the error diminishes as the size of the\ninput graph increases, and that common analyses like fitting a\npower-law can be carried out very accurately."}
{"Title": "A Linear-time Graph Kernel", "Abstract": "Abstract\u2014The design of a good kernel is fundamental for\nknowledge discovery from graph-structured data. Existing\ngraph kernels exploit only limited information about the graph\nstructures but are still computationally expensive. We propose\na novel graph kernel based on the structural characteristics of\ngraphs. The key is to represent node labels as binary arrays and\ncharacterize each node using logical operations on the label set\nof the connected nodes. Our kernel has a linear time complexity\nwith respect to the number of nodes times the average number\nof neighboring nodes in the given graphs. The experimental\nresult shows that the proposed kernel performs comparable and\nmuch faster than a state-of-the-art graph kernel for benchmark\ndata sets and shows high scalability for new applications with\nlarge graphs."}
{"Title": "GSML: A Unified Framework for Sparse Metric Learning", "Abstract": "There has been significant recent interest in sparse met-\nric learning (SML) in which we simultaneously learn both\na good distance metric and a low-dimensional representa-\ntion. Unfortunately, the performance of existing sparse met-\nric learning approaches is usually limited because the au-\nthorsassumedcertainproblemrelaxationsortheytargetthe\nSMLobjectiveindirectly. Inthispaper, weproposeaGener-\nalized Sparse Metric Learning method (GSML). This novel\nframework offers a unified view for understanding many of\nthe popular sparse metric learning algorithms including the\nSparse Metric Learning framework proposed in [15], the\nLarge Margin Nearest Neighbor (LMNN) [21][22], and the\nD-ranking Vector Machine (D-ranking VM) [14]. More-\nover, GSML also establishes a close relationship with the\nPairwise Support Vector Machine [20]. Furthermore, the\nproposed framework is capable of extending many current\nnon-sparse metric learning models such as Relevant Vector\nMachine (RCA) [4] and a state-of-the-art method proposed\nin [23] into their sparse versions. We present the detailed\nframework, provide theoretical justifications, build various\nconnectionswithothermodels, andproposeapracticaliter-\native optimization method, making the framework both the-\noretically important and practically scalable for medium or\nlarge datasets. A series of experiments show that the pro-\nposed approach can outperform previous methods in terms\nof both test accuracy and dimension reduction, on six real-\nworld benchmark datasets."}
{"Title": "GRAPE: A Graph-Based Framework for Disambiguating People Appearances in\nWeb Search", "Abstract": "Abstract\u2014Finding information about people using search\nengines is one of the most common activities on the Web.\nHowever, search engines usually return a long list of Web pages,\nwhich may be relevant to many namesakes, especially given the\nexplosive growth of Web data. To address the challenge caused\nby name ambiguity in Web people search, this paper proposes a\nnovel graph-based framework, GRAPE (abbr. a Graph-based\nfRamework for disAmbiguating People appEarances in Web\nsearch). In GRAPE, people tag information (e.g., people name,\norganization, and email address) surrounding the queried\npeople name is extracted from the search results, a graph-\nbased unsupervised algorithm is then developed to cluster the\nextracted tags, where a new method, Cohesion, is introduced\nto measure the importance of a tag for clustering, and each final\ncluster of tags represents a unique people entity. Experimental\nresults show that our proposed framework outperforms the\nstate-of-the-art Web people name disambiguation approaches."}
{"Title": "A Tree-based Framework for Difference Summarization", "Abstract": "Abstract\u2014Understanding the differences between two\ndatasets is a fundamental data mining question and is also\nubiquitously important across many real world scientific ap-\nplications. In this paper, we propose a tree-based framework to\nprovide a parsimonious explanation of the difference between\ntwo distributions based on rigorous two-sample statistical test.\nWe develop two efficient approaches. The first one is a dynamic\nprogramming approach that finds a minimal number of data\nsubsets that describe the difference between two data sets.\nThe second one is a greedy approach that approximates the\ndynamic programming approach . We employ the well-known\nFriedman\u2019s MST (minimal spanning tree) statistics for two-\nsample statistical tests in our summarization tree construction,\nand develop novel techniques to speedup its computational\nprocedure. We performed a detailed experimental evaluation\non both real and synthetic datasets and demonstrated the\neffectiveness of our tree-summarization approach."}
{"Title": "TrBagg: A Simple Transfer Learning Method\nand Its Application to Personalization in Collaborative Tagging", "Abstract": "Abstract\u2014The aim of transfer learning is to improve pre-\ndiction accuracy on a target task by exploiting the training\nexamples for tasks that are related to the target one. Transfer\nlearning has received more attention in recent years, because\nthis technique is considered to be helpful in reducing the\ncost of labeling. In this paper, we propose a very simple\napproach to transfer learning: TrBagg, which is the extension\nof bagging. TrBagg is composed of two stages: Many weak\nclassifiers are first generated as in standard bagging, and\nthese classifiers are then filtered based on their usefulness\nfor the target task. This simplicity makes it easy to work\nreasonably well without severe tuning of learning parameters.\nFurther, our algorithm equips an algorithmic scheme to avoid\nnegative transfer. We applied TrBagg to personalized tag\nprediction tasks for social bookmarks Our approach has several\nconvenient characteristics for this task such as adaptation to\nmultiple tasks with low computational cost."}
{"Title": "PEGASUS: A Peta-Scale Graph Mining System - Implementation and Observations", "Abstract": "Abstract\u2014In this paper, we describe P E G A S US , an open\nsource Peta Graph Mining library which performs typical\ngraph mining tasks such as computing the diameter of the\ngraph, computing the radius of each node and finding the\nconnected components. As the size of graphs reaches several\nGiga-, Tera- or Peta-bytes, the necessity for such a library\ngrows too. To the best of our knowledge, P E G A S US is the first\nsuch library, implemented on the top of the H ADOOP platform,\nthe open source version of M AP R EDUCE .\nMany graph mining operations (PageRank, spectral cluster-\ning, diameter estimation, connected components etc.) are es-\nsentially a repeated matrix-vector multiplication. In this paper\nwe describe a very important primitive for P E G A S US , called\nGIM-V (Generalized Iterated Matrix-Vector multiplication).\nGIM-V is highly optimized, achieving (a) good scale-up on the\nnumber of available machines (b) linear running time on the\nnumber of edges, and (c) more than 5 times faster performance\nover the non-optimized version of GIM-V.\nOur experiments ran on M45, one of the top 50 supercom-\nputers in the world. We report our findings on several real\ngraphs, including one of the largest publicly available Web\nGraphs, thanks to Yahoo!, with \u2248 6,7 billion edges."}
{"Title": "Efficient Discovery of Frequent Correlated Subgraph Pairs", "Abstract": "Abstract\u2014The recent proliferation of graph data in a wide\nspectrum of applications has led to an increasing demand\nfor advanced data analysis techniques. In view of this, many\ngraph mining techniques, such as frequent subgraph mining\nand correlated subgraph mining, have been proposed. In many\napplications, both frequency and correlation play an important\nrole. Thus, this paper studies a new problem of mining the set\nof frequent correlated subgraph pairs. A simple algorithm that\ncombines existing algorithms for mining frequent subgraphs\nand correlated subgraphs results in a multiplication of the\nmining operations, the majority of which are redundant. We\ndiscover that most of the graphs correlated to a common\ngraph are also highly correlated. We establish theoretical\nfoundations for this finding and derive a tight lower bound\non the correlation of any two graphs that are correlated to a\ncommon graph. This theoretical result leads to the design of\na very effective skipping mechanism, by which we skip the\nprocessing of a majority of graphs in the mining process.\nOur algorithm, FCP-Miner, is a fast approximate algorithm,\nbut we show that the missing pairs are only a small set of\nmarginally correlated pairs. Extensive experiments verify both\nthe efficiency and effectiveness of FCP-Miner."}
{"Title": "Self-Adaptive Anytime Stream Clustering", "Abstract": "Abstract\u2014Clustering streaming data requires algorithms\nwhich are capable of updating clustering results for the incom-\ning data. As data is constantly arriving, time for processing is\nlimited. Clustering has to be performed in a single pass over\nthe incoming data and within the possibly varying inter-arrival\ntimes of the stream. Likewise, memory is limited, making\nit impossible to store all data. For clustering, we are faced\nwith the challenge of maintaining a current result that can be\npresented to the user at any given time.\nIn this work, we propose a parameter free algorithm that\nautomatically adapts to the speed of the data stream. It makes\nbest use of the time available under the current constraints\nto provide a clustering of the objects seen up to that point.\nOur approach incorporates the age of the objects to reflect\nthe greater importance of more recent data. Moreover, we\nare capable of detecting concept drift, novelty and outliers in\nthe stream. For efficient and effective handling, we introduce\nthe ClusTree, a compact and self-adaptive index structure for\nmaintaining stream summaries. Our experiments show that\nour approach is capable of handling a multitude of different\nstream characteristics for accurate and scalable anytime stream\nclustering."}
{"Title": "Improving SVM Classification on Imbalanced Data Sets in Distance Spaces", "Abstract": "Abstract\u2014Imbalanced data sets present a particular chal-\nlenge to the data mining community. Often, it is the rare event\nthat is of interest and the cost of misclassifying the rare event\nis higher than misclassifying the usual event. When the data is\nhighly skewed toward the usual, it can be very difficult for a\nlearning system to accurately detect the rare event. There have\nbeen many approaches in recent years for handling imbalanced\ndata sets, from under-sampling the majority class to adding\nsynthetic points to the minority class in feature space. Distances\nbetween time series are known to be non-Euclidean and non-\nmetric, since comparing time series requires warping in time.\nThis fact makes it impossible to apply standard methods like\nSMOTE to insert synthetic data points in feature spaces. We\npresent an innovative approach that augments the minority\nclass by adding synthetic points in distance spaces. We then use\nSupport Vector Machines for classification. Our experimental\nresults on standard time series show that our synthetic points\nsignificantly improve the classification rate of the rare events,\nand in many cases also improves the overall accuracy of SVM."}
{"Title": "CoCoST: A Computational Cost Sensitive Classifier", "Abstract": "Abstract\u2014Computational cost of classification is as impor-\ntant as accuracy in on-line classification systems. The compu-\ntational cost is usually dominated by the cost of computing\nimplicit features of the raw input data. Very few efforts\nhave been made to design classifiers which perform effectively\nwith limited computational power; instead, feature selection is\nusually employed as a pre-processing step to reduce the cost of\nrunning traditional classifiers. We present CoCoST, a novel and\neffective approach for building classifiers which achieve state-\nof-the-art classification accuracy, while keeping the expected\ncomputational cost of classification low, even without feature\nselection. CoCost employs a wide range of novel cost-aware\ndecision trees, each of which is tuned to specialize in classifying\ninstances from a subset of the input space, and judiciously\nconsults them depending on the input instance in accordance\nwith a cost-aware meta-classifier. Experimental results on a\nnetwork flow detection application show that, our approach\ncan achieve better accuracy than classifiers such as SVM and\nrandom forests, while achieving 75%-90% reduction in the\ncomputational costs."}
{"Title": "Semi-Naive Exploitation of One-Dependence Estimators", "Abstract": "Abstract\u2014It is well known that the key of Bayesian classifier\nlearning is to balance the two important issues, that is, the\nexploration of attribute dependencies in high orders for ensur-\ning a sufficient flexibility in approximating the ground-truth\ndependencies, and the exploration of low orders for ensuring\na stable probability estimate from limited training samples.\nBy allowing one-order attribute dependencies, one-dependence\nestimators (ODEs) have been shown to be able to approximate\nthe ground-truth attribute dependencies whilst keeping the\neffectiveness of probability estimation, and therefore leading\nto excellent performance. In previous studies, however, ODEs\nwere exploited in simple ways, such as by averaging, for clas-\nsification. In this paper, we propose a semi-naive exploitation\nof ODEs that fits a function of ODEs to pursue higher-order\nattribute dependencies. Extensive experiments show that the\nproposed SNODE approach can achieve better performance\nthan many state-of-the-art Bayesian classifiers."}
{"Title": "A Framework for Computing the Privacy Scores of Users in Online Social Networks", "Abstract": "Abstract\u2014A large body of work has been devoted to address\ncorporate-scale privacy concerns related to social networks.\nThe main focus was on how to share social networks owned\nby organizations without revealing the identities or sensitive\nrelationships of the users involved. Not much attention has been\ngiven to the privacy risk of users posed by their information-\nsharing activities.\nIn this paper, we approach the privacy concerns arising in\nonline social networks from the individual users\u2019 viewpoint:\nwe propose a framework to compute a privacy score of a\nuser, which indicates the potential privacy risk caused by his\nparticipation in the network. Our definition of privacy score\nsatisfies the following intuitive properties: the more sensitive\nthe information revealed by a user, the higher his privacy\nrisk. Also, the more visible the disclosed information becomes\nin the network, the higher the privacy risk. We develop\nmathematical models to estimate both sensitivity and visibility\nof the information. We apply our methods to synthetic and\nreal-world data and demonstrate their efficacy and practical\nutility."}
{"Title": "Least Square Incremental Linear Discriminant Analysis", "Abstract": "Abstract\u2014Linear discriminant analysis (LDA) is a well-\nknown dimension reduction approach, which projects high-\ndimensional data into a low-dimensional space with the best\nseparation of different classes. In many tasks, the data accumu-\nlates over time, and thus incremental LDA is more desirable\nthan batch LDA. Several incremental LDA algorithms have\nbeen developed and achieved success; however, the eigen-\nproblem involved requires a large computation cost, which\nhampers the efficiency of these algorithms. In this paper, we\npropose a new incremental LDA algorithm, LS-ILDA, based\non the least square solution of LDA. When new samples\nare received, LS-ILDA incrementally updates the least square\nsolution of LDA. Our analysis discloses that this algorithm\nproduces the exact least square solution of batch LDA, while\nits computational cost is O(min(n,d) \u00d7 d) for one update\non dataset containing n instances in d-dimensional space.\nExperimental results show that comparing with state-of-the-art\nincremental LDA algorithms, our proposed LS-ILDA achieves\nhigh accuracy with low time cost."}
{"Title": "Unified Solution to Nonnegative Data Factorization Problems \u2217", "Abstract": "In this paper, we restudy the non-convex data fac-\ntorization problems (regularized or not, unsupervised or\nsupervised), where the optimization is confined in the\nnonnegative orthant, and provide a unified convergency\nprovable solution based on multiplicative nonnegative\nupdate rules. This solution is general for optimization\nproblems with block-wisely quadratic objective functions,\nand thus direct update rules can be derived by skipping\nover the tedious specific procedure deduction process and\nalgorithmic convergence proof. By taking this unified solu-\ntion as a general template, we i) re-explain several existing\nnonnegative data factorization algorithms, ii) develop a\nvariant of nonnegative matrix factorization formulation for\nhandling out-of-sample data, and iii) propose a new non-\nnegative data factorization algorithm, called Correlated\nCo-Decomposition (CCD), to simultaneously factorize two\nfeature spaces by exploring the inter-correlated informa-\ntion. Experiments on both face recognition and multi-label\nimage annotation tasks demonstrate the wide applicability\nof the unified solution as well as the effectiveness of two\nproposed new algorithms."}
{"Title": "Extended Boolean Matrix Decomposition", "Abstract": "Abstract\u2014With the vast increase in collection and storage of\ndata, the problem of data summarization is most critical for\neffective data management. Since much of this data is categor-\nical in nature, it can be viewed in terms of a Boolean matrix.\nBoolean matrix decomposition (BMD) has been used to provide\nconcise and interpretable representations of Boolean data sets.\nA Boolean matrix can be expressed as a product of two Boolean\nmatrices, where the first matrix represents a set of meaningful\nconcepts, and the second describes how the observed data can\nbe expressed as combinations of those concepts. Typically, the\ncombination is only in terms of the set union. In other words, a\nsuccessful Boolean matrix decomposition gives a set of concepts\nand shows how every column of the input data can be expressed\nas a union of some subset of those concepts. However, this way\nof modeling only incompletely represents real data semantics.\nEssentially, it ignores a critical component \u2013 the set difference\noperation: a column can be expressed as the combination of\nunion of certain concepts as well as the exclusion of other\nconcepts. This has two significant benefits. First, the total\nnumber of concepts required to describe the data may itself\nbe reduced. Second, a more succinct summarization may be\nfound for every column. In this paper, we propose the extended\nBoolean matrix decomposition (EBMD) problem, which aims\nto factor Boolean matrices using both the set union and set\ndifference operations. We study several variants of the problem,\nshow that they are NP-hard, and propose efficient heuristics\nto solve them. Extensive experimental results demonstrate the\npower of EBMD."}
{"Title": "Active Learning with Adaptive Heterogeneous Ensembles", "Abstract": "Abstract\u2014One common approach to active learning is to\niteratively train a single classifier by choosing data points\nbased on its uncertainty, but it is nontrivial to design uncer-\ntainty measures unbiased by the choice of classifier. Query by\ncommittee [1] suggests that given an ensemble of diverse but\naccurate classifiers, the most informative data points are those\nthat cause maximal disagreement among the predictions of the\nensemble members. However the method for finding ensembles\nappropriate to a given data set remains an open question. In\nthis paper, the random subspace method is combined with\nactive learning to create multiple instances of different classifier\ntypes, and an algorithm is introduced that adapts the ratio\nof different classifier types in the ensemble towards better\noverall accuracy. Here we show that the proposed algorithm\noutperforms C4.5 with uncertainty sampling, Naive Bayes\nwith uncertainty sampling, bagging, boosting and the random\nsubspace method with random sampling. To the best of our\nknowledge, our work is the first to adapt the ratio of classifiers\nin a heterogeneous ensemble for active learning."}
{"Title": "Non-negative Laplacian Embedding", "Abstract": "Abstract\u2014Laplacian embedding provides a low dimensional\nrepresentation for a matrix of pairwise similarity data using\nthe eigenvectors of the Laplacian matrix. The true power of\nLaplacian embedding is that it provides an approximation\nof the Ratio Cut clustering. However, Ratio Cut clustering\nrequires the solution to be nonnegative. In this paper, we\npropose a new approach, nonnegative Laplacian embedding,\nwhich approximates Ratio Cut clustering in a more direct\nway than traditional approaches. From the solution of our\napproach, clustering structures can be read off directly. We\nalso propose an efficient algorithm to optimize the objective\nfunction utilized in our approach. Empirical studies on many\nreal world datasets show that our approach leads to more\naccurate Ratio Cut solution and improves clustering accuracy\nat the same time."}
{"Title": "Scalable Algorithms for Distribution Search", "Abstract": "Distribution data naturally arise in countless domains,\nsuch as meteorology, biology, geology, industry and eco-\nnomics. However, relatively little attention has been paid\nto data mining for large distribution sets. Given n dis-\ntributions of multiple categories and a query distribution\nQ, we want to find similar clouds (i.e., distributions), to\ndiscover patterns, rules and outlier clouds. For example,\nconsider the numerical case of sales of items, where, for\neach item sold, we record the unit price and quantity; then,\neach customer is represented as a distribution of 2-d points\n(one for each item he/she bought). We want to find similar\nusers, e.g., for market segmentation, anomaly/fraud detec-\ntion. We propose to address this problem and present D-\nSearch, whichincludesfastandeffectivealgorithmsforsim-\nilarity search in large distribution datasets. Our main con-\ntributions are (1) approximate KL divergence, which can\nspeed up cloud-similarity computations, (2) multi-step se-\nquential scan, which efficiently prunes a significant number\nof search candidates and leads to a direct reduction in the\nsearch cost. We also introduce an extended version of D-\nSearch : (3) time-series distribution mining, which finds\nsimilar subsequences in time-series distribution datasets.\nExtensive experiments on real multi-dimensional datasets\nshow that our solution achieves up to 2,300 faster wall-\nclock time over the naive implementation while it does not\nsacrifice accuracy."}
{"Title": "A Deep Non-Linear Feature Mapping for Large-Margin kNN Classification", "Abstract": "KNN is one of the most popular data mining methods\nfor classification, but it often fails to work well with inap-\npropriatechoiceof distancemetric or due to thepresence of\nnumerousclass-irrelevant features. Linear feature transfor-\nmation methods have been widely applied to extract class-\nrelevantinformationtoimprovekNN classification,whichis\nvery limited in many applications. Kernels have also been\nused to learn powerful non-linear feature transformations,\nbut these methods fail to scale to large datasets. In this\npaper, we present a scalable non-linear feature mapping\nmethod based on a deep neural network pretrained with\nRestricted Boltzmann Machines for improving kNN classi-\nfication in a large-margin framework, which we call DNet-\nkNN. DNet-kNN can be used for both classification and for\nsupervised dimensionality reduction. The experimental re-\nsults on two benchmark handwritten digit datasets and one\nnewsgroup text dataset show that DNet-kNN has much bet-\nterperformancethanlarge-marginkNN usingalinearmap-\nping and kNN based on a deep autoencoderpretrained with\nRestricted Boltzmann Machines."}
{"Title": "Finding Time Series Motifs in Disk-Resident Data", "Abstract": "Abstract\u2014Time series motifs are sets of very similar\nsubsequences of a long time series. They are of interest in their\nown right, and are also used as inputs in several higher-level\ndata mining algorithms including classification, clustering,\nrule-discovery and summarization. In spite of extensive\nresearch in recent years, finding exact time series motifs in\nmassive databases is an open problem. Previous efforts either\nfound approximate motifs or considered relatively small\ndatasets residing in main memory. In this work, we describe for\nthe first time a disk-aware algorithm to find exact time series\nmotifs in multi-gigabyte databases which contain on the order\nof tens of millions of time series.\nWe have evaluated our algorithm on datasets from diverse\nareas including medicine, anthropology, computer networking\nand image processing and show that we can find interesting\nand meaningful motifs in datasets that are many orders of\nmagnitude larger than anything considered before."}
{"Title": "Relevant Subspace Clustering: Mining the Most Interesting\nNon-Redundant Concepts in High Dimensional Data", "Abstract": "Abstract\u2014Subspace clustering aims at detecting clusters in\nany subspace projection of a high dimensional space. As the\nnumber of possible subspace projections is exponential in the\nnumber of dimensions, the result is often tremendously large.\nRecent approaches fail to reduce results to relevant subspace\nclusters. Their results are typically highly redundant, i.e. many\nclusters are detected multiple times in several projections.\nIn this work, we propose a novel model for relevant subspace\nclustering (RESCU). We present a global optimization which\ndetects the most interesting non-redundant subspace clusters.\nWe prove that computation of this model is NP-hard. For\nRESCU, we propose an approximative solution that shows\nhigh accuracy with respect to our relevance model. Thorough\nexperiments on synthetic and real world data show that RESCU\nsuccessfully reduces the result to manageable sizes. It reliably\nachieves top clustering quality while competing approaches\nshow greatly varying performance"}
{"Title": "Stacked Gaussian Process Learning", "Abstract": "Abstract\u2014Triggered by a market relevant application that\ninvolves making joint predictions of pedestrian and public\ntransit flows in urban areas, we address the question of how\nto utilize hidden common cause relations among variables of\ninterest in order to improve performance in the two related\nregression tasks. Specifically, we propose stacked Gaussian\nprocess learning, a meta-learning scheme in which a base Gaus-\nsian process is enhanced by adding the posterior covariance\nfunctions of other related tasks to its covariance function in a\nstage-wise optimization. The idea is that the stacked posterior\ncovariances encode the hidden common causes among variables\nof interest that are shared across the related regression tasks.\nStacked Gaussian process learning is efficient, capable of\ncapturing shared common causes, and can be implemented\nwith any kind of standard Gaussian process regression model\nsuch as sparse approximations and relational variants. Our\nexperimental results on real-world data from the market\nrelevant application show that stacked Gaussian processes\nlearning can significantly improve prediction performance of a\nstandard Gaussian process."}
{"Title": "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data", "Abstract": "Recently a number of modeling techniques have been\ndeveloped for data mining and machine learning in rela-\ntional and network domains where the instances are not in-\ndependent and identically distributed (i.i.d.). These meth-\nods specifically exploit the statistical dependencies among\ninstances in order to improve classification accuracy. How-\never, there has been little focus on how these same de-\npendencies affect our ability to draw accurate conclusions\nabout the performance of the models. More specifically, the\ncomplex link structure and attribute dependencies in net-\nwork data violate the assumptions of many conventional\nstatistical tests and make it difficult to use these tests to\nassess the models in an unbiased manner. In this work,\nwe examine the task of within-network classification and\nthe question of whether two algorithms will learn models\nwhich will result in significantly different levels of perfor-\nmance. We show that the commonly-used form of evalua-\ntion (paired t-test on overlapping network samples) can re-\nsult in an unacceptable level of Type I error. Furthermore\nwe show that Type I error increases as (1) the correlation\namong instances increases and (2) the size of the evalua-\ntion set increases (i.e., the proportion of labeled nodes in\nthe network decreases). We propose a method for network\ncross-validation that combined with paired t-tests produces\nmore acceptable levels of Type I error while still providing\nreasonable levels of statistical power (i.e., Type II error)."}
{"Title": "Discovering Excitatory Networks from Discrete Event Streams\nwith Applications to Neuronal Spike Train Analysis", "Abstract": "Abstract\u2014Mining temporal network models from discrete\nevent streams is an important problem with applications in\ncomputational neuroscience, physical plant diagnostics, and\nhuman-computer interaction modeling. We focus in this paper\non temporal models representable as excitatory networks where\nall connections are stimulative, rather than inhibitive. Through\nthis emphasis on excitatory networks, we show how they can\nbe learned by creating bridges to frequent episode mining.\nSpecifically, we show that frequent episodes help identify nodes\nwith high mutual information relationships and which can\nbe summarized into a dynamic Bayesian network (DBN). To\ndemonstrate the practical feasibility of our approach, we show\nhow excitatory networks can be inferred from both mathemat-\nical models of spiking neurons as well as real neuroscience\ndatasets."}
{"Title": "Clustering Trajectories of Moving Objects in an Uncertain World", "Abstract": "Mining Trajectory Databases (TD) has recently\ngained great interest due to the popularity of tracking\ndevices. On the other hand, the inherent presence of\nuncertainty in TD (e.g., due to GPS errors) has not\nbeen taken yet into account during the mining process.\nIn this paper, we study the effect of uncertainty in TD\nclustering and introduce a three-step approach to deal\nwith it. First, we propose an intuitionistic point vector\nrepresentation of trajectories that encompasses the\nunderlying uncertainty and introduce an effective\ndistance metric to cope with uncertainty. Second, we\ndevise CenTra, a novel algorithm which tackles the\nproblem of discovering the Centroid Trajectory of a\ngroup of movements. Third, we propose a variant of\nthe Fuzzy C-Means (FCM) clustering algorithm, which\nembodies CenTra at its update procedure. The\nexperimental  evaluation  over  real  world  TD\ndemonstrates the efficiency and effectiveness of our\napproach."}
{"Title": "Semi-Supervised Sequence Labeling with Self-Learned Features", "Abstract": "Abstract\u2014Typical information extraction (IE) systems can be\nseen as tasks assigning labels to words in a natural language\nsequence. The performance is restricted by the availability\nof labeled words. To tackle this issue, we propose a semi-\nsupervised approach to improve the sequence labeling pro-\ncedure in IE through a class of algorithms with self-learned\nfeatures (SLF). A supervised classifier can be trained with\nannotated text sequences and used to classify each word in\na large set of unannotated sentences. By averaging predicted\nlabels over all cases in the unlabeled corpus, SLF training\nbuilds class label distribution patterns for each word (or word\nattribute) in the dictionary and re-trains the current model\niteratively adding these distributions as extra word features.\nBasic SLF models how likely a word could be assigned to target\nclass types. Several extensions are proposed, such as learning\nwords\u2019 class boundary distributions. SLF exhibits robust and\nscalable behaviour and is easy to tune. We applied this\napproach on four classical IE tasks: named entity recognition\n(German and English), part-of-speech tagging (English) and\none gene name recognition corpus. Experimental results show\neffective improvements over the supervised baselines on all\ntasks. In addition, when compared with the closely related\nself-training idea, this approach shows favorable advantages."}
{"Title": "Semi-Markov kMeans Clustering And Activity Recognition From Body-Worn\nSensors", "Abstract": "Abstract\u2014Subsequence clustering aims to find patterns that\nappear repeatedly in time series data. We introduce a novel\nsubsequence clustering technique that we call semi-Markov\nkmeans clustering. The clustering results in ideal examples of\nthe repeating patterns and in labeled segmentations that can be\nused as training data for sophisticated discriminative methods\nlike max-margin semi-Markov models. We are applying the\nnew clustering technique to activity recognition from body-\nworn sensors by showing how it can enable a system to learn\nfrom data that is only annotated by an ordered list of activity\ntypes that have been undertaken. This kind of annotation,\nunlike a detailed segmentation of the sensor data, is easily\nprovided by a non-expert user. We show that we can achieve\nequally good results using only an ordered list of activity types\nfor training as when using a full detailed labeled segmentation."}
{"Title": "A Sparsification Approach for Temporal Graphical Model Decomposition", "Abstract": "Abstract\u2014Temporal causal modeling can be used to recover\nthe causal structure among a group of relevant time series\nvariables. Several methods have been developed to explicitly\nconstruct temporal causal graphical models. However, how to\nbest understand and conceptualize these complicated causal\nrelationships is still an open problem. In this paper, we propose\na decomposition approach to simplify the temporal graphical\nmodel. Our method clusters time series variables into groups\nsuch that strong interactions appear among the variables within\neach group and weak (or no) interactions exist for cross-\ngroup variable pairs. Specifically, we formulate the cluster-\ning problem for temporal graphical models as a regression-\ncoefficient sparsification problem and define an interesting\nobjective function which balances the model prediction power\nand its cluster structure. We introduce an iterative optimization\napproach utilizing the Quasi-Newton method and generalized\nridge regression to minimize the objective function and to\nproduce a clustered temporal graphical model. We also present\na novel optimization procedure utilizing a graph theoretical\ntool based on the maximum weight independent set problem\nto speed up the Quasi-Newton method for a large number\nof variables. Finally, our detailed experimental study on both\nsynthetic and real datasets demonstrates the effectiveness of\nour methods."}
{"Title": "Resolving Identity Uncertainty with Learned Random Walks", "Abstract": "A pervasive problem in large relational databases is\nidentity uncertainty which occurs when multiple entries in\na database refer to the same underlying entity in the world.\nRelational databases exhibit rich graphical structure and\nare naturally modeled as graphs whose nodes represent en-\ntities and whose typed-edges represent relations between\nthem. We propose using random walk models for resolv-\ning identity uncertainty since they have proven effective for\nfinding points which are proximately located in a network.\nBecause not all types of relations are equally helpful in al-\nleviating identity uncertainty, we develop a supervised ap-\nproach to learning the usefulness of different database re-\nlations from a training set of database entries whose true\nidentities are known. When tested on the task of resolving\nuncertainty of ambiguously named authors in bibliograph-\nical data, the learned random walk models yield perfor-\nmancesuperiortosupportvectormachines, andtoarelated\nspectral clustering method."}
{"Title": "Discriminative Mixed-membership Models", "Abstract": "Although mixed-membership models have achieved\ngreat success in unsupervised learning, they have not been\nwidely applied to classification problems. In this paper,\nwe propose a family of discriminative mixed-membership\nmodels for classification by combining unsupervised mixed-\nmembership models with multi-class logistic regression. In\nparticular, we propose two variants respectively applica-\nble to text classification based on latent Dirichlet alloca-\ntion and usual feature vector classification based on mixed-\nmembership naive Bayes models. The proposed models al-\nlow the number of components in the mixed membership\nto be different from the number of classes. We propose\ntwo variational inference based algorithms for learning the\nmodels, including a fast variational inference which is sub-\nstantially more efficient than mean-field variational approx-\nimation. Through extensive experiments on UCI and text\nclassification benchmark datasets, we show that the models\nare competitive with the state of the art, and can discover\ncomponents not explicitly captured by the class labels."}
{"Title": "Argumentation based constraint acquisition", "Abstract": "Abstract\u2014Efficient acquisition of constraint networks is a\nkey factor for the applicability of constraint problem solving\nmethods. Current techniques learn constraint networks from\nsets of training examples, where each example is classified as\neither a solution or non-solution of a target network. However,\nin addition to this classification, an expert can usually provide\narguments as to why examples should be rejected or accepted.\nGenerally speaking domain specialists have partial knowledge\nabout the theory to be acquired which can be exploited for\nknowledge acquisition. Based on this observation, we discuss\nthe various types of arguments an expert can formulate and\ndevelop a knowledge acquisition algorithm for processing these\ntypes of arguments which gives the expert the possibility to\ninput arguments in addition to the learning examples. The\nresult of this approach is a significant reduction in the number\nof examples which must be provided to the learner in order\nto learn the target constraint network."}
{"Title": "Extending Semi-supervised Learning Methods for Inductive Transfer Learning", "Abstract": "Abstract\u2014Inductive transfer learning and semi-supervised\nlearning are two different branches of machine learning. The\nformer tries to reuse knowledge in labeled out-of-domain\ninstances while the later attempts to exploit the usefulness of\nunlabeled in-domain instances. In this paper, we bridge the two\nbranches by pointing out that many semi-supervised learning\nmethods can be extended for inductive transfer learning, if the\nstep of labeling an unlabeled instance is replaced by re-weighting\na diff-distribution instance. Based on this recognition, we\ndevelop a new transfer learning method, namely COITL, by\nextending the co-training method in semi-supervised learning.\nExperimental results reveal that COITL can achieve significantly\nhigher generalization and robustness, compared with two state-\nof-the-art methods in inductive transfer learning."}
{"Title": "iTopicModel: Information Network-Integrated Topic Modeling", "Abstract": "Abstract\u2014Document networks, i.e. , networks associated with\ntext information, are becoming increasingly popular due to\nthe ubiquity of Web documents, blogs, and various kinds of\nonline data. In this paper, we propose a novel topic modeling\nframework for document networks, which builds a unified\ngenerative topic model that is able to consider both text and\nstructure information for documents. A graphical model is\nproposed to describe the generative model. On the top layer of\nthis graphical model, we define a novel multivariate Markov\nRandom Field for topic distribution random variables for\neach document, to model the dependency relationships among\ndocuments over the network structure. On the bottom layer, we\nfollow the traditional topic model to model the generation of\ntext for each document. A joint distribution function for both\nthe text and structure of the documents is thus provided. A\nsolution to estimate this topic model is given, by maximizing the\nlog-likelihood of the joint probability. Some important practical\nissues in real applications are also discussed, including how to\ndecide the topic number and how to choose a good network\nstructure. We apply the model on two real datasets, DBLP\nand Cora, and the experiments show that this model is more\neffective in comparison with the state-of-the-art topic modeling\nalgorithms."}
{"Title": "Uncovering Groups via Heterogeneous Interaction Analysis", "Abstract": "Abstract\u2014With the pervasive availability of Web 2.0 and\nsocial networking sites, people can interact with each other\neasily through various social media. For instance, popular sites\nlike Del.icio.us, Flickr, and YouTube allow users to comment\nshared content (bookmark, photos, videos), and users can tag\ntheir own favorite content. Users can also connect to each other,\nand subscribe to or become a fan or a follower of others.\nThese diverse individual activities result in a multi-dimensional\nnetwork among actors, forming cross-dimension group struc-\ntures with group members sharing certain similarities. It is\nchallenging to effectively integrate the network information\nof multiple dimensions in order to discover cross-dimension\ngroup structures. In this work, we propose a two-phase strategy\nto identify the hidden structures shared across dimensions\nin multi-dimensional networks. We extract structural features\nfrom each dimension of the network via modularity analysis,\nand then integrate them all to find out a robust community\nstructure among actors. Experiments on synthetic and real-\nworld data validate the superiority of our strategy, enabling the\nanalysis of collective behavior underneath diverse individual\nactivities in a large scale"}
{"Title": "Significance of Episodes Based on Minimal Windows", "Abstract": "Abstract\u2014Discovering episodes, frequent sets of events from\na sequence has been an active field in pattern mining. Tradi-\ntionally, a level-wise approach is used to discover all frequent\nepisodes. While this technique is computationally feasible it\nmay result in a vast number of patterns, especially when low\nthresholds are used.\nIn this paper we propose a new quality measure for episodes.\nWe say that an episode is significant if the average length\nof its minimal windows deviates greatly when compared to\nthe expected length according to the independence model.\nWe can apply this measure as a post-pruning step to test\nwhether the discovered frequent episodes are truly interesting\nand consequently to reduce the number of output.\nAs a main contribution we introduce a technique that allows\nus to compute the distribution of lengths of minimal windows\nusing the independence model. Such a computation task is\nsurpisingly complex and in order to solve it we compute\nthe distribution iteratively starting from simple episodes and\nprogressively moving towards the more complex ones. In\nour experiments we discover candidate episodes that have a\nsufficient amount of minimal windows and test each candidate\nfor significance. The experimental results demonstrate that our\napproach finds significant episodes while ignoring uninteresting\nones."}
{"Title": "Convex Non-Negative Matrix Factorization in the Wild", "Abstract": "Abstract\u2014Non-negative matrix factorization (NMF) has re-\ncently received a lot of attention in data mining, information\nretrieval, and computer vision. It factorizes a non-negative\ninput matrix V into two non-negative matrix factors V = WH\nsuch that W describes \u201dclusters\u201d of the datasets. Analyzing\ngenotypes, social networks, or images, it can be beneficial to\nensure V to contain meaningful \u201ccluster centroids\u201d, i.e., to\nrestrict W to be convex combinations of data points. But\nhow can we run this convex NMF in the wild, i.e., given\nmillions of data points? Triggered by the simple observation\nthat each data point is a convex combination of vertices of\nthe data convex hull, we propose to restrict W further to be\nvertices of the convex hull. The benefits of this convex-hull NMF\napproach are twofold. First, the expected size of the convex\nhull of, for example, n random Gaussian points in the plane is\n\u2126( \u221a logn), i.e., the candidate set typically grows much slower\nthan the data set. Second, distance preserving low-dimensional\nembeddings allow one to compute candidate vertices efficiently.\nOur extensive experimental evaluation shows that convex-hull\nNMF compares favorably to convex NMF for large data sets\nboth in terms of speed and reconstruction quality. Moreover,\nwe show that our method can easily be applied to large-scale,\nreal-world data sets, in our case consisting of 1.6 million images\nrespectively 150 million votes on World of Warcraft\nR ? guilds."}
{"Title": "On K-Means Cluster Preservation using Quantization Schemes", "Abstract": "This work examines under what conditions compression\nmethodologies can retain the outcome of clustering oper-\nations. We focus on the popular k-Means clustering algo-\nrithm and we demonstrate how a properly constructed com-\npression scheme based on post-clustering quantization is\ncapableofmaintainingtheglobalclusterstructure. Ouran-\nalytical derivations indicate that a 1-bit moment preserving\nquantizer per cluster is sufficient to retain the original data\nclusters. Merits of the proposed compression technique\ninclude: a) reduced storage requirements with clustering\nguarantees, b) data privacy on the original values, and\nc) shape preservation for data visualization purposes. We\nevaluate quantization scheme on various high-dimensional\ndatasets, including 1-dimensional and 2-dimensional time-\nseries (shape datasets) and demonstrate the cluster preser-\nvation property. We also compare with previously proposed\nsimplification techniques in the time-series area and show\nsignificant improvements both on the clustering and shape\npreservation of the compressed datasets."}
{"Title": "Scalable Classification in Large Scale\nSpatiotemporal Domains Applied to\nVoltage-Sensitive Dye Imaging", "Abstract": "Abstract\u2014We present an approach for learning models that\nobtain accurate classification of large scale data objects, collected\nin spatiotemporal domains. The model generation is structured\nin three phases: pixel selection (spatial dimension reduction),\nspatiotemporal features extraction and feature selection. Novel\ntechniques for the first two phases are presented, with two\nalternatives for the middle phase. Model generation based on\nthe combinations of techniques from each phase is explored. The\nintroduced methodology is applied on datasets from the Voltage-\nSensitive Dye Imaging (VSDI) domain, where the generated\nclassification models successfully decode neuronal population re-\nsponses in the visual cortex of behaving animals. VSDI currently\nis the best technique enabling simultaneous high spatial (10,000\npoints) and temporal (10 ms or less) resolution imaging from\nneuronal population in the cortex. We demonstrate that not\nonly our approach is scalable enough to handle computationally\nchallenging data, but it also contributes to the neuroimaging field\nof study with its decoding abilities."}
{"Title": "Extracting Output Metadata from Scientific Deep Web Data Sources", "Abstract": "Abstract\u2014Increasingly, many data sources appear as online\ndatabases, hidden behind query forms, thus forming the deep\nweb. The popularity of this new medium for data dissemination\nis leading to new problems in data integration. Particularly, to\nenable data integration from multiple deep web data sources,\none needs to obtain the metadata for each of the data sources.\nObtaining the metadata, particularly, the output schema, can\nbe very challenging. This is because, given an input query,\nmany deep web data sources only return a subset of the output\nschema attributes, i.e, the ones that have a non-NULL value\nfor the corresponding input.\nIn this paper, we propose two approaches, which are the\nsampling model approach and the mixture model approach,\nrespectively, to efficiently obtain an approximately complete\nset of output schema attributes from a deep web data source.\nOur experiments show while each of the above two approaches\nhas limitations, a hybrid strategy, where we combine the two\napproaches, achieves high recall with good precision for most\ndata sources."}
{"Title": "Semi-Supervised Multi-Task Learning with Task Regularizations", "Abstract": "Abstract\u2014Multi-task learning refers to the learning problem\nof performing inference by jointly considering multiple related\ntasks. There have already been many research efforts on\nsupervised multi-task learning. However, collecting sufficient\nlabeled data for each task is usually time consuming and\nexpensive. In this paper, we consider the semi-supervised multi-\ntask learning (SSMTL) problem, where we are given a small\nportion of labeled points together with a large pool of unlabeled\ndata within each task. We assume that the different tasks\ncan form some task clusters and the task in the same cluster\nshare similar classifier parameters. The final learning problem\nis relaxed to a convex one and an efficient gradient descent\nstrategy is proposed. Finally the experimental results on both\nsynthetic and real world data sets are presented to show the\neffectiveness of our method."}
{"Title": "Fast Online Training of Ramp Loss Support Vector Machines", "Abstract": "Abstract\u2014A fast online algorithm OnlineSVM R for training\nRamp-Loss Support Vector Machines (SVM R s) is proposed. It\nfinds the optimal SVM R for t+1 training examples using SVM R\nbuilt on t previous examples. The algorithm retains the\nKarush\u2013Kuhn\u2013Tucker conditions on all previously observed\nexamples. This is achieved by an SMO-style incremental\nlearning and decremental unlearning under the Concave-\nConvex Procedure framework. Further speedup of training\ntime could be achieved by dropping the requirement of\noptimality. A variant, called OnlineASVM R , is a greedy\napproach that approximately optimizes the SVM R objective\nfunction and is suitable for online active learning. The\nproposed algorithms were comprehensively evaluated on 9\nlarge benchmark data sets. The results demonstrate that\nOnlineSVM R (1) has the similar computational cost as its\noffline counterpart; (2) outperforms IDSVM, its competing\nonline algorithm that uses hinge-loss, in terms of accuracy,\nmodel sparsity and training time. The experiments on online\nactive learning show that for a fixed number of label queries\nOnlineASVM R (1) achieves consistently better accuracy than\nQueryAll and competitive accuracy to Greedy approach; (2)\noutperforms the active learning version of IDSVM."}
{"Title": "Mining Peculiarity Groups in Day-by-Day Behavioral Datasets", "Abstract": "Abstract\u2014Behavior mining is one of the most important\nissues in data mining. The growing interest in the study of\nbehavior mining has been credited to the availability of a\nlarge amount of individual behavioral data. Some objects\ncontaining common behavioral patterns in the dataset are\ndramatically different from other individual objects and show\ntheir peculiarities. It is very important for behavior analysis to\nmine these peculiar objects\u2019 groups as this has great potential\nin practice. However, to the best of our knowledge, it has\nnot been explored before. In this paper, we identify this\ninteresting and practical problem of behavior mining: mining\npeculiarity groups and defining a measurement of the degree\nof peculiarity. As the first attempt to tackle the problem,\nwe present a set-value-oriented day-by-day behavioral data\nexpression mode considering that daily behaviors with respect\nto an object should be recorded as a set of behaviors, and\ndevise a peculiarity group mining algorithm in view of the\nset-value-oriented data expression which cannot be very well\nhandled by existing methods. Furthermore, we show that our\nmethod is practical and efficient using real datasets."}
{"Title": "Online System Problem Detection by Mining Patterns of Console Logs", "Abstract": "Abstract\u2014We describe a novel application of using data min-\ning and statistical learning methods to automatically monitor\nand detect abnormal execution traces from console logs in an\nonline setting. Different from existing solutions, we use a two\nstage detection system. The first stage uses frequent pattern\nmining and distribution estimation techniques to capture the\ndominant patterns (both frequent sequences and time dura-\ntion). The second stage use principal component analysis based\nanomaly detection technique to identify actual problems. Using\nreal system data from a 203-node Hadoop [1] cluster, we show\nthat we can not only achieve highly accurate and fast problem\ndetection, but also help operators better understand execution\npatterns in their system."}
{"Title": "Synthesizing Novel Dimension Reduction Algorithms in Matrix Trace\nOriented Optimization Framework", "Abstract": "Dimension  Reduction  (DR)  algorithms  are\ngenerally categorized into feature extraction and\nfeature selection algorithms. In the past, few works\nhave been done to contrast and unify the two\nalgorithm categories. In this work, we introduce a\nmatrix trace oriented optimization framework to\nprovide a unifying view for both feature extraction and\nselection algorithms. We show that the unified view of\nDR algorithms allows us to discover some essential\nrelationships among many state-of- the-art DR\nalgorithms. Inspired by these essential insights, we\npropose to synthesize unlimited number of novel DR\nalgorithms by combining, mapping and integrating the\nstate- of-the-art algorithms. We present examples of\nnewly synthesized DR algorithms with experimental\nresults to show the effectiveness of our automatically\nsynthesized algorithms."}
{"Title": "Peculiarity Analysis for Classifications", "Abstract": "Abstract\u2014Peculiarity-oriented mining (POM) is a new data\nmining method consisting of peculiar data identification and\npeculiar data analysis. Peculiarity factor (PF) and local pe-\nculiarity factor (LPF) are important concepts employed to\ndescribe the peculiarity of points in the identification step.\nOne can study the notions at both attribute and record levels.\nIn this paper, a new record LPF called distance based record\nLPF (D-record LPF) is proposed, which is defined as the sum of\ndistances between a point and its nearest neighbors. It is proved\nmathematically that D-record LPF can characterize accurately\nthe probability density function of a continuous m-dimensional\ndistribution. This provides a theoretical basis for some existing\ndistance based anomaly detection techniques. More important,\nit also provides an effective method for describing the class-\nconditional probabilities in the Bayesian classifier. The result\nenables us to apply peculiarity analysis for classification prob-\nlems. A novel algorithm called LPF-Bayes classifier and its\nkernelized implementation are presented, which have some\nconnection to the Bayesian classifier. Experimental results on\nseveral benchmark data sets demonstrate that the proposed\nclassifiers are effective."}
{"Title": "Filtering and Refinement: A Two-Stage Approach for Efficient and Effective\nAnomaly Detection", "Abstract": "Abstract\u2014Anomaly detection is an important data mining\ntask. Most existing methods treat anomalies as inconsistencies\nand spend the majority amount of time on modeling normal\ninstances. A recently proposed, sampling-based approach may\nsubstantially boost the efficiency in anomaly detection but may\nalso lead to weaker accuracy and robustness. In this study, we\npropose a two-stage approach to find anomalies in complex\ndatasets with high accuracy as well as low time complexity\nand space cost. Instead of analyzing normal instances, our\nalgorithm first employs an efficient deterministic space par-\ntition algorithm to eliminate obvious normal instances and\ngenerates a small set of anomaly candidates with a single scan\nof the dataset. It then checks each candidate with density-\nbased multiple criteria to determine the final results. This two-\nstage framework also detects anomalies of different notions.\nOur experiments show that this new approach finds anomalies\nsuccessfully in different conditions and ensures a good balance\nof efficiency, accuracy, and robustness."}
{"Title": "Mining Data Streams with Labeled and Unlabeled Training Examples \u2217", "Abstract": "In this paper, we propose a framework to build predic-\ntion models from data streams which contain both labeled\nand unlabeled examples. We argue that due to the increas-\ningdatacollectionabilitybut limitedresources forlabeling,\nstream data collected at hand may only have a small num-\nber of labeledexamples, whereas a large portion of data re-\nmainunlabeledbut can be beneficialfor learning. Unleash-\ning the full potential of the unlabeled instances for stream\ndata mining is, however, a significant challenge, consider\nthat even fully labeled data streams may suffer from the\nconcept drifting, and inappropriate uses of the unlabeled\nsamples may only make the problem even worse. To build\nprediction models, we first categorize the stream data into\nfour different categories, each of which corresponds to the\nsituation where concept drifting may or may not exist in the\nlabeled and unlabeled data. After that, we propose a rela-\ntional k-means based transfer semi-supervised SVM learn-\ning framework (RK-TS 3 VM), which intends to leverage la-\nbeled and unlabeled samples to build prediction models.\nExperimental results and comparisons on both synthetic\nand real-world data streams demonstrate that the proposed\nframework is able to help build prediction models more ac-\ncurate than other simple approaches can offer."}
{"Title": "Maximum Margin Clustering with Multivariate Loss Function", "Abstract": "This paper presents a simple but powerful extension\nof the maximum margin clustering (MMC) algorithm that\noptimizes multivariate performance measure specifically\ndefined for clustering, including Normalized Mutual In-\nformation, Rand Index and F-measure. Different from\nprevious MMC algorithms that always employ the error\nrate as the loss function, our formulation involves a\nmultivariate loss function that is a non-linear combination\nof the individual clustering results. Computationally, we\npropose a cutting plane algorithm to approximately solve\nthe resulting optimization problem with a guaranteed accu-\nracy. Experimental evaluations show clear improvements\nin clustering performance of our method over previous\nmaximum margin clustering algorithms."}
{"Title": "Efficient Discovery of Confounders in Large Data Sets", "Abstract": "Abstract\u2014Given a large transaction database, association\nanalysis is concerned with efficiently finding strongly related\nobjects. Unlike traditional associate analysis, where relation-\nships among variables are searched at a global level, we\nexamine confounding factors at a local level. Indeed, many\nreal-world phenomena are localized to specific regions and\ntimes. These relationships may not be visible when the entire\ndata set is analyzed. Specially, confounding effects that change\nthe direction of correlation is the most significant. Along\nthis line, we propose to efficiently find confounding effects\nattributable to local associations. Specifically, we derive an\nupper bound by a necessary condition of confounders, which\ncan help us prune the search space and efficiently identify\nconfounders. Experimental results show that the proposed\nCONFOUND algorithm can effectively identify confounders\nand the computational performance is an order of magnitude\nfaster than benchmark methods."}
{"Title": "Vague One-Class Learning for Data Streams ", "Abstract": "In this paper, we formulate a new research problem of\nlearning from vaguely labeled one-class data streams,\nwhere the main objective is to allow users to label instance\ngroups, instead of single instances, as positive samples for\nlearning. The batch-labeling, however, raises serious issues\nbecause labeled groups may contain non-positive samples,\nand users may change their labeling interests at any time.\nTo solve this problem, we propose a Vague One-Class\nLearning (VOCL) framework which employs a double\nweighting approach, at both instance and classifier levels,\nto build an ensembling framework for learning. At instance\nlevel, both local and global filterings are considered for\ninstance weight adjustment. Two solutions are proposed to\ntake instance weight values into the classifier training\nprocess. At classifier level, a weight value is assigned to\neach classifier of the ensemble to ensure that learning can\nquickly adapt to users\u2019 interests. Experimental results on\nsynthetic and real-world data streams demonstrate that the\nproposed VOCL framework significantly outperforms\nother methods for vaguely labeled one-class data streams."}
{"Title": "Inverse Time Dependency in Convex Regularized Learning", "Abstract": "Abstract\u2014In the conventional regularized learning, training\ntime increases as the training set expands. Recent work on L 2\nlinear SVM challenges this common sense by proposing the\ninverse time dependency on the training set size. In this paper,\nwe first put forward a Primal Gradient Solver (PGS) to\neffectively solve the convex regularized learning problem. This\nsolver is based on the stochastic gradient descent method and\nthe Fenchel conjugate adjustment, employing the well-known\nonline strongly convex optimization algorithm with logarithmic\nregret. We then theoretically prove the inverse dependency\nproperty of our PGS, embracing the previous work of the L 2\nlinear SVM as a special case and enable the \ud835\udcf5 \ud835\udc91 -norm\noptimization to run within a bounded sphere, which qualifies\nmore convex loss functions in PGS. We further illustrate this\nsolver in three examples: SVM, logistic regression and\nregularized least square. Experimental results substantiate the\nproperty of the inverse dependency on training data size."}
{"Title": "P-packSVM: Parallel Primal grAdient desCent Kernel SVM", "Abstract": "Abstract\u2014It is an extreme challenge to produce a nonlinear\nSVM classifier on very large scale data. In this paper we\ndescribe a novel P-packSVM algorithm that can solve the\nSupport Vector Machine (SVM) optimization problem with an\narbitrary kernel. This algorithm embraces the best known\nstochastic gradient descent method to optimize the primal\nobjective, and has \ud835\udfcf/\ud835\udf50 dependency in complexity to obtain a\nsolution of optimization error \ud835\udf50. The algorithm can be highly\nparallelized with a special packing strategy, and experiences\nsub-linear speed-up with hundreds of processors. We\ndemonstrate that P-packSVM achieves accuracy sufficiently\nclose to that of SVM-light, and overwhelms the state-of-the-art\nparallel SVM trainer PSVM in both accuracy and efficiency.\nAs an illustration, our algorithm trains CCAT dataset with\n800k samples in 13 minutes and 95% accuracy, while PSVM\nneeds 5 hours but only has 92% accuracy. We at last\ndemonstrate the capability of P-packSVM on 8 million training\nsamples."}
{"Title": "An L \u221e Norm Visual Classifier", "Abstract": "Abstract\u2014We introduce a mathematical framework, based on\nthe L \u221e norm distance metric, to describe human interactions\nin a visual data mining environment. We use the framework to\nbuild a classifier that involves an algebra on hyper-rectangles.\nOur classifier, called VisClassifier, generates set-wise rules from\nsimple gestures in an exploratory visual GUI. Logging these\nrules allows us to apply our analysis to a new sample or batch\nof data so that we can assess the predictive power of our visual-\nprocessing motivated classifier. The accuracy of this classifier\non widely-used benchmark datasets rivals the accuracy of\ncompetitive classifiers."}
{"Title": "Outlier Detection using Inductive Logic Programming", "Abstract": "Abstract\u2014We present a novel definition of outlier in the\ncontext of inductive logic programming. Given a set of positive\nand negative examples, the definition aims at singling out\nthe examples showing anomalous behavior. We note that the\ntask here pursued is different from noise removal, and, in\nfact, the anomalous observations we discover are different in\nnature from noisy ones. We discuss pecularities of the novel\napproach, present an algorithm for detecting outliers, discuss\nsome examples of knowledge mined, and compare it with\nalternative approaches."}
{"Title": "Joint Emotion-Topic Modeling for Social Affective Text Mining", "Abstract": "This paper is concerned with the problem of social af-\nfective text mining, which aims to discover the connections\nbetween social emotions and affective terms based on user-\ngeneratedemotion labels. We propose a joint emotion-topic\nmodelbyaugmentinglatentDirichlet allocationwith anad-\nditional layer for emotion modeling. It first generates a set\nof latent topics from emotions, followed by generating af-\nfective terms from each topic. Experimental results on an\nonline news collection show that the proposed model can\neffectively identify meaningful latent topics for each emo-\ntion. Evaluation on emotion prediction further verifies the\neffectiveness of the proposed model."}
{"Title": "Algorithms for Large, Sparse Network Alignment Problems", "Abstract": "Abstract\u2014We propose a new distributed algorithm for sparse\nvariants of the network alignment problem, which occurs\nin a variety of data mining areas including systems biology,\ndatabase matching, and computer vision. Our algorithm uses\na belief propagation heuristic and provides near optimal\nsolutions for this NP-hard combinatorial optimization problem.\nWe show that our algorithm is faster and outperforms or\nties existing algorithms on synthetic problems, a problem in\nbioinformatics, and a problem in ontology matching. We also\nprovide a unified framework for studying and comparing all\nnetwork alignment solvers."}
{"Title": "Dirichlet Mixture Allocation for Multiclass Document Collections Modeling", "Abstract": "Abstract\u2014Topic model, Latent Dirichlet Allocation (LDA),\nis an effective tool for statistical analysis of large collections of\ndocuments. In LDA, each document is modeled as a mixture\nof topics and the topic proportions are generated from the\nunimodal Dirichlet distribution prior. When a collection of\ndocuments are drawn from multiple classes, this unimodal\nprior is insufficient for data fitting. To solve this problem, we\nexploit the multimodal Dirichlet mixture prior, and propose the\nDirichlet mixture allocation (DMA). We report experiments on\nthe popular TDT2 Corpus demonstrating that DMA models\na collection of documents more precisely than LDA when the\ndocuments are obtained from multiple classes."}
{"Title": "SLIDER: Mining correlated motifs in protein-protein interaction networks", "Abstract": "Abstract\u2014Correlated motif mining (CMM) is the problem\nto find overrepresented pairs of patterns, called motif pairs,\nin interacting protein sequences. Algorithmic solutions for\nCMM thereby provide a computational method for predicting\nbinding sites for protein interaction. In this paper, we adopt\na motif-driven approach where the support of candidate motif\npairs is evaluated in the network. We experimentally establish\nthe superiority of the Chi-square-based support measure over\nother support measures. Furthermore, we obtain that CMM\nis an NP-hard problem for a large class of support measures\n(including Chi-square) and reformulate the search for corre-\nlated motifs as a combinatorial optimization problem. We then\npresent the method SLIDER which uses local search with a\nneighborhood function based on sliding motifs and employs\nthe Chi-square-based support measure. We show that SLIDER\noutperforms existing motif-driven CMM methods and scales to\nlarge protein-protein interaction networks."}
{"Title": "Effective Anomaly Detection in Sensor Networks Data Streams", "Abstract": "Abstract\u2014This paper addresses a major challenge in data\nmining applications where the full information about the\nunderlying processes, such as sensor networks or large on-\nline database, cannot be practically obtained due to physical\nlimitations such as low bandwidth or memory, storage, or\ncomputing power. Motivated by the recent theory on direct\ninformation sampling called compressed sensing (CS), we\npropose a framework for detecting anomalies from these large-\nscale data mining applications where the full information is\nnot practically possible to obtain. Exploiting the fact that\nthe intrinsic dimension of the data in these applications are\ntypically small relative to the raw dimension and the fact that\ncompressed sensing is capable of capturing most information\nwith few measurements, our work show that spectral methods\nthat used for volume anomaly detection can be directly applied\nto the CS data with guarantee on performance. Our theoretical\ncontributions are supported by extensive experimental results\non large datasets which show satisfactory performance."}
{"Title": "Hierarchical Bayesian Models for Collaborative Tagging Systems", "Abstract": "Abstract\u2014Collaborative tagging systems with user generated\ncontent have become a fundamental element of websites such as\nDelicious, Flickr or CiteULike. By sharing common knowledge,\nmassively linked semantic data sets are generated that provide\nnew challenges for data mining. In this paper, we reduce the\ndata complexity in these systems by finding meaningful topics\nthat serve to group similar users and serve to recommend tags\nor resources to users. We propose a well-founded probabilistic\napproach that can model every aspect of a collaborative\ntagging system. By integrating both user information and tag\ninformation into the well-known Latent Dirichlet Allocation\nframework, the developed models can be used to solve a\nnumber of important information extraction and retrieval\ntasks."}
{"Title": "Efficient Algorithm for Computing Link-based Similarity in\nReal World Networks", "Abstract": "Abstract\u2014Similarity calculation has many applications, such\nas information retrieval, and collaborative filtering, among\nmany others. It has been shown that link-based similarity\nmeasure, such as SimRank, is very effective in characterizing\nthe object similarities in networks, such as the Web, by\nexploiting the object-to-object relationship. Unfortunately, it is\nprohibitively expensive to compute the link-based similarity in\na relatively large graph. In this paper, based on the\nobservation that link-based similarity scores of real world\ngraphs follow the power-law distribution, we propose a new\napproximate  algorithm,  namely  Power-SimRank,  with\nguaranteed error bound to efficiently compute link-based\nsimilarity measure. We also prove the convergence of the\nproposed algorithm. Extensive experiments conducted on real\nworld datasets and synthetic datasets show that the proposed\nalgorithm outperforms SimRank by four-five times in terms of\nefficiency while the error generated by the approximation is\nsmall."}
{"Title": "Fine-Grain Perturbation for Privacy Preserving Data\nPublishing", "Abstract": "Abstract\u2014 Recent work [12] shows that conventional privacy\npreserving publishing techniques based on anonymity-groups\nare susceptible to corruption attacks. In a corruption attack, if\nthe sensitive information of any anonymity-group member is\nuncovered, then the remaining group members are at risk. In\nthis study, we abandon anonymity-groups and hide sensitive\ninformation through perturbation on the sensitive attribute.\nWith each record being perturbed independently, corruption\nattacks cannot be effectively carried out. Previous anti-\ncorruption work did not minimize information loss. This paper\nproposes to address this issue by allowing fine-grain privacy\nspecification. We demonstrate the power of our approach\nthrough experiments on real medical and synthetic datasets."}
{"Title": "Accelerated Gradient Method for Multi-Task Sparse Learning Problem", "Abstract": "Abstract\u2014Many real world learning problems can be recast\nas multi-task learning problems which utilize correlations\namong different tasks to obtain better generalization per-\nformance than learning each task individually. The feature\nselection problem in multi-task setting has many applications in\nfields of computer vision, text classification and bio-informatics.\nGenerally, it can be realized by solving a L-1-infinity regu-\nlarized optimization problem. And the solution automatically\nyields the joint sparsity among different tasks. However, due\nto the nonsmooth nature of the L-1-infinity norm, there lacks\nan efficient training algorithm for solving such problem with\ngeneral convex loss functions. In this paper, we propose an\naccelerated gradient method based on an \u201coptimal\u201d first order\nblack-box method named after Nesterov and provide the con-\nvergence rate for smooth convex loss functions. For nonsmooth\nconvex loss functions, such as hinge loss, our method still\nhas fast convergence rate empirically. Moreover, by exploiting\nthe structure of the L-1-infinity ball, we solve the black-box\noracle in Nesterov\u2019s method by a simple sorting scheme. Our\nmethod is suitable for large-scale multi-task learning problem\nsince it only utilizes the first order information and is very\neasy to implement. Experimental results show that our method\nsignificantly outperforms the most state-of-the-art methods in\nboth convergence speed and learning accuracy."}
{"Title": "CoFKM: a centralized method for multiple-view clustering", "Abstract": "Abstract\u2014This paper deals with clustering for multi-view\ndata, i.e. objects described by several sets of variables or\nproximity matrices. Many important domains or applications\nsuch as Information Retrieval, biology, chemistry and mar-\nketing are concerned by this problematic. The aim of this\ndata mining research field is to search for clustering patterns\nthat perform a consensus between the patterns from different\nviews. This requires to merge information from each view\nby performing a fusion process that identifies the agreement\nbetween the views and solves the conflicts. Various fusion\nstrategies can be applied, occurring either before, after or\nduring the clustering process. We draw our inspiration from\nthe existing algorithms based on a centralized strategy. We\npropose a fuzzy clustering approach that generalizes the three\nfusion strategies and outperforms the main existing multi-view\nclustering algorithm both on synthetic and real datasets."}
{"Title": "Active Selection of Sensor Sites in Remote Sensing Applications", "Abstract": "Abstract\u2014 In a data-mining approach, a model for\nestimation of Aerosol Optical Depth (AOD) from satellite\nobservations is learned using collocated satellite and ground-\nbased observations. For accurate learning of such a spatio-\ntemporal model, it is important to collect ground-based data\nfrom a large number of sites. The objective of this project is\nto determine appropriate locations for the next set of\nground-based data collection sites to maximize accuracy of\nAOD estimation. Ideally, a new site should capture the most\nsignificant unseen aerosol patterns and should be the least\ncorrelated with the previously observed patterns. We\npropose achieving this aim by selecting the locations on\nwhich the existing prediction model is the most uncertain.\nSeveral criteria were considered for site selection, including\nuncertainty, spatial diversity, similarity in temporal pattern,\nand their combination. Extensive experiments on globally\ndistributed data over 90 AERONET sites from the years\n2005 and 2006 provide strong evidence that sites selected\nusing the proposed algorithms improve the overall AOD\nprediction accuracy at a faster rate than those selected\nrandomly or based on spatial diversity among sites."}
{"Title": "Large Scale Relation Acquisition Using Class Dependent Patterns", "Abstract": "Abstract\u2014This paper proposes a minimally supervised\nmethod for acquiring high-level semantic relations such as\ncausality and prevention from the Web. Our method learns\nlinguistic patterns that express causality such as \u201cx gave\nrise to y\u201d, and uses them to extract causal noun pairs like\n(global warming, malaria epidemic) from sentences like \u201cglobal\nwarming gave rise to a new malaria epidemic\u201d. The novelty of\nour method lies in the use of semantic word classes acquired by\nlarge scale clustering for learning class dependent patterns. We\ndemonstrate the effectiveness of this class based approach on\nthree large-scale relation mining tasks from 50 million Japanese\nWeb pages. In two of these tasks we obtained more than 30,000\nrelation instances with over 80% precision, outperforming a\nstate-of-the-art system by a large margin."}
{"Title": "Large Scale Relation Acquisition Using Class Dependent Patterns", "Abstract": "Abstract\u2014This paper proposes a minimally supervised\nmethod for acquiring high-level semantic relations such as\ncausality and prevention from the Web. Our method learns\nlinguistic patterns that express causality such as \u201cx gave\nrise to y\u201d, and uses them to extract causal noun pairs like\n(global warming, malaria epidemic) from sentences like \u201cglobal\nwarming gave rise to a new malaria epidemic\u201d. The novelty of\nour method lies in the use of semantic word classes acquired by\nlarge scale clustering for learning class dependent patterns. We\ndemonstrate the effectiveness of this class based approach on\nthree large-scale relation mining tasks from 50 million Japanese\nWeb pages. In two of these tasks we obtained more than 30,000\nrelation instances with over 80% precision, outperforming a\nstate-of-the-art system by a large margin."}
{"Title": "Bayesian Overlapping Subspace Clustering", "Abstract": "Given a data matrix, the problem of finding\ndense/uniform sub-blocks in the matrix is becoming\nimportant in several applications. The problem is inher-\nently combinatorial since the uniform sub-blocks may\ninvolve arbitrary subsets of rows and columns and may\neven be overlapping. While there are a few existing\nmethods based on co-clustering or subspace clustering,\nthey typically rely on local search heuristics and in general\ndo not have a systematic model for such data. We present a\nBayesian Overlapping Subspace Clustering (BOSC) model\nwhich is a hierarchical generative model for matrices with\npotentially overlapping uniform sub-block structures. The\nBOSC model can also handle matrices with missing entries.\nWe propose an EM-style algorithm based on approximate\ninference using Gibbs sampling and parameter estimation\nusing coordinate descent for the BOSC model. Through\nexperiments on both simulated and real datasets, we\ndemonstrate that the proposed algorithm outperforms the\nstate-of-the-art."}
{"Title": "Unsupervised Relation Extraction by Massive Clustering", "Abstract": "Abstract\u2014The goal of Information Extraction is to auto-\nmatically generate structured pieces of information from the\nrelevant information contained in text documents.\nMachine Learning techniques have been applied to reduce\nthe cost of Information Extraction system adaptation. However,\nelements of human supervision strongly bias the learning\nprocess. Unsupervised learning approaches can avoid these\nbiases.\nIn this paper, we propose an unsupervised approach to\nlearning for Relation Detection, based on the use of massive\nclustering ensembles.\nThe results obtained on the ACE Relation Mention Detection\ntask outperform in terms of F1 score by 5 points the state of the\nart of unsupervised techniques for this evaluation framework,\nin addition to being simpler and more flexible."}
{"Title": "Regression Learning Vector Quantization", "Abstract": "Abstract\u2014 Learning Vector Quantization (LVQ) is a popular\nclass  of  nearest  prototype  classifiers  for  multiclass\nclassification. Learning algorithms from this family are widely\nused because of their intuitively clear learning process and ease\nof implementation. In this paper we propose an extension of\nthe LVQ algorithm to regression. Just like the LVQ algorithm,\nthe proposed modification uses a supervised learning\nprocedure to learn the best prototype positions, but unlike\nLVQ algorithm for classification, it also learns the best\nprototype target values. This results in the effective partition of\nthe feature space, similar to the one the K-means algorithm\nwould make. Experimental results on benchmark datasets\nshowed that the proposed Regression LVQ algorithm performs\nbetter than the nearest prototype competitors that choose\nprototypes  randomly  or  through  K-means  clustering,\nclassification LVQ on quantized target values, and similarly to\nthe memory-based Parzen Window and Nearest Neighbor\nalgorithms."}
{"Title": "Projective Clustering Ensembles", "Abstract": "Recent advances in data clustering concern clustering\nensembles and projective clustering methods, each address-\ning different issues in clustering problems. In this paper, we\nconsider for the first time the projective clustering ensemble\n(PCE) problem, whose main goal is to derive a proper pro-\njective consensus partition from an ensemble of projective\nclustering solutions. We formalize PCE as an optimization\nproblem which does not rely on any particular clustering\nensemble algorithm, and which has the ability to handle\nhard as well as soft data clustering, and different feature\nweightings. We provide two formulations for PCE, namely\na two-objective and a single-objective problem, in which\nthe object-based and feature-based representations of the\nensemble solutions are taken into account differently. Ex-\nperiments have demonstrated that the proposed methods for\nPCE show clear improvements in terms of accuracy of the\noutput consensus partition."}
{"Title": "Knowledge Discovery from Citation Networks", "Abstract": "Abstract\u2014Knowledge discovery from scientific articles has\nreceived increasing attentions recently since huge repositories\nare made available by the development of the Internet and\ndigital databases. In a corpus of scientific articles such as a\ndigital library, documents are connected by citations and one\ndocument plays two different roles in the corpus: document\nitself and a citation of other documents. In the existing topic\nmodels, little effort is made to differentiate these two roles.\nWe believe that the topic distributions of these two roles are\ndifferent and related in a certain way. In this paper we propose\na Bernoulli Process Topic (BPT) model which models the corpus\nat two levels: document level and citation level. In the BPT\nmodel, each document has two different representations in the\nlatent topic space associated with its roles. Moreover, the multi-\nlevel hierarchical structure of the citation network is captured\nby a generative process involving a Bernoulli process. The\ndistribution parameters of the BPT model are estimated by a\nvariational approximation approach. In addition to conducting\nthe experimental evaluations on the document modeling task,\nwe also apply the BPT model to a well known scientific corpus\nto discover the latent topics. The comparisons against state-of-\nthe-art methods demonstrate a very promising performance."}
{"Title": "An effective approach to inverse frequent set mining", "Abstract": "Abstract\u2014The inverse frequent set mining problem is the\nproblem of computing a database on which a given collection\nof itemsets must emerge to be frequent. Earlier studies focused\non investigating computational and approximability properties\nof this problem. In this paper, we face it under the pragmatic\nperspective of defining heuristic solution approaches that are\neffective and scalable in real scenarios. In particular, a general\nformulation of the problem is considered where minimum\nand maximum support constraints can be defined on each\nitemset, and where no bound is given beforehand on the\nsize of the resulting output database. Within this setting,\nan algorithm is proposed that always satisfies the maximum\nsupport constraints, but which treats minimum support con-\nstraints as soft ones that are enforced as long as possible. A\nthorough experimentation evidences that minimum support\nconstraints are hardly violated in practice, and that such\nnegligible degradation in accuracy (which is unavoidable due\nto the theoretical intractability of the problem) is compensated\nby very good scaling performances."}
{"Title": "Parallel PathFinder Algorithms for\nMining Structures from Graphs", "Abstract": "Abstract\u2014 PathFinder networks are increasingly used in Data\nMining for different purposes, like network visualization or\nknowledge extraction. This novel way of representing graphical\ndata has been proven to give better results than other link\nreduction algorithms, like minimum spanning networks.\nHowever, this increase in quality comes with a high computation\ncost, typically of the order of n^3 or higher, where n is the\nnumber of nodes in the graph. While this problem has\npreviously been tackled by using mathematical properties to\nspeed up the algorithm, in this paper, we propose two new\nalgorithms to speed up PathFinder computation based on\nparallelization techniques to take advantage of the increasingly\navailable multi-core hardware platform. Experiments show that\nboth new algorithms are more efficient than the state of the art\nalgorithms; one of them can achieve speed-ups of up to x127\nwith an average of x23 on recent hardware (2007)."}
{"Title": "A bootstrap approach to eigenvalue correction", "Abstract": "Abstract\u2014Eigenvalue analysis is an important aspect in many\ndata modeling methods. Unfortunately, the eigenvalues of the\nsample covariance matrix (sample eigenvalues) are biased esti-\nmates of the eigenvalues of the covariance matrix of the data\ngenerating process (population eigenvalues). We present a new\nmethod based on bootstrapping to reduce the bias in the sample\neigenvalues: the eigenvalue estimates are updated in several\niterations, where in each iteration synthetic data is generated\nto determine how to update the population eigenvalue estimates.\nComparison of the bootstrap eigenvalue correction with a state\nof the art correction method by Karoui shows that depending\non the type of population eigenvalue distribution, sometimes the\nKaroui method performs better and sometimes our bootstrap\nmethod."}
{"Title": "Modeling Syntactic Structures of Topics with a Nested HMM-LDA", "Abstract": "Abstract\u2014Latent Dirichlet Allocation (LDA) is a commonly\nused topic modeling method for text analysis and mining.\nStandard LDA treats documents as bags of words, ignoring\nthe syntactic structures of sentences. In this paper, we propose\na hybrid model that embeds hidden Markov models (HMMs)\nwithin LDA topics to jointly model both the topics and the\nsyntactic structures within each topic. Our model is general and\nsubsumes standard LDA and HMM as special cases. Compared\nwith standard LDA and HMM, our model can simultaneously\ndiscover both topic-specific content words and background\nfunctional words shared among topics. Our model can also\nautomatically separate content words that play different roles\nwithin a topic. Using perplexity as evaluation metric, our model\nreturns lower perplexity for unseen test documents compared\nwith standard LDA, which shows its better generalization\npower than LDA."}
{"Title": "Redistricting using Heuristic-Based Polygonal Clustering", "Abstract": "Abstract\u2014 Redistricting is the process of dividing a geographic\narea into districts or zones. This process has been considered in\nthe past as a problem that is computationally too complex for\nan automated system to be developed that can produce un-\nbiased plans. In this paper we present a novel method for redi-\nstricting a geographic area using a heuristic-based approach\nfor polygonal spatial clustering. While clustering geospatial\npolygons several complex issues need to be addressed \u2013 such\nas: removing order dependency, clustering all polygons assum-\ning no outliers, and strategically utilizing domain knowledge to\nguide the clustering process. In order to address these special\nneeds, we have developed the Constrained Polygonal Spatial\nClustering (CPSC) algorithm that holistically integrates do-\nmain knowledge in the form of cluster-level and instance-level\nconstraints and uses heuristic functions to grow clusters. In\norder to illustrate the usefulness of our algorithm we have ap-\nplied it to the problem of formation of unbiased congressional\ndistricts. Furthermore, we compare and contrast our algo-\nrithm with two other approaches proposed in the literature for\nredistricting, namely \u2013 graph partitioning and simulated an-\nnealing."}
{"Title": "A Walk from 2-Norm SVM to 1-Norm SVM", "Abstract": "Abstract\u2014This paper studies how useful the standard 2-\nnorm regularized SVM is in approximating the 1-norm SVM\nproblem. To this end, we examine a general method that is\nbased on iteratively re-weighting the features and solving a\n2-norm optimization problem. The convergence rate of this\nmethod is unknown. Previous work indicates that it might\nrequire an excessive number of iterations. We study how well\nwe can do with just a small number of iterations. In theory the\nconvergence rate is fast, except for coordinates of the current\nsolution that are close to zero. Our empirical experiments\nconfirm this. In many problems with irrelevant features,\nalready one iteration is often enough to produce accuracy as\ngood as or better than that of the 1-norm SVM. Hence, it seems\nthat in these problems we do not need to converge to the 1-norm\nSVM solution near zero values. The benefit of this approach is\nthat we can build something similar to the 1-norm regularized\nsolver based on any 2-norm regularized solver. This is quick to\nimplement and the solution inherits the good qualities of the\nsolver such as scalability and stability."}
{"Title": "Semi-Supervised Density-Based Clustering", "Abstract": "Most of the effort in the semi-supervised clustering liter-\nature was devoted to variations of the K-means algorithm.\nIn this paper we show how background knowledge can be\nused to bias a partitional density-based clustering algo-\nrithm. Our work describes how labeled objects can be used\nto help the algorithm detecting suitable density parameters\nfor the algorithm to extract density-based clusters in spe-\ncific parts of the feature space. Considering the set of con-\nstraints established by the labeled dataset we show that our\nalgorithm, called SSDBSCAN, automatically finds density\nparameters for each natural cluster in a dataset. Four of\nthe most interesting characteristics of SSDBSCAN are that\n(1) it only requires a single, robust input parameter, (2) it\ndoes not need any user intervention, (3) it automatically\nfinds the noise objects according to the density of the nat-\nural clusters and (4) it is able to find the natural cluster\nstructure even when the density among clusters vary widely.\nThe algorithm presented in this paper is evaluated with arti-\nficial and real-world datasets, demonstrating better results\nwhen compared to other unsupervised and semi-supervised\ndensity-based approaches."}
{"Title": "VIF Regression: A Fast Regression Algorithm For Large Data", "Abstract": "Abstract\u2014We propose a fast regression algorithm that can\nsubstantially reduce the computational complexity of searching,\nyet retain good accuracy. It also guarantees to discover corre-\nlated features that are collectively predictive, and avoid model\nover-fitting. Its capability of controlling mFDR (marginal False\nDiscovery Rate) statistically enables the one-pass search of the\nfast algorithm and guarantees the accuracy of the sparse model\nchosen by the algorithm without cross validation. Numerical\nresults show that our algorithm is much faster than any other\nalgorithm and is competitively as accurate as the best but\nslower algorithms."}
{"Title": "Sparse Norm-Regularized Reconstructive Coefficients Learning", "Abstract": "Abstract\u2014Inspired by the fact that the final decision rule\nis mainly affected by a small subset of the training samples,\ni.e., Support Vector Machine(SVM) shows that the decision\nfunction relies on the few samples that are on or over the\nmargin. We propose a new framework that explicitly strengthen\nthis intuitive fact by adding an l 1 -norm regularizer. We give\ndifferent formulations for our framework in different scenarios,\nand the experiments show that our framework can not only\nlead to high sparse solutions but also better performance than\ntraditional methods."}
{"Title": "A Contrast Pattern based Clustering Quality Index\nfor Categorical Data", "Abstract": "Since clustering is unsupervised and highly explorative, clustering\nvalidation (i.e. assessing the quality of clustering solutions) has\nbeen an important and long standing research problem. Existing\nvalidity measures have significant shortcomings. This paper\nproposes a novel Contrast Pattern based Clustering Quality index\n(CPCQ) for categorical data, by utilizing the quality and diversity\nof the contrast patterns (CPs) which contrast the clusters in\nclusterings. High quality CPs can characterize clusters and\ndiscriminate them against each other. Experiments show that the\nCPCQ index (1) can recognize that expert-determined classes are\nthe best clusters for many datasets from the UCI repository; (2)\ndoes not give inappropriate preference to larger number of clusters;\n(3) does not require a user to provide a distance function."}
{"Title": "On the (In)Security and (Im)Practicality of\nOutsourcing Precise Association Rule Mining", "Abstract": "Abstract\u2014The recent interest in outsourcing IT services onto\nthe cloud raises two main concerns: security and cost. One\ntask that could be outsourced is data mining. In VLDB 2007,\nWong et al. propose an approach for outsourcing association\nrule mining [1]. Their approach maps a set of real items\ninto a set of pseudo items, then maps each transaction non-\ndeterministically.\nThis paper, analyzes both the security and costs associated\nwith outsourcing association rule mining. We show how to\nbreak the encoding scheme from [1] without using context\nspecific information and reduce the security to a one-to-\none mapping. We present a stricter notion of security than\nused in [1], and then consider the practicality of outsourcing\nassociation rule mining. Our results indicate that outsourcing\nassociation rule mining may not be practical, if the data owner\nis concerned with data confidentiality."}
{"Title": "Multi-Document Summarization by Information Distance", "Abstract": "Abstract\u2014Fast changing knowledge on the Internet can be\nacquired more efficiently with the help of automatic document\nsummarization and updating techniques. This paper described\na novel approach for multi-document update summarization.\nThe best summary is defined to be the one which has the\nminimum information distance to the entire document set.\nThe best update summary has the minimum conditional\ninformation distance to a document cluster given that a prior\ndocument cluster has already been read. Experiments on the\nDUC 2007 dataset 1 and the TAC 2008 dataset 2 have proved\nthat our method closely correlates with the human summaries\nand outperforms other programs such as LexRank in many\ncategories under the ROUGE evaluation criterion."}
{"Title": "Promoting Total Efficiency in Text Clustering via\nIterative and Interactive Metric Learning", "Abstract": "Abstract\u2014In this paper, we propose a framework to make\nthe text clustering process, as a whole, efficient. In a real text\nclustering task, an analyst usually has some expectation on the\nresults in mind. However, a single run of a clustering algorithm\non the preprocessed data would not satisfy the expectation.\nThen the analyst faces labor-intensive trials for improving the\nresults that involve repetitive feature refinement and parameter\ntuning.\nWe develop the Iterative and Interactive Metric Learning\nSystem (IIMLS) for addressing the challenge. Specifically,\nIIMLS allows analysts to input feedback on a current clustering\nresult. Given the feedback, IIMLS optimizes metric in the\nfeature space so that the clustering algorithm applied with\nthe refined metric would reflect the feedback. As a byproduct,\nlearned metric may be used for a similar dataset. Illustrative\nexamples on a real-world dataset show IIMLS can dramatically\nimprove efficiency of a text clustering task. The learned\n\u201cknowledge\u201d, or the metric, is visualized for gaining insights\nof the optimized feature metric."}
{"Title": "A New Clustering Algorithm Based on Regions of Influence\nwith Self-Detection of the Best Number of Clusters", "Abstract": "Abstract\u2014Clustering methods usually require to know the\nbest number of clusters, or another parameter, e.g. a threshold,\nwhich is not ever easy to provide. This paper proposes a new\ngraph-based clustering method called \u201cGBC\u201d which detects\nautomatically the best number of clusters, without requiring\nany other parameter. In this method based on regions of\ninfluence, a graph is constructed and the edges of the graph\nhaving the higher values are cut according to a hierarchical\ndivisive procedure. An index is calculated from the size average\nof the cut edges which self-detects the more appropriate\nnumber of clusters. The results of GBC for 3 quality indices\n(Dunn, Silhouette and Davies-Bouldin) are compared with\nthose of K-Means, Ward\u2019s hierarchical clustering method and\nDBSCAN on 8 benchmarks. The experiments show the good\nperformance of GBC in the case of well separated clusters,\neven if the data are unbalanced, non-convex or with presence\nof outliers, whatever the shape of the clusters."}
{"Title": "Automatically Extracting Dialog Models from Conversation Transcripts", "Abstract": "Abstract\u2014There is a growing need for task-oriented natural\nlanguage dialog systems that can interact with a user to accom-\nplish a given objective. Recent work on building task-oriented\ndialog systems have emphasized the need for acquiring task-\nspecific knowledge from un-annotated conversational data. In\nour work we acquire task-specific knowledge by defining sub-\ntask as the key unit of a task-oriented conversation. We propose\nan unsupervised, apriori like algorithm that extracts the sub-\ntasks and their valid orderings from un-annotated human-\nhuman conversations. Modeling dialogues as a combination\nof sub-tasks and their valid orderings easily captures the\nvariability in conversations. It also provides us the ability to\nmap our dialogue model to AIML constructs and therefore use\noff-the-shelf AIML interpreters to build task-oriented chat-\nbots. We conduct experiments on real world data sets to\nestablish the effectiveness of the sub-task extraction process. We\ncodify the extracted sub-tasks in an AIML knowledge base and\nbuild a chatbot using this knowledge base. We also show the\nusefulness of the chatbot in automatically handling customer\nrequests by performing a user evaluation study."}
{"Title": "To Trust or Not to Trust? Predicting Online Trusts using Trust Antecedent\nFramework", "Abstract": "Abstract\u2014This paper analyzes the trustor and trustee factors\nthat lead to inter-personal trust using a well studied Trust\nAntecedent framework in management science [10]. To apply\nthese factors to trust ranking problem in online rating systems,\nwe derive features that correspond to each factor and develop\ndifferent trust ranking models. The advantage of this approach\nis that features relevant to trust can be systematically derived\nso as to achieve good prediction accuracy. Through a series of\nexperiments on real data from Epinions, we show that even a\nsimple model using the derived features yields good accuracy\nand outperforms MoleTrust, a trust propagation based model.\nSVM classifiers using these features also show improvements."}
{"Title": "Permutation Tests for Studying Classifier Performance", "Abstract": "Abstract\u2014We explore the framework of permutation-based\np-values for assessing the behavior of the classification error. In\nthis paper we study two simple permutation tests. The first test\nestimates the null distribution by permuting the labels in the\ndata; this has been used extensively in classification problems in\ncomputational biology. The second test produces permutations\nof the features within classes, inspired by restricted random-\nization techniques traditionally used in statistics. We study the\nproperties of these tests and present an extensive empirical\nevaluation on real and synthetic data. Our analysis shows\nthat studying the classification error via permutation tests is\neffective; in particular, the restricted permutation test clearly\nreveals whether the classifier exploits the interdependency\nbetween the features in the data."}
{"Title": "Interaction-based Clustering of Multivariate Time Series", "Abstract": "Abstract\u2014In this paper, we present a novel approach to\nclustering multivariate time series. In contrast to previous\napproaches, we base our cluster notion on the interactions\nbetween the univariate time series within a data object. Our\nobjective is to assign objects with a similar intrinsic interaction\npattern to a common cluster. To formalize this idea, we define a\ncluster by a set of mathematical models describing the cluster-\nspecific interaction pattern. In addition, we propose interaction\nK-means (IKM), an efficient algorithm for partitioning cluster-\ning of multivariate time series. The cluster-specific interaction\npatterns detected by IKM provide valuable information for\ninterpretation of the cluster content. An extensive experimental\nevaluation on synthetic and real world data demonstrates the\neffectiveness and efficiency of our approach."}
{"Title": "PUB: A Class Description Technique based on\nPartial Coverage of Subspace", "Abstract": "Abstract\u2014A good description of a class should be accurate\nand interpretable. Previous works describe classes either by\nanalyzing the correlation of each attribute with the class, or\nby producing rules as in building a classifier. These solutions\nsuffer from issues in accuracy and interpretability.\nA description naturally consists of sentences, where each\nsentence consists of a set of terms. Normally, a sentence is\ndefined as a disjunction or conjunction of several terms, each\nof which specifies a constraint (range/set of values) on an\nattribute. From the data analysis point of view, a sentence\nspecifies a subspace in the database. In this paper, we create\na richer yet interpretable form of a sentence, i.e., a sentence\ndescribes an object if any k attributes of that object satisfy the\nspecified constraints.\nTo that end, we design P UB , an algorithm that produces\ndescriptions with our form of sentences. While constructing\na sentence (within the description), P UB finds the optimal\nrange/set of values for each attribute in linear time. We also\nempirically show that P UB is efficient, and able to produce more\naccurate, concise and interpretable descriptions than current\napproaches on various real datasets."}
{"Title": "Online and Batch Learning of Generalized Cosine Similarities", "Abstract": "Abstract\u2014In this paper, we define an online algorithm to\nlearn the generalized cosine similarity measures for k-NN\nclassification and hence a similarity matrix A corresponding to\na bilinear form. In contrary to the standard cosine measure, the\nnormalization is itself dependent on the similarity matrix which\nmakes it impossible to use directly the algorithms developed\nfor learning Mahanalobis distances, based on positive, semi-\ndefinite (PSD) matrices. We follow the approach where we\nfirst find an appropriate matrix and then project it onto the\nset of PSD matrices, which we have adapted to the particular\nform of generalized cosine similarities, and more particularly\nto the fact that such measures are normalized. The resulting\nonline algorithm as well as its batch version is fast and has\ngot better accuracy as compared with state-of-the-art methods\non standard data sets."}
{"Title": "Discovering Organizational Structure in Dynamic Social Network", "Abstract": "Abstract\u2014Applying the concept of organizational structure\nto social network analysis may well represent the power of\nmembers and the scope of their power in a social network.\nIn this paper, we propose a data structure, called Community\nTree, to represent the organizational structure in the social\nnetwork. We combine the PageRank algorithm and random\nwalks on graph to derive the community tree from the social\nnetwork. In the real world, a social network is constantly\nchanging. Hence, the organizational structure in the social\nnetwork is also constantly changing. In order to present\nthe organizational structure in a dynamic social network,\nwe propose a tree learning algorithm to derive an evolving\ncommunity tree. The evolving community tree enables a smooth\ntransition between the two community trees and well represents\nthe evolution of organizational structure in the dynamic social\nnetwork. Experiments conducted on real data show our meth-\nods are effective at discovering the organizational structure\nand representing the evolution of organizational structure in a\ndynamic social network."}
{"Title": "Kernel Conditional Quantile Estimation via Reduction Revisited", "Abstract": "Abstract\u2014Quantile regression refers to the process of es-\ntimating the quantiles of a conditional distribution and has\nmany important applications within econometrics and data\nmining, among other domains. In this paper, we show how\nto estimate these conditional quantile functions within a Bayes\nrisk minimization framework using a Gaussian process prior.\nThe resulting non-parametric probabilistic model is easy to\nimplement and allows non-crossing quantile functions to be\nenforced. Moreover, it can directly be used in combination\nwith tools and extensions of standard Gaussian Processes\nsuch as principled hyperparameter estimation, sparsification,\nand quantile regression with input-dependent noise rates. No\nexisting approach enjoys all of these desirable properties.\nExperiments on benchmark datasets show that our method\nis competitive with state-of-the-art approaches."}
{"Title": "Naive Bayes Classification of Uncertain Data", "Abstract": "Abstract\u2014Traditional machine learning algorithms assume\nthat data are exact or precise. However, this assumption\nmay not hold in some situations because of data uncertainty\narising from measurement errors, data staleness, and repeated\nmeasurements, etc. With uncertainty, the value of each data\nitem is represented by a probability distribution function (pdf).\nIn this paper, we propose a novel naive Bayes classification\nalgorithm for uncertain data with a pdf. Our key solution is to\nextend the class conditional probability estimation in the Bayes\nmodel to handle pdf\u2019s. Extensive experiments on UCI datasets\nshow that the accuracy of naive Bayes model can be improved\nby taking into account the uncertainty information.\nKeywords-Uncertain data mining; naive Bayes mode"}
{"Title": "Constraint-based Pattern Mining in Dynamic Graphs", "Abstract": "Abstract\u2014Dynamic graphs are used to represent relation-\nships between entities that evolve over time. Meaningful pat-\nterns in such structured data must capture strong interactions\nand their evolution over time. In social networks, such patterns\ncan be seen as dynamic community structures, i.e., sets of\nindividuals who strongly and repeatedly interact. In this paper,\nwe propose a constraint-based mining approach to uncover\nevolving patterns. We propose to mine dense and isolated\nsubgraphs defined by two user-parameterized constraints. The\ntemporal evolution of such patterns is captured by associating\na temporal event type to each identified subgraph. We consider\nfive basic temporal events: The formation, dissolution, growth,\ndiminution and stability of subgraphs from one time stamp to\nthe next. We propose an algorithm that finds such subgraphs in\na time series of graphs processed incrementally. The extraction\nis feasible due to efficient patterns and data pruning strategies.\nWe demonstrate the applicability of our method on several\nreal-world dynamic graphs and extract meaningful evolving\ncommunities."}
{"Title": "Global Slope Change Synopses for Measurement Maps", "Abstract": "Abstract\u2014Quality control using scalar quality measures is\nstandard practice in manufacturing. However, there are also\nquality measures that are determined at a large number\nof positions on a product, since the spatial distribution is\nimportant. We denote such a mapping of local coordinates\non the product to values of a measure as a measurement\nmap. In this paper, we examine how measurement maps\ncan be clustered according to a novel notion of similarity\u2014\nmapscape similarity\u2014that considers the overall course of the\nmeasure on the map. We present a class of synopses called\nglobal slope change that uses the profile of the measure along\nseveral lines from a reference point to different points on\nthe borders to represent a measurement map. We conduct an\nevaluation of global slope change using a real-world data set\nfrom manufacturing and demonstrate its superiority over other\nsynopses."}
{"Title": "Aspect Guided Text Categorization with Unobserved Labels", "Abstract": "This paper proposes a novel multiclass classification\nmethod and exhibits its advantage in the domain of text cat-\negorization with a large label space and, most importantly,\nwhen some of the labels were not observed in the training\ndata. The key insight is the introduction of intermediate as-\npect variables that encode properties of the labels. Aspect\nvariables serve as a joint representation for observed and\nunobserved labels. This way the classification problem can\nbe viewed as a structure learning problem with natural con-\nstraintsonassignmentstotheaspectvariables. Wesolvethe\nproblem as a constrained optimization problem over multi-\nple learners and show significant improvement in classify-\ning short sentences into a large label space of categories,\nincluding previously unobserved categories."}
{"Title": "A Fully Automated Method for Discovering Community Structures in High\nDimensional Data", "Abstract": "Abstract\u2014Identifying modules, or natural communities, in\nlarge complex networks is fundamental in many fields, in-\ncluding social sciences, biological sciences and engineering.\nRecently several methods have been developed to automatically\nidentify communities from complex networks by optimizing the\nmodularity function. The advantage of this type of approaches\nis that the algorithm does not require any parameter to be\ntuned. However, the modularity-based methods for community\ndiscovery assume that the network structure is given explicitly\nand is correct. In addition, these methods work best if the\nnetwork is unweighted and/or sparse. In reality, networks are\noften not directly defined, or may be given as an affinity\nmatrix. In the first case, each node of the network is defined\nas a point in a high dimensional space and different networks\ncan be obtained with different network construction methods,\nresulting in different community structures. In the second case,\nan affinity matrix may define a dense weighted graph, for which\nmodularity-based methods do not perform well. In this work,\nwe propose a very simple algorithm to automatically identify\ncommunity structures from these two types of data. Our\napproach utilizes a k-nearest-neighbor network construction\nmethod to capture the topology embedded in high dimensional\ndata, and applies a modularity-based algorithm to identify\nthe optimal community structure. A key to our approach\nis that the network construction is incorporated with the\ncommunity identification process and is totally parameter-\nfree. Furthermore, our method can suggest appropriate pre-\nprocessing / normalization of the data to improve the results\nof community identification. We tested our methods on several\nsynthetic and real data sets, and evaluated its performance by\ninternal or external accuracy indices. Compared with several\nexisting approaches, our method is not only fully automatic,\nbut also has the best accuracy overall."}
{"Title": "Hierarchical Probabilistic Segmentation Of Discrete Events", "Abstract": "Abstract\u2014Segmentation, the task of splitting a long sequence\nof discrete symbols into chunks, can provide important infor-\nmation about the nature of the sequence that is understandable\nto humans. Algorithms for segmenting mostly belong to the\nsupervised learning family, where a labeled corpus is available\nto the algorithm in the learning phase. We are interested,\nhowever, in the unsupervised scenario, where the algorithm\nnever sees examples of successful segmentation, but still needs\nto discover meaningful segments.\nIn this paper we present an unsupervised learning algorithm\nfor segmenting sequences of symbols or categorical events.\nOur algorithm, Hierarchical Multigram, hierarchically builds\na lexicon of segments and computes a maximum likelihood\nsegmentation given the current lexicon. Thus, our algorithm\nis most appropriate to hierarchical sequences, where smaller\nsegments are grouped into larger segments. Our probabilistic\napproach also allows us to suggest conditional entropy as a\nmeasurement of the quality of a segmentation in the absence\nof labeled data.\nWe compare our algorithm to two previous approaches\nfrom the unsupervised segmentation literature, showing it to\nprovide superior segmentation over a number of benchmarks.\nWe also compare our algorithm to previous approaches over\na segmentation of the unlabeled interactions of a web service\nand its client."}
{"Title": "Topic Modeling for Sequences of Temporal Activities", "Abstract": "Temporally-ordered activity sequences are popular in\nmany real-world domains. This paper presents an LDA-\nstyle topic model for sequences of temporal activities that\ncaptures three features of such sequences: 1) the counts\nof unique activities, 2) the Markov transition dependence\nand 3) the absolute or relative timestamp on each activity.\nIn modeling the first two features we propose the concept\nof global transition probability and distinguish it with local\ntransition probability used in previous work. In modeling\nthe third feature, we employ a continuous time distribution\nto depict the time range of latent topics. The combination of\nthe global transition probability and the temporal informa-\ntion helps to refine the mixture distribution over topics for\ntemporal sequence analysis. We present results on the data\nof system call traces, showing better next activity prediction\nand sequence clustering."}
{"Title": "Combining Super-structuring and Abstraction on Sequence Classification", "Abstract": "Abstract\u2014We present an approach to adapting the data rep-\nresentation used by a learner on sequence classification tasks.\nOur approach that exploits the complementary strengths of\nsuper-structuring (constructing complex features by combining\nexisting features) and abstraction (grouping of similar features\nto generate more abstract features), yields smaller and, at the\nsame time, accurate models. Super-structuring provides a way\nto increase the predictive accuracy of the learned models by\nenriching the data representation (and hence, increases the\ncomplexity of the learned models) whereas abstraction helps\nreduce the number of model parameters by simplifying the\ndata representation. The results of our experiments on two\ndata sets drawn from macromolecular sequence classification\napplications show that adapting data representation by com-\nbining super-structuring and abstraction, makes it possible\nto construct predictive models that use significantly smaller\nnumber of features (by one to three orders of magnitude) than\nthose that are obtained using super-structuring alone, without\nsacrificing predictive accuracy. Our experiments also show that\nsimplifying data representation using abstraction yields better\nperforming models than those obtained using feature selection."}
{"Title": "A Global-Model Naive Bayes Approach\nto the Hierarchical Prediction of Protein Functions", "Abstract": "Abstract\u2014In this paper we propose a new global\u2013model\napproach for hierarchical classification, where a single global\nclassification model is built by considering all the classes in the\nhierarchy \u2013 rather than building a number of local classification\nmodels as it is more usual in hierarchical classification. The\nmethod is an extension of the flat classification algorithm naive\nBayes. We present the extension made to the original algorithm\nas well as its evaluation on eight protein function hierarchical\nclassification datasets. The achieved results are positive and\nshow that the proposed global model is better than using a\nlocal model approach."}
{"Title": "Spatio-temporal Energy based Gait Recognition", "Abstract": "Abstract\u2014Recently there has been lot of interest in using the\ngait energy image (GEI) of human walk sequence for individual\nrecognition. Researchers have reported very good recognition\nrates using both unsupervised and supervised methods for\nnormal walk sequences. However, the performance degrades\nwhen there is a variant like change in clothing or carrying a\nbag. This paper shows that the performance for the variant\nsituations can be improved by constructing the GEI with sway\nalignment instead of upper body alignment, and dynamically\nselecting just the required number of rows from the bottom of\nthe silhouette as inputs for an unsupervised feature selection\napproach. The improvement in recognition rates are established\nwith performance testing on a large gait dataset."}
{"Title": "Feature Selection in the Tensor Product Feature Space", "Abstract": "Abstract\u2014Classifying objects that are sampled jointly from\ntwo or more domains has many applications. The tensor\nproduct feature space is useful for modeling interactions\nbetween feature sets in different domains but feature selection\nin the tensor product feature space is challenging. Conventional\nfeature selection methods ignore the structure of the feature\nspace and may not provide the optimal results. In this paper we\npropose methods for selecting features in the original feature\nspaces of different domains. We obtained sparsity through two\napproaches, one using integer quadratic programming and\nanother using L1-norm regularization. Experimental studies\non biological data sets validate our approach."}
{"Title": "Topic Distributions over Links on Web", "Abstract": "Abstract\u2014It is well known that Web users create links with\ndifferent intentions. However, a key question, which is not well\nstudied, is how to categorize the links and how to quantify the\nstrength of the influence of a web page on another if there is\na link between the two linked web pages. In this paper, we\nfocus on the problem of link semantics analysis, and propose\na novel supervised learning approach to build a model, based\non a training link-labeled and link-weighted graph where a\nlink-label represents the category of a link and a link-weight\nrepresents the influence of one web page on the other in a link.\nBased on the model built, we categorize links and quantify the\ninfluence of web pages on the others in a large graph in the\nsame application domain. We discuss our proposed approach,\nnamely Pairwise Restricted Boltzmann Machines (PRBMs),\nand conduct extensive experimental studies to demonstrate the\neffectiveness of our approach using large real datasets."}
{"Title": "Clustering with Multiple Graphs", "Abstract": "Abstract\u2014In graph-based learning models, entities are often\nrepresented as vertices in an undirected graph with weighted\nedges describing the relationships between entities. In many\nreal-world applications, however, entities are often associated\nwith relations of different types and/or from different sources,\nwhich can be well captured by multiple undirected graphs over\nthe same set of vertices. How to exploit such multiple sources\nof information to make better inferences on entities remains\nan interesting open problem. In this paper, we focus on the\nproblem of clustering the vertices based on multiple graphs\nin both unsupervised and semi-supervised settings. As one of\nour contributions, we propose Linked Matrix Factorization\n(LMF) as a novel way of fusing information from multiple\ngraph sources. In LMF, each graph is approximated by matrix\nfactorization with a graph-specific factor and a factor common\nto all graphs, where the common factor provides features for all\nvertices. Experiments on SIAM journal data show that (1) we\ncan improve the clustering accuracy through fusing multiple\nsources of information with several models, and (2) LMF yields\nsuperior or competitive results compared to other graph-based\nclustering methods."}
{"Title": "Two Heads Better Than One: Metric+Active Learning and Its Applications for IT\nService Classification", "Abstract": "Abstract\u2014Large IT service providers track service requests\nand their execution through problem/change tickets. It is\nimportant to classify the tickets based on the problem/change\ndescription in order to understand service quality and to opti-\nmize service processes. However, two challenges exist in solving\nthis classification problem: 1) ticket descriptions from different\nclasses are of highly diverse characteristics, which invalidates\nmost standard distance metrics; 2) it is very expensive to obtain\nhigh-quality labeled data.\nTo address these challenges, we develop two seemingly\nindependent methods 1) Discriminative Neighborhood Metric\nLearning (DNML) and 2) Active Learning with Median Se-\nlection (ALMS), both of which are, however, based on the\nsame core technique: iterated representative selection. A case\nstudy on real IT service classification application is presented\nto demonstrate the effectiveness and efficiency of our proposed\nmethods."}
{"Title": "Maximum Margin Clustering on Data Manifolds", "Abstract": "Abstract\u2014Clustering is one of the most fundamental and\nimportant problems in computer vision and pattern recognition\ncommunities. Maximum Margin Clustering(MMC) is a recently\nproposed clustering technique which has shown promising\nexperimental results. The main theme behind MMC is to\nextend the standard maximum margin principle in Support\nVector Machine (SVM) to the unsupervised scenario. This\npaper will consider the problem of maximum margin cluster-\ning on data manifolds. Specifically, we propose an approach\ncalled Manifold Regularized Maximum Margin Clustering\n(MRMMC) which combines both the maximum margin data\ndiscrimination and data manifold information in a unified\nclustering objective and propose an efficient algorithm to solve\nit. Finally the experimental results on several real world data\nsets are presented to show the effectiveness of our method."}
{"Title": "Discovering Contexts and Contextual Outliers Using Random Walks in Graphs", "Abstract": "Abstract\u2014The identifying of contextual outliers allows the\ndiscovery of anomalous behavior that other forms of outlier\ndetection cannot find. What may appear to be normal behavior\nwith respect to the entire data set can be shown to be anomalous\nby subsetting the data according to specific spatial or temporal\ncontext. However, in many real-world applications, we may\nnot have sufficient a priori contextual information to discover\nthese contextual outliers. This paper addresses the problem by\nproposing a probabilistic approach based on random walks,\nwhich can simultaneously explore meaningful contexts and\nscore contextual outliers therein. Our approach has several\nadvantages including producing outlier scores which can be\ninterpreted as stationary expectations and their calculation in\nclosed form in polynomial time. In addition, we show that point\noutlier detection using the stationary distribution is a special\ncase of our approach. It allows us to find both global and\ncontextual outliers simultaneously and to create a meaningful\nranked list consisting of both types of outliers. This is a major\ndeparture from existing work where an algorithm typically\nidentifies one type of outlier. The effectiveness of our method is\njustified by empirical results on real data sets, with comparison\nto related work."}
{"Title": "Effective Criterion Functions for Efficient Agglomerative Clustering on Very Large\nNetworks", "Abstract": "Abstract\u2014As the agglomerative clustering algorithm is\nwidely used in data mining, image processing, bioinformatics\nand pattern recognition. it has attracted great interests from\nboth academical and industrial communities. However, existing\nstudies neglect the decisive factor of the efficiency of the\nagglomerative clustering algorithm for large complex networks\nand usually use criterion functions which lead to inefficiency.\nIn this paper, we propose three effective criterion functions for\nimproving performance of agglomerative clustering algorithm.\nWe note that clustering efficiency is determined by two factors:\na) the number of neighbors of two merged clusters in each\nmerge step; b) the number of neighbors shared by the two\nclusters. Based on these observations, we propose a framework\nfor designing criterion functions in order to efficiently find\nclusters in very large networks. We devise three criterion\nfunctions that can effectively control the number of neighbors\nof clusters, and they can efficiently produce high-quality\nclusters. We have implemented our method and compared with\nexisting studies on real networks, and our method outperforms\nstate-of-the-art approaches significantly on large networks."}
{"Title": "Binomial Matrix Factorization for Discrete Collaborative Filtering", "Abstract": "Abstract\u2014Matrix factorization (MF) models have proved\nefficient and well scalable for collaborative filtering (CF)\nproblems. Many researchers also present the probabilistic\ninterpretation of MF. They usually assume that the factor\nvectors of users and items are from normal distributions, and\nso are the ratings when the user and item factors are given.\nThen they can derive the exact MF algorithm by finding a MAP\nestimate of the model parameters. In this paper we suggest a\nnew probabilistic perspective on MF for discrete CF problems.\nWe assume that all ratings are from binomial distributions with\ndifferent preference parameters instead of the original normal\ndistributions. The new interpretation is more reasonable for\ndiscrete CF problems since they only allow several legal discrete\nrating values. We also present two effective algorithms to learn\nthe new model and make predictions. They are applied to the\nNetflix Prize data set and acquire considerably better accuracy\nthan those of MF."}
{"Title": "Bi-Relational Network Analysis Using a Fast Random Walk with Restart", "Abstract": "Abstract\u2014Identification of nodes relevant to a given node\nin a relational network is a basic problem in network analysis\nwith great practical importance. Most existing network analysis\nalgorithms utilize one single relation to define relevancy among\nnodes. However, in real world applications multiple relation-\nships exist between nodes in a network. Therefore, network\nanalysis algorithms that can make use of more than one relation\nto identify the relevance set for a node are needed. In this paper,\nwe show how the Random Walk with Restart (RWR) approach\ncan be used to study relevancy in a bi-relational network from\nthe bibliographic domain, and show that making use of two\nrelations results in better results as compared to approaches\nthat use a single relation. As relational networks can be very\nlarge, we also propose a fast implementation for RWR by\nadapting an existing Iterative Aggregation and Disaggregation\n(IAD) approach. The IAD-based RWR exploits the block-wise\nstructure of real world networks. Experimental results show\nsignificant increase in running time for the IAD-based RWR\ncompared to the traditional power method based RWR."}
{"Title": "A New MCA-based Divisive Hierarchical Algorithm for Clusteing Categorical Data", "Abstract": "Abstract\u2014 Clustering categorical data faces two challenges,\none is lacking of inherent similarity measure, and the other is\nthat the clusters are prone to being embedded in different\nsubspace. In this paper, we propose the first divisive\nhierarchical clustering algorithm for categorical data. The\nalgorithm, which is based on Multiple Correspondence\nAnalysis (MCA), is systematic, efficient and effective. In our\nalgorithm, MCA plays an important role in analyzing the data\nglobally. The proposed algorithm has five merits. First, our\nalgorithm yields a dendrogram representing nested groupings\nof patterns and similarity levels at different granularities.\nSecond, it is parameter-free, fully automatic and, most\nimportantly, requires no assumption regarding the number of\nclusters. Third, it is independent of the order in which the data\nare processed. Forth, it is scalable to large data sets; and\nfinally, using the novel data representation and Chi-square\ndistance measures makes our algorithm capable of seamlessly\ndiscovering  the  clusters  embedded  in  the  subspaces.\nExperiments on both synthetic and real data demonstrate the\nsuperior performance of our algorithm."}
{"Title": "Non-Sparse Multiple Kernel Learning for Fisher Discriminant Analysis", "Abstract": "Abstract\u2014We consider the problem of learning a linear\ncombination of pre-specified kernel matrices in the Fisher\ndiscriminant analysis setting. Existing methods for such a task\nimpose an \u2018 1 norm regularisation on the kernel weights, which\nproduces sparse solution but may lead to loss of information.\nIn this paper, we propose to use \u2018 2 norm regularisation instead.\nThe resulting learning problem is formulated as a semi-infinite\nprogram and can be solved efficiently. Through experiments on\nboth synthetic data and a very challenging object recognition\nbenchmark, the relative advantages of the proposed method\nand its \u2018 1 counterpart are demonstrated, and insights are\ngained as to how the choice of regularisation norm should\nbe made."}
{"Title": "Multirelational Topic Models", "Abstract": "Abstract\u2014In this paper we propose the multirelational topic\nmodel (MRTM) for multiple types of link modeling such as\ncitation and coauthor links in document networks. In the\ncitation network, the MRTM models the citation link between\neach pair of documents as a binary variable conditioned on\ntheir topic distributions. In the coauthor network, the MRTM\nmodels the coauthor link between each pair of authors as a\nbinary variable conditioned on their expertise distributions.\nThe topic discovery is collectively regularized by multiple\nrelations in both citation and coauthor networks. This model\ncan summarize topics from the document network, predict\ncitation links between documents and coauthor links between\nauthors. Efficient inference and learning algorithms are derived\nbased on Gibbs sampling. Experiments demonstrate that the\nMRTM significantly outperforms other state-of-the-art single-\nrelational link modeling methods for large scientific document\nnetworks."}
{"Title": "Learning Local Components to Understand Large Bayesian Networks", "Abstract": "Abstract\u2014Bayesian networks are known for providing an\nintuitive and compact representation of probabilistic infor-\nmation and allowing the creation of models over a large\nand complex domain. Bayesian learning and reasoning are\nnontrivial for a large Bayesian network. In parallel, it is\na tough job for users (domain experts) to extract accurate\ninformation from a large Bayesian network due to dimensional\ndifficulty. We define a formulation of local components and\npropose a clustering algorithm to learn such local components\ngiven complete data. The algorithm groups together most inter-\nrelevant attributes in a domain. We evaluate its performance\non three benchmark Bayesian networks and provide results in\nsupport. We further show that the learned components may\nrepresent local knowledge more precisely in comparison to the\nfull Bayesian networks when working with a small amount of\ndata."}
{"Title": "RING: An Integrated Method for Frequent Representative Subgraph Mining", "Abstract": "Abstract\u2014We propose a novel representative based subgraph\nmining model. A series of standards and methods are proposed\nto select invariants. Patterns are mapped into invariant vectors\nin a multidimensional space. To find qualified patterns, only\na subset of frequent patterns is generated as representatives,\nsuch that every frequent pattern is close to one of the rep-\nresentative patterns while representative patterns are distant\nfrom each other. We devise the RING algorithm, integrating\nthe representative selection into the pattern mining process.\nMeanwhile, we use R-trees to assist this mining process. Last\nbut not least, a large number of real and synthetic datasets\nare employed for the empirical study, which show the benefits\nof the representative model and the efficiency of the RING\nalgorithm."}
{"Title": "A Cost-Effective LSH Filter for Fast Pairwise Mining", "Abstract": "Abstract\u2014The pairwise mining problem is to discover pair-\nwise objects having measures greater than the user-specified\nminimum threshold from a collection of objects. It is essential\nin a large variety of database and data-mining applications. Of\nlate, there has been increasing interest in applying a Locality-\nSensitive Hashing (LSH) scheme for pairwise mining. LSH-type\nmethods have shown themselves to be simply implementable\nand capable of achieving significant performance gain in run-\nning time over most exact methods. However, the present LSH-\ntype methods still suffer from some bottlenecks, such as \u201dthe\ncurse of threshold\u201d. In this paper, we proposed a novel LSH-\nbased method, namely Cost-effective LSH filter (Ce-LSH for\nshort), for pairwise mining. Compared with previous LSH-type\nmethods, it uses a lower fixed number of LSH functions and is\nthus more cost-effective. Substantial experiments evidence that\nour method gives significant improvement in running time over\nexisting LSH-type methods and some recently reported method\nbased on upper-bound. Experimental results also indicate that\nit scales well even for a relatively low minimum threshold and\nfor a fairly small miss ratio."}
{"Title": "A New Kernel-based Classification Algorithm", "Abstract": "A new kernel-based learning algorithm called\nkernel affine subspace nearest point (KASNP)\napproach is proposed in this paper. Inspired by the\ngeometrical explanation of Support Vector Machines\n(SVMs) and its nearest point problem in convex hulls,\nwe extend the convex hull of each class to its\ncorresponding affine subspace in high dimensional\nspace induced by kernel. In two class affine subspaces,\nKASNP finds the nearest points and then constructs a\nseparating hyperplane, which bisects the line segment\njoining them. The nearest point problem of KASNP is\nonly an unconstrained optimal problem whose solution\ncan be directly computed. Compared with SVM,\nKASNP avoids solving convex quadratic programming.\nExperiments on two-spiral dataset, two UCI credit\ndatasets, and face recognition datasets show that our\nproposed KASNP is effective for data classification."}
{"Title": "Top 10 algorithms in data mining", "Abstract": "This paper presents the top 10 data mining algorithms identified by the IEEE\nInternational Conference on Data Mining (ICDM) in December 2006: C4.5, k-Means, SVM,\nApriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART. These top 10 algorithms\nare among the most influential data mining algorithms in the research community. With each\nalgorithm,weprovideadescriptionofthealgorithm,discusstheimpactofthealgorithm,and\nreviewcurrentandfurtherresearchonthealgorithm.These10algorithmscoverclassification,clustering, statistical learning, association analysis, and link mining, which are all among the\nmost important topics in data mining research and development."}
{"Title": "Detecting Novel Discrepancies in Communication Networks", "Abstract": "Abstract\u2014We address the problem of detecting charac-\nteristic patterns in communication networks. We introduce\na scalable approach based on set-system discrepancy. By\nimplicitly labeling each network edge with the sequence\nof times in which its two endpoints communicate, we\nview an entire communication network as a set-system.\nThis view allows us to use combinatorial discrepancy as\na mechanism to \u201cobserve\u201d system behavior at different\ntime scales. We illustrate our approach, called Discrepancy-\nbased Novelty Detector (DND), on networks obtained from\nemails, bluetooth connections, IP traffic, and tweets. DND\nhas almost 1 linear runtime complexity and linear storage\ncomplexity in the number of communications. Examples\nof novel discrepancies that it detects are (i) asynchronous\ncommunications and (ii) disagreements in the firing rates\nof nodes and edges relative to the communication network\nas a whole."}
{"Title": "Learning Markov Network Structure with Decision Trees", "Abstract": "Abstract\u2014Traditional Markov network structure learning\nalgorithms perform a search for globally useful features.\nHowever, these algorithms are often slow and prone to finding\nlocal optima due to the large space of possible structures.\nRavikumar et al. [1] recently proposed the alternative idea\nof applying L1 logistic regression to learn a set of pairwise\nfeatures for each variable, which are then combined into\na global model. This paper presents the DTSL algorithm,\nwhich uses probabilistic decision trees as the local model. Our\napproach has two significant advantages: it is more efficient,\nand it is able to discover features that capture more complex\ninteractions among the variables. Our approach can also be\nseen as a method for converting a dependency network into\na consistent probabilistic model. In an extensive empirical\nevaluation on 13 datasets, our algorithm obtains comparable\naccuracy to three standard structure learning algorithms while\nrunning 1-4 orders of magnitude faster."}
{"Title": "Discovering Overlapping Groups in Social Media", "Abstract": "Abstract\u2014The increasing popularity of social media is short-\nening the distance between people. Social activities, e.g., tagging\nin Flickr, bookmarking in Delicious, twittering in Twitter, etc.\nare reshaping people\u2019s social life and redefining their social\nroles. People with shared interests tend to form their groups\nin social media, and users within the same community likely\nexhibit similar social behavior (e.g., going for the same movies,\nhaving similar political viewpoints), which in turn reinforces\nthe community structure. The multiple interactions in social\nactivities entail that the community structures are often over-\nlapping, i.e., one person is involved in several communities.\nWe propose a novel co-clustering framework, which takes\nadvantage of networking information between users and tags in\nsocial media, to discover these overlapping communities. In our\nmethod, users are connected via tags and tags are connected to\nusers. This explicit representation of users and tags is useful for\nunderstanding group evolution by looking at who is interested\nin what. The efficacy of our method is supported by empirical\nevaluation in both synthetic and online social networking data."}
{"Title": "Discovering author impact: A PageRank perspective", "Abstract": "This article provides an alternative perspective for measuring author impact by applying\nPageRank algorithm to a coauthorship network. A weighted PageRank algorithm consider-\ning citation and coauthorship network topology is proposed. We test this algorithm under\ndifferent damping factors by evaluating author impact in the informetrics research com-\nmunity. In addition, we also compare this weighted PageRank with the h-index, citation,\nand program committee (PC) membership of the International Society for Scientometrics\nand Informetrics (ISSI) conferences. Findings show that this weighted PageRank algorithm\nprovides reliable results in measuring author impact."}
{"Title": "Mining and modeling linkage information from citation context for\nimproving biomedical literature retrieval", "Abstract": "Mining linkage information from the citation graph has been shown to be effective in iden-\ntifying important literatures. However, the question of how to utilize linkage information\nfrom the citation graph to facilitate literature retrieval still remains largely unanswered. In\nthis paper, given the context of biomedical literature retrieval, we first conduct a case study\nin order to find out whether applying PageRank and HITS algorithms directly to the citation\ngraph is the best way of utilizing citation linkage information for improving biomedical lit-\nerature retrieval. Second, we propose a probabilistic combination framework for integrat-\ning citation information into the content-based information retrieval weighting model.\nBased on the observations of the case study, we present two strategies for modeling the\nlinkage information contained in the citation graph. The proposed framework provides a\ntheoretical support for the combination of content and linkage information. Under this\nframework, exhaustive parameter tuning can be avoided. Extensive experiments on three\nTREC Genomics collections demonstrate the advantages and effectiveness of our proposed\nmethods."}
{"Title": "Popular and/or prestigious? Measures of scholarly esteem", "Abstract": "Citation analysis does not generally take the quality of citations into account: all citations\nare weighted equally irrespective of source. However, a scholar may be highly cited but not\nhighly regarded: popularity and prestige are not identical measures of esteem. In this study\nwe define popularity as the number of times an author is cited and prestige as the number\nof times an author is cited by highly cited papers. Information retrieval (IR) is the test field.\nWe compare the 40 leading researchers in terms of their popularity and prestige over time.\nSome authors are ranked high on prestige but not on popularity, while others are ranked\nhigh on popularity but not on prestige. We also relate measures of popularity and prestige\nto date of Ph.D. award, number of key publications, organizational affiliation, receipt of\nprizes/honors, and gender."}
{"Title": "The Anatomy of a Large-Scale Hypertextual\nWeb Search Engine", "Abstract": "In this paper, we present Google, a prototype of a large-scale search engine which makes heavy\nuse of the structure present in hypertext. Google is designed to crawl and index the Web efficiently\nand produce much more satisfying search results than existing systems. The prototype with a full\ntext and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/\nTo engineer a search engine is a challenging task. Search engines index tens to hundreds of\nmillions of web pages involving a comparable number of distinct terms. They answer tens of\nmillions of queries every day. Despite the importance of large-scale search engines on the web,\nvery little academic research has been done on them. Furthermore, due to rapid advance in\ntechnology and web proliferation, creating a web search engine today is very different from three\nyears ago. This paper provides an in-depth description of our large-scale web search engine -- the\nfirst such detailed public description we know of to date. Apart from the problems of scaling\ntraditional search techniques to data of this magnitude, there are new technical challenges involved\nwith using the additional information present in hypertext to produce better search results. This\npaper addresses this question of how to build a practical large-scale system which can exploit the\nadditional information present in hypertext. Also we look at the problem of how to effectively deal\nwith uncontrolled hypertext collections where anyone can publish anything they want."}
{"Title": "A Generation Model to Unify Topic Relevance\nand Lexicon-based Sentiment for Opinion Retrieval", "Abstract": "Opinion retrieval is a task of growing interest in social life and\nacademic research, which is to find relevant and opinionate\ndocuments according to a user\u2019s query. One of the key issues is\nhow to combine a document\u2019s opinionate score (the ranking score\nof to what extent it is subjective or objective) and topic relevance\nscore. Current solutions to document ranking in opinion retrieval\nare generally ad-hoc linear combination, which is short of\ntheoretical foundation and careful analysis. In this paper, we focus\non lexicon-based opinion retrieval. A novel generation model that\nunifies topic-relevance and opinion generation by a quadratic\ncombination is proposed in this paper. With this model, the\nrelevance-based ranking serves as the weighting factor of the\nlexicon-based sentiment ranking function, which is essentially\ndifferent from the popular heuristic linear combination approaches.\nThe effect of different sentiment dictionaries is also discussed.\nExperimental results on TREC blog datasets show the significant\neffectiveness of the proposed unified model. Improvements of\n28.1% and 40.3% have been obtained in terms of MAP and p@10\nrespectively. The conclusion is not limited to blog environment.\nBesides the unified generation model, another contribution is that\nour work demonstrates that in the opinion retrieval task, a\nBayesian approach to combining multiple ranking functions is\nsuperior to using a linear combination. It is also applicable to\nother result re-ranking applications in similar scenario."}
{"Title": "BrowseRank: Letting Web Users Vote for Page Importance", "Abstract": "This paper proposes a new method for computing page importance,\nreferred to as BrowseRank. The conventional approach to com-\npute page importance is to exploit the link graph of the web and\nto build a model based on that graph. For instance, PageRank is\nsuch an algorithm, which employs a discrete-time Markov process\nas the model. Unfortunately, the link graph might be incomplete\nand inaccurate with respect to data for determining page impor-\ntance, because links can be easily added and deleted by web con-\ntent creators. In this paper, we propose computing page impor-\ntance by using a \u2019user browsing graph\u2019 created from user behav-\nior data. In this graph, vertices represent pages and directed edges\nrepresent transitions between pages in the users\u2019 web browsing his-\ntory. Furthermore, the lengths of staying time spent on the pages\nby users are also included. The user browsing graph is more re-\nliable than the link graph for inferring page importance. This pa-\nper further proposes using the continuous-time Markov process on\nthe user browsing graph as a model and computing the stationary\nprobability distribution of the process as page importance. An effi-\ncient algorithm for this computation has also been devised. In this\nway, we can leverage hundreds of millions of users\u2019 implicit voting\non page importance. Experimental results show that BrowseRank\nindeed outperforms the baseline methods such as PageRank and\nTrustRank in several tasks."}
{"Title": "Combining Learn-based and Lexicon-based Techniques\nfor Sentiment Detection without Using Labeled Examples", "Abstract": "In this work, we propose a novel scheme for sentiment\nclassification (without labeled examples) which combines the\nstrengths of both \u201clearn-based\u201d and \u201clexicon-based\u201d approaches\nas follows: we first use a lexicon-based technique to label a\nportion of informative examples from given task (or domain);\nthen learn a new supervised classifier based on these labeled ones;\nfinally apply this classifier to the task. The experimental results\nindicate that proposed scheme could dramatically outperform\n\u201clearn-based\u201d and \u201clexicon-based\u201d techniques."}
{"Title": "Deep Classification in Large-scale Text Hierarchies", "Abstract": "Most classification algorithms are best at categorizing the Web\ndocuments into a few categories, such as the top two levels in the\nOpen Directory Project. Such a classification method does not give\nvery detailed topic-related class information for the user because the\nfirst two levels are often too coarse. However, classification on a\nlarge-scale hierarchy is known to be intractable for many target\ncategories with cross-link relationships among them. In this paper,\nwe propose a novel deep-classification approach to categorize Web\ndocuments into categories in a large-scale taxonomy. The approach\nconsists of two stages: a search stage and a classification stage. In\nthe first stage, a category-search algorithm is used to acquire the\ncategory candidates for a given document. Based on the category\ncandidates, we prune the large-scale hierarchy to focus our\nclassification effort on a small subset of the original hierarchy. As a\nresult, the classification model is trained on the small subset before\nbeing applied to assign the category for a new document. Since the\ncategory candidates are sufficiently close to each other in the\nhierarchy, a statistical-language-model based classifier using n-gram\nfeatures is exploited. Furthermore, the structure of the taxonomy can\nbe utilized in this stage to improve the performance of classification.\nWe demonstrate the performance of our proposed algorithms on the\nOpen Directory Project with over 130,000 categories. Experimental\nresults show that our proposed approach can reach 51.8% on the\nmeasure of Mi-F1 at the 5th level, which is 77.7% improvement\nover top-down based SVM classification algorithms."}
{"Title": "Discriminative Probabilistic Models for Passage Based\nRetrieval", "Abstract": "The approach of using passage-level evidence for document\nretrieval has shown mixed results when it is applied to a\nvariety of test beds with different characteristics. One main\nreason of the inconsistent performance is that there exists\nno unified framework to model the evidence of individual\npassages within a document. This paper proposes two prob-\nabilistic models to formally model the evidence of a set of\ntop ranked passages in a document. The first probabilistic\nmodel follows the retrieval criterion that a document is rele-\nvant if any passage in the document is relevant, and models\neach passage independently. The second probabilistic model\ngoes a step further and incorporates the similarity correla-\ntions among the passages. Both models are trained in a\ndiscriminative manner. Furthermore, we present a combi-\nnation approach to combine the ranked lists of document\nretrieval and passage-based retrieval.\nAn extensive set of experiments have been conducted on\nfour different TREC test beds to show the effectiveness of\nthe proposed discriminative probabilistic models for passage-\nbased retrieval. The proposed algorithms are compared with\na state-of-the-art document retrieval algorithm and a lan-\nguage model approach for passage-based retrieval. Further-\nmore, our combined approach has been shown to provide\nbetter results than both document retrieval and passage-\nbased retrieval approaches."}
{"Title": "Efficient Top-k Querying over Social-Tagging Networks", "Abstract": "Online communities have become popular for publishing and search-\ning content, as well as for finding and connecting to other users.\nUser-generatedcontent includes, for example, personal blogs, book-\nmarks, and digital photos. These items can be annotated and rated\nby different users, and these social tags and derived user-specific\nscores can be leveraged for searching relevant content and discov-\nering subjectively interesting items. Moreover, the relationships\namong users can also be taken into consideration for ranking search\nresults, the intuition being that you trust the recommendations of\nyour close friends more than those of your casual acquaintances.\nQueries for tag or keyword combinations that compute and rank\nthe top-k results thus face a large variety of options that complicate\nthe query processing and pose efficiency challenges. This paper ad-\ndresses these issues by developing an incremental top-k algorithm\nwith two-dimensional expansions: social expansion considers the\nstrength of relations among users, and semantic expansion consid-\ners the relatedness of different tags. It presents a new algorithm,\nbased on principles of threshold algorithms, by folding friends and\nrelated tags into the search space in an incremental on-demand\nmanner. The excellent performance of the method is demonstrated\nby an experimental evaluation on three real-world datasets, crawled\nfrom deli.cio.us, Flickr, and LibraryThing."}
{"Title": "EigenRank: A Ranking-Oriented Approach to Collaborative\nFiltering", "Abstract": "A recommender system must be able to suggest items that\nare likely to be preferred by the user. In most systems, the\ndegree of preference is represented by a rating score. Given a\ndatabase of users\u2019 past ratings on a set of items, traditional\ncollaborative filtering algorithms are based on predicting the\npotential ratings that a user would assign to the unrated\nitems so that they can be ranked by the predicted ratings\nto produce a list of recommended items. In this paper, we\npropose a collaborative filtering approach that addresses the\nitem ranking problem directly by modeling user preferences\nderived from the ratings. We measure the similarity be-\ntween users based on the correlation between their rankings\nof the items rather than the rating values and propose new\ncollaborative filtering algorithms for ranking items based on\nthe preferences of similar users. Experimental results on\nreal world movie rating data sets show that the proposed\napproach outperforms traditional collaborative filtering al-\ngorithms significantly on the NDCG measure for evaluating\nranked results."}
{"Title": "Enhancing Text Clustering by Leveraging Wikipedia\nSemantics", "Abstract": "Most traditional text clustering methods are based on \u201cbag of\nwords\u201d (BOW) representation based on frequency statistics in\na set of documents. BOW, however, ignores the important\ninformation on the semantic relationships between key terms.\nTo overcome this problem, several methods have been\nproposed to enrich text representation with external resource\nin the past, such as WordNet. However, many of these\napproaches suffer from some limitations: 1) WordNet has\nlimited coverage and has a lack of effective word-sense\ndisambiguation ability; 2) Most of the text representation\nenrichment strategies, which append or replace document\nterms with their hypernym and synonym, are overly simple.\nIn this paper, to overcome these deficiencies, we first\npropose a way to build a concept thesaurus based on the\nsemantic relations (synonym, hypernym, and associative\nrelation) extracted from Wikipedia. Then, we develop a\nunified framework to leverage these semantic relations in\norder to enhance traditional content similarity measure for\ntext clustering. The experimental results on Reuters and\nOHSUMED datasets show that with the help of Wikipedia\nthesaurus, the clustering performance of our method is\nimproved as compared to previous methods. In addition,\nwith the optimized weights for hypernym, synonym, and\nassociative concepts that are tuned with the help of a few\nlabeled data users provided, the clustering performance can\nbe further improved."}
{"Title": "Exploiting Subjectivity Analysis in Blogs to Improve\nPolitical Leaning Categorization", "Abstract": "In this paper, we address a relatively new and interesting\ntext categorization problem: classify a political blog as ei-\nther liberal or conservative, based on its political leaning.\nOur subjectivity analysis based method is twofold: 1) we\nidentify subjective sentences that contain at least two strong\nsubjective clues based on the General Inquirer dictionary;\n2) from subjective sentences identified, we extract opinion\nexpressions and other features to build political leaning clas-\nsifiers. Experimental results with a political blog corpus we\nbuilt show that by using features from subjective sentences\ncan significantly improve the classification performance. In\naddition, by extracting opinion expressions from subjective\nsentences, we are able to reveal opinions that are character-\nistic of a specific political leaning to some extent."}
{"Title": "Adaptive p-Posterior Mixture-Model Kernels\nfor Multiple Instance Learning", "Abstract": "In multiple instance learning (MIL), how the\ninstances determine the bag-labels is an es-\nsential issue, both algorithmically and in-\ntrinsically. In this paper, we show that the\nmechanism of how the instances determine\nthe bag-labels is different for different ap-\nplication domains, and does not necessarily\nobey the traditional assumptions of MIL. We\ntherefore propose an adaptive framework for\nMIL that adapts to different application do-\nmains by learning the domain-specific mech-\nanisms merely from labeled bags. Our ap-\nproach is especially attractive when we are\nencountered with novel application domains,\nfor which the mechanisms may be different\nand unknown. Specifically, we exploit mix-\nture models to represent the composition of\neach bag and an adaptable kernel function to\nrepresent the relationship between the bags.\nWe validate on synthetic MIL datasets that\nthe kernel function automatically adapts to\ndifferent mechanisms of how the instances\ndetermine the bag-labels. We also compare\nour approach with state-of-the-art MIL tech-\nniques on real-world benchmark datasets."}
{"Title": "Learning Query Intent from Regularized Click Graphs", "Abstract": "This work presents the use of click graphs in improving query\nintent classifiers, which are critical if vertical search and\ngeneral-purpose search services are to be offered in a uni-\nfied user interface. Previous works on query classification\nhave primarily focused on improving feature representation\nof queries, e.g., by augmenting queries with search engine re-\nsults. In this work, we investigate a completely orthogonal\napproach \u2014 instead of enriching feature representation, we\naim at drastically increasing the amounts of training data by\nsemi-supervised learning with click graphs. Specifically, we\ninfer class memberships of unlabeled queries from those of\nlabeled ones according to their proximities in a click graph.\nMoreover, we regularize the learning with click graphs by\ncontent-based classification to avoid propagating erroneous\nlabels. We demonstrate the effectiveness of our algorithms\nin two different applications, product intent and job intent\nclassification. In both cases, we expand the training data\nwith automatically labeled queries by over two orders of\nmagnitude, leading to significant improvements in classifi-\ncation performance. An additional finding is that with a\nlarge amount of training data obtained in this fashion, clas-\nsifiers using only query words/phrases as features can work\nremarkably well."}
{"Title": "Learning to Rank with SoftRank and Gaussian Processes", "Abstract": "In this paper we address the issue of learning to rank for doc-\nument retrieval using Thurstonian models based on sparse\nGaussian processes. Thurstonian models represent each doc-\nument for a given query as a probability distribution in a\nscore space; these distributions over scores naturally give\nrise to distributions over document rankings. However, in\ngeneral we do not have observed rankings with which to\ntrain the model; instead, each document in the training set\nis judged to have a particular relevance level: for example\n\u201cBad\u201d, \u201cFair\u201d, \u201cGood\u201d, or \u201cExcellent\u201d. The performance of\nthe model is then evaluated using information retrieval (IR)\nmetrics such as Normalised Discounted Cumulative Gain\n(NDCG). Recently Taylor et al. presented a method called\nSoftRank which allows the direct gradient optimisation of\na smoothed version of NDCG using a Thurstonian model.\nIn this approach, document scores are represented by the\noutputs of a neural network, and score distributions are cre-\nated artificially by adding random noise to the scores. The\nSoftRank mechanism is a general one; it can be applied to\ndifferent IR metrics, and make use of different underlying\nmodels. In this paper we extend the SoftRank framework to\nmake use of the score uncertainties which are naturally pro-\nvided by a Gaussian process (GP), which is a probabilistic\nnon-linear regression model. We further develop the model\nby using sparse Gaussian process techniques, which give im-\nproved performance and efficiency, and show competitive re-\nsults against baseline methods when tested on the publicly\navailable LETOR OHSUMED data set. We also explore\nhow the available uncertainty information can be used in\nprediction and how it affects model performance."}
{"Title": "Query Dependent Ranking Using K-Nearest Neighbor\n\u2217", "Abstract": "Many ranking models have been proposed in information re-\ntrieval, and recently machine learning techniques have also been\napplied to ranking model construction. Most of the existing meth-\nods do not take into consideration the fact that significant differ-\nences exist between queries, and only resort to a single function\nin ranking of documents. In this paper, we argue that it is nec-\nessary to employ different ranking models for different queries and\nconduct what we call query-dependent ranking. As the first such at-\ntempt, we propose a K-Nearest Neighbor (KNN) method for query-\ndependent ranking. We first consider an online method which cre-\nates a ranking model for a given query by using the labeled neigh-\nbors of the query in the query feature space and then rank the docu-\nments with respect to the query using the created model. Next, we\ngive two offline approximations of the method, which create the\nranking models in advance to enhance the efficiency of ranking.\nAnd we prove a theory which indicates that the approximations are\naccurate in terms of difference in loss of prediction, if the learning\nalgorithm used is stable with respect to minor changes in training\nexamples. Our experimental results show that the proposed online\nand offline methods both outperform the baseline method of using\na single ranking function."}
{"Title": "Question Classification with Semantic Tree Kernel", "Abstract": "Question Classification plays an important role in most Question\nAnswering systems. In this paper, we exploit semantic features in\nSupport Vector Machines (SVMs) for Question Classification.\nWe propose a semantic tree kernel to incorporate semantic\nsimilarity information. A diverse set of semantic features is\nevaluated. Experimental results show that SVMs with semantic\nfeatures, especially semantic classes, can significantly outperform\nthe state-of-the-art systems."}
{"Title": "Ranking Opinionated Blog Posts using OpinionFinder", "Abstract": "The aim of an opinion finding system is not just to retrieve\nrelevant documents, but to also retrieve documents that ex-\npress an opinion towards the query target entity. In this\nwork, we propose a way to use and integrate an opinion-\nidentification toolkit, OpinionFinder, into the retrieval pro-\ncess of an Information Retrieval (IR) system, such that opin-\nionated, relevant documents are retrieved in response to a\nquery. In our experiments, we vary the number of top-\nranked documents that must be parsed in response to a\nquery, and investigate the effect on opinion retrieval per-\nformance and required parsing time. We find that opin-\nion finding retrieval performance is improved by integrating\nOpinionFinder into the retrieval system, and that retrieval\nperformance grows as more posts are parsed by Opinion-\nFinder. However, the benefit eventually tails off at a deep\nrank, suggesting that an optimal setting for the system has\nbeen achieved."}
{"Title": "SOPING: A Chinese Customer Review Mining System", "Abstract": "With the booming development of the Web, popular Chinese forums\nenable people to find experienced customers\u2019 reviews for products.\nIn order to get an all-around opinion about one product, users need\nto go through plenty of web pages, which is time-consuming and\ninefficient.  Consequently,  automatic  review  mining  and\nsummarization has become a hot research topic recently. However,\nprevious approaches are not applicable for mining Chinese customer\nreviews. In this paper, we introduce SOPING, a Chinese customer\nreview mining system that mines reviews from forums. Specifically,\nwe propose a novel search-based approach to extract product\nfeatures and a feature-oriented sentence orientation determination\nmethod. Our experimental results show that our proposed techniques\nare highly effective."}
{"Title": "Topic-bridged PLSA for Cross-Domain Text Classification", "Abstract": "In many Web applications, such as blog classification and news-\ngroup classification, labeled data are in short supply. It often hap-\npens that obtaining labeled data in a new domain is expensive and\ntime consuming, while there may be plenty of labeled data in a\nrelated but different domain. Traditional text classification ap-\nproaches are not able to cope well with learning across different\ndomains. In this paper, we propose a novel cross-domain text\nclassification algorithm which extends the traditional probabilis-\ntic latent semantic analysis (PLSA) algorithm to integrate labeled\nand unlabeled data, which come from different but related do-\nmains, into a unified probabilistic model. We call this new model\nTopic-bridged PLSA, or TPLSA. By exploiting the common top-\nics between two domains, we transfer knowledge across different\ndomains through a topic-bridge to help the text classification in\nthe target domain. A unique advantage of our method is its ability\nto maximally mine knowledge that can be transferred between\ndomains, resulting in superior performance when compared to\nother state-of-the-art text classification approaches. Experimental\nevaluation on different kinds of datasets shows that our proposed\nalgorithm can improve the performance of cross-domain text clas-\nsification significantly."}
{"Title": "A Study of Inter-Annotator Agreement for Opinion Retrieval", "Abstract": "Evaluation of sentiment analysis, like large-scale IR evalu-\nation, relies on the accuracy of human assessors to create\njudgments. Subjectivity in judgments is a problem for rel-\nevance assessment and even more so in the case of senti-\nment annotations. In this study we examine the degree to\nwhich assessors agree upon sentence-level sentiment anno-\ntation. We show that inter-assessor agreement is not con-\ntingent on document length or frequency of sentiment but\ncorrelates positively with automated opinion retrieval per-\nformance. We also examine the individual annotation cate-\ngories to determine which categories pose most difficulty for\nannotators."}
{"Title": "Context-Aware Query Classification", "Abstract": "Understanding users\u2019 search intent expressed through their\nsearch queries is crucial to Web search and online adver-\ntisement. Web query classification (QC) has been widely\nstudied for this purpose. Most previous QC algorithms clas-\nsify individual queries without considering their context in-\nformation. However, as exemplified by the well-known ex-\nample on query \u201cjaguar\u201d, many Web queries are short and\nambiguous, whose real meanings are uncertain without the\ncontext information. In this paper, we incorporate context\ninformation into the problem of query classification by using\nconditional random field (CRF) models. In our approach,\nwe use neighboring queries and their corresponding clicked\nURLs (Web pages) in search sessions as the context infor-\nmation. We perform extensive experiments on real world\nsearch logs and validate the effectiveness and efficiency of\nour approach. We show that we can improve the F 1 score\nby 52% as compared to other state-of-the-art baselines."}
{"Title": "Deeper Text Understanding for IR with Contextual Neural\nLanguage Modeling", "Abstract": "Neural networks provide new possibilities to automatically learn\ncomplex language patterns and query-document relations. Neu-\nral IR models have achieved promising results in learning query-\ndocument relevance patterns, but few explorations have been done\non understanding the text content of a query or a document. This\npaper studies leveraging a recently-proposed contextual neural lan-\nguage model, BERT, to provide deeper text understanding for IR.\nExperimental results demonstrate that the contextual text represen-\ntations from BERT are more effective than traditional word embed-\ndings. Compared to bag-of-words retrieval models, the contextual\nlanguage model can better leverage language structures, bringing\nlarge improvements on queries written in natural languages. Com-\nbining the text understanding ability with search knowledge leads\nto an enhanced pre-trained BERT model that can benefit related\nsearch tasks where training data are limited."}
{"Title": "Harvesting Drug Effectiveness from Social Media", "Abstract": "Drug effectiveness describes the capacity of a drug to cure a disease,\nwhich is of great importance for drug safety. To get this informa-\ntion,anumberofreal-worldpatient-orientedoutcomesarerequired.\nHowever, current surveillance systems can only capture a small\nportion of them, and there is a time lag in processing the reported\ndata. Since social media provides quantities of patient-oriented\nuser posts in real-time, it is of great value to automatically extract\ndrug effectiveness from these data. To this end, we build a dataset\ncontaining 25K tweets describing drug use, and further harvest\ndrug effectiveness by performing Relation Extraction (RE) between\nchemicals and diseases. Most prior works about RE deal with men-\ntion pairs independently, which is not suitable for our task since\ninteractions across mention pairs are widespread. In this paper, we\npropose a model regarding mention pairs as nodes connected by\nmultiple types of edges. With the help of graph-based information\ntransfers over time, it deals with all mention pairs simultaneously\nto capture their interactions. Besides, a novel idea is used to per-\nform multiple instance learning, a big challenge in general RE tasks.\nExtensive experimental results show that our model outperforms\nprevious work by a substantial margin."}
{"Title": "Investigating Passage-level Relevance and Its Role in\nDocument-level Relevance Judgment", "Abstract": "The understanding of the process of relevance judgment helps to\ninspire the design of retrieval models. Traditional retrieval models\nusually estimate relevance based on document-level signals. Recent\nworks consider a more fne-grain, passage-level relevance informa-\ntion, which can further enhance retrieval performance. However, it\nlacks a detailed analysis of how passage-level relevance signals de-\ntermineorinfuencetherelevancejudgmentofthewholedocument.\nTo investigate the role of passage-level relevance in the document-\nlevel relevance judgment, we construct an ad-hoc retrieval dataset\nwithbothpassage-levelanddocument-levelrelevancelabels.Athor-\nough analysis reveals that: 1) there is a strong correlation between\nthe document-level relevance and the fractions of irrelevant pas-\nsages to highly relevant passages; 2) the position, length and query\nsimilarity of passages play diferent roles in the determination of\ndocument-levelrelevance;3)Thesequentialpassage-levelrelevance\nwithin a document is a potential indicator for the document-level\nrelevance. Based on the relationship between passage-level and\ndocument-level relevance, we also show that utilizing passage-level\nrelevance signals can improve existing document ranking models.\nThis study helps us better understand how users perceive relevance\nfor a document and inspire the designing of novel ranking models\nleveraging fne-grain, passage-level relevance signals."}
{"Title": "Effective Medical Archives Processing Using Knowledge Graphs", "Abstract": "Medical archives processing is a very important task in a\nmedical information system. It generally consists of three\nsteps: medical archives recognition, feature extraction and\ntext classification. In this paper, we focus on empowering the\nmedical archives processing with knowledge graphs. We first\nbuild a semantic-rich medical knowledge graph. Then, we\nrecognize texts from medical archives using several popular\noptical character recognition (OCR) engines, and extract\nkeywords from texts using a knowledge graph based feature\nextraction algorithm. Third, we define a semantic measure\nbased on knowledge graph to evaluate the similarity between\nmedical texts, and perform the text classification task. This\nmeasure can value semantic relatedness between medical\ndocuments, to enhance the text classification. We use medical\narchives collected from real hospitals for validation. The\nresults show that our algorithms can significantly outperform\ntypical baselines that employs only term statistics."}
{"Title": "Personal Knowledge Base Construction from Text-based\nLifelogs", "Abstract": "Previous work on lifelogging focuses on life event extraction from\nimage, audio, and video data via wearable sensors. In contrast to\nwearing an extra camera to record daily life, people are used to\nlog their life on social media platforms. In this paper, we aim to\nextract life events from textual data shared on Twitter and\nconstruct personal knowledge bases of individuals. The issues to\nbe tackled include (1) not all text descriptions are related to life\nevents, (2) life events in a text description can be expressed\nexplicitly or implicitly, (3) the predicates in the implicit events are\noften absent, and (4) the mapping from natural language\npredicates to knowledge base relations may be ambiguous. A joint\nlearning approach is proposed to detect life events in tweets and\nextract event components including subjects, predicates, objects,\nand time expressions. Finally, the extracted information is\ntransformed to knowledge base facts. The evaluation is\nperformed on a collection of lifelogs from 18 Twitter users.\nExperimental results show our proposed system is effective in life\nevent extraction, and the constructed personal knowledge bases\nare expected to be useful to memory recall applications."}
{"Title": "CORALS: Who are My Potential New Customers? Tapping into\nthe Wisdom of Customers\u2019 Decisions", "Abstract": "Identifying and recommending potential new customers for local\nbusinessesare crucialto thesurvivaland successoflocalbusinesses.\nA key component to identifying the right customers is to under-\nstand the decision-making process of choosing a business over the\nothers. However, modeling this process is an extremely challenging\ntask as a decision is influenced by multiple factors. These factors\ninclude but are not limited to an individual\u2019s taste or preference,\nthe location accessibility of a business, and the reputation of a busi-\nness from social media. Most of the recommender systems lack\nthe power to integrate multiple factors together and are hardly\nextensible to accommodate new incoming factors. In this paper, we\nintroduce a unified framework, CORALS, which considers the per-\nsonalpreferencesofdifferentcustomers,thegeographicalinfluence,\nand the reputation of local businesses in the customer recommen-\ndation task. To evaluate the proposed model, we conduct a series of\nexperiments to extensively compare with 12 state-of-the-art meth-\nods using two real-world datasets. The results demonstrate that\nCORALS outperforms all these baselines by a significant margin in\nmost scenarios. In addition to identifying potential new customers,\nwe also break down the analysis for different types of businesses to\nevaluate the impact of various factors that may affect customers\u2019\ndecisions. This information, in return, provides a great resource for\nlocal businesses to adjust their advertising strategies and business\nservices to attract more prospective customers."}
{"Title": "ExFaKT: A Framework for Explaining Facts\nover Knowledge Graphs and Text", "Abstract": "Fact-checking is a crucial task for accurately populating, updat-\ning and curating knowledge graphs. Manually validating candidate\nfactsistime-consuming.Priorworkonautomatingthistaskfocuses\non estimating truthfulness using numerical scores which are not\nhuman-interpretable. Others extract explicit mentions of the can-\ndidate fact in the text as an evidence for the candidate fact, which\ncan be hard to directly spot. In our work, we introduce ExFaKT, a\nframework focused on generating human-comprehensible expla-\nnations for candidate facts. ExFaKT uses background knowledge\nencoded in the form of Horn clauses to rewrite the fact in question\ninto a set of other easier-to-spot facts. The final output of our frame-\nwork is a set of semantic traces for the candidate fact from both\ntext and knowledge graphs. The experiments demonstrate that our\nrewritings significantly increase the recall of fact-spotting while\npreservinghighprecision.Moreover,weshowthattheexplanations\neffectively help humans to perform fact-checking and can also be\nexploited for automating this task."}
{"Title": "Relevance Search over Schema-Rich Knowledge Graphs", "Abstract": "Relevance search over a knowledge graph (KG) has gained much\nresearch attention. Given a query entity in a KG, the problem is\nto find its most relevant entities. However, the relevance function\nis hidden and dynamic. Different users for different queries may\nconsider relevance from different angles of semantics. The ambi-\nguity in a query is more noticeable in the presence of thousands\nof types of entities and relations in a schema-rich KG, which has\nchallenged the effectiveness and scalability of existing methods. To\nmeet the challenge, our approach called RelSUE requests a user\nto provide a small number of answer entities as examples, and\nthen automatically learns the most likely relevance function from\nthese examples. Specifically, we assume the intent of a query can\nbe characterized by a set of meta-paths at the schema level. RelSUE\nsearches a KG for diversified significant meta-paths that best char-\nacterize the relevance of the user-provided examples to the query\nentity. It reduces the large search space of a schema-rich KG using\ndistance and degree-based heuristics, and performs reasoning to\ndeduplicate meta-paths that represent equivalent query-specific\nsemantics. Finally, a linear model is learned to predict meta-path\nbased relevance. Extensive experiments demonstrate that RelSUE\noutperforms several state-of-the-art methods."}
{"Title": "A State Transition Model for Mobile Notifications\nvia Survival Analysis", "Abstract": "Mobilenotificationshave becomeamajor communicationchannel\nforsocialnetworking servicestokeepusersinformedandengaged.\nAs more mobile applications push notifications to users, they con-\nstantly face decisions on what to send, when and how. A lack of\nresearch and methodology commonly leads to heuristic decision\nmaking. Many notifications arrive at an inappropriate moment or\nintroduce too many interruptions, failing to provide value to users\nand spurring users\u2019 complaints. In this paper we explore unique\nfeatures of interactions between mobile notifications and user en-\ngagement. We propose a state transition framework to quantita-\ntivelyevaluatetheeffectiveness ofnotifications.Withinthisframe-\nwork,wedevelopasurvivalmodelforbadgingnotificationsassum-\ning a log-linear structure and a Weibull distribution. Our results\nshow that this model achieves more flexibility for applications and\nsuperior prediction accuracy than a logistic regression model. In\nparticular, we provide an online use case on notification delivery\ntime optimization to show how we make better decisions, drive\nmore user engagement, and provide more value to users."}
{"Title": "Beyond News Contents:\nThe Role of Social Context for Fake News Detection", "Abstract": "Social media is becoming popular for news consumption due to\nits fast dissemination, easy access, and low cost. However, it also\nenables the wide propagation of fake news, i.e., news with intention-\nally false information. Detecting fake news is an important task,\nwhich not only ensures users receive authentic information but\nalso helps maintain a trustworthy news ecosystem. The majority\nof existing detection algorithms focus on finding clues from news\ncontents, which are generally not effective because fake news is of-\nten intentionally written to mislead users by mimicking true news.\nTherefore, we need to explore auxiliary information to improve\ndetection. The social context during news dissemination process\non social media forms the inherent tri-relationship, the relationship\namong publishers, news pieces, and users, which has potential to\nimprove fake news detection. For example, partisan-biased publish-\ners are more likely to publish fake news, and low-credible users\nare more likely to share fake news. In this paper, we study the\nnovel problem of exploiting social context for fake news detection.\nWe propose a tri-relationship embedding framework TriFN, which\nmodels publisher-news relations and user-news interactions simul-\ntaneously for fake news classification. We conduct experiments\non two real-world datasets, which demonstrate that the proposed\napproach significantly outperforms other baseline methods for fake\nnews detection."}
{"Title": "Interactive Anomaly Detection on Attributed Networks", "Abstract": "Performing anomaly detection on attributed networks concerns\nwith finding nodes whose patterns or behaviors deviate signifi-\ncantly from the majority of reference nodes. Its success can be\neasily found in many real-world applications such as network intru-\nsion detection, opinion spam detection and system fault diagnosis,\nto name a few. Despite their empirical success, a vast majority of\nexisting efforts are overwhelmingly performed in an unsupervised\nscenario due to the expensive labeling costs of ground truth anom-\nalies. In fact, in many scenarios, a small amount of prior human\nknowledge of the data is often effortless to obtain, and getting it\ninvolved in the learning process has shown to be effective in ad-\nvancing many important learning tasks. Additionally, since new\ntypes of anomalies may constantly arise over time especially in\nan adversarial environment, the interests of human expert could\nalso change accordingly regarding to the detected anomaly types. It\nbrings further challenges to conventional anomaly detection algo-\nrithms as they are often applied in a batch setting and are incapable\nto interact with the environment. To tackle the above issues, in this\npaper, we investigate the problem of anomaly detection on attrib-\nuted networks in an interactive setting by allowing the system to\nproactively communicate with the human expert in making a lim-\nited number of queries about ground truth anomalies. Our objective\nis to maximize the true anomalies presented to the human expert af-\nter a given budget is used up. Along with this line, we formulate the\nproblem through the principled multi-armed bandit framework and\ndevelop a novel collaborative contextual bandit algorithm, named\nGraphUCB. In particular, our developed algorithm: (1) explicitly\nmodels the nodal attributes and node dependencies seamlessly in\na joint framework; and (2) handles the exploration-exploitation\ndilemma when querying anomalies of different types. Extensive\nexperiments on real-world datasets show the improvement of the\nproposed algorithm over the state-of-the-art algorithms."}
{"Title": "Weakly Supervised Co-Training of Query Rewriting and\nSemantic Matching for e-Commerce", "Abstract": "Relevance is the core problem of a search engine, and one\nof the main challenges is the vocabulary gap between user\nqueries and documents. This problem is more serious in e-\ncommerce, because language in product titles is more profes-\nsional. Query rewriting and semantic matching are two key\ntechniques to bridge the semantic gap between them to im-\nprove relevance. Recently, deep neural networks have been\nsuccessfully applied to the two tasks and enhanced the rel-\nevance performance. However, such approaches suffer from\nthe sparseness of training data in e-commerce scenario. In\nthis study, we investigate the instinctive connection between\nquery rewriting and semantic matching tasks, and propose a\nco-training framework to address the data sparseness prob-\nlem when training deep neural networks. We first build a\nhuge unlabeled dataset from search logs, on which the two\ntasks can be considered as two different views of the rele-\nvance problem. Then we iteratively co-train them via labeled\ndata generated from this unlabeled set to boost their perfor-\nmance simultaneously.\nWe conduct a series of offline and online experiments on a\nreal-world e-commerce search engine, and the results demon-\nstrate that the proposed method improves relevance signifi-\ncantly."}
{"Title": "Review-Driven Answer Generation for Product-Related\nQuestions in E-Commerce", "Abstract": "The users often have many product-related questions before they\nmake a purchase decision in E-commerce. However, it is often\ntime-consuming to examine each user review to identify the de-\nsired information. In this paper, we propose a novel r eview-driven\nframework for a nswer g eneration for product-related questions in\nE -commerce, named RAGE. We develope RAGE on the basis of the\nmulti-layer convolutional architecture to facilitate speed-up of an-\nswer generation with the parallel computation. For each question,\nRAGE first extracts the relevant review snippets from the reviews\nofthecorrespondingproduct.Then,wedeviseamechanismtoiden-\ntify the relevant information from the noise-prone review snippets\nand incorporate this information to guide the answer generation.\nThe experiments on two real-world E-Commerce datasets show\nthat the proposed RAGE significantly outperforms the existing al-\nternatives in producing more accurate and informative answers in\nnatural language. Moreover, RAGE takes much less time for both\nmodel training and answer generation than the existing RNN based\ngeneration models."}
{"Title": "Product-Aware Answer Generation\nin E-Commerce Question-Answering", "Abstract": "Ine-commerceportals,generatinganswersforproduct-relatedques-\ntions has become a crucial task. In this paper, we propose the task of\nproduct-aware answer generation, which tends to generate an accu-\nrate and complete answer from large-scale unlabeled e-commerce\nreviews and product attributes. Unlike existing question-answering\nproblems, answer generation in e-commerce confronts three main\nchallenges: (1) Reviews are informal and noisy; (2) joint modeling\nof reviews and key-value product attributes is challenging; (3) tra-\nditional methods easily generate meaningless answers. To tackle\nabove challenges, we propose an adversarial learning based model,\nnamed PAAG, which is composed of three components: a question-\naware review representation module, a key-value memory network\nencoding attributes, and a recurrent neural network as a sequence\ngenerator. Specifically, we employ a convolutional discriminator to\ndistinguish whether our generated answer matches the facts. To ex-\ntract the salience part of reviews, an attention-based review reader\nis proposed to capture the most relevant words given the question.\nConducted on a large-scale real-world e-commerce dataset, our\nextensive experiments verify the effectiveness of each module in\nour proposed model. Moreover, our experiments show that our\nmodel achieves the state-of-the-art performance in terms of both\nautomatic metrics and human evaluations."}
{"Title": "Learning to Selectively Transfer: Reinforced Transfer Learning\nfor Deep Text Matching", "Abstract": "Deep text matching approaches have been widely studied for many\napplications including question answering and information re-\ntrieval systems. To deal with a domain that has insufficient labeled\ndata, these approaches can be used in a Transfer Learning (TL)\nsetting to leverage labeled data from a resource-rich source domain.\nTo achieve better performance, source domain data selection is\nessential in this process to prevent the \u201cnegative transfer\" problem.\nHowever, the emerging deep transfer models do not fit well with\nmostexistingdataselectionmethods,becausethedataselectionpol-\nicy and the transfer learning model are not jointly trained, leading\nto sub-optimal training efficiency.\nIn this paper, we propose a novel reinforced data selector to\nselect high-quality source domain data to help the TL model. Specif-\nically, the data selector \u201cacts\" on the source domain data to find a\nsubset for optimization of the TL model, and the performance of\nthe TL model can provide \u201crewards\" in turn to update the selec-\ntor. We build the reinforced data selector based on the actor-critic\nframework and integrate it to a DNN based transfer learning model,\nresulting in a Reinforced Transfer Learning (RTL) method. We per-\nform a thorough experimental evaluation on two major tasks for\ntext matching, namely, paraphrase identification and natural lan-\nguage inference. Experimental results show the proposed RTL can\nsignificantly improve the performance of the TL model. We further\ninvestigate different settings of states, rewards, and policy optimiza-\ntion methods to examine the robustness of our method. Last, we\nconduct a case study on the selected data and find our method is\nable to select source domain data whose Wasserstein distance is\nclose to the target domain data. This is reasonable and intuitive as such source domain data can provide more transferability power\nto the model."}
{"Title": "Discovering Correlations between Sparse Features in Distant\nSupervision for Relation Extraction", "Abstract": "The recent art in relation extraction is distant supervision which\ngenerates training data by heuristically aligning a knowledge base\nwith free texts and thus avoids human labelling. However, the\nconcerned relation mentions often use the bag-of-words represen-\ntation, which ignores inner correlations between features located\nin different dimensions and makes relation extraction less effec-\ntive. To capture the complex characteristics of relation expression\nand tighten the correlated features, we attempt to discover and\nutilise informative correlations between features by the following\nfour phases: 1) formulating semantic similarities between lexical\nfeatures using the embedding method; 2) constructing generative\nrelation for lexical features with different sizes of side windows; 3)\ncomputing correlation scores between syntactic features through a\nkernel-based method; and 4) conducting a distillation process for\nthe obtained correlated feature pairs and integrating informative\npairs with existing relation extraction models. The extensive ex-\nperiments demonstrate that our method can effectively discover\ncorrelation information and improve the performance of state-of-\nthe-art relation extraction methods."}
{"Title": "Across-Time Comparative Summarization of News Articles", "Abstract": "Comparative summarization is an effective strategy to discover\nimportant similarities and differences in collections of documents\nbiased to users\u2019 interests. A natural method of this task is to find\nimportant and corresponding content. In this paper, we propose\na novel research task of automatic query-based across-time sum-\nmarization in news archives as well as we introduce an effective\nmethodtosolvethistask.Theproposedmodelfirstlearnsanorthog-\nonal transformation between temporally distant news collections.\nThen, it generates a set of corresponding sentence pairs based on\na concise integer linear programming framework. We experimen-\ntally demonstrate the effectiveness of our method on the New York\nTimes Annotated Corpus."}
{"Title": "CluWords: Exploiting Semantic Word Clustering Representation\nfor Enhanced Topic Modeling", "Abstract": "In this paper, we advance the state-of-the-art in topic modeling\nby means of a new document representation based on pre-trained\nword embeddings for non-probabilistic matrix factorization. Specif-\nically, our strategy, called CluWords, exploits the nearest words\nof a given pre-trained word embedding to generate meta-words\ncapableofenhancingthedocumentrepresentation,intermsofboth,\nsyntactic and semantic information. The novel contributions of our\nsolution include: (i) the introduction of a novel data representation\nfor topic modeling based on syntactic and semantic relationships\nderived from distances calculated within a pre-trained word em-\nbedding space and (ii) the proposal of a new TF-IDF-based strategy,\nparticularly developed to weight the CluWords. In our extensive\nexperimentation evaluation, covering 12 datasets and 8 state-of-\nthe-art baselines, we exceed (with a few ties) in almost cases, with\ngains of more than 50% against the best baselines (achieving up to\n80% against some runner-ups). Finally, we show that our method is\nable to improve document representation for the task of automatic\ntext classification."}
{"Title": "ATAR: A Aspect-Based T Temporal A Analog R Retrieval System for\nDocument Archives", "Abstract": "In recent years, we have witnessed a rapid increase of text con-\ntent stored in digital archives such as newspaper archives or web\narchives. With the passage of time, it is however difficult to effec-\ntively perform search within such collections due to vocabulary\nand context change. In this paper, we present a system that helps to\nfind analogical terms across temporal text collections by applying\nnon-linear transformation. We implement two approaches for ana-\nlog retrieval where one of them allows users to also input an aspect\nterm specifying particular perspective of a query. The current pro-\ntotype system permits temporal analog search across two different\ntime periods based on New York Times Annotated Corpus."}
{"Title": "An Approach to the Problem of Annotation of Research\nPublications", "Abstract": "An approach to multiple labelling research papers is ex-\nplored. We develop techniques for annotating/labelling re-\nsearch papers in informatics and computer sciences with key\nphrases taken from the ACM Computing Classification Sys-\ntem. The techniques utilise a phrase-to-text relevance mea-\nsure so that only those phrases that are most relevant go\nto the annotation. Three phrase-to-text relevance measures\nare experimentally compared in this setting. The measures\nare: (a) cosine relevance score between conventional vector\nspace representations of the texts coded with tf-idf weight-\ning; (b) popular characteristic of probability of term genera-\ntion BM25; and (c) an in-house characteristic of conditional\nprobability of symbols averaged over matching fragments in\nsuffix trees representing texts and phrases, CPAMF. In an\nexperiment conducted over a set of texts published in jour-\nnals of the ACM and manually annotated by their authors,\nCPAMF outperforms both the cosine measure and BM25 by\na wide margin."}
{"Title": "Automatic Gloss Finding for a Knowledge Base\nusing Ontological Constraints", "Abstract": "While there has been much research on automatically construct-\ning structured Knowledge Bases (KBs), most of it has focused on\ngenerating facts to populate a KB. However, a useful KB must go\nbeyond facts. For example, glosses (short natural language defi-\nnitions) have been found to be very useful in tasks such as Word\nSense Disambiguation. However, the important problem of Auto-\nmatic Gloss Finding, i.e., assigning glosses to entities in an ini-\ntially gloss-free KB, is relatively unexplored. We address that gap\nin this paper. In particular, we propose GLOFIN, a hierarchical\nsemi-supervised learning algorithm for this problem which makes\neffective use of limited amounts of supervision and available onto-\nlogical constraints. To the best of our knowledge, GLOFIN is the\nfirst system for this task.\nThroughextensiveexperimentsonreal-worlddatasets, wedemon-\nstrateGLOFIN\u2019seffectiveness. ItisencouragingtoseethatGLOFIN\noutperformsotherstate-of-the-artSSLalgorithms, especiallyinlow\nsupervision settings. We also demonstrate GLOFIN\u2019s robustness to\nnoise through experiments on a wide variety of KBs, ranging from\nuser contributed (e.g., Freebase) to automatically constructed (e.g.,\nNELL). To facilitate further research in this area, we have made the\ndatasets and code used in this paper publicly available."}
{"Title": "Back to the Past: Supporting Interpretations of Forgotten\nStories by Time-aware Re-Contextualization", "Abstract": "Fully understanding an older news article requires context\nknowledge from the time of article creation. Finding infor-\nmation about such context is a tedious and time-consuming\ntask, which distracts the reader. Simple contextualization\nvia Wikification is not sufficient here. The retrieved context\ninformation has to be time-aware, concise (not full Wiki\npages) and focused on the coherence of the article topic.\nIn this paper, we present an approach for time-aware re-\ncontextualization, which takes those requirements into ac-\ncount in order to improve reading experience. For this pur-\npose, we propose (1) different query formulation methods\nfor retrieving contextualization candidates and (2) ranking\nmethods taking into account topical and temporal relevance\nas well as complementarity with respect to the original text.\nWe evaluate our proposed approaches through extensive ex-\nperiments using real-world datasets and ground-truth con-\nsisting of over 9,400 article/context pairs. To this end, our\nexperimental results show that our approaches retrieve con-\ntextualization information for older articles from the New\nYork Times Archive with high precision and outperform\nbaselines significantly."}
{"Title": "Boosting Search with Deep Understanding\nof Contents and Users", "Abstract": "Recent years have witnessed dramatic changes in how people\ninteract with search engines. Search engines are expected to be\nmore intelligent in understanding users\u2019 intention and fulfilling\nusers\u2019 needs with direct answers rather than raw information.\nFurthermore, search engines are expected to be equipped with\nrecommendation and dialogue capabilities, making the interaction\nwith users more natural and smoother. In this talk, I will introduce\nBaidu's work on how to make some of them come true through\nthe deep understanding of users, queries and web pages, and\ndiscuss challenges behind these technologies."}
{"Title": "Concept Graph Learning from Educational Data", "Abstract": "This paper addresses an open challenge in educational data\nmining, i.e., the problem of using observed prerequisite re-\nlations among courses to learn a directed universal con-\ncept graph, and using the induced graph to predict un-\nobserved prerequisite relations among a broader range of\ncourses. This is particularly useful to induce prerequisite\nrelations among courses from different providers (universi-\nties, MOOCs, etc.). We propose a new framework for in-\nference within and across two graphs\u2014at the course level\nand at the induced concept level\u2014which we call Concept\nGraph Learning (CGL). In the training phase, our system\nprojects the course-level links onto the concept space to in-\nduce directed concept links; in the testing phase, the concept\nlinks are used to predict (unobserved) prerequisite links for\ntest-set courses within the same institution or across insti-\ntutions. The dual mappings enable our system to perform\nan interlingua-style transfer learning, e.g. treating the con-\ncept graph as the interlingua, and inducing prerequisite links\nin a transferable manner across different universities. Ex-\nperiments on our newly collected data sets of courses from\nMIT, Caltech, Princeton and CMU show promising results,\nincluding the viability of CGL for transfer learning."}
{"Title": "Distributed Graph Algorithmics: Theory and Practice", "Abstract": "As a fundamental tool in modeling and analyzing social,\nand information networks, large-scale graph mining is an\nimportant component of any tool set for big data analy-\nsis. Processing graphs with hundreds of billions of edges\nis only possible via developing distributed algorithms under\ndistributed graph mining frameworks such as MapReduce,\nPregel, Gigraph, and alike. For these distributed algorithms\nto work well in practice, we need to take into account several\nmetrics such as the number of rounds of computation and\nthe communication complexity of each round. For example,\ngiven the popularity and ease-of-use of MapReduce frame-\nwork, developing practical algorithms with good theoretical\nguarantees for basic graph algorithms is a problem of great\nimportance.\nIn this tutorial, we first discuss how to design and imple-\nment algorithms based on traditional MapReduce architec-\nture. In this regard, we discuss various basic graph theoretic\nproblems such as computing connected components, maxi-\nmum matching, MST, counting triangle and overlapping or\nbalanced clustering. We discuss a computation model for\nMapReduce and describe the sampling, filtering, local ran-\ndom walk, and core-set techniques to develop efficient al-\ngorithms in this framework. At the end, we explore the\npossibility of employing other distributed graph processing\nframeworks. In particular, we study the effect of augment-\ning MapReduce with a distributed hash table (DHT) service\nand also discuss the use of a new graph processing frame-\nwork called ASYMP based on asynchronous message-passing\nmethod. In particular, we will show that using ASyMP, one can improve the CPU usage, and achieve significantly im-\nproved running time."}
{"Title": "Driven by Food: Modeling Geographic Choice", "Abstract": "In this work we study the dynamics of geographic choice, i.e., how\nusers choose one from a set of objects in a geographic region. We\npostulate a model in which an object is selected from a slate of can-\ndidates with probability that depends on how far it is (distance) and\nhow many closer alternatives exist (rank). Under a discrete choice\nformulation, we argue that there exists a factored form in which\nunknown functions of rank and distance may be combined to pro-\nduce an accurate estimate of the likelihood that a user will select\neach alternative. We then learn these hidden functions and show\nthat each can be closely approximated by an appropriately parame-\nterized lognormal, even though the respective marginals look quite\ndifferent. We give a theoretical justification to support the presence\nof lognormal distributions.\nWe then apply this framework to study restaurant choices in map\nsearch logs. We show that a four-parameter model based on com-\nbinations of lognormals has excellent performance at predicting\nrestaurant choice, even compared to baseline models with access to\nthe full (densely parameterized) marginal distribution for rank and\ndistance. Finally, we show how this framework can be extended to\nsimultaneously learn a per-restaurant quality score representing the\nresidual likelihood of choice after distance and rank have been ac-\ncounted for. We show that, compared to a per-place score that pre-\ndicts likelihood without factoring out rank and distance, our score\nis a significantly better predictor of user quality judgments."}
{"Title": "Finding Subgraphs with Maximum Total Density\nand Limited Overlap", "Abstract": "Finding dense subgraphs in large graphs is a key primitive\nin a variety of real-world application domains, encompass-\ning social network analytics, event detection, biology, and\nfinance. In most such applications, one typically aims at\nfinding several (possibly overlapping) dense subgraphs which\nmight correspond to communities in social networks or in-\nteresting events. While a large amount of work is devoted\nto finding a single densest subgraph, perhaps surprisingly,\nthe problem of finding several dense subgraphs with limited\noverlap has not been studied in a principled way, to the best\nof our knowledge. In this work we define and study a natural\ngeneralization of the densest subgraph problem, where the\nmain goal is to find at most k subgraphs with maximum to-\ntal aggregate density, while satisfying an upper bound on the\npairwise Jaccard coefficient between the sets of nodes of the\nsubgraphs. After showing that such a problem is NP-Hard,\nwe devise an efficient algorithm that comes with provable\nguarantees in some cases of interest, as well as, an efficient\npractical heuristic. Our extensive evaluation on large real-\nworld graphs confirms the efficiency and effectiveness of our\nalgorithms."}
{"Title": "FLAME: A Probabilistic Model Combining Aspect Based\nOpinion Mining and Collaborative Filtering", "Abstract": "Aspect-based opinion mining from online reviews has at-\ntracted a lot of attention recently. Given a set of reviews,\nthe main task of aspect-based opinion mining is to extract\nmajor aspects of the items and to infer the latent aspect\nratings from each review. However, users may have differ-\nent preferences which might lead to different opinions on the\nsame aspect of an item. Even if fine-grained aspect rating\nanalysis is provided for each review, it is still difficult for a\nuser to judge whether a specific aspect of an item meets his\nown expectation. In this paper, we study the problem of\nestimating personalized sentiment polarities on different as-\npects of the items. We propose a unified probabilistic model\ncalled Factorized Latent Aspect ModEl (FLAME), which\ncombines the advantages of collaborative filtering and aspect\nbased opinion mining. FLAME learns users\u2019 personalized\npreferences on different aspects from their past reviews, and\npredicts users\u2019 aspect ratings on new items by collective in-\ntelligence. Experiments on two online review datasets show\nthat FLAME outperforms state-of-the-art methods on the\ntasks of aspect identification and aspect rating prediction."}
{"Title": "Global Optimization for Display Ad", "Abstract": "Online display advertisement has been examined by\nnumerous studies. Most online display ad systems take the\ngreedy approach, namely they display, for each user, the\nset of ads that match best with the user\u2019s interests. One\nshortcoming of the greedy approach is that it does not take\ninto account the budget limitation of each advertiser. As a\nresult, we often observed that some ads are popular and\nmatch with the interests of millions of users; but due to the\nbudget restriction, these ads can only be presented by a\nlimited times, leading to a suboptimal performance.\nTo make our point clear, let\u2019s consider a simple case where\nwe only have two advertisers (i.e. A and B), and two users\n(i.e. a and b). We assume that both advertisers have only a\nbudget of one display. We further assume that user a is\ninterested in both ads even though he is more interested in\nad A, while user b is only interested in ad A. Now, if we\ntake the greedy approach, we will always present ad A to\nuser a; as a result, if user a comes before user b, we will\nhave no appropriate ad to be displayed for user b. On the\nother hand, if we can take into account the budget\nlimitation of both advertisers, a better approach is to\npresent ad B to user a and ad A to user b. This simple\nexample motivates us to develop the global optimization\napproach for online display advertisement that explicitly\ntake into account the budget limitation of advertisers when\ndeciding the ad presentation for individual users.\nThe key idea of the proposed approach is to compute a\nuser-ad assignment matrix that maximizes the number of\nclicks under the constraint of ad budgets from individual\nadvertisers. The main computational challenge is the size\nof variable to be optimized: since the number of users and\nadvertisements involved in our system are 1 billion and ten\nthousands, respectively, we need to estimate a matrix of\nbillions times ten thousands. We address this challenge by\nconverting the original optimization problem into its dual\nproblem, in which the number of variables is reduced to\nonly ten thousands. A distributed computing algorithm,\nbased on the Nesterov\u2019s method and map-reduce\nframework, was developed to efficiently solve the related\noptimization problem. We have observed that, the\nproposed  algorithm  significantly  improves  the\neffectiveness of ad presentation compared to the greedy\nalgorithm."}
{"Title": "HIA\u201915: Heterogeneous Information Access Workshop\nat WSDM 2015", "Abstract": "The HIA\u201915 workshop aims to bring together information re-\ntrieval practitioners from industry and academic researchers\nconcerned with heterogeneous information access and search\nfederation. We would like to create a forum to encourage\ndiscussion and exchange of ideas on heterogeneous informa-\ntion access in different contexts. To facilitate the discus-\nsion, we encourage submissions on ideas and results from\ndifferent aspects of heterogeneous information access includ-\ning aggregated search, composite retrieval, personal search,\nstructured search, etc. Another objective of the workshop is\nto encourage submissions with novel ideas (e.g. new appli-\ncations) on heterogeneous information access and potential\nfuture directions of this area."}
{"Title": "Hiring Behavior Models for Online Labor Markets", "Abstract": "In an online labor marketplace employers post jobs, receive\nfreelancer applications and make hiring decisions. These hir-\ning decisions are based on the freelancer\u2019s observed (e.g. ed-\nucation) and latent (e.g. ability) characteristics. Because\nof the heterogeneity that appears in the observed charac-\nteristics, and the existence of latent ones, identifying and\nhiring the best possible applicant is a very challenging task.\nIn this work we study and model the employer\u2019s hiring be-\nhavior. We assume that employers are utility maximizers\nand make rational decisions by hiring the best possible ap-\nplicant at hand. Based on this premise, we propose a se-\nries of probabilistic models that estimate the hiring proba-\nbility of each applicant. We train and test our models on\nmore than 600,000 job applications obtained by oDesk.com,\nand we show evidence that the proposed models outperform\ncurrently in-use baselines. To get further insights, we con-\nduct an econometric analysis and observe that the attributes\nthat are strongly correlated with the hiring probability are\nwhether or not the freelancer and the employer have pre-\nviously worked together, the available information on the\nfreelancer\u2019s profile, the countries of the employer and the\nfreelancer and the skillset of the freelancer. Finally, we find\nthat the faster a freelancer applies to an opening, the higher\nis the probability to get the job."}
{"Title": "Incorporating Phrase-level Sentiment Analysis on Textual\nReviews for Personalized Recommendation", "Abstract": "Previous research on Recommender Systems (RS), especially\nthe continuously popular approach of Collaborative Filtering\n(CF), has been mostly focusing on the information resource\nof explicit user numerical ratings or implicit (still numerical)\nfeedbacks. However, the ever-growing availability of textual\nuser reviews has become an important information resource,\nwhere a wealth of explicit product attributes/features and\nuser attitudes/sentiments are expressed therein. This infor-\nmation rich resource of textual reviews have clearly exhib-\nited brand-new approaches to solving many of the important\nproblems that have been perplexing the research community\nfor years, such as the paradox of cold-start, the explana-\ntion of recommendation, and the automatic generation of\nuser or item profiles. However, it is only recently that the\nfundamental importance of textual reviews has gained wide\nrecognition, perhaps mainly because of the difficulty in for-\nmatting, structuring and analyzing the free-texts. In this\nresearch, we stress the importance of incorporating textual\nreviews for recommendation through phrase-level sentiment\nanalysis, and further investigate the role that the texts play\nin various important recommendation tasks."}
{"Title": "Just in Time Recommendations - Modeling the Dynamics\nof Boredom in Activity Streams", "Abstract": "Recommendation methods have mainly dealt with the problem\nof recommending new items to the user while user visitation be-\nhavior to the familiar items (items which have been consumed\nbefore) are little understood. In this paper, we analyze user ac-\ntivity streams and show that user\u2019s temporal consumption of fa-\nmiliar items is driven by boredom. Specifically, users move on to\na different item when bored and return to the same item when\ntheir interest is restored. To model this behavior we include two\nlatent psychological states of preference for items - sensitization\nand boredom. In the sensitization state the user is highly engaged\nwith the item, while in the boredom state the user is disinterested.\nWe model this behavior using a Hidden Semi-Markov Model for\nthe gaps between user consumption activities. We show that our\nmodel performs much better than the state-of-the-art temporal\nrecommendation models at predicting the revisit time to the item.\nMoreover, we attribute two main reasons for this: (1) recom-\nmending items that are not in the bored state for the user, (2)\nrecommending items where user has restored her interests."}
{"Title": "Learning About Health and Medicine from Internet Data", "Abstract": "Surveys show that around 70% of US Internet users consult\nthe Internet when they require medical information. Peo-\nple seek this information using both traditional search en-\ngines and via social media. The information created using\nthe search process offers an unprecedented opportunity for\napplications to monitor and improve the quality of life of\npeople with a variety of medical conditions. In recent years,\nresearch in this area has addressed public-health questions\nsuch as the effect of media on development of anorexia, de-\nveloped tools for measuring influenza rates and assessing\ndrug safety, and examined the effects of health information\non individual wellbeing. This tutorial will show how Internet\ndata can facilitate medical research, providing an overview of\nthe state-of-the-art in this area. During the tutorial we will\ndiscuss the information which can be gleaned from a variety\nof Internet data sources, including social media, search en-\ngines, and specialized medical websites. We will provide an\noverview of analysis methods used in recent literature, and\nshow how results can be evaluated using publicly-available\nhealth information and online experimentation. Finally, we\nwill discuss ethical and privacy issues and possible techno-\nlogical solutions. This tutorial is intended for researchers of\nuser generated content who are interested in applying their\nknowledge to improve health and medicine."}
{"Title": "Leveraging In-Batch Annotation Bias\nfor Crowdsourced Active Learning", "Abstract": "Data annotation bias is found in many situations. Often\nit can be ignored as just another component of the noise\nfloor. However, it is especially prevalent in crowdsourcing\ntasks and must be actively managed. Annotation bias on\nsingle data items has been studied with regard to data diffi-\nculty, annotator bias, etc., while annotation bias on batches\nof multiple data items simultaneously presented to anno-\ntators has not been studied. In this paper, we verify the\nexistence of \u201cin-batch annotation bias\u201d between data items\nin the same batch. We propose a factor graph based batch\nannotation model to quantitatively capture the in-batch an-\nnotation bias, and measure the bias during a crowdsourcing\nannotation process of inappropriate comments in LinkedIn.\nWe discover that annotators tend to make polarized annota-\ntions for the entire batch of data items in our task. We fur-\nther leverage the batch annotation model to propose a novel\nbatch active learning algorithm. We test the algorithm on\na real crowdsourcing platform and find that it outperforms\nin-batch bias na\u00a8\u0131ve algorithms."}
{"Title": "Modeling and Predicting Retweeting Dynamics on\nMicroblogging Platforms", "Abstract": "Popularity prediction on microblogging platforms aims to\npredict the future popularity of a message based on its\nretweeting dynamics in the early stages. Existing works\nmainly focus on exploring effective features for prediction,\nwhile ignoring the underlying arrival process of retweets.\nAlso, the effect of user activity variation on the retweeting\ndynamics in the early stages has been neglected. In this\npaper, we propose an extended reinforced Poisson process\nmodel with time mapping process to model the retweeting\ndynamics and predict the future popularity. The proposed\nmodel explicitly characterizes the process through which\na message gain its retweets, by capturing a power-law\ntemporal relaxation function corresponding to the aging\nin the ability of the message to attract new retweets and\nan exponential reinforcement mechanism characterizing the\n\u201cricher-get-richer\u201d phenomenon. Further, we introduce the\nnotation of weibo time and integrate a time mapping process\ninto the proposed model to eliminate the effect of user\nactivity variation. Extensive experiments on two Weibo\ndatasets, with 10K and 18K messages respectively, well\ndemonstrate the effectiveness of our proposed model in\npopularity prediction."}
{"Title": "Listwise Approach for Rank Aggregation in Crowdsourcing", "Abstract": "Inferring a gold-standard ranking over a set of objects, such\nas documents or images, is a key task to build test collections\nfor various applications like Web search and recommender\nsystems. Crowdsourcing services provide an efficient and in-\nexpensive way to collect judgments via labeling by sets of\nannotators. We thus study the problem of finding a con-\nsensus ranking from crowdsourced judgments. In contrast\nto conventional rank aggregation methods which minimize\nthe distance between predicted ranking and input judgments\nfrom either pointwise or pairwise perspective, we argue that\nit is critical to consider the distance in a listwise way to\nemphasize the position importance in ranking. Therefore,\nwe introduce a new listwise approach in this paper, where\nranking measure based objective functions are utilized for\noptimization. In addition, we also incorporate the annota-\ntor quality into our model since the reliability of annotators\ncan vary significantly in crowdsourcing. For optimization,\nwe transform the optimization problem to the Linear Sum\nAssignment Problem, and then solve it by a very efficient\nalgorithm named CrowdAgg guaranteeing the optimal so-\nlution. Experimental results on two benchmark data sets\nfrom different crowdsourcing tasks show that our algorithm\nis much more effective, efficient and robust than traditional\nmethods."}
{"Title": "Modeling Website Popularity Competition in the\nAttention-Activity Marketplace", "Abstract": "How does a new startup drive the popularity of compet-\ning websites into oblivion like Facebook famously did to\nMySpace? This question is of great interest to academics,\ntechnologists, and financial investors alike. In this work\nwe exploit the singular way in which Facebook wiped out\nthe popularity of MySpace, Hi5, Friendster, and Multiply\nto guide the design of a new popularity competition model.\nOur model provides new insights into what Nobel Laure-\nate Herbert A. Simon called the\u201cmarketplace of attention,\u201d\nwhich we recast as the attention-activity marketplace. Our\nmodel design is further substantiated by user-level activity\nof 250,000 MySpace users obtained between 2004 and 2009.\nThe resulting model not only accurately fits the observed\nDaily Active Users (DAU) of Facebook and its competitors\nbut also predicts their fate four years into the future."}
{"Title": "Negative Link Prediction in Social Media", "Abstract": "Signed network analysis has attracted increasing attention in\nrecent years. This is in part because research on signed net-\nwork analysis suggests that negative links have added value\nin the analytical process. A major impediment in their effec-\ntive use is that most social media sites do not enable users\nto specify them explicitly. In other words, a gap exists be-\ntween the importance of negative links and their availability\nin real data sets. Therefore, it is natural to explore whether\none can predict negative links automatically from the com-\nmonly available social network data. In this paper, we in-\nvestigate the novel problem of negative link prediction with\nonly positive links and content-centric interactions in social\nmedia. We make a number of important observations about\nnegative links, and propose a principled framework NeLP,\nwhich can exploit positive links and content-centric interac-\ntions to predict negative links. Our experimental results on\nreal-world social networks demonstrate that the proposed\nNeLP framework can accurately predict negative links with\npositive links and content-centric interactions. Our detailed\nexperiments also illustrate the relative importance of various\nfactors to the effectiveness of the proposed framework."}
{"Title": "Offline Evaluation and Optimization for Interactive Systems", "Abstract": "Evaluating and optimizing an interactive system (like\nsearch engines, recommender and advertising systems) from\nhistorical data against a predefined online metric is chal-\nlenging, especially when that metric is computed from user\nfeedback such as clicks and payments. The key challenge is\ncounterfactual in nature: we only observe a user\u2019s feed-\nback for actions taken by the system, but we do not know\nwhat that user would have reacted to a different action.\nThe golden standard to evaluate such metrics of a user-\ninteracting system is online A/B experiments (a.k.a. ran-\ndomized controlled experiments), which can be expensive\nin terms of both time and engineering resources. Offline\nevaluation/optimization (sometimes referred to as off-policy\nlearning in the literature) thus becomes critical, aiming to\nevaluate the same metrics without running (many) expensive\nA/B experiments on live users.\nOne approach to offline evaluation is to build a user model\nthat simulates user behavior (clicks, purchases, etc.) under\nvarious contexts, and then evaluate metrics of a system with\nthis simulator. While being straightforward and common in\npractice, the reliability of such model-based approaches re-\nlies heavily on how well the user model is built. Furthermore,it is often difficult to know a priori whether a user model is\ngood enough to be trustable.\nRecent years have seen a growing interest in another so-\nlution to the offline evaluation problem. Using statistical\ntechniques like importance sampling and doubly robust es-\ntimation, the approach can give unbiased estimates of met-\nrics for a wide range of problems. It enjoys other benefits as\nwell. For example, it often allows data scientists to obtain a\nconfidence interval for the estimate to quantify the amount\nof uncertainty; it does not require building user models, so\nis more robust and easier to apply. All these benefits make\nthe approach particularly attractive to a wide range of prob-\nlems. Successful applications have been reported in the last\nfew years by some of the industrial leaders.\nThis tutorial gives a review of the basic theory and rep-\nresentative techniques. Applications of these techniques are\nillustrated through several case studies done at Microsoft\nand Yahoo!."}
{"Title": "On Integrating Network and Community Discovery", "Abstract": "The problem of community detection has recently been stud-\nied widely in the context of the web and social media net-\nworks. Most algorithms for community detection assume\nthat the entire network is available for online analysis. In\npractice, this is not really true, because only restricted por-\ntions of the network may be available at any given time for\nanalysis. Many social networks such as Facebook have pri-\nvacy constraints, which do not allow the discovery of the en-\ntire structure of the social network. Even in the case of more\nopen networks such as Twitter, it may often be challenging\nto crawl the entire network from a practical perspective. For\nmany other scenarios such as adversarial networks, the dis-\ncovery of the entire network may itself be a costly task, and\nonly a small portion of the network may be discovered at any\ngiven time. Therefore, it can be useful to investigate whether\nnetwork mining algorithms can integrate the network discov-\nery process tightly into the mining process, so that the best\nresults are achieved for particular constraints on discovery\ncosts. In this context, we will discuss algorithms for inte-\ngrating community detection with network discovery. We\nwill tightly integrate with the cost of actually discovering a\nnetwork with the community detection process, so that the\ntwo processes can support each other and are performed in\na mutually cohesive way. We present experimental results\nillustrating the advantages of the approach"}
{"Title": "On Tag Recommendation for Expertise Profiling:\nA Case Study in the Scientific Domain", "Abstract": "Building expertise profiles is a crucial step towards iden-\ntifying experts in different knowledge areas. However, sum-\nmarizing the topics of expertise of a given individual is a\nchallenging task, primarily due to the semi-structured and\nheterogeneous nature of the documentary evidence available\nfor this task. In this paper, we investigate the suitability of\ntag recommendation as a mechanism to produce effective ex-\npertise profiles. In particular, we perform a large-scale user\nstudy with academic experts from different knowledge areas\nto assess the effectiveness of multiple supervised and unsu-\npervised tag recommendation approaches as well as multiple\nsources of textual evidence. Our analysis reveals that tradi-\ntional content-based tag recommenders perform well at iden-\ntifying expertise-oriented tags, with article keywords being\na particularly effective source of evidence across profiles in\ndifferent knowledge areas and with various levels of sparsity.\nMoreover, by combining multiple recommenders and sources\nof evidence as learning signals, we further demonstrate the\neffectiveness of tag recommendation for expertise profiling."}
{"Title": "Pairwise Ranking Aggregation in a Crowdsourced Setting", "Abstract": "Inferring rankings over elements of a set of objects, such as\ndocuments or images, is a key learning problem for such im-\nportant applications as Web search and recommender sys-\ntems. Crowdsourcing services provide an inexpensive and\nefficient means to acquire preferences over objects via la-\nbeling by sets of annotators. We propose a new model to\npredict a gold-standard ranking that hinges on combining\npairwise comparisons via crowdsourcing. In contrast to tra-\nditional ranking aggregation methods, the approach learns\nabout and folds into consideration the quality of contribu-\ntions of each annotator. In addition, we minimize the cost of\nassessment by introducing a generalization of the traditional\nactive learning scenario to jointly select the annotator and\npair to assess while taking into account the annotator quali-\nty, the uncertainty over ordering of the pair, and the current\nmodel uncertainty. We formalize this as an active learn-\ning strategy that incorporates an exploration-exploitation\ntradeoff and implement it using an efficient online Bayesian\nupdating scheme. Using simulated and real-world data, we\ndemonstrate that the active learning strategy achieves sig-\nnificant reductions in labeling cost while maintaining accu-\nracy."}
{"Title": "Personalized Mobile App Recommendation: Reconciling\nApp Functionality and User Privacy Preference", "Abstract": "Recent years have witnessed a rapid adoption of mobile de-\nvices and a dramatic proliferation of mobile applications\n(Apps for brevity). However, the large number of mobile\nApps makes it difficult for users to locate relevant Apps.\nTherefore, recommending Apps becomes an urgent task.\nTraditional recommendation approaches focus on learning\nthe interest of a user and the functionality of an item (e.g.,\nan App) from a set of user-item ratings, and they recom-\nmend an item to a user if the item\u2019s functionality well matches\nthe user\u2019s interest. However, Apps could have privileges to\naccess a user\u2019s sensitive resources (e.g., contact, message,\nand location). As a result, a user chooses an App not only\nbecause of its functionality, but also because it respects the\nuser\u2019s privacy preference.\nTo the best of our knowledge, this paper presents the first\nsystematic study on incorporating both interest-functionality\ninteractions and users\u2019 privacy preferences to perform per-\nsonalized App recommendations. Specifically, we first con-\nstruct a new model to capture the trade-off between func-\ntionality and user privacy preference. Then we crawled a\nreal-world dataset (16,344 users, 6,157 Apps, and 263,054\nratings) from Google Play and use it to comprehensively\nevaluate our model and previous methods. We find that\nour method consistently and substantially outperforms the\nstate-of-the-art approaches, which implies the importance of\nuser privacy preference on personalized App recommenda-\ntions. Moreover, we explore the impact of different levels\nof privacy information on the performances of our method,\nwhich gives us insights on what resources are more likely to\nbe treated as private by users and influence users\u2019 behaviors\nat selecting Apps."}
{"Title": "Predicting The Next App That You Are Going To Use", "Abstract": "Given the large number of installed apps and the limited\nscreen size of mobile devices, it is often tedious for users to\nsearch for the app they want to use. Although some mo-\nbile OSs provide categorization schemes that enhance the\nvisibility of useful apps among those installed, the emerging\ncategory of homescreen apps aims to take one step further by\nautomatically organizing the installed apps in a more intel-\nligent and personalized way. In this paper, we study how to\nimprove homescreen apps\u2019 usage experience through a pre-\ndiction mechanism that allows to show to users which app\nshe is going to use in the immediate future. The prediction\ntechnique is based on a set of features representing the real-\ntime spatiotemporal contexts sensed by the homescreen app.\nWe model the prediction of the next app as a classification\nproblem and propose an effective personalized method to\nsolve it that takes full advantage of human-engineered fea-\ntures and automatically derived features. Furthermore, we\nstudy how to solve the two naturally associated cold-start\nproblems: app cold-start and user cold-start. We conduct\nlarge-scale experiments on log data obtained from Yahoo\nAviate, showing that our approach can accurately predict\nthe next app that a person is going to use."}
{"Title": "Real-Time Bidding: A New Frontier of\nComputational Advertising Research", "Abstract": "In display and mobile advertising, the most significant de-\nvelopment in recent years is the Real-Time Bidding (RTB),\nwhich allows selling and buying in real-time one ad impres-\nsion at a time. Since then, RTB has fundamentally changed\nthe landscape of the digital marketing by scaling the buy-\ning process across a large number of available inventories.\nThe demand for automation, integration and optimisation in\nRTB brings new research opportunities in the IR/DM/ML\nfields. However, despite its rapid growth and huge poten-\ntial, many aspects of RTB remain unknown to the research\ncommunity for many reasons. In this tutorial, together with\ninvited distinguished speakers from online advertising indus-\ntry, we aim to bring the insightful knowledge from the real-\nworld systems to bridge the gaps and provide an overview\nof the fundamental infrastructure, algorithms, and techni-\ncal and research challenges of this new frontier of computa-\ntional advertising. We will also introduce to researchers the\ndatasets, tools, and platforms which are publicly available\nthus they can get hands-on quickly."}
{"Title": "Regressing Towards Simpler Prediction Systems", "Abstract": "This talk will focus on our experience in managing the\ncomplexity of Sibyl, a large scale machine learning system\nthat is widely used within Google. We believe that a large\nfraction of the challenges faced by Sibyl are inherent to\nlarge scale production machine learning and that other\nproduction systems are likely to encounter them as well[1].\nThus, these challenges present interesting opportunities for\nfuture research.\nThe Sibyl system is complex for a number of reasons.\nWe have learnt that a complete end-to-end machine learning\nsolution has to have subsystems to address a variety of\ndifferent needs: data ingestion, data analysis, data\nverification, experimentation, model analysis, model\nserving, configuration, data transformations, support for\ndifferent kinds of loss functions and modeling, machine\nlearning algorithm implementations, etc. Machine learning\nalgorithms themselves constitute a relatively small fraction\nof the overall system.\nEach subsystem consists of a number of distinct components\nto support the variety of product needs. For example, Sibyl\nsupports more than 5 different model serving systems, each\nwith its own idiosyncrasies and challenges. In addition,\nSibyl configuration contains more lines of code than the\ncore Sibyl learner itself. Finally existing solutions for some\nof the challenges don't feel adequate and we believe these\nchallenges present opportunities for future research.\nThough the overall system is complex, our users need to be\nable to deploy solutions quickly. This is because a machine\nlearning deployment is typically an iterative process\nof model improvements. At each iteration, our users\nexperiment with new features, find those that improve the\nmodel's prediction capability, and then \u201claunch\u201d a new\nmodel with those improved features. A user may go through\n10 or more such productive launches. Not only is speed of\niteration crucial to our users, but they are often willing to\nsacrifice the improved prediction quality of a high quality\nbut cumbersome system for the speed of iteration of a lower\nquality but nimble system.\nIn this talk I will give an example of how simplification\ndrives systems design and sometimes the design of novel\nalgorithms."}
{"Title": "Sarcasm Detection on Twitter:\nA Behavioral Modeling Approach", "Abstract": "Sarcasm is a nuanced form of language in which individuals\nstate the opposite of what is implied. With this intentional\nambiguity, sarcasm detection has always been a challenging\ntask, even for humans. Current approaches to automatic sar-\ncasm detection rely primarily on lexical and linguistic cues.\nThis paper aims to address the difficult task of sarcasm de-\ntection on Twitter by leveraging behavioral traits intrinsic\nto users expressing sarcasm. We identify such traits using\nthe user\u2019s past tweets. We employ theories from behavioral\nand psychological studies to construct a behavioral mod-\neling framework tuned for detecting sarcasm. We evaluate\nour framework and demonstrate its efficiency in identifying\nsarcastic tweets."}
{"Title": "WSDM\u201915 Workshop Summary / Scalable Data Analytics:\nTheory and Applications", "Abstract": "The SDA workshop at WSDM 2015 is the fifth International\nWorkshop on Scalable Data Analytics, following the previ-\nous four workshops of SDA respectively held at IEEE Big\nData 2013, PAKDD 2014, IEEE Big Data 2014, and IEEE\nICDM 2014. This series of workshops aims to provide pro-\nfessionals, researchers, and technologists with a single forum\nwhere they can discuss and share the state-of-the-art theo-\nries and applications of scalable data analytics technologies.\nIn particular, in the era of information explosion, the sci-\nentific, biomedical, and engineering research communities\nare undergoing a profound transformation where discover-\nies and innovations increasingly rely on massive amounts of\ndata. The characteristics of volume, velocity, variety and\nveracity originated in the massive big data then bring chal-\nlenges to current data analytics techniques. The focus of the\nfifth SDA is to discuss how we can scale up data analytics\ntechniques for modeling and analyzing big data from various\ndomains."}
{"Title": "Semantic Matching in APP Search", "Abstract": "Past years, with the growth of smart-phones and applications,\nAPP market has become an important mobile internet portal. As\nan important function in application market, APP search gains lots\nof attentions.However, mismatch between queries and APP is the\nmost critical problem in APP search because of less text within\nterm matching search engine.\nIn this talk, we describe a semantic matching architecture in APP\nsearch\uff0cwhich mining topics and tags in big data. It enriches\nquery and APP representations with topics and tags to achieve\nsemantic matching in search.\nSome challenge must be considered:\n1) How to extract tag-APP relationship from large web text.\n2) How to use machine learning technologies to process de-\nnoising and computing confidence.\n3) How to hybrid ranking apps retrieved by different matching\nmethod.\nThese will be introduced in some of our related works and as\nexamples to describe how semantic matching is used in Tencent\nMyApp, an application market which serving hundreds of\nmillions of users."}
{"Title": "Sentiment-Specific Representation Learning for\nDocument-Level Sentiment Analysis", "Abstract": "In this paper, we propose a representation learning research\nframework for document-level sentiment analysis. Given a\ndocument as the input, document-level sentiment analysis\naims to automatically classify its sentiment/opinion (such\nas thumbs up or thumbs down) based on the textural in-\nformation. Despite the success of feature engineering in\nmany previous studies, the hand-coded features do not well\ncapture the semantics of texts. In this research, we argue\nthat learning sentiment-specific semantic representations of\ndocuments is crucial for document-level sentiment analysis.\nWe decompose the document semantics into four cascad-\ned constitutes: (1) word representation, (2) sentence struc-\nture, (3) sentence composition and (4) document compo-\nsition. Specifically, we learn sentiment-specific word rep-\nresentations, which simultaneously encode the contexts of\nwords and the sentiment supervisions of texts into the con-\ntinuous representation space. According to the principle of\ncompositionality, we learn sentiment-specific sentence struc-\ntures and sentence-level composition functions to produce\nthe representation of each sentence based on the representa-\ntions of the words it contains. The semantic representations\nof documents are obtained through document composition,\nwhich leverages the sentiment-sensitive discourse relations\nand sentence representations."}
{"Title": "SimApp: A Framework for Detecting Similar Mobile\nApplications by Online Kernel Learning", "Abstract": "With the popularity of smart phones and mobile devices,\nthe number of mobile applications (a.k.a. \u201capps\u201d) has been\ngrowing rapidly. Detecting semantically similar apps from\na large pool of apps is a basic and important problem, as\nit is beneficial for various applications, such as app recom-\nmendation, app search, etc. However, there is no systematic\nand comprehensive work so far that focuses on addressing\nthis problem. In order to fill this gap, in this paper, we ex-\nplore multi-modal heterogeneous data in app markets (e.g.,\ndescription text, images, user reviews, etc.), and present\n\u201cSimApp\u201d\u2013 a novel framework for detecting similar apps us-\ning machine learning. Specifically, it consists of two stages:\n(i) a variety of kernel functions are constructed to measure\napp similarity for each modality of data; and (ii) an online\nkernel learning algorithm is proposed to learn the optimal\ncombination of similarity functions of multiple modalities.\nWe conduct an extensive set of experiments on a real-world\ndataset crawled from Google Play to evaluate SimApp, from\nwhich the encouraging results demonstrate that SimApp is\neffective and promising."}
{"Title": "Topics, Tasks & Beyond: Learning Representations for\nPersonalization", "Abstract": "Accurate understanding of a user\u2019s interests, preferences and\nbehaviours is possibly one of the most critical research chal-\nlenges faced while developing personalized systems for be-\nhavior targeting and information access. We intend to de-\nvelop comprehensive latent variable models for web search\npersonalization which jointly models user\u2019s topical interests\nalong with user\u2019s click based relevance preferences while at\nthe same time taking into account user\u2019s intended search\ntasks along with information about other similar users. We\nfurther augment this model by incorporating topic-level rel-\nevance parameters, which, to the best of our knowledge, is\nthe first attempt at modeling result ranking preferences at\nthe topic level. Additionally, we intend to explore the pos-\nsibility of modeling users in terms of the search tasks they\nperform thereby coupling users\u2019 topical interests with their\nsearch task behavior to learn user representations. Finally,\nwe wish to evaluate the proposition of extending user rep-\nresentations to hierarchical structures as an alternative to\nexisting flat representations. The evaluation of these alter-\nnative approaches for user modeling is based on their per-\nformance on a variety of tasks such as collaborative query\nrecommendations, user cohort modeling and search result\npersonalization. This proposal provides the motivation to\npursue these research directions, summarizes key research\nproblems being targeted, glances through potential ways of\ntackling these research challenges and highlights some initial\nresults obtained."}
{"Title": "User Modeling for a Personal Assistant", "Abstract": "We present a user modeling system that serves as the foun-\ndation of a personal assistant. The system ingests web\nsearch history for signed-in users, and identifies coherent\ncontexts that correspond to tasks, interests, and habits. Un-\nlike past work which focused on either in-session tasks or\ntasks over a few days, we look at several months of his-\ntory in order to identify not just short-term tasks, but also\nlong-term interests and habits. The features we use for iden-\ntifying coherent contexts yield substantially higher precision\nand recall than past work. We also present an algorithm for\nidentifying contexts that is 8 to 30 times faster than previous\nalgorithms. The user modeling system has been deployed in\nproduction. It runs over hundreds of millions of users, and\nupdates the models with a 10-minute latency. The contexts\nidentified by the system serve as the foundation for gener-\nating recommendations in Google Now."}
{"Title": "WorkerRank: Using Employer Implicit Judgements To Infer\nWorker Reputation", "Abstract": "In online labor marketplaces two parties are involved; employers\nand workers. An employer posts a job in the marketplace to receive\napplications from interested workers. After evaluating the match to\nthe job, the employer hires one (or more workers) to accomplish the\njob via an online contract. At the end of the contract, the employer\ncan provide his worker with some rating that becomes visible in the\nworker online profile. This form of explicit feedback guides future\nhiring decisions, since it is indicative of worker true ability. In this\npaper, first we discuss some of the shortcomings of the existing\nreputation systems that are based on the end-of-contract ratings.\nThen we propose a new reputation mechanism that uses Bayesian\nupdates to combine employer implicit feedback signals in a link-\nanalysis approach. The new system addresses the shortcomings\nof existing approaches, while yielding better signal for the worker\nquality towards hiring decision."}
{"Title": "Driven by Food: Modeling Geographic Choice", "Abstract": "In this work we study the dynamics of geographic choice, i.e., how\nusers choose one from a set of objects in a geographic region. We\npostulate a model in which an object is selected from a slate of can-\ndidates with probability that depends on how far it is (distance) and\nhow many closer alternatives exist (rank). Under a discrete choice\nformulation, we argue that there exists a factored form in which\nunknown functions of rank and distance may be combined to pro-\nduce an accurate estimate of the likelihood that a user will select\neach alternative. We then learn these hidden functions and show\nthat each can be closely approximated by an appropriately parame-\nterized lognormal, even though the respective marginals look quite\ndifferent. We give a theoretical justification to support the presence\nof lognormal distributions.\nWe then apply this framework to study restaurant choices in map\nsearch logs. We show that a four-parameter model based on com-\nbinations of lognormals has excellent performance at predicting\nrestaurant choice, even compared to baseline models with access to\nthe full (densely parameterized) marginal distribution for rank and\ndistance. Finally, we show how this framework can be extended to\nsimultaneously learn a per-restaurant quality score representing the\nresidual likelihood of choice after distance and rank have been ac-\ncounted for. We show that, compared to a per-place score that pre-\ndicts likelihood without factoring out rank and distance, our score\nis a significantly better predictor of user quality judgments."}
{"Title": "You Are Where You Go: Inferring Demographic Attributes\nfrom Location Check-ins", "Abstract": "User profiling is crucial to many online services. Several recent\nstudiessuggestthatdemographicattributesarepredictablefromdif-\nferent online behavioral data, such as users\u2019 \u201cLikes\u201d on Facebook,\nfriendship relations, and the linguistic characteristics of tweets. But\nlocation check-ins, as a bridge of users\u2019 offline and online lives,\nhave by and large been overlooked in inferring user profiles.\nIn this paper, we investigate the predictive power of location\ncheck-ins for inferring users\u2019 demographics and propose a simple\nyet general location to profile (L2P) framework. More specifically,\nwe extract rich semantics of users\u2019 check-ins in terms of spatiality,\ntemporality, and location knowledge, where the location knowledge\nis enriched with semantics mined from heterogeneous domains in-\ncluding both online customer review sites and social networks. Ad-\nditionally, tensor factorization is employed to draw out low dimen-\nsional representations of users\u2019 intrinsic check-in preferences con-\nsidering the above factors. Meanwhile, the extracted features are\nused to train predictive models for inferring various demographic\nattributes.\nWe collect a large dataset consisting of profiles of 159,530 ver-\nified users from an online social network. Extensive experimental\nresults based upon this dataset validate that: 1) Location check-\nins are diagnostic representations of a variety of demographic at-\ntributes, such as gender, age, education background, and marital\nstatus; 2) The proposed framework substantially outperforms com-\npared models for profile inference in terms of various evaluation\nmetrics, such as precision, recall, F-measure, and AUC."}
{"Title": "EgoSet: Exploiting Word Ego-networks and\nUser-generated Ontology for Multifaceted Set Expansion", "Abstract": "A key challenge of entity set expansion is that multifaceted input\nseeds can lead to significant incoherence in the result set. In this pa-\nper, we present a novel solution to handling multifaceted seeds by\ncombining existing user-generated ontologies with a novel word-\nsimilarity metric based on skip-grams. By blending the two re-\nsources we are able to produce sparse word ego-networks that are\ncentered on the seed terms and are able to capture semantic equiv-\nalence among words. We demonstrate that the resulting networks\npossess internally-coherent clusters, which can be exploited to pro-\nvide non-overlapping expansions, in order to reflect different se-\nmantic classes of the seeds. Empirical evaluation against state-of-\nthe-art baselines shows that our solution, EgoSet, is able to not only\ncapture multiple facets in the input query, but also generate expan-\nsions for each facet with higher precision."}
{"Title": "Barbara Made the News: Mining the Behavior of Crowds\nfor Time-Aware Learning to Rank", "Abstract": "In Twitter, and other microblogging services, the generation\nof new content by the crowd is often biased towards immedi-\nacy: what is happening now. Prompted by the propagation\nof commentary and information through multiple mediums,\nusers on the Web interact with and produce new posts about\nnewsworthy topics and give rise to trending topics. This\npaper proposes to leverage on the behavioral dynamics of\nusers to estimate the most relevant time periods for a topic.\nOur hypothesis stems from the fact that when a real-world\nevent occurs it usually has peak times on the Web: a higher\nvolume of tweets, new visits and edits to related Wikipedia\narticles, and news published about the event.\nIn this paper, we propose a novel time-aware ranking\nmodel that leverages on multiple sources of crowd signals.\nOur approach builds on two major novelties. First, a unifying\napproach that given query q , mines and represents tempo-\nral evidence from multiple sources of crowd signals. This\nallows us to predict the temporal relevance of documents for\nquery q . Second, a principled retrieval model that integrates\ntemporal signals in a learning to rank framework, to rank\nresults according to the predicted temporal relevance. Evalu-\nation on the TREC 2013 and 2014 Microblog track datasets\ndemonstrates that the proposed model achieves a relative\nimprovement of 13.2% over lexical retrieval models and 6.2%\nover a learning to rank baseline."}
{"Title": "Exploiting New Sentiment-Based Meta-leve", "Abstract": "In this paper we address the problem of automatically learn-\ning to classify the sentiment of short messages/reviews by\nexploiting information derived from meta-level features i.e.,\nfeatures derived primarily from the original bag-of-words\nrepresentation. We propose new meta-level features espe-\ncially designed for the sentiment analysis of short messages\nsuch as: (i) information derived from the sentiment distri-\nbution among the k nearest neighbors of a given short test\ndocument x, (ii) the distribution of distances of x to their\nneighbors and (iii) the document polarity of these neighbors\ngiven by unsupervised lexical-based methods. Our approach\nis also capable of exploiting information from the neighbor-\nhood of document x regarding (highly noisy) data obtained\nfrom 1.6 million Twitter messages with emoticons. The set\nof proposed features is capable of transforming the original\nfeature space into a new one, potentially smaller and more\ninformed. Experiments performed with a substantial num-\nber of datasets (nineteen) demonstrate that the effectiveness\nof the proposed sentiment-based meta-level features is not\nonly superior to the traditional bag-of-word representation\n(by up to 16%) but is also superior in most cases to state-of-\nart meta-level features previously proposed in the literature\nfor text classification tasks that do not take into account\nsome idiosyncrasies of sentiment analysis. Our proposal is\nalso largely superior to the best lexicon-based methods as\nwell as to supervised combinations of them. In fact, the\nproposed approach is the only one to produce the best re-\nsults in all tested datasets in all scenarios."}
{"Title": "Feedback Control of Real-Time Display Advertising", "Abstract": "Real-Time Bidding (RTB) is revolutionising display adver-\ntising by facilitating per-impression auctions to buy ad im-\npressions as they are being generated. Being able to use\nimpression-level data, such as user cookies, encourages user\nbehaviour targeting, and hence has significantly improved\nthe effectiveness of ad campaigns. However, a fundamental\ndrawback of RTB is its instability because the bid decision\nis made per impression and there are enormous fluctuations\nin campaigns\u2019 key performance indicators (KPIs). As such,\nadvertisers face great difficulty in controlling their campaign\nperformance against the associated costs. In this paper, we\npropose a feedback control mechanism for RTB which helps\nadvertisers dynamically adjust the bids to effectively control\nthe KPIs, e.g., the auction winning ratio and the effective\ncost per click. We further formulate an optimisation frame-\nwork to show that the proposed feedback control mechanism\nalso has the ability of optimising campaign performance. By\nsettling the effective cost per click at an optimal reference\nvalue, the number of campaign\u2019s ad clicks can be maximised\nwith the budget constraint. Our empirical study based on\nreal-world data verifies the effectiveness and robustness of\nour RTB control system in various situations. The proposed\nfeedback control mechanism has also been deployed on a\ncommercial RTB platform and the online test has shown its\nsuccess in generating controllable advertising performance."}
{"Title": "Geographic Segmentation via Latent Poisson Factor Model", "Abstract": "Discovering latent structures in spatial data is of critical\nimportance to understanding the user behavior of location-\nbased services. In this paper, we study the problem of geo-\ngraphic segmentation of spatial data, which involves dividing\na collection of observations into distinct geo-spatial regions\nand uncovering abstract correlation structures in the data.\nWe introduce a novel, Latent Poisson Factor (LPF) model\nto describe spatial count data. The model describes the spa-\ntial counts as a Poisson distribution with a mean that factors\nover a joint item-location latent space. The latent factors\nare constrained with weak labels to help uncover interesting\nspatial dependencies. We study the LPF model on a mobile\napp usage data set and a news article readership data set.\nWe empirically demonstrate its effectiveness on a variety of\nprediction tasks on these two data sets."}
{"Title": "How Relevant is the Irrelevant Data:\nLeveraging the Tagging Data for a Learning-to-Rank Model", "Abstract": "For the task of tag-based item recommendations, the underlying\ntensor model faces several challenges such as high data sparsity\nand inferring latent factors effectively. To overcome the inherent\nsparsity issue of tensor models, we propose the graded-relevance\ninterpretation scheme that leverages the tagging data effectively.\nUnlike the existing schemes, the graded-relevance scheme inter-\nprets the tagging data richly, differentiates the non-observed tag-\nging data insightfully, and annotates each entry as one of the\n\u201crelevant\u201d, \u201clikely relevant\u201d, \u201cirrelevant\u201d, or \u201cindecisive\u201d labels.\nTo infer the latent factors of tensor models correctly to produce\nthe high quality recommendation, we develop a novel learning-to-\nrank method, Go-Rank, that optimizes Graded Average Precision\n(GAP). Evaluating the proposed method on real-world datasets,\nwe show that the proposed interpretation scheme produces a dens-\ner tensor model by revealing \u201crelevant\u201d entries from the previous-\nly assumed \u201cirrelevant\u201d entries. Optimizing GAP as the ranking\nmetric, the quality of the recommendations generated by Go-Rank\nis found superior against the benchmarking methods."}
{"Title": "Inferring Latent Triggers of Purchases\nwith Consideration of Social Effects and Media\nAdvertisements", "Abstract": "This paper proposes a method for inferring from single-\nsource data the factors that trigger purchases. Here, single-\nsource data are the histories of item purchases and media\nadvertisement views for each individual. We assume a se-\nquence of purchase events to be a stochastic process incor-\nporating the following three factors: (a) user preference, (b)\nsocial effects received from other users, and (c) media ad-\nvertising effects. As our user-purchase model incorporates\nthe latent relationships between users and advertisers, it can\ninfer the latent triggers of purchases. Experiments on real\nsingle-source data show that our model can (a) achieve high\nprediction accuracy for purchases, (b) discover the key infor-\nmation, i.e., popular items, influential users, and influential\nadvertisers, (c) estimate the relative impact of the three fac-\ntors on purchases, and (d) find user segments according to\nthe estimated factors."}
{"Title": "Modeling and Predicting Learning Behavior in MOOCs", "Abstract": "Massive Open Online Courses (MOOCs), which collect complete\nrecords of all student interactions in an online learning environ-\nment, offer us an unprecedented opportunity to analyze students\u2019\nlearning behavior at a very fine granularity than ever before.\nUsing dataset from xuetangX, one of the largest MOOCs from\nChina, we analyze key factors that influence students\u2019 engagement\nin MOOCs and study to what extent we could infer a student\u2019s\nlearning effectiveness. We observe significant behavioral hetero-\ngeneity in students\u2019 course selection as well as their learning pat-\nterns. For example, students who exert higher effort and ask more\nquestions are not necessarily more likely to get certificates. Addi-\ntionally, the probability that a student obtains the course certificate\nincreases dramatically (3\u00d7 higher) when she has one or more \u201ccer-\ntificate friends\u201d.\nMoreover, we develop a unified model to predict students\u2019 learn-\ning effectiveness, by incorporating user demographics, forum ac-\ntivities, and learning behavior. We demonstrate that the proposed\nmodelsignificantlyoutperforms(+2.03-9.03%byF1-score)several\nalternative methods in predicting students\u2019 performance on assign-\nments and course certificates. The model is flexible and can be\napplied to various settings. For example, we are deploying a new\nfeature into xuetangX to help teachers dynamically optimize the\nteaching process."}
{"Title": "Portrait of an Online Shopper:\nUnderstanding and Predicting Consumer Behavior", "Abstract": "Consumer spending accounts for a large fraction of economic foot-\nprint of modern countries. Increasingly, consumer activity is mov-\ning to the web, where digital receipts of online purchases pro-\nvide valuable data sources detailing consumer behavior. We con-\nsider such data extracted from emails and combined with with con-\nsumers\u2019 demographic information, which we use to characterize,\nmodel, and predict purchasing behavior. We analyze such behavior\nof consumers in different age and gender groups, and find inter-\nesting, actionable patterns that can be used to improve ad targeting\nsystems. For example, we found that the amount of money spent on\nonline purchases grows sharply with age, peaking in the late 30s,\nwhile shoppers from wealthy areas tend to purchase more expen-\nsive items and buy them more frequently. Furthermore, we look at\nthe influence of social connections on purchasing habits, as well as\nat the temporal dynamics of online shopping where we discovered\ndaily and weekly behavioral patterns. Finally, we build a model\nto predict when shoppers are most likely to make a purchase and\nhow much will they spend, showing improvement over baseline\napproaches. The presented results paint a clear picture of a modern\nonline shopper, and allow better understanding of consumer be-\nhavior that can help improve marketing efforts and make shopping\nmore pleasant and efficient experience for online customers."}
{"Title": "Project Success Prediction in Crowdfunding Environments", "Abstract": "Crowdfunding has gained widespread attention in recent\nyears. Despite the huge success of crowdfunding platforms,\nthe percentage of projects that succeed in achieving their\ndesired goal amount is only around 40%. Moreover, many\nof these crowdfunding platforms follow \u201call-or-nothing\u201d pol-\nicy which means the pledged amount is collected only if the\ngoal is reached within a certain predefined time duration.\nHence, estimating the probability of success for a project is\none of the most important research challenges in the crowd-\nfunding domain. To predict the project success, there is\na need for new prediction models that can potentially com-\nbine the power of both classification (which incorporate both\nsuccessful and failed projects) and regression (for estimat-\ning the time for success). In this paper, we formulate the\nproject success prediction as a survival analysis problem\nand apply the censored regression approach where one can\nperform regression in the presence of partial information.\nWe rigorously study the project success time distribution\nof crowdfunding data and show that the logistic and log-\nlogistic distributions are a natural choice for learning from\nsuch data. We investigate various censored regression mod-\nels using comprehensive data of 18K Kickstarter (a popular\ncrowdfunding platform) projects and 116K corresponding\ntweets collected from Twitter. We show that the models\nthat take complete advantage of both the successful and\nfailed projects during the training phase will perform sig-\nnificantly better at predicting the success of future projects\ncompared to the ones that only use the successful projects.\nWe provide a rigorous evaluation on many sets of relevant\nfeatures and show that adding few temporal features that\nare obtained at the project\u2019s early stages can dramatically\nimprove the performance."}
{"Title": "Quantifying Controversy in Social Media", "Abstract": "Which topics spark the most heated debates in social media?\nIdentifying these topics is a first step towards creating sys-\ntems which pierce echo chambers. In this paper, we perform\na systematic methodological study of controversy detection\nusing social media network structure and content.\nUnlike previous work, rather than identifying controversy\nin a single hand-picked topic and use domain-specific knowl-\nedge, we focus on comparing topics in any domain. Our\napproach to quantifying controversy is a graph-based three-\nstage pipeline, which involves ( i ) building a conversation\ngraph about a topic, which represents alignment of opinion\namong users; ( ii ) partitioning the conversation graph to iden-\ntify potential sides of the controversy; and ( iii ) measuring\nthe amount of controversy from characteristics of the graph.\nWe perform an extensive comparison of controversy mea-\nsures, as well as graph building approaches and data sources.\nWe use both controversial and non-controversial topics on\nTwitter, as well as other external datasets. We find that our\nnew random-walk-based measure outperforms existing ones\nin capturing the intuitive notion of controversy, and show\nthat content features are vastly less helpful in this task."}
{"Title": "Semantic Documents Relatedness using Concept Graph\nRepresentation", "Abstract": "We deal with the problem of document representation for\nthe task of measuring semantic relatedness between docu-\nments. A document is represented as a compact concept\ngraph where nodes represent concepts extracted from the\ndocument through references to entities in a knowledge base\nsuch as DBpedia. Edges represent the semantic and struc-\ntural relationships among the concepts. Several methods\nare presented to measure the strength of those relation-\nships. Concepts are weighted through the concept graph\nusing closeness centrality measure which reflects their rel-\nevance to the aspects of the document. A novel similarity\nmeasure between two concept graphs is presented. The simi-\nlarity measure first represents concepts as continuous vectors\nby means of neural networks. Second, the continuous vectors\nare used to accumulate pairwise similarity between pairs of\nconcepts while considering their assigned weights. We eval-\nuate our method on a standard benchmark for document\nsimilarity. Our method outperforms state-of-the-art meth-\nods including ESA (Explicit Semantic Annotation) while our\nconcept graphs are much smaller than the concept vectors\ngenerated by ESA. Moreover, we show that by combining\nour concept graph with ESA, we obtain an even further im-\nprovement."}
{"Title": "TargetAd 2016: Int\u2019l Workshop on Ad Targeting at Scale", "Abstract": "The 2 nd International Workshop on Ad Targeting at Scale will be\nheldinSanFrancisco, California, USAonFebruary22 nd , 2016, co-\nlocated with the 9 th ACM International Conference on Web Search\nand Data Mining (WSDM). The main objective of the workshop\nis to address the challenges of ad targeting in web-scale settings.\nThe workshop brings together interdisciplinary researchers in com-\nputational advertising, recommender systems, personalization, and\nrelated areas, to share, exchange, learn, and develop preliminary re-\nsults, new concepts, ideas, principles, and methodologies on apply-\ning data mining technologies to ad targeting. We have constructed\nan exciting program of eight refereed papers and several invited\ntalks that will help us better understand the future of ad targeting."}
{"Title": "Understanding and Identifying Advocates for\nPolitical Campaigns on Social Media", "Abstract": "Social media is increasingly being used to access and dis-\nseminate information on sociopolitical issues like gun rights\nand general elections. The popularity and openness of so-\ncial media makes it conducive for some individuals, known\nas advocates, who use social media to push their agendas on\nthese issues strategically. Identifying these advocates will\ncaution social media users before reading their information\nand also enable campaign managers to identify advocates for\ntheir digital political campaigns. A significant challenge in\nidentifying advocates is that they employ nuanced strategies\nto shape user opinion and increase the spread of their mes-\nsages, making it difficult to distinguish them from random\nusers posting on the campaign. In this paper, we draw from\nsocial movement theories and design a quantitative frame-\nwork to study the nuanced message strategies, propagation\nstrategies, and community structure adopted by advocates\nfor political campaigns in social media. Based on observa-\ntions of their social media activities manifesting from these\nstrategies, we investigate how to model these strategies for\nidentifying them. We evaluate the framework using two\ndatasets from Twitter, and our experiments demonstrate its\neffectiveness in identifying advocates for political campaigns\nwith ramifications of this work directed towards assisting\nusers as they navigate through social media spaces."}
{"Title": "Understanding Offline Political System", "Abstract": "\u201cMan is by nature a political animal\u201d, as asserted by Aris-\ntotle. This political nature manifests itself in the data we\nproduce and the traces we leave online. In this tutorial, we\naddress a number of fundamental issues regarding mining\nof political data: What types of data could be considered\npolitical? What can we learn from such data? Can we use\nthe data for prediction of political changes, etc? How can\nthese prediction tasks be done efficiently? Can we use online\nsocio-political data in order to get a better understanding of\nour political systems and of recent political changes? What\nare the pitfalls and inherent shortcomings of using online\ndata for political analysis? In recent years, with the abun-\ndance of data, these questions, among others, have gained\nimportance, especially in light of the global political turmoil\nand the upcoming 2016 US presidential election. We intro-\nduce relevant political science theory, describe the challenges\nwithin the framework of computational social science and\npresent state of the art approaches bridging social network\nanalysis, graph mining, and natural language processing."}
{"Title": "WSDM Cup 2016 \u2013 Entity Ranking Challenge", "Abstract": "In this paper, we describe the WSDM Cup entity ranking\nchallenge held in conjunction with the 2016 Web Search and Data\nMining conference (WSDM 2016). Participants in the challenge\nwere provided access to the Microsoft Academic Graph (MAG), a\nlarge heterogeneous graph of academic entities, and were invited\nto calculate the query-independent importance of each publication\nin the graph. Submissions for the challenge were open from\nAugust through November 2015, and a public leaderboard\ndisplayed teams\u2019 progress against a set of training judgements.\nFinal evaluations were performed against a separate, withheld\nportion of the evaluation judgements. The top eight performing\nteams were then invited to submit papers to the WSDM Cup\nworkshop, held at the WSDM 2016 conference."}
{"Title": "Can you Trust the Trend? Discovering Simpson\u2019s\nParadoxes in Social Data", "Abstract": "We investigate how Simpson\u2019s paradox affects analysis of trends\nin social data. According to the paradox, the trends observed in\ndata that has been aggregated over an entire population may be\ndifferent from, and even opposite to, those of the underlying sub-\ngroups. Failure to take this effect into account can lead analysis to\nwrong conclusions. We present a statistical method to automati-\ncally identify Simpson\u2019s paradox in data by comparing statistical\ntrends in the aggregate data to those in the disaggregated sub-\ngroups. We apply the approach to data from Stack Exchange, a\npopular question-answering platform, to analyze factors affecting\nanswerer performance, specifically, the likelihood that an answer\nwritten by a user will be accepted by the asker as the best answer to\nhis or her question. Our analysis confirms a known Simpson\u2019s para-\ndox and identifies several new instances. These paradoxes provide\nnovel insights into user behavior on Stack Exchange."}
{"Title": "Deep Neural Architecture for Multi-Modal Retrieval based on\nJoint Embedding Space for Text and Images", "Abstract": "Recent advances in deep learning and distributed representations\nof images and text have resulted in the emergence of several neu-\nral architectures for cross-modal retrieval tasks, such as searching\ncollections of images in response to textual queries and assigning\ntextual descriptions to images. However, the multi-modal retrieval\nscenario,whenaquerycanbeeitheratextoranimageandthegoal\nis to retrieve both a textual fragment and an image, which should\nbe considered as an atomic unit, has been signifcantly less studied.\nIn this paper, we propose a gated neural architecture to project im-\nageandkeywordqueriesaswellasmulti-modalretrievalunitsinto\nthe same low-dimensional embedding space and perform seman-\ntic matching in this space. The proposed architecture is trained to\nminimize structured hinge loss and can be applied to both cross-\nand multi-modal retrieval. Experimental results for six diferent\ncross- and multi-modal retrieval tasks obtained on publicly avail-\nable datasets indicate superior retrieval accuracy of the proposed\narchitecture in comparison to the state-of-art baselines."}
{"Title": "A Discrete Choice Model for Subset Selection", "Abstract": "Multinomiallogistic regression isaclassicaltechnique formodeling\nhow individuals choose an item from a finite set of alternatives.\nThismethodologyisaworkhorseinbothdiscretechoicetheoryand\nmachine learning. However, it is unclear how to generalize multi-\nnomial logistic regression to subset selection, allowing the choice of\nmore than one item at a time. We present a new model for subset\nselection derived from the perspective of random utility maximiza-\ntion in discrete choice theory. In our model, the quality of a subset\nis determined by the quality of its elements, plus an optional cor-\nrection. Given a budget on the number of subsets that may receive\ncorrection, we develop a framework for learning the quality scores\nfor each item, the choice of subsets, and the correction for each\nsubset. We show that, given the subsets to receive correction, we\ncan efficiently and optimally learn the remaining model parameters\njointly. We show further that learning the optimal subsets is both\nNP-hard and non-submodular, but there are efficient heuristics that\nperform well in practice. We combine these pieces to provide an\noverall learning solution and apply it to subset prediction tasks.\nWe find that with reasonably-sized budgets, there are significant\ngains in average per-choice likelihood ranging from 7% to 8x de-\npending on the dataset and also substantial improvements over a\ndeterminantal point process model."}
{"Title": "Neural Graph Learning: Training Neural Networks Using Graphs", "Abstract": "Label propagation is a powerful and flexible semi-supervised learn-\ning technique on graphs. Neural networks, on the other hand, have\nproven track records in many supervised learning tasks. In this\nwork, we propose a training framework with a graph-regularised\nobjective, namely Neural Graph Machines, that can combine the\npower of neural networks and label propagation. This work gener-\nalises previous literature on graph-augmented training of neural\nnetworks, enabling it to be applied to multiple neural architectures\n(Feed-forward NNs, CNNs and LSTM RNNs) and a wide range of\ngraphs. The new objective allows the neural networks to harness\nboth labeled and unlabeled data by: (a) allowing the network to\ntrain using labeled data as in the supervised setting, (b) biasing the\nnetwork to learn similar hidden representations for neighboring\nnodes on a graph, in the same vein as label propagation. Such ar-\nchitectures with the proposed objective can be trained efficiently\nusing stochastic gradient descent and scaled to large graphs, with\na runtime that is linear in the number of edges. The proposed joint\ntraining approach convincingly outperforms many existing meth-\nods on a wide range of tasks (multi-label classification on social\ngraphs, news categorization, document classification and semantic\nintentclassification),withmultipleformsofgraphinputs(including\ngraphs with and without node-level features) and using different\ntypes of neural networks."}
{"Title": "Putting Data in the Driver\u2019s Seat:\nOptimizing Earnings for On-Demand Ride-Hailing", "Abstract": "On-demand ride-hailing platforms like Uber and Lyft are helping\nreshape urban transportation, by enabling car owners to become\ndrivers for hire with minimal overhead. Although there are many\nstudies that consider ride-hailing platforms holistically, e.g., from\nthe perspective of supply and demand equilibria, little emphasis\nhas been placed on optimization for the individual, self-interested\ndrivers that currently comprise these fleets. While some individu-\nals drive opportunistically either as their schedule allows or on a\nfixed schedule, we show that strategic behavior regarding when\nand where to drive can substantially increase driver income. In this\npaper, we formalize the problem of devising a driver strategy to\nmaximize expected earnings, describe a series of dynamic program-\nming algorithms to solve these problems under different sets of\nmodeled actions available to the drivers, and exemplify the models\nand methods on a large scale simulation of driving for Uber in\nNYC. In our experiments, we use a newly-collected dataset that\ncombines the NYC taxi rides dataset along with Uber API data, to\nbuild time-varying traffic and payout matrices for a representative\nsix-month time period in greater NYC. From this input, we can\nreason about prospective itineraries and payoffs. Moreover, the\nframework enables us to rigorously reason about and analyze the\nsensitivity of our results to perturbations in the input data. Among\nour main findings is that repositioning throughout the day is key\nto maximizing driver earnings, whereas \u2018chasing surge\u2019 is typically\nmisguided and sometimes a costly move."}
{"Title": "Convolutional Neural Networks for Sof-Matching N-Grams in\nAd-hoc Search", "Abstract": "Tis paper presents Conv-KNRM , a Convolutional Kernel-based Neu-\nral Ranking Model that models n-gram sof matches for ad-hoc\nsearch. Instead of exact matching query and document n-grams,\nConv-KNRM uses Convolutional Neural Networks to represent n-\ngrams of various lengths and sof matches them in a unifed embed-\nding space. Te n-gram sof matches are then utilized by the kernel\npooling and learning-to-rank layers to generate the fnal ranking\nscore. Conv-KNRM can be learned end-to-end and fully optimized\nfrom user feedback. Te learned model\u2019s generalizability is inves-\ntigated by testing how well it performs in a related domain with\nsmall amounts of training data. Experiments on English search\nlogs, Chinese search logs, and TREC Web track tasks demonstrated\nconsistent advantages of Conv-KNRM over prior neural IR methods\nand feature-based methods."}
{"Title": "Ballpark Crowdsourcing: The Wisdom of Rough Group\nComparisons", "Abstract": "Crowdsourcing has become a popular method for collecting labeled\ntraining data. However, in many practical scenarios traditional\nlabeling can be difficult for crowdworkers (for example, if the data\nis high-dimensional or unintuitive, or the labels are continuous).\nIn this work, we develop a novel model for crowdsourcing that\ncancomplementstandardpracticesbyexploitingpeople\u2019sintuitions\nabout groups and relations between them. We employ a recent\nmachinelearningsetting,calledBallparkLearning,thatcanestimate\nindividual labels given only coarse, aggregated signal over groups\nof data points. To address the important case of continuous labels,\nwe extend the Ballpark setting (which focused on classification)\nto regression problems. We formulate the problem as a convex\noptimization problem and propose fast, simple methods with an\ninnate robustness to outliers.\nWe evaluate our methods on real-world datasets, demonstrating\nhowusefulconstraintsaboutgroupscanbeharnessedfromacrowd\nof non-experts. Our methods can rival supervised models trained\non many true labels, and can obtain considerably better results\nfrom the crowd than a standard label-collection process (for a lower\nprice).Bycollectingroughguessesongroupsofinstancesandusing\nmachine learning to infer the individual labels, our lightweight\nframework is able to address core crowdsourcing challenges and\ntrain machine learning models in a cost-effective way."}
{"Title": "Co-PACRR:\nA Context-Aware Neural IR Model for Ad-hoc Retrieval", "Abstract": "Neural IR models, such as DRMM and PACRR, have achieved strong\nresults by successfully capturing relevance matching signals. We\nargue that the context of these matching signals is also important.\nIntuitively, when extracting, modeling, and combining matching\nsignals, one would like to consider the surrounding text (local\ncontext) as well as other signals from the same document that can\ncontribute to the overall relevance score. In this work, we highlight\nthree potential shortcomings caused by not considering context\ninformationandproposethreeneuralingredientstoaddressthem:a\ndisambiguation component, cascade k-max pooling, and a shuffling\ncombinationlayer.IncorporatingthesecomponentsintothePACRR\nmodel yields Co-PACRR, a novel context-aware neural IR model.\nExtensive comparisons with established models on Trec Web Track\ndata confirm that the proposed model can achieve superior search\nresults.Inaddition,anablationanalysisisconductedtogaininsights\ninto the impact of and interactions between different components.\nWe release our code to enable future comparisons 1 ."}
{"Title": "Topic Chronicle Forest for Topic Discovery and Tracking", "Abstract": "To ease the comprehension of existing time-stamped corpora, we\nextend topic models to handle both the specifcity and temporality\nof topics; this is a signifcant advance over previous models which\nfail to provide both aspects simultaneously. Our proposed model\ncombines the Topic Chronicle Forest (TCF) and Thematic Dirichlet\nProcesses (TDP). TCF is a set of Topic Chronicle Trees, where each\ntree is a hierarchy of topics that becomes more specialized toward\nthe leaves. Only one tree is defned in each time interval, a region,\nand is processed by TDP to generate a document. The advantage of\nour approach is that it provides more compact topic organization,\nwhile preserving both the semantics of the given corpus and the\ntheme of each document. Experiments show that TCF is a useful\nextension for longitudinal topic disco"}
{"Title": "REV2: Fraudulent User Prediction in Rating Platforms", "Abstract": "Rating platforms enable large-scale collection of user opinion about\nitems (e.g., products or other users). However, fraudulent users give\nfake ratings for excessive monetary gains. In this paper, we present\nRev2, a system to identify such fraudulent users. We propose three\ninterdependent intrinsic quality metrics\u2014fairness of a user, reliabil-\nityofaratingandgoodnessofaproduct.Thefairnessandreliability\nquantify the trustworthiness of a user and rating, respectively, and\ngoodness quantifies the quality of a product. Intuitively, a user is\nfair if it provides reliable scores that are close to the goodness of\nproducts. We propose six axioms to establish the interdependency\nbetween the scores, and then, formulate a mutually recursive def-\ninition that satisfies these axioms. We extend the formulation to\naddress cold start problem and incorporate behavior properties. We\ndevelop the Rev2 algorithm to calculate these intrinsic scores for\nall users, ratings, and products by combining network and behavior\nproperties. We prove that this algorithm is guaranteed to converge\nand has linear time complexity. By conducting extensive experi-\nments on five rating datasets, we show that Rev2 outperforms nine\nexisting algorithms in detecting fraudulent users. We reported the\n150 most unfair users in the Flipkart network to their review fraud\ninvestigators, and 127 users were identified as being fraudulent\n(84.6% accuracy). The Rev2 algorithm is being deployed at Flipkart."}
{"Title": "Joint Non-negative Matrix Factorization for\nLearning Ideological Leaning on Twitter", "Abstract": "People are shifting from traditional news sources to online news at\nanincrediblyfastrate.However,thetechnologybehindonlinenews\nconsumption promotes content that confirms the users\u2019 existing\npoint of view. This phenomenon has led to polarization of opinions\nand intolerance towards opposing views. Thus, a key problem is\nto model information filter bubbles on social media and design\nmethodstoeliminatethem.Inthispaper,weuseamachine-learning\napproach to learn a liberal-conservative ideology space on Twitter,\nand show how we can use the learned latent space to tackle the\nfilter bubble problem.\nWe model the problem of learning the liberal-conservative ideol-\nogy space of social media users and media sources as a constrained\nnon-negativematrix-factorizationproblem.Ourmodelincorporates\nthe social-network structure and content-consumption informa-\ntion in a joint factorization problem with shared latent factors. We\nvalidate our model and solution on a real-world Twitter dataset\nconsisting of controversial topics, and show that we are able to\nseparate users by ideology with over 90% purity. When applied\nto media sources, our approach estimates ideology scores that are\nhighly correlated (Pearson correlation 0 . 9) with ground-truth ide-\nology scores. Finally, we demonstrate the utility of our model in\nreal-worldscenarios,byillustratinghowthelearnedideologylatent\nspace can be used to develop exploratory and interactive interfaces\nthat can help users in diffusing their information filter bubble."}
{"Title": "Learning to Discover Domain-Specific Web Content", "Abstract": "The ability to discover all content relevant to an information do-\nmain has many applications, from helping in the understanding of\nhumanitarian crises to countering human and arms trafficking. In\nsuch applications, time is of essence: it is crucial to both maximize\ncoverage and identify new content as soon as it becomes available,\nso that appropriate actions can be taken. In this paper, we propose\nnew methods for efficient domain-specific re-crawling that maxi-\nmize the yield for new content. By learning patterns of pages that\nhave a high yield, our methods select a small set of pages that can\nbe re-crawled frequently, increasing the coverage and freshness\nwhile conserving resources. Unlike previous approaches to this\nproblem, our methods combine different factors to optimize the\nre-crawling strategy, do not require full snapshots for the learning\nstep, and dynamically adapt the strategy as the crawl progresses. In\nan empirical evaluation, we have simulated the framework over 600\npartial crawl snapshots in three different domains. The results show\nthat our approach can achieve 150% higher coverage compared to\nexisting, state-of-the-art techniques. In addition, it is also able to\ncapture 80% of new relevant content within less than 4 hours of\npublication."}
{"Title": "Leveraging Implicit Contribution Amounts to Facilitate\nMicrofinancing Requests", "Abstract": "The emergence of online microfinancing platforms provides new\nopportunities for people to seek financial assistance from a large\nnumber of potential contributors. However, these platforms deal\nwith a huge number of requests, making it hard for the requesters\nto get assistance for their financial needs. Designing algorithms to\nidentify potential contributors for a given request will assist in sat-\nisfying financial needs of requesters and improve the effectiveness\nof microfinancing platforms. Existing work correlates requests with\ncontributorinterestsandprofilestodesignfeaturebasedapproaches\nfor recommending projects to prospective contributors. However,\ncontributing money to financial requests has a cost on contributors\nwhich can affect his inclination to contribute in the future . Litera-\nture in economic behavior has investigated the manner in which\nmemory of past contribution amounts affects user inclination to\ncontribute to a given request. To systematically investigate whether\nthese characteristics of economic behavior would help to facilitate\nrequests in online microfinancing platforms, we present a novel\nframework to identify contributors for a given request from their\npast financial information. Individual contribution amounts are not\npublicly available, so we draw from financial modeling literature\nto model the implicit contribution amounts made to past requests.\nWe evaluate the framework on two microfinancing platforms to\ndemonstrate its effectiveness in identifying contributors."}
{"Title": "Measuring the Latency of Depression Detection in Social Media", "Abstract": "Detecting depression is a key public health challenge, as almost\n12% of all disabilities can be attributed to depression. Computa-\ntional models for depression detection must prove not only that\ncan they detect depression, but that they can do it early enough\nfor an intervention to be plausible. However, current evaluations\nof depression detection are poor at measuring model latency. We\nidentify several issues with the currently popular ERDE metric, and\nproposealatency-weightedF1metricthataddressestheseconcerns.\nWe then apply this evaluation to several models from the recent\neRisk 2017 shared task on depression detection, and show how our\nproposed measure can better capture system differences."}
{"Title": "Customer Purchase Behavior Prediction from Payment Datasets", "Abstract": "With the advances in the development of mobile payments, a huge\namount of payment data are collected by banks. User payment data\nofferagooddatasettodepictcustomerbehaviorpatterns.Acompre-\nhensive understanding of customers\u2019 purchase behavior is crucial\nto developing good marketing strategies, which may trigger much\ngreater purchase amounts. For example, by exploring customer be-\nhavior patterns, given a target store, a set of potential customers is\nable to be identified. In other words, personalized campaigns at the\nright time and in the right place can be treated as the last stage of\nconsumption. Here we propose a probability graphical model that\nexploits the payment data to discover customer purchase behavior\nin the s patial, t emporal, p ayment amount and product c ategory\naspects, named STPC-PGM. As a result, the mobility behavior of\nan individual user could be predicted with a probabilistic graphical\nmodel that accounts for all aspects of each customer\u2019s relationship\nwith the payment platform. To achieve real time advertising, we\nthen develop an online framework that efficiently computes the\nprediction results. Our experiment results show that STPC-PGM is\neffective in discovering customers\u2019 profiling features, and outper-\nforms the state-of-the-art methods in purchase behavior prediction.\nIn addition, the prediction results are being deployed in the market-\ning of real-world credit card users, and have presented a significant\ngrowth in the advertising conversion rate."}
{"Title": "Why People Search for Images using Web Search Engines", "Abstract": "Whataretheintentsorgoalsbehindhumaninteractionswithimage\nsearch engines? Knowing why people search for images is of major\nconcern to Web image search engines because user satisfaction may\nvary as intent varies. Previous analyses of image search behavior\nhave mostly been query-based, focusing on what images people\nsearch for, rather than intent-based, that is, why people search for\nimages. To date, there is no thorough investigation of how diferent\nimage search intents afect users\u2019 search behavior.\nIn this paper, we address the following questions: (1) Why do\npeople search for images in text-based Web image search systems?\n(2) How does image search behavior change with user intent?\n(3) Can we predict user intent efectively from interactions during\nthe early stages of a search session? To this end, we conduct both a\nlab-based user study and a commercial search log analysis.\nWe show that user intents in image search can be grouped into\nthree classes: Explore/Learn, Entertain, and Locate/Acquire. Our\nlab-based user study reveals diferent user behavior patterns under\nthese three intents, such as frst click time, query reformulation,\ndwell time and mouse movement on the result page. Based on user\ninteraction features during the early stages of an image search ses-\nsion, that is, before mouse scroll, we develop an intent classifer that\nis able to achieve promising results for classifying intents into our\nthree intent classes. Given that all features can be obtained online\nand unobtrusively, the predicted intents can provide guidance for\nchoosing ranking methods immediately after scrolling."}
{"Title": "Dynamic Word Embeddings for Evolving Semantic Discovery", "Abstract": "Word evolution refers to the changing meanings and associations\nof words throughout time, as a byproduct of human language evo-\nlution. By studying word evolution, we can infer social trends and\nlanguage constructs over different periods of human history. How-\never, traditional techniques such as word representation learning\ndo not adequately capture the evolving language structure and\nvocabulary. In this paper, we develop a dynamic statistical model to\nlearn time-aware word vector representation. We propose a model\nthat simultaneously learns time-aware embeddings and solves the\nresulting \u201calignment problem\u201d. This model is trained on a crawled\nNYTimes dataset. Additionally, we develop multiple intuitive eval-\nuation strategies of temporal word embeddings. Our qualitative\nand quantitative tests indicate that our method not only reliably\ncaptures this evolution over time, but also consistently outperforms\nstate-of-the-art temporal embedding approaches on both semantic\naccuracy and alignment quality."}
{"Title": "Event Mining over Distributed Text Streams", "Abstract": "This research presents a new set of techniques to deal with event\nmining from different text sources, a complex set of NLP tasks\nwhich aim to extract events of interest and their components in-\ncluding authors, targets, locations, and event categories. Our focus\nis on distributed text streams, such as tweets from different news\nagencies, in order to accurately retrieve events and its components\nby combining such sources in different ways using text stream\nmining. Therefore this research project aims to fill the gap between\nbatch event mining, text stream mining and distributed data mining\nwhich have been used separately to address related learning tasks.\nWe propose a multi-task and multi-stream mining approach to com-\nbine information from multiple text streams to accurately extract\nand categorise events under the assumptions of stream mining.\nOur approach also combines ontology matching to boost accuracy\nunder imbalanced distributions. In addition, we plan to address two\nrelatively unexplored event mining tasks: event coreference and\nevent synthesis. Preliminary results show the appropriateness of\nour proposal, which is giving an increase of around 20% on macro\nprequential metrics for the event classification task."}
{"Title": "Beyond Who and What: Data Driven Approaches for User\nCharacterization", "Abstract": "Socialmediaandtechnologyhavedrasticallytransformedthesocial\nand information networks around us. They have impacted how we\ncommunicate with others, search for information, and even how\nwe express our personal opinions. Further, in this era of big data,\nnot only are the online services collecting vast variety of user data,\nbut we, as users, are also readily divulging significant amounts\nof information. Together, massive datasets obtained from diverse\nsources such as organizations and user generated content give us\nthe opportunity to explore and understand complex behavior of\nboth individuals and communities. This proposal aims at designing\ngeneralizable and scalable data-driven frameworks to gain a deeper\nunderstanding of the users, explain their actions and preferences,\nand infer personal traits. The proposed models will enable us to\nmove beyond asking the conventional questions of who and what,\nand reveal answers about how and why. Given the varying digital\npersona of users motivated by their personal preferences and social\nattributes, we characterize users in two distinct domains: online\nhealth and peace studies. The models are designed to solve various\nreal-world challenges to maximize their broader impact."}
{"Title": "Conversational Semantic Search:\nLooking Beyond Web Search, Q&A and Dialog Systems", "Abstract": "User expectations of web search are changing. They are expecting\nsearch engines to answer questions, to be more conversational,\nand to offer means to complete tasks on their behalf. At the same\ntime, to increase the breadth of tasks that personal digital assistants\n(PDAs), such as Microsoft\u2019s Cortana or Amazon\u2019s Alexa, are capa-\nble of, PDAs need to better utilize information about the world, a\nsignificant amount of which is available in the knowledge bases\nand answers built for search engines. It thus seems likely that the\nunderlying systems that power web search and PDAs will con-\nverge. This demonstration presents a system that merges elements\nof traditional multi-turn dialog systems with web based question\nanswering. This demo focuses on the automatic composition of\nsemantic functional units, Botlets, to generate responses to user\u2019s\nnatural language (NL) queries. We show that such a system can be\ntrained to combine information from search engine answers with\nPDA tasks to enable new user experiences."}
{"Title": "Athlytics: Winning in Sports with Data", "Abstract": "Data and analytics have been part of the sports industry from as\nearly as the 1870s, when the first boxscore in baseball was recorded.\nHowever,itisonlyrecentlythatadvanceddataminingandmachine\nlearning techniques have been utilized for facilitating the opera-\ntions of sports franchises. While part of the reason is related with\nthe ability to collect more fine-grained data, an equally important\nfactor for this turn to analytics is the huge success and competitive\nadvantage that early adopters of investment in analytics enjoyed\n(popularized by the best-seller \u201cMoneyball\u201d that described the suc-\ncess that Oakland Athletics had with analytics). Draft selection,\ngame-day decision making and player evaluation are just a few of\nthe applications where sports analytics play a crucial role today.\nApart from the sports clubs, other stakeholders in the industry (e.g.,\nthe leagues\u2019 offices, media, etc.) invest in analytics. The leagues in-\ncreasingly rely on data in order to decide on potential rule changes.\nFor instance, the most recent rule change in NFL, i.e., the kickoff\ntouchback, was a result of thorough data analysis of concussion\ninstances. In this tutorial we will review the literature in data min-\ning and machine learning techniques for sports analytics. We will\nintroduce the audience to the design and methodologies behind\nadvanced metrics such as the adjusted plus/minus for evaluating\nbasketball players, spatial metrics for evaluating the ability of a\nplayer to spread the defense in basketball, and the Player Efficiency\nRating (PER). We will also go in depth in advanced data mining\nmethods,andinparticulartensormining,thatcananalyzeheteroge-\nnous data similar to the ones available in today\u2019s sports world."}
{"Title": "Mining Knowledge Graphs From Text", "Abstract": "Knowledge graphs have become an increasingly crucial com-\nponent in machine intelligence systems, powering ubiquitous\ndigital assistants and inspiring several large scale academic\nprojects across the globe. Our tutorial explains why knowl-\nedge graphs are important, how knowledge graphs are con-\nstructed, and where new research opportunities exist for\nimproving the state-of-the-art. In this tutorial, we cover the\nmany sophisticated approaches that complete and correct\nknowledge graphs. We organize this exploration into two\nmain classes of models. The first include probabilistic logi-\ncal frameworks that use graphical models, random walks, or\nstatistical rule mining to construct knowledge graphs. The\nsecond class of models includes latent space models such as\nmatrix and tensor factorization and neural networks. We\nconclude the tutorial with a critical comparison of techniques\nand results. We will offer practical advice for novices to iden-\ntify common empirical challenges and concrete data sets for\ninitial experimentation. Finally, we will highlight promising\nareas of current and future work."}
{"Title": "HeteroNAM: International Workshop on\nHeterogeneous Networks Analysis and Mining", "Abstract": "The first International Workshop onHeterogeneous Networks Anal-\nysis and Mining is held in Los Angeles, California, USA on February\n9th, 2018 and is co-located with the 11th ACM International Con-\nference on Web Search and Data Mining. The goal of this workshop\nis to bring together computing researchers and practitioners to\naddress challenges in the mining and analysis of real-world hetero-\ngeneous networks. This workshop has an exciting program that\nspans a number of subareas including: graph mining, learning from\nstructured data, statistical relational learning, and network science\nin general. The program includes six invited speakers, lively dis-\ncussion on emerging topics, and presentations of several original\nresearch papers."}
{"Title": "iPhone\u2019s Digital Marketplace:\nCharacterizing the Big Spenders", "Abstract": "With mobile shopping surging in popularity, people are spending\never more money on digital purchases through their mobile devices\nand phones. However, few large-scale studies of mobile shopping\nexist. In this paper we analyze a large data set consisting of more\nthan 776M digital purchases made on Apple mobile devices that\ninclude songs, apps, and in-app purchases. We find that 61% of all\nthe spending is on in-app purchases and that the top 1% of users\nare responsible for 59% of all the spending. These big spenders\nare more likely to be male and older, and less likely to be from\nthe US. We study how they adopt and abandon individual app, and\nfind that, after an initial phase of increased daily spending, users\ngradually lose interest: the delay between their purchases increases\nand the spending decreases with a sharp drop toward the end. Fi-\nnally, we model the in-app purchasing behavior in multiple steps:\n1) we model the time between purchases; 2) we train a classifier\nto predict whether the user will make a purchase from a new app\nor continue purchasing from the existing app; and 3) based on the\noutcome of the previous step, we attempt to predict the exact app,\nnew or existing, from which the next purchase will come. The re-\nsults yield new insights into spending habits in the mobile digital\nmarketplace."}
{"Title": "Raising Graphs From Randomness to Reveal\nInformation Networks", "Abstract": "We analyze the fine-grained connections between the aver-\nage degree and the power-law degree distribution exponent\nin growing information networks. Our starting observation\nis a power-law degree distribution with a decreasing expo-\nnent and increasing average degree as a function of the net-\nwork size. Our experiments are based on three Twitter at-\nmention networks and three more from the Koblenz Network\nCollection. We observe that popular network models cannot\nexplain decreasing power-law degree distribution exponent\nand increasing average degree at the same time.\nWe propose a model that is the combination of exponential\ngrowth, and a power-law developing network, in which new\n\u201chomophily\u201d edges are continuously added to nodes propor-\ntional to their current homophily degree. Parameters of the\naverage degree growth and the power-law degree distribu-\ntion exponent functions depend on the ratio of the network\ngrowth exponent parameters. Specifically, we connect the\ngrowth of the average degree to the decreasing exponent of\nthe power-law degree distribution. Prior to our work, only\none of the two cases were handled. Existing models and even\ntheir combinations can only reproduce some of our key new\nobservations in growing information networks."}
{"Title": "Anticipating Information Needs Based on Check-in Activity", "Abstract": "In this work we address the development of a smart personal assis-\ntant that is capable of anticipating a user\u2019s information needs based\non a novel type of context: the person\u2019s activity inferred from her\ncheck-inrecordsonalocation-basedsocialnetwork. Ourmaincon-\ntribution is a method that translates a check-in activity into an infor-\nmation need, which is in turn addressed with an appropriate infor-\nmation card. This task is challenging because of the large number\nof possible activities and related information needs, which need to\nbe addressed in a mobile dashboard that is limited in size. Our ap-\nproach considers each possible activity that might follow after the\nlast (and already finished) activity, and selects the top information\ncards such that they maximize the likelihood of satisfying the user\u2019s\ninformation needs for all possible future scenarios. The proposed\nmodelsalsoincorporateknowledgeaboutthetemporaldynamicsof\ninformation needs. Using a combination of historical check-in data\nand manual assessments collected via crowdsourcing, we show ex-\nperimentally the effectiveness of our approach."}
{"Title": "Reducing Controversy by Connecting Opposing Views", "Abstract": "Society is often polarized by controversial issues that split\nthe population into groups with opposing views. When such\nissues emerge on social media, we often observe the creation\nof \u2018echo chambers\u2019, i.e., situations where like-minded people\nreinforce each other\u2019s opinion, but do not get exposed to\nthe views of the opposing side. In this paper we study\nalgorithmic techniques for bridging these chambers, and thus\nreduce controversy. Specifically, we represent the discussion\non a controversial issue with an endorsement graph, and\ncast our problem as an edge-recommendation problem on\nthis graph. The goal of the recommendation is to reduce\nthe controversy score of the graph, which is measured by a\nrecently-developed metric based on random walks. At the\nsame time, we take into account the acceptance probability\nof the recommended edge, which represents how likely the\nedge is to materialize in the endorsement graph.\nWe propose a simple model based on a recently-developed\nuser-level controversy score, that is competitive with state-\nof-the-art link-prediction algorithms. Our goal then becomes\nfinding the edges that produce the largest reduction in the\ncontroversy score, in expectation. To solve this problem, we\npropose an efficient algorithm that considers only a fraction of\nall the possible combinations of edges. Experimental results\nshow that our algorithm is more efficient than a simple greedy\nheuristic, while producing comparable score reduction. Fi-\nnally, a comparison with other state-of-the-art edge-addition\nalgorithms shows that this problem is fundamentally different\nfrom what has been studied in the literature."}
{"Title": "Understanding and Discovering Deliberate Self-harm\nContent in Social Media", "Abstract": "Studies suggest that self-harm users found it easier to discuss\nself-harm-related thoughts and behaviors using social media\nthan in the physical world. Given the enormous and increas-\ning volume of social media data, on-line self-harm content\nis likely to be buried rapidly by other normal content. To\nenable voices of self-harm users to be heard, it is important\nto distinguish self-harm content from other types of con-\ntent. In this paper, we aim to understand self-harm content\nand provide automatic approaches to its detection. We first\nperform a comprehensive analysis on self-harm social media\nusing different input cues. Our analysis, the first of its kind\nin large scale, reveals a number of important findings. Then\nwe propose frameworks that incorporate the findings to dis-\ncover self-harm content under both supervised and unsuper-\nvised settings. Our experimental results on a large social\nmedia dataset from Flickr demonstrate the effectiveness of\nthe proposed frameworks and the importance of our findings\nin discovering self-harm content."}
{"Title": "Harnessing the Web for Population-Scale Physiological\nSensing: A Case Study of Sleep and Performance", "Abstract": "Human cognitive performance is critical to productivity, learning,\nand accident avoidance. Cognitive performance varies throughout\neach day and is in part driven by intrinsic, near 24-hour circa-\ndian rhythms. Prior research on the impact of sleep and circadian\nrhythms on cognitive performance has typically been restricted to\nsmall-scale laboratory-based studies that do not capture the vari-\nability of real-world conditions, such as environmental factors, mo-\ntivation, and sleep patterns in real-world settings. Given these limi-\ntations, leading sleep researchers have called for larger in situ mon-\nitoring of sleep and performance [39]. We present the largest study\nto date on the impact of objectively measured real-world sleep on\nperformance enabled through a reframing of everyday interactions\nwith a web search engine as a series of performance tasks. Our\nanalysis includes 3 million nights of sleep and 75 million interac-\ntion tasks. We measure cognitive performance through the speed of\nkeystroke and click interactions on a web search engine and corre-\nlate them to wearable device-defined sleep measures over time. We\ndemonstrate that real-world performance varies throughout the day\nand is influenced by both circadian rhythms, chronotype (morn-\ning/evening preference), and prior sleep duration and timing. We\ndevelopastatisticalmodelthatoperationalizesalargebodyofwork\non sleep and performance and demonstrates that our estimates of\ncircadian rhythms, homeostatic sleep drive, and sleep inertia align\nwith expectations from laboratory-based sleep studies. Further, we\nquantify the impact of insufficient sleep on real-world performance\nand show that two consecutive nights with less than six hours of\nsleep are associated with decreases in performance which last for a\nperiod of six days. This work demonstrates the feasibility of using\nonline interactions for large-scale physiological sensing."}
{"Title": "Does Document Relevance Affect the Searcher\u2019s\nPerception of Time?", "Abstract": "Time plays an essential role in multiple areas of Information Re-\ntrieval (IR) studies such as search evaluation, user behavior anal-\nysis, temporal search result ranking and query understanding. Es-\npecially, in search evaluation studies, time is usually adopted as a\nmeasure to quantify users\u2019 efforts in search processes. Psychologi-\ncal studies have reported that the time perception of human beings\ncan be affected by many stimuli, such as attention and motivation,\nwhich are closely related to many cognitive factors in search. Con-\nsidering the fact that users\u2019 search experiences are affected by their\nsubjective feelings of time, rather than the objective time measured\nby timing devices, it is necessary to look into the different factors\nthat have impacts on search users\u2019 perception of time. In this work,\nwe make a first step towards revealing the time perception mecha-\nnism of search users with the following contributions: (1) We es-\ntablish an experimental research framework to measure the subjec-\ntive perception of time while reading documents in search scenario,\nwhich originates from but is also different from traditional time\nperception measurements in psychological studies. (2) With the\nframework, we show that while users are reading result documents,\ndocument relevance has small yet visible effect on search users\u2019\nperception of time. By further examining the impact of other fac-\ntors, we demonstrate that the effect on relevant documents can also\nbe influenced by individuals and tasks. (3) We conduct a prelim-\ninary experiment in which the difference between perceived time\nand dwell time is taken into consideration in a search evaluation\ntask. We found that the revised framework achieved a better cor-\nrelation with users\u2019 satisfaction feedbacks. This work may help us\nbetter understand the time perception mechanism of search users\nand provide insights in how to better incorporate time factor in\nsearch evaluation studies."}
{"Title": "Generating Illustrative Snippets for Open Data on the Web", "Abstract": "To embrace the open data movement, increasingly many\ndatasets have been published on the Web to be reused.\nUsers, when assessing the usefulness of an unfamiliar dataset,\nneed means to quickly inspect its contents. To satisfy the\nneeds, we propose to automatically extract an optimal small\nportion from a dataset, called a snippet, to concisely illus-\ntrate the contents of the dataset. We consider the qual-\nity of a snippet from three aspects: coverage, familiarity,\nand cohesion, which are jointly formulated in a new combi-\nnatorial optimization problem called the maximum-weight-\nand-coverage connected graph problem (MwcCG). We give\na constant-factor approximation algorithm for this NP-hard\nproblem, and experiment with our solution on real-world\ndatasets. Our quantitative analysis and user study show\nthat our approach outperforms a baseline approach."}
{"Title": "Investigation of User Search Behavior While Facing\nHeterogeneous Search Services", "Abstract": "With Web users\u2019 search tasks becoming increasingly\ncomplex, a single information source cannot necessarily\nsatisfy their information needs. Searchers may rely on\nheterogeneous sources to complete their tasks, such as\nsearch engines, Community Question Answering (CQA),\nencyclopedia sites, and crowdsourcing platforms. Previous\nworks focus on interaction behaviors with federated search\nresults, including how to compose a federated Web search\nresult page and what factors affect users\u2019 interaction\nbehavior on aggregated search interfaces. However, little is\nknown about which factors are crucial in determining\nusers\u2019 search outcomes while facing multiple heterogeneous\nsearch services. In this paper, we design a lab-based user\nstudy to analyze what explicit and implicit factors affect\nsearch outcomes (information gain and user satisfaction)\nwhen users have access to heterogeneous information\nsources. In the study, each participant can access three\ndifferent kinds of search services: a general search engine\n(Bing), a general CQA portal (Baidu Knows), and a\nhigh-quality CQA portal (Zhihu). Using questionnaires\nand interaction log data, we extract explicit and implicit\nsignals to analyze how users\u2019 search outcomes are\ncorrelated with their behaviors on different information\nsources. Experimental results indicate that users\u2019 search\nexperiences on CQA portals (such as users\u2019 perceived\nusefulness and number of result clicks) positively affect\nsearch outcome (information gain), while search\nsatisfaction is significantly correlated with some other\nfactors such as users\u2019 familiarity, interest and difficulty of\nthe task. Besides, users\u2019 search satisfaction can be more\naccurately predicted by the implicit factors than search\noutcomes."}
{"Title": "Click Through Rate Prediction for Local Search Results", "Abstract": "With the ubiquity of internet access and location services\nprovided by smartphone devices, the volume of queries is-\nsued by users to find products and services that are located\nnear them is rapidly increasing. Local search engines help\nusers in this task by matching queries with a predefined ge-\nographical connotation (\u201clocal queries\u201d) against a database\nof local business listings.\nLocal search differs from traditional web-search because\nto correctly capture users\u2019 click behavior, the estimation of\nrelevance between query and candidate results must be in-\ntegrated with geographical signals, such as distance. The\nintuition is that users prefer businesses that are physically\ncloser to them. However, this notion of closeness is likely to\ndepend upon other factors, like the category of the business,\nthe quality of the service provided, the density of businesses\nin the area of interest, etc.\nIn this paper we perform an extensive analysis of online\nusers\u2019 behavior and investigate the problem of estimating the\nclick-through rate on local search (LCTR) by exploiting the\ncombination of standard retrieval methods with a rich col-\nlection of geo and business-dependent features. We validate\nour approach on a large log collected from a real-world lo-\ncal search service. Our evaluation shows that the non-linear\ncombination of business information, geo-local and textual\nrelevance features leads to a significant improvements over\nstate of the art alternative approaches based on a combina-\ntion of relevance, distance and business reputation."}
{"Title": "Document Retrieval Model Through Semantic Linking", "Abstract": "This paper addresses the task of document retrieval based\non the degree of document relatedness to the meanings of\na query by presenting a semantic-enabled language model.\nOur model relies on the use of semantic linking systems for\nforming a graph representation of documents and queries,\nwhere nodes represent concepts extracted from documents\nand edges represent semantic relatedness between concepts.\nBased on this graph, our model adopts a probabilistic rea-\nsoning model for calculating the conditional probability of a\nquery concept given values assigned to document concepts.\nWe present an integration framework for interpolating other\nretrieval systems with the presented model in this paper.\nOur empirical experiments on a number of TREC collections\nshow that the semantic retrieval has a synergetic impact on\nthe results obtained through state of the art keyword-based\napproaches, and the consideration of semantic information\nobtained from entity linking on queries and documents can\ncomplement and enhance the performance of other retrieval\nmodels."}
{"Title": "Beyond the Words: Predicting User Personality from\nHeterogeneous Information", "Abstract": "An incisive understanding of user personality is not only essen-\ntial to many scientific disciplines, but also has a profound business\nimpact on practical applications such as digital marketing, person-\nalized recommendation, mental diagnosis, and human resources\nmanagement. Previous studies have demonstrated that language\nusage in social media is effective in personality prediction. How-\never, except for single language features, a less researched direc-\ntion is how to leverage the heterogeneous information on social\nmedia to have a better understanding of user personality. In this\npaper, we propose a Heterogeneous Information Ensemble frame-\nwork, called HIE, to predict users\u2019 personality traits by integrating\nheterogeneous information including self-language usage, avatar,\nemoticon, and responsive patterns. In our framework, to improve\nthe performance of personality prediction, we have designed differ-\nent strategies extracting semantic representations to fully leverage\nheterogeneous information on social media. We evaluate our meth-\nods with extensive experiments based on a real-world data covering\nboth personality survey results and social media usage from thou-\nsands of volunteers. The results reveal that our approaches signif-\nicantly outperform several widely adopted state-of-the-art baseline\nmethods. To figure out the utility of HIE in a real-world interac-\ntive setting, we also present DiPsy, a personalized chatbot to pre-\ndict user personality through heterogeneous information in digital\ntraces and conversation logs."}
{"Title": "Constructing and Embedding Abstract Event\nCausality Networks from Text Snippets", "Abstract": "In this paper, we formally define the problem of repre-\nsenting and leveraging abstract event causality to power\ndownstream applications. We propose a novel solution to\nthis problem, which build an abstract causality network and\nembed the causality network into a continuous vector space.\nThe abstract causality network is generalized from a specific\none, with abstract event nodes represented by frequently co-\noccurring word pairs. To perform the embedding task, we\ndesign a dual cause-effect transition model. Therefore, the\nproposed method can obtain general, frequent, and simple\ncausality patterns, meanwhile, simplify event matching.\nGiven the causality network and the learned embeddings,\nour model can be applied to a wide range of applications such\nas event prediction, event clustering and stock market move-\nment prediction. Experimental results demonstrate that 1)\nthe abstract causality network is effective for discovering\nhigh-level causality rules behind specific causal events; 2) the\nembedding models perform better than state-of-the-art link\nprediction techniques in predicting events; and 3) the event\ncausality embedding is an easy-to-use and sophisticated\nfeature for downstream applications such as stock market\nmovement prediction."}
{"Title": "Fun Facts: Automatic Trivia Fact Extraction from Wikipedia", "Abstract": "A significant portion of web search queries directly refers to\nnamed entities. Search engines explore various ways to im-\nprove the user experience for such queries. We suggest aug-\nmenting search results with trivia facts about the searched\nentity. Trivia is widely played throughout the world, and\nwas shown to increase users\u2019 engagement and retention.\nMost random facts are not suitable for the trivia sec-\ntion. There is skill (and art) to curating good trivia. In\nthis paper, we formalize a notion of trivia-worthiness and\npropose an algorithm that automatically mines trivia facts\nfrom Wikipedia. We take advantage of Wikipedia\u2019s category\nstructure, and rank an entity\u2019s categories by their trivia-\nquality. Our algorithm is capable of finding interesting facts,\nsuch as Obama\u2019s Grammy or Elvis\u2019 stint as a tank gunner.\nIn user studies, our algorithm captures the intuitive notion of\n\u201cgood trivia\u201d45% higher than prior work. Search-page tests\nshow a 22% decrease in bounce rates and a 12% increase in\ndwell time, proving our facts hold users\u2019 attention."}
{"Title": "Related Event Discovery", "Abstract": "We consider the problem of discovering local events on the web,\nwhere events are entities extracted from webpages. Examples of\nsuch local events include small venue concerts, farmers markets,\nsports activities, etc. Given an event entity, we propose a graph-\nbased framework for retrieving a ranked list of related events that a\nuser is likely to be interested in attending. Due to the difficulty of\nobtaining ground-truth labels for event entities, which are temporal\nand areconstrained by location, our retrievalframework is unsuper-\nvised, and its graph-based formulation addresses (a) the challenge\nof feature sparseness and noisiness, and (b) the semantic mismatch\nproblem in a self-contained and principled manner.\nTo validate our methods, we collect human annotations and con-\nduct a comprehensive empirical study, analyzing the performance\nof our methods with regard to relevance, recall, and diversity. This\nstudy shows that our graph-based framework is significantly bet-\nter than any individual feature source, and can be further improved\nwith minimal supervision."}
{"Title": "Lightweight Multilingual Entity Extraction and Linking", "Abstract": "Text analytics systems often rely heavily on detecting and\nlinking entity mentions in documents to knowledge bases for\ndownstream applications such as sentiment analysis, ques-\ntion answering and recommender systems. A major chal-\nlenge for this task is to be able to accurately detect en-\ntities in new languages with limited labeled resources. In\nthis paper we present an accurate and lightweight 1 multi-\nlingual named entity recognition (NER) and linking (NEL)\nsystem. The contributions of this paper are three-fold: 1)\nLightweight named entity recognition with competitive ac-\ncuracy; 2) Candidate entity retrieval that uses search click-\nlog data and entity embeddings to achieve high precision\nwith a low memory footprint; and 3) efficient entity dis-\nambiguation. Our system achieves state-of-the-art perfor-\nmance on tac kbp 2013 multilingual data and on English\naida-conll data."}
{"Title": "Predicting Completeness in Knowledge Bases", "Abstract": "Knowledge bases such as Wikidata, DBpedia, or YAGO con-\ntain millions of entities and facts. In some knowledge bases,\nthe correctness of these facts has been evaluated. However,\nmuch less is known about their completeness, i.e., the pro-\nportion of real facts that the knowledge bases cover. In this\nwork, we investigate different signals to identify the areas\nwhere the knowledge base is complete. We show that we\ncan combine these signals in a rule mining approach, which\nallows us to predict where facts may be missing. We also\nshow that completeness predictions can help other applica-\ntions such as fact inference."}
{"Title": "Synthesis of Forgiving Data Extractors", "Abstract": "We address the problem of synthesizing a robust data-extractor\nfrom a family of websites that contain the same kind of informa-\ntion. This problem is common when trying to aggregate informa-\ntion from many web sites, for example, when extracting informa-\ntion for a price-comparison site.\nGiven a set of example annotated web pages from multiple sites\nin a family, our goal is to synthesize a robust data extractor that per-\nforms well on all sites in the family (not only on the provided ex-\nample pages). The main challenge is the need to trade off precision\nfor generality and robustness. Our key contribution is the introduc-\ntion of forgiving extractors that dynamically adjust their precision\nto handle structural changes, without sacrificing precision on the\ntraining set. Our approach uses decision tree learning to create a\ngeneralized extractor and converts it into a forgiving extractor, in\nthe form of an XPath query. The forgiving extractor captures a se-\nries of pruned decision trees with monotonically decreasing preci-\nsion, and monotonically increasing recall, and dynamically adjusts\nprecision to guarantee sufficient recall.\nWe have implemented our approach in a tool called T REEX and\napplied it to synthesize extractors for real-world large scale web\nsites. We evaluate the robustness and generality of the forgiving\nextractors by evaluating their precision and recall on: (i) different\npages from sites in the training set (ii) pages from different versions\nof sites in the training set (iii) pages from different (unseen) sites.\nWe compare the results of our synthesized extractor to those of\nclassifier-based extractors, and pattern-based extractors, and show\nthat T REEX significantly improves extraction accuracy"}
{"Title": "Concept Embedded Convolutional Semantic Model for\nQuestion Retrieval", "Abstract": "The question retrieval, which aims to find similar questions of a\ngiven question, is playing pivotal role in various question answer-\ning (QA) systems. This task is quite challenging mainly on three\naspects: lexical gap, polysemy and word order. In this paper, we\npropose a unified framework to simultaneously handle these three\nproblems. We use word combined with corresponding concept in-\nformation to handle the polysemous problem. The concept embed-\nding and word embedding are learned at the same time from both\ncontext-dependent and context-independent view. The lexical gap\nproblem is handled since the semantic information has been encod-\nedintotheembedding. Then, weproposetouseahigh-levelfeature\nembedded convolutional semantic model to learn the question em-\nbedding by inputting the concept embedding and word embedding\nwithout manually labeling training data. The proposed framework\nnicely represent the hierarchical structures of word information and\nconcept information in sentences with their layer-by-layer compo-\nsition and pooling. Finally, the framework is trained in a weakly-\nsupervised manner on question answer pairs, which can be direct-\nly obtained without manually labeling. Experiments on two real\nquestion answering datasets show that the proposed framework can\nsignificantly outperform the state-of-the-art solutions"}
{"Title": "Joint Deep Modeling of Users and Items Using Reviews for\nRecommendation", "Abstract": "A large amount of information exists in reviews written by\nusers. This source of information has been ignored by most\nof the current recommender systems while it can potentially\nalleviate the sparsity problem and improve the quality of rec-\nommendations. In this paper, we present a deep model to\nlearn item properties and user behaviors jointly from review\ntext. The proposed model, named Deep Cooperative Neural\nNetworks (DeepCoNN), consists of two parallel neural net-\nworks coupled in the last layers. One of the networks focuses\non learning user behaviors exploiting reviews written by the\nuser, and the other one learns item properties from the re-\nviews written for the item. A shared layer is introduced on\nthe top to couple these two networks together. The shared\nlayer enables latent factors learned for users and items to\ninteract with each other in a manner similar to factoriza-\ntion machine techniques. Experimental results demonstrate\nthat DeepCoNN significantly outperforms all baseline rec-\nommender systems on a variety of datasets."}
{"Title": "Groove Radio: A Bayesian Hierarchical Model for\nPersonalized Playlist Generation", "Abstract": "This paper describes an algorithm designed for Microsoft\u2019s\nGroove music service, which serves millions of users world\nwide. We consider the problem of automatically generating\npersonalized music playlists based on queries containing a\n\u201cseed\u201d artist and the listener\u2019s user ID. Playlist generation\nmay be informed by a number of information sources in-\ncluding: user specific listening patterns, domain knowledge\nencoded in a taxonomy, acoustic features of audio tracks,\nand overall popularity of tracks and artists. The importance\nassigned to each of these information sources may vary de-\npending on the specific combination of user and seed artist.\nThe paper presents a method based on a variational Bayes\nsolution for learning the parameters of a model containing a\nfour-level hierarchy of global preferences, genres, sub-genres\nand artists. The proposed model further incorporates a per-\nsonalization component for user-specific preferences. Em-\npirical evaluations on both proprietary and public datasets\ndemonstrate the effectiveness of the algorithm and showcase\nthe contribution of each of its components."}
{"Title": "Social Collaborative Viewpoint Regression\nwith Explainable Recommendations", "Abstract": "A recommendation is called explainable if it not only predicts a\nnumerical rating for an item, but also generates explanations for\nusers\u2019 preferences. Most existing methods for explainable recom-\nmendation apply topic models to analyze user reviews to provide\ndescriptions along with the recommendations they produce. So far,\nsuch methods have neglected user opinions and influences from so-\ncial relations as a source of information for recommendations, even\nthough these are known to improve the rating prediction.\nIn this paper, we propose a latent variable model, called social\ncollaborative viewpoint regression (sCVR), for predicting item rat-\nings based on user opinions and social relations. To this end, we\nuse so-called viewpoints, represented as tuples of a concept, topic,\nand a sentiment label from both user reviews and trusted social re-\nlations. In addition, such viewpoints can be used as explanations.\nWe apply a Gibbs EM sampler to infer posterior distributions of\nsCVR. Experiments conducted on three large benchmark datasets\nshow the effectiveness of our proposed method for predicting item\nratings and for generating explanations."}
{"Title": "Recurrent Recommender Networks", "Abstract": "Recommender systems traditionally assume that user pro-\nfiles and movie attributes are static. Temporal dynamics are\npurely reactive, that is, they are inferred after they are ob-\nserved, e.g. after a user\u2019s taste has changed or based on hand-\nengineered temporal bias corrections for movies. We propose\nRecurrent Recommender Networks (RRN) that are able to\npredict future behavioral trajectories. This is achieved by\nendowing both users and movies with a Long Short-Term\nMemory (LSTM) [14] autoregressive model that captures\ndynamics, in addition to a more traditional low-rank factor-\nization. On multiple real-world datasets, our model offers\nexcellent prediction accuracy and it is very compact, since\nwe need not learn latent state but rather just the state tran-\nsition function."}
{"Title": "Neural Survival Recommender", "Abstract": "The ability to predict future user activity is invaluable when\nit comes to content recommendation and personalization.\nFor instance, knowing when users will return to an online\nmusic service and what they will listen to increases user\nsatisfaction and therefore user retention.\nWe present a model based on Long-Short Term Memory\nto estimate when a user will return to a site and what their\nfuture listening behavior will be. In doing so, we aim to\nsolve the problem of Just-In-Time recommendation, that is,\nto recommend the right items at the right time. We use tools\nfrom survival analysis for return time prediction and expo-\nnential families for future activity analysis. We show that\nthe resulting multitask problem can be solved accurately,\nwhen applied to two real-world datasets."}
{"Title": "Directed Edge Recommender System", "Abstract": "Recommender systems have become ubiquitous in online ap-\nplications where companies personalize the user experience\nbased on explicit or inferred user preferences. Most mod-\nern recommender systems concentrate on finding relevant\nitems for each individual user. In this paper, we describe\nthe problem of directed edge recommendations where the\nsystem recommends the best item that a user can gift, share\nor recommend to another user that he/she is connected to.\nWe propose algorithms that utilize the preferences of both\nthe sender and the recipient by integrating individual user\npreference models (e.g., based on items each user purchased\nfor themselves) with models of sharing preferences (e.g., gift\npurchases for others) into the recommendation process. We\ncompare our work to group recommender systems and social\nnetwork edge labeling, showing that incorporating the task\ncontext leads to more accurate recommendations."}
{"Title": "Online Actions with Offline Impact: How Online Social\nNetworks Influence Online and Offline User Behavior", "Abstract": "Many of today\u2019s most widely used computing applications utilize\nsocial networking features and allow users to connect, follow each\nother, share content, and comment on others\u2019 posts. However, de-\nspite the widespread adoption of these features, there is little un-\nderstanding of the consequences that social networking has on user\nretention, engagement, and online as well as offline behavior.\nHere, we study how social networks influence user behavior in\na physical activity tracking application. We analyze 791 million\nonline and offline actions of 6 million users over the course of 5\nyears, and show that social networking leads to a significant in-\ncrease in users\u2019 online as well as offline activities. Specifically,\nwe establish a causal effect of how social networks influence user\nbehavior. We show that the creation of new social connections in-\ncreases user online in-application activity by 30%, user retention\nby 17%, and user offline real-world physical activity by 7% (about\n400 steps per day). By exploiting a natural experiment we distin-\nguish the effect of social influence of new social connections from\nthe simultaneous increase in user\u2019s motivation to use the app and\ntake more steps. We show that social influence accounts for 55%\nof the observed changes in user behavior, while the remaining 45%\ncan be explained by the user\u2019s increased motivation to use the app.\nFurther, weshowthat subsequent, individual edgeformationsin the\nsocial network lead to significant increases in daily steps. These ef-\nfects diminish with each additional edge and vary based on edge at-\ntributes and user demographics. Finally, we utilize these insights to\ndevelop a model that accurately predicts which users will be most\ninfluenced by the creation of new social network connections."}
{"Title": "Social Incentive Optimization in Online Social Networks", "Abstract": "Most online social networks provide a mechanism for users\nto broadcast messages to their personalized network through\nactions like shares, likes and tweets. Receiving positive feed-\nback from the network such as likes, comments and retweets\nin response to such actions can provide a strong incentive\nfor users to broadcast more often in the future. We call such\nfeedback by the network, that influences a user to perform\ncertain desirable future actions, social incentives. For ex-\nample, after a user shares an article to her social network,\nreceiving positive feedback such as a \u201clike\u201d from a friend\ncan potentially encourage her to continue sharing more reg-\nularly. Typically, for every user\u2019s visit to an online social\nnetwork site, good messages need to be ranked and selected\nby a recommender system from a large set of candidate mes-\nsages (broadcasted by the user\u2019s network). In this paper,\nwe propose a novel recommendation problem: How should\nwe recommend messages to users to incentivize neighbors\nin their personal network to perform desirable actions in\nthe future with high likelihood, without significantly hurt-\ning overall engagement for the entire system? For instance,\nmessages could be content shared by neighbors. The goal\nin this case would be to encourage more content shares in\nthe future. We call this problem social incentive optimiza-\ntion and study an instance of it for LinkedIn\u2019s news feed.\nWe observe that a user who receives positive social feed-\nback from neighbors has a higher likelihood of broadcasting\nmore frequently. Using this observation, we develop a novel\nrecommendation framework that incentivize users to broad-\ncast more often, without significantly hurting overall feed\nengagement. We demonstrate the effectiveness of our ap-\nproach through causal analysis on retrospective data and\nonline A/B experiments."}
{"Title": "Representation Learning with Pair-wise Constraints\nfor Collaborative Ranking", "Abstract": "Last decades have witnessed a vast amount of interest and research\nin recommendation systems. Collaborative filtering which uses the\nknown preferences of a group of users to make recommendations\nor predictions of the unknown preferences for other users, is one of\nthe most successful approaches to build recommendation systems.\nMost previous collaborative filterin approaches employ the ma-\ntrix factorization techniques to learn latent user feature profile and\nitem feature profiles Also many subsequent works are proposed\nto incorporate users\u2019 social network information and items\u2019 attri-\nbutions to further improve recommendation performance under the\nmatrix factorization framework. However, the matrix factorization\nbased methods may not make full use of the rating information,\nleading to unsatisfying performance. Recently deep learning has\nbeen approved tobe able to fin good representations innatural lan-\nguage processing, image classification and so on. Along this line,\nwe propose a collaborative ranking framework via REpresentAtion\nlearning with Pair-wise constraints (REAP for short), in which au-\ntoencoder is used to simultaneously learn the latent factors of both\nusers and items and pair-wise ranked loss define by (user, item)\npairs is considered. Extensive experiments are conducted on fve\ndata sets to demonstrate the effectiveness of the proposed frame-\nwork."}
{"Title": "Managing Risk of Bidding in Display Advertising", "Abstract": "In this paper, we deal with the uncertainty of bidding for\ndisplay advertising. Similar to the financial market trad-\ning, real-time bidding (RTB) based display advertising em-\nploys an auction mechanism to automate the impression\nlevel media buying; and running a campaign is no differ-\nent than an investment of acquiring new customers in re-\nturn for obtaining additional converted sales. Thus, how\nto optimally bid on an ad impression to drive the profit\nand return-on-investment becomes essential. However, the\nlarge randomness of the user behaviors and the cost uncer-\ntainty caused by the auction competition may result in a\nsignificant risk from the campaign performance estimation.\nIn this paper, we explicitly model the uncertainty of user\nclick-through rate estimation and auction competition to\ncapture the risk. We borrow an idea from finance and de-\nrive the value at risk for each ad display opportunity. Our\nformulation results in two risk-aware bidding strategies that\npenalize risky ad impressions and focus more on the ones\nwith higher expected return and lower risk. The empirical\nstudy on real-world data demonstrates the effectiveness of\nour proposed risk-aware bidding strategies: yielding profit\ngains of 15.4% in offline experiments and up to 17.5% in\nan online A/B test on a commercial RTB platform over the\nwidely applied bidding strategies."}
{"Title": "Predicting Online Purchase Conversion for Retargeting\n\u2217", "Abstract": "Generally 2% of shoppers make a purchase on the first visit to an\nonline store while the other 98% enjoys only window-shopping.\nTo bring people back to the store and close the deal, \u201cretargeting\u201d\nhas been a vital online advertising strategy that leads to \u201cconver-\nsion\u201d of window-shoppers into buyers. As such retargeting is more\neffective as a focused tool, in this paper, we study the problem of\nidentifying a conversion rate for a given product and its current cus-\ntomers, which is an important analytics metric for retargeting pro-\ncess. Compared to existing approaches using either of customer-\nor product-level conversion pattern, we propose a joint modeling\nof both level patterns based on the well-studied buying decision\nprocess. To evaluate the effectiveness of our method, we perform\nextensive experiments on the simulated dataset generated based on\na set of real-world web logs. The evaluation results show that con-\nversion predictions by our approach are consistently more accurate\nand robust than those by existing baselines in dynamic market en-\nvironment."}
{"Title": "Modeling Air Travel Choice Behavior with Mixed Kernel\nDensity Estimations", "Abstract": "Understanding air travel choice behavior of air passengers\nis of great significance for various purposes such as travel\ndemand prediction and trip recommendation. Existing ap-\nproaches based on surveys can only provide aggregate level\nair travel choice behavior of passengers and they fail to pro-\nvide comprehensive information for personalized services. In\nthis paper we focus on modeling individual level air travel\nchoice behavior of passengers, which is valuable for recom-\nmendations and personalized services. We employ a proba-\nbilistic model to represent individual level air travel choice\nbehavior based on a large dataset of historical booking records,\nleveraging several key factors, such as takeoff time, arrival\ntime, elapsed time between reservation and takeoff, price,\nand seat class. However, each passenger has only a limited\nnumber of historical booking records, causing a serious data\nsparsity problem. To this end, we propose a mixed ker-\nnel density estimation (mix-KDE) approach for each pas-\nsenger with a mixture model that combines probabilistic es-\ntimation of both regularity of the individual himself and so-\ncial conformity of similar passengers. The proposed model\nis trained and evaluated via the expectation-maximization\n(EM) algorithm with a huge dataset of booking records of\nover 10 million air passengers from a popular online travel\nagency in China. Experimental results demonstrate that\nour mix-KDE approach outperforms the Gaussian mixture\nmodel (GMM) and the simple kernel density estimation in\nthe presence of the sparsity issue."}
{"Title": "Real-Time Bidding by Reinforcement Learning\nin Display Advertising", "Abstract": "The majority of online display ads are served through real-\ntime bidding (RTB) \u2014 each ad display impression is auc-\ntioned off in real-time when it is just being generated from a\nuser visit. To place an ad automatically and optimally, it is\ncritical for advertisers to devise a learning algorithm to clev-\nerly bid an ad impression in real-time. Most previous works\nconsider the bid decision as a static optimization problem of\neither treating the value of each impression independently\nor setting a bid price to each segment of ad volume. How-\never, the bidding for a given ad campaign would repeatedly\nhappen during its life span before the budget runs out. As\nsuch, each bid is strategically correlated by the constrained\nbudget and the overall effectiveness of the campaign (e.g.,\nthe rewards from generated clicks), which is only observed\nafter the campaign has completed. Thus, it is of great inter-\nest to devise an optimal bidding strategy sequentially so that\nthe campaign budget can be dynamically allocated across all\nthe available impressions on the basis of both the immedi-\nate and future rewards. In this paper, we formulate the bid\ndecision process as a reinforcement learning problem, where\nthe state space is represented by the auction information\nand the campaign\u2019s real-time parameters, while an action is\nthe bid price to set. By modeling the state transition via\nauction competition, we build a Markov Decision Process\nframework for learning the optimal bidding policy to opti-\nmize the advertising performance in the dynamic real-time\nbidding environment. Furthermore, the scalability problem\nfrom the large real-world auction volume and campaign bud-\nget is well handled by state value approximation using neural\nnetworks. The empirical study on two large-scale real-world\ndatasets and the live A/B testing on a commercial platform\nhave demonstrated the superior performance and high effi-\nciency compared to state-of-the-art methods."}
{"Title": "Probabilistic Social Sequential Model for\nTour Recommendation", "Abstract": "The pervasive growth of location-based services such as\nFoursquare and Yelp has enabled researchers to incorpo-\nrate better personalization into recommendation models by\nleveraging the geo-temporal breadcrumbs left by a plethora\nof travelers. In this paper, we explore Travel path recommen-\ndation, which is one of the applications of intelligent urban\nnavigation that aims in recommending sequence of point of\ninterest (POIs) to tourists. Currently, travelers rely on a\ntedious and time-consuming process of searching the web,\nbrowsing through websites such as Trip Advisor, and read-\ning travel blogs to compile an itinerary. On the other hand,\npeople who do not plan ahead of their trip find it extremely\ndifficult to do this in real-time since there are no automated\nsystems that can provide personalized itinerary for travelers.\nTo tackle this problem, we propose a tour recommendation\nmodel that uses a probabilistic generative framework to in-\ncorporate user\u2019s categorical preference, influence from their\nsocial circle, the dynamic travel transitions (or patterns) and\nthe popularity of venues to recommend sequence of POIs for\ntourists. Through comprehensive experiments over a rich\ndataset of travel patterns from Foursquare, we show that\nour model is capable of outperforming the state-of-the-art\nprobabilistic tour recommendation model by providing con-\ntextual and meaningful recommendation for travelers."}
{"Title": "Deep Memory Networks for Attitude Identification", "Abstract": "We consider the task of identifying attitudes towards a given set\nof entities from text. Conventionally, this task is decomposed into\ntwo separate subtasks: target detection that identifies whether each\nentity is mentioned in the text, either explicitly or implicitly, and\npolarity classification that classifies the exact sentiment towards an\nidentified entity (the target) into positive, negative, or neutral.\nInstead, we show that attitude identification can be solved with\nan end-to-end machine learning architecture, in which the two sub-\ntasks are interleaved by a deep memory network. In this way, sig-\nnals produced in target detection provide clues for polarity classi-\nfication, and reversely, the predicted polarity provides feedback to\nthe identification of targets. Moreover, the treatments for the set of\ntargets also influence each other \u2013 the learned representations may\nshare the same semantics for some targets but vary for others. The\nproposed deep memory network, the AttNet, outperforms methods\nthat do not consider the interactions between the subtasks or those\namong the targets, including conventional machine learning meth-\nods and the state-of-the-art deep learning models."}
{"Title": "Modeling Document Networks with Tree-Averaged Copula\nRegularization", "Abstract": "Document network is a kind of intriguing dataset which pro-\nvides both topical (texts) and topological (links) informa-\ntion. Most previous work assumes that documents closely\nlinked with each other share common topics. However, the\nassociations among documents are usually complex, which\nare not limited to the homophily (i.e., tendency to link to\nsimilar others). Actually, the heterophily (i.e., tendency to\nlink to different others) is another pervasive phenomenon\nin social networks. In this paper, we introduce a new tool,\ncalled copula, to separately model the documents and links,\nso that different copula functions can be applied to cap-\nture different correlation patterns. In statistics, a copula\nis a powerful framework for explicitly modeling the depen-\ndence of random variables by separating the marginals and\ntheir correlations. Though widely used in Economics, cop-\nulas have not been paid enough attention to by researchers\nin machine learning field. Besides, to further capture the\npotential associations among the unconnected documents,\nwe apply the tree-averaged copula instead of a single copula\nfunction. This improvement makes our model achieve bet-\nter expressive power, and also more elegant in algebra. We\nderive efficient EM algorithms to estimate the model param-\neters, and evaluate the performance of our model on three\ndifferent datasets. Experimental results show that our ap-\nproach achieves significant improvements on both topic and\nlink modeling compared with the current state of the art."}
{"Title": "Random Semantic Tensor Ensemble for\nScalable Knowledge Graph Link Prediction", "Abstract": "Link prediction on knowledge graphs is useful in numerous\napplication areas such as semantic search, question answer-\ning, entity disambiguation, enterprise decision support, rec-\nommender systems and so on. While many of these applica-\ntions require a reasonably quick response and may operate\non data that is constantly changing, existing methods often\nlack speed and adaptability to cope with these requirements.\nThis is aggravated by the fact that knowledge graphs are\noften extremely large and may easily contain millions of en-\ntities rendering many of these methods impractical. In this\npaper, we address the weaknesses of current methods by\nproposing Random Semantic Tensor Ensemble (RSTE), a\nscalable ensemble-enabled framework based on tensor factor-\nization. Our proposed approach samples a knowledge graph\ntensor in its graph representation and performs link predic-\ntion via ensembles of tensor factorization. Our experiments\non both publicly available datasets and real world enter-\nprise/sales knowledge bases have shown that our approach\nis not only highly scalable, parallelizable and memory effi-\ncient, but also able to increase the prediction accuracy sig-\nnificantly across all datasets."}
{"Title": "Unsupervised Ranking using Graph Structures and Node\nAttributes", "Abstract": "PageRank has been the signature unsupervised ranking model\nfor ranking node importance in a graph. One potential draw-\nback of PageRank is that its computation depends only on\ninput graph structures, not considering external informa-\ntion such as the attributes of nodes. This work proposes\nAttriRank, an unsupervised ranking model that considers\nnot only graph structure but also the attributes of nodes.\nAttriRank is unsupervised and domain-independent, which\nis different from most of the existing works requiring either\nground-truth labels or specific domain knowledge. Combin-\ning two reasonable assumptions about PageRank and node\nattributes, AttriRank transfers extra node information into\na Markov chain model to obtain the ranking. We further\ndevelop approximation for AttriRank and reduce its com-\nplexity to be linear to the number of nodes or links in the\ngraph, which makes it feasible for large network data. The\nexperiments show that AttriRank outperforms competing\nmodels in diverse graph ranking applications."}
{"Title": "How Users Explore Ontologies on the Web:\nA Study of NCBO\u2019s BioPortal Usage Logs", "Abstract": "Ontologies in the biomedical domain are numerous, highly\nspecialized and very expensive to develop. Thus, a cru-\ncial prerequisite for ontology adoption and reuse is effective\nsupport for exploring and finding existing ontologies. To-\nwards that goal, the National Center for Biomedical Ontol-\nogy (NCBO) has developed BioPortal\u2014an online repository\ncontaining more than 500 biomedical ontologies. In 2016,\nBioPortal represents one of the largest portals for explo-\nration of semantic biomedical vocabularies and terminolo-\ngies, which is used by many researchers and practitioners.\nWhile usage of this portal is high, we know very little about\nhow exactly users search and explore ontologies and what\nkind of usage patterns or user groups exist in the first place.\nDeeper insights into user behavior on such portals can pro-\nvide valuable information to devise strategies for a better\nsupport of users in exploring and finding existing ontologies,\nand thereby enable better ontology reuse. To that end, we\nstudy and group users according to their browsing behavior\non BioPortal and use data mining techniques to character-\nize and compare exploration strategies across ontologies. In\nparticular, we were able to identify seven distinct browsing\ntypes, all relying on different functionality provided by Bio-\nPortal. For example, Search Explorers extensively use the\nsearch functionality while Ontology Tree Explorers mainly\nrely on the class hierarchy for exploring ontologies. Further,\nwe show that specific characteristics of ontologies influence\nthe way users explore and interact with the website. Our\nresults may guide the development of more user-oriented\nsystems for ontology exploration on the Web."}
{"Title": "Utilizing Knowledge Graphs in\nText-centric Information Retrieval", "Abstract": "The past decade has witnessed the emergence of several pub-\nlicly available and proprietary knowledge graphs (KGs). The\nincreasing depth and breadth of content in KGs makes them\nnot only rich sources of structured knowledge by themselves\nbut also valuable resources for search systems. A surge of\nrecent developments in entity linking and retrieval methods\ngave rise to a new line of research that aims at utilizing KGs\nfor text-centric retrieval applications, making this an ideal\ntime to pause and report current findings to the community,\nsummarizing successful approaches, and soliciting new ideas.\nThis tutorial is the first to disseminate the progress in this\nemerging field to researchers and practitioners. "}
{"Title": "Social Media Anomaly Detection:\nChallenges and Solutions", "Abstract": "Anomaly detection is of critical importance to prevent ma-\nlicious activities such as bullying, terrorist attack planning,\nand fraud information dissemination. With the recent pop-\nularity of social media, new types of anomalous behaviors\narise, causing concerns from various parties. While a large\nbody of work haven been dedicated to traditional anomaly\ndetection problems, we observe a surge of research inter-\nests in the new realm of social media anomaly detection.\nIn this tutorial, we survey existing work on social media\nanomaly detection, focusing on the new anomalous phenom-\nena in social media and most recent techniques to detect\nthose special types of anomalies. We aim to provide a gen-\neral overview of the problem domain, common formulations,\nexisting methodologies and future directions."}
{"Title": "WSDM 2017 Workshop on Mining Online Health Reports", "Abstract": "The workshop on Mining Online Health Reports (MOHRS)\ndraws upon the rapidly developing field of Computational\nHealth, focusing on textual content that has been gener-\nated through various activities on the Web. Online user-\ngenerated information mining, especially from social media\nplatforms and search engines, has been in the forefront of\nmany research efforts, especially in the fields of Informa-\ntion Retrieval and Natural Language Processing. The in-\ncorporation of such data and techniques in a number of\nhealth-oriented applications has provided strong evidence\nof the potential benefits, which include better population\ncoverage, timeliness and applicability to places with less\nestablished health infrastructure. The workshop provides\nan opportunity to present relevant state-of-the-art research,\nand a venue for discussion between researchers with cross-\ndisciplinary backgrounds. It will focus on the characterisa-\ntion of data sources, the essential methods for mining this\ntextual information, as well as potential real-world applica-\ntions and the arising ethical issues. MOHRS \u201917 will feature\n3 keynote talks and 4 accepted paper presentations, as well\nas a panel discussion."}
{"Title": "A Neural Network-based Framework for Non-factoid Question\nAnswering", "Abstract": "In this paper, we present a neural network based framework for\nanswering non-factoid questions. The framework consists of two\nmain components: Answer Retriever and Answer Ranker. In the\nfirst component, we leverage off-the-shelf retrieval models (e.g.\nbm25) to retrieve a pool of candidate answers regarding to the\ninput question. Answer Ranker is then used to select the most\nsuitable answer. In this work, we adopt two typical deep learning\nbased frameworks for our Answer Ranker component. One is based\non Siamese architecture and the other is the Compare-Aggregate\nframework. The Answer Ranker component is evaluated separately\nbased on popular answer selection datasets. Our overall system is\nevaluated using FiQA dataset, a newly released dataset for financial\ndomain and shows promising results"}
{"Title": "Aspect-based Financial Sentiment Analysis with Deep Neural\nNetworks", "Abstract": "Aspect-based financial sentiment analysis, which aims to classify\nthe text instance into a pre-defined aspect class and predict the\nsentiment score for the mentioned target. In this paper, we propose\na neural network model, Attention-based LSTM model with the\nAspect information (ALA), to solve the financial opinion mining\nproblem introduced by the WWW 2018 shared task. The proposed\nneural network model can adapt to the financial dataset so that the\nneuralnetworkcaneffectivelyunderstandthesemanticinformation\nof the short text. We evaluate our model with the 10-fold cross-\nvalidation, and we compare our model with a variety of related\ndeep neural network models."}
{"Title": "Aspect-Based Financial Sentiment Analysis using Deep Learning", "Abstract": "Aspect based sentiment analysis aims to detect an aspect (i.e. fea-\ntures) in a given text and then perform sentiment analysis of the\ntext with respect to that aspect. This paper aims to give a solution\nfor the FiQA 2018 challenge subtask 1 1 . We perform aspect-based\nsentiment analysis on the microblogs and headlines of financial\ndomain. We use a multi-channel convolutional neural network for\nsentiment analysis and a recurrent neural network with bidirec-\ntional long short-term memory units to extract aspect from a given\nheadline or microblog. Our proposed model produces a weighted\naverage F1 score of 0.69 for the aspect extraction task and predicts\nsentiment intensity scores with a mean squared error of 0.112 on\n10-fold cross validation. We believe that the developed system has\ndirect applications in the financial domain."}
{"Title": "Field-weighted Factorization Machines for Click-Through Rate\nPrediction in Display Advertising", "Abstract": "Click-through rate (CTR) prediction is a critical task in online dis-\nplay advertising. The data involved in CTR prediction are typically\nmulti-field categorical data, i.e., every feature is categorical and\nbelongs to one and only one field. One of the interesting charac-\nteristics of such data is that features from one field often interact\ndifferently with features from different other fields. Recently, Field-\naware Factorization Machines (FFMs) have been among the best\nperforming models for CTR prediction by explicitly modeling such\ndifference. However, the number of parameters in FFMs is in the\norder of feature number times field number, which is unacceptable\nin the real-world production systems. In this paper, we propose\nField-weighted Factorization Machines (FwFMs) to model the dif-\nferent feature interactions between different fields in a much more\nmemory-efficient way. Our experimental evaluations show that\nFwFMs can achieve competitive prediction performance with only\nas few as 4% parameters of FFMs. When using the same number of\nparameters, FwFMs can bring 0 . 92% and 0 . 47% AUC lift over FFMs\non two real CTR prediction data sets."}
{"Title": "Financial Aspect and Sentiment Predictions with Deep Neural\nNetworks: An Ensemble Approach", "Abstract": "In this paper, we describe our ensemble approach for sentiment\nand aspect predictions in the financial domain for a given text. This\nensemble approach uses Convolutional Neural Networks (CNNs)\nand Recurrent Neural Networks (RNNs) with a ridge regression\nand a voting strategy for sentiment and aspect predictions, and\ntherefore,doesnotrelyonanyhandcraftedfeature.Basedon5-cross\nvalidation on the released training set, the results show that CNNs\noverall perform better than RNNs on both tasks, and the ensemble\napproach can boost the performance further by leveraging different\ntypes of deep learning approaches."}
{"Title": "Identifying Your Representative Work Based on Credit\nAllocation", "Abstract": "With the rapid development of scientific impact quantifica-\ntion in the field of science of success, the ability to identify\nthe representative work of a researcher has important impli-\ncations in a wide range of areas, including hiring, funding,\nand promotion systems. In this paper, we propose a two-step\ncredit allocation algorithm (TSCA) for identifying the rep-\nresentative work of a researcher. This algorithm explicitly\ncaptures the importance of a paper, its relevance to other\npapers, and the unequally distributed contribution of each\ncitation. We validate TSCA by applying it on the citation\ndata from American Physical Society (APS) in the scenario of\nidentifying the Nobel prize winning papers of the Nobel laure-\nates. Experiments demonstrate that the proposed algorithm\ncan significantly outperform the existing methods."}
{"Title": "That Makes Sense: Joint Sense Retrofitting from Contextual\nand Ontological Information", "Abstract": "While recent word embedding models demonstrate their abilities\nto capture syntactic and semantic information, the demand for\nsense level embedding is getting higher. In this study, we\npropose a novel joint sense embedding learning model that\nretrofits word representation into sense representation from\ncontextual and ontological information. The experiment shows\nthe effectiveness and robustness of our model that outperforms\nprevious approaches in four public available benchmark datasets."}
{"Title": "A Fresh Look at Understanding News Events Evolution", "Abstract": "This paper proposes a novel approach to retrieve news articles\nrelated to a specific event and generate a storyline to help people\nunderstand the event evolution. First, a similarity calculation\nmethod is proposed to retrieve news articles related to the\nspecific event, which combines textual similarity, temporal\nsimilarity and entity similarity. Then a multi-view attribute\ngraph is constructed to represent the relationship between\nretrieved articles. Finally, a community detection algorithm is\ndeveloped to segment and chain subevents in the graph.\nExperimental results on real-world datasets demonstrate that the\nproposed approach achieve better results than existing\nmethods. 0F0F "}
{"Title": "Hierarchical Type Constrained Topic Entity Detection for\nKnowledge Base Question Answering", "Abstract": "Topic entity detection is to find out the main entity asked in a ques-\ntion, which is significant in question answering. Traditional meth-\nods ignore the information of entities, especially entity types and\ntheir hierarchical structures, restricting the performance. To take\nfull advantage of Knowledge Base(KB) and detect topic entities cor-\nrectly, we propose a deep neural model to leverage type hierarchy\nand relations of entities in KB. Experimental results demonstrate\nthe effectiveness of the proposed method."}
{"Title": "Multi-task Learning for Author Profiling with\nHierarchical Features", "Abstract": "Author profiling is an important but challenging task. In this pa-\nper, we propose a novel Multi-Task learning framework for Author\nProfiling (MTAP), in which a document modeling module is shared\nacross three different author profiling tasks (i.e., age, gender and\njob classification tasks). To further boost author profiling, we inte-\ngrate hierarchical features learned by different models. Concretely,\nwe employ CNN, LSTM and topic model to learn the character-\nlevel, word-level and topic-level features, respectively. MTAP thus\nleverages the benefits of supervised deep neural neural networks\nas well as an unsupervised probabilistic generative model to en-\nhance the document representation learning. Experimental results\non a real-life blog dataset show that MTAP has robust superiority\nover competitors and sets state-of-the-art for all the three author\nprofiling tasks 2 ."}
{"Title": "Open Information Extraction with Global Structure Constraints", "Abstract": "Extracting entities and their relations from text is an important task\nfor understanding massive text corpora. Open information extrac-\ntion (IE) systems mine relation tuples (i.e., entity arguments and a\npredicate string to describe their relation) from sentences. However,\ncurrent open IE systems ignore the fact that global statistics in a\nlarge corpus can be collectively leveraged to identify high-quality\nsentence-level extractions. In this paper, we propose a novel open\nIE system, called ReMine, which integrates local context signal\nand global structural signal in a unified framework with distant\nsupervision. The new system can be efficiently applied to different\ndomains as it uses facts from external knowledge bases as super-\nvision; and can effectively score sentence-level tuple extractions\nbased on corpus-level statistics. Specifically, we design a joint opti-\nmization problem to unify (1) segmenting entity/relation phrases\nin individual sentences based on local context; and (2) measuring\nthe quality of sentence-level extractions with a translating-based\nobjective. Experiments on real-world corpora from different do-\nmains demonstrate the effectiveness and robustness of ReMine\nwhen compared to other open IE systems."}
{"Title": "A Fast Deep Learning Model for Textual Relevance\nin Biomedical Information Retrieval", "Abstract": "Publications in the life sciences are characterized by a large tech-\nnical vocabulary, with many lexical and semantic variations for\nexpressing the same concept. Towards addressing the problem of\nrelevance in biomedical literature search, we introduce a deep learn-\ning model for the relevance of a document\u2019s text to a keyword style\nquery. Limited by a relatively small amount of training data, the\nmodel uses pre-trained word embeddings. With these, the model\nfirst computes a variable-length Delta matrix between the query\nand document, representing a difference between the two texts,\nwhich is then passed through a deep convolution stage followed by\na deep feed-forward network to compute a relevance score. This\nresults in a fast model suitable for use in an online search engine.\nThe model is robust and outperforms comparable state-of-the-art\ndeep learning approaches."}
{"Title": "Demystifying Advertising Campaign for CPA Goal Optimization", "Abstract": "In cost-per-click (CPC) or cost-per-impression (CPM) advertising\ncampaigns, advertisers always run the risk of spending the bud-\nget without getting enough conversions. Moreover, the bidding on\nadvertising inventory has few connections with propensity that\ncan reach to cost-per-acquisition (CPA) goals. To address this prob-\nlem, this paper presents a bid optimization scenario to achieve the\ndesired CPA goals for advertisers. In particular, we build the op-\ntimization engine to make a decision by solving the constrained\noptimization problem. The proposed model can naturally recom-\nmend the bid that meets the advertisers\u2019 expectations by making\ninference over history auction behaviors. The bid optimization\nmodel outperforms the baseline methods on real-world campaigns,\nand can be applied into a wide range of scenarios for performance\nimprovement and revenue liftup."}
{"Title": "Matching Resumes to Jobs via Deep Siamese Network", "Abstract": "In this paper we investigate the important and challenging task\nof recommending appropriate jobs for job seeking candidates by\nmatching semi structured resumes of candidates to job descriptions.\nTo perform this task, we propose to use a siamese adaptation of\nconvolutional neural network. The proposed approach effectively\ncaptures the underlying semantics thus enabling to project simi-\nlar resumes and job descriptions closer to each other, and make\ndissimilar resumes and job descriptions distant from each other\nin the semantic space. Our experimental results on a set of 1314\nresumes and a set of 3809 job descriptions (5 , 005 , 026 resume-job\ndescription pairs) demonstrate that our approach is better than the\ncurrent state-of-the-art approaches."}
{"Title": "Visualizing the Flow of Discourse with a Concept Ontology", "Abstract": "Understanding and visualizing human discourse has long\nbeing a challenging task. Although recent work on argument\nmining have shown success in classifying the role of various\nsentences, the task of recognizing concepts and understand-\ning the ways in which they are discussed remains challenging.\nGiven an email thread or a transcript of a group discussion,\nour task is to extract the relevant concepts and understand\nhow they are referenced and re-referenced throughout the\ndiscussion. In the present work, we present a preliminary\napproach for extracting and visualizing group discourse by\nadapting Wikipedia\u2019s category hierarchy to be an external con-\ncept ontology. From a user study, we found that our method\nachieved better results than 4 strong alternative approaches,\nand we illustrate our visualization method based on the ex-\ntracted discourse flows."}
{"Title": "Patterns of Volunteer Behaviour Across Online Citizen Science", "Abstract": "Human-computer systems are increasingly applied to data\nreduction problems; citizen science platforms (e.g. the Zooniverse)\nare one type of such a system. These platforms function as social\nmachines, combining volunteer efforts with automated processes to\nenable distributed data analysis. The rapid growth of this approach\nis increasing the need to understand how we can improve volunteer\ninteraction and engagement. Here, we utilize the most\ncomprehensive collection of online citizen science data gathered to\ndate to examine multiple variables across 63 Zooniverse projects.\nOur analyses reveal how subtle design changes can influence many\nfacets of volunteer interaction, generating insights that have\nimplications for the design and study of citizen science projects, and\nfuture research."}
{"Title": "FORank: Fast ObjectRank for Large Heterogeneous Graphs", "Abstract": "ObjectRank is one of the popular graph mining methods that en-\nablesustoevaluatetheimportanceofeachvertexonheterogeneous\ngraphs.However,itiscomputationallyexpensivetoapplyittolarge\ngraphs since ObjectRank needs to compute the importance of all\nvertices iteratively. In this work, we present a fast ObjectRank algo-\nrithm, FORank, that accurately approximates the keyword search\nresults. FORank iteratively prunes vertices whose convergence\nscore likely has less impact on the results during iterative computa-\ntion. The experiments showed that FORank runs 7 times faster than\nObjectRank computation with over 90% accuracy approximation."}
{"Title": "Contextual Web Summarization: A Supervised Ranking\nApproach", "Abstract": "We investigate the task of reading context-biased web summariza-\ntion, where the goal is to extract information relevant to the current\nreading context from a cited web article. In certain kind of linked\ndocument sets such as Wikipedia articles, scientific papers as well\nas news and blogs, such contextual summaries can be useful in\nproviding additional related information to the user helping in the\nreading task. In this work, we focus on web articles only and try\nto find out the set of key components that contribute to building\nup the reading context. We build a supervised model for ranking\nsentences from the cited document according to their contextual\nsalience. Initial evaluation based on annotated data-set of web arti-\ncles show that our ranking model performs better than the generic\nsummaries as well as baseline context-biased summaries."}
{"Title": "Homophily at Academic Conferences", "Abstract": "Academic conferences are a backbone for the exchange of ideas\nin scientific communities. However, so far little is known about\nthe communication networks emerging at those venues. Besides\npersonal knowledge, network homophily has been identified as\na driving factor for establishing contacts and followerships in so-\ncial networks, i.e., people are more likely to engage with others\nif they are similar with respect to certain attributes. In this paper,\nwe describe work in progress on investigating homophily at four\nacademic conferences based on face-to-face (F2F) contact data col-\nlected using wearable sensors between conference participants. In\nparticular, we study which personal attributes are predictive for\nface-to-face contacts. For that purpose, we obtained diverse per-\nsonal attributes from online sources in order to elicit a variety of\nhypotheses,whichcanthenbecomparedusingdescriptivestatistics\nand a Bayesian method for comparing hypotheses in networks. Our\nresults suggest that personal knowledge (as derived from DBLP and\nResearchGate networks) and homophilic behavior with respect to\nseveral attributes, e.g., gender or country of origin, are important\nfactors for contacts at academic conferences."}
{"Title": "Machine Learning for the Peer Assessment Credibility", "Abstract": "The peer assessment approach is considered to be one of the best\nsolutions for scaling both assessment and peer learning to global\nclassrooms, such as MOOCs. However, some academic staff hesitate\nto use a peer assessment approach for their classes due to concerns\nabout its credibility and reliability. The focus of our research is to\ndetectthecredibilitylevelofeachassessmentperformedbystudents\nduring peer assessment. We found three major scopes in assessing\nthe credibility level of evaluations, 1) Informativity, 2) Accuracy,\nand 3) Consistency. We collect assessments, including comments\nandgradesprovidedbystudentsduringthepeerassessmentprocess\nand then each feedback-and-grade pair is labeled with its credibility\nlevel by Mechanical Turk evaluators. We extract relevant features\nfrom each labeled assessment and use them to build a classifier\nthat attempts to automatically assess its level of credibility in C5.0\nDecision Tree classifier. The evaluation results show that the model\ncan be used to automatically classify peer assessments as credible\nor non-credible, with accuracy in the range of 88%."}
{"Title": "Generating Semantic Trajectories\nUsing a Car Signal Ontology", "Abstract": "In this paper, we use semantic technologies for enriching trajectory\ndata in the automotive industry for offline analysis. We proposed\nto re-use a combination of existing ontologies and we designed a\nVehicle Signal Specification ontology to provide an environment in\nwhich we developed an application that analyzes the variations of\nsignal values and enables to infer the \u201cdriving smoothness\u201d that we\nrepresent as additional annotations of semantic trajectories."}
{"Title": "SmartPub: A Platform for Long-Tail Entity Extraction from\nScientific Publications", "Abstract": "This demo presents SmartPub , a novel web-based platform that\nsupports the exploration and visualization of shallow meta-data\n(e.g.,authorlist,keywords)anddeepmeta-data \u2013longtailnameden-\ntities which are rare, and often relevant only in specific knowledge\ndomain \u2013 from scientific publications. The platform collects docu-\nments from different sources (e.g. DBLP and Arxiv), and extracts\nthe domain-specific named entities from the text of the publications\nusing Named Entity Recognizers (NERs) which we can train with\nminimal human supervision even for rare entity types. The plat-\nform further enables the interaction with the Crowd for filtering\npurposes or training data generation, and provides extended visual-\nizationandexplorationcapabilities. SmartPub willbedemonstrated\nusing sample collection of scientific publications focusing on the\ncomputer science domain and will address the entity types Dataset\n(i.e. dataset presented or used in a publication), and Methods (i.e.\nalgorithms used to create/enrich/analyse a data set)."}
{"Title": "Social Smart Meter: Identifying Energy Consumption Behavior\nin User-Generated Content", "Abstract": "Having a thorough understanding of energy consumption behav-\nior is an important element of sustainability studies. Traditional\nsources of information about energy consumption, such as smart\nmeter devices and surveys, can be costly to deploy, may lack con-\ntextual information or have infrequent updates. In this paper, we\nexamine the possibility of extracting energy consumption-related\ninformation from user-generated content. More specifically, we\ndevelop a pipeline that helps identify energy-related content in\nTwitter posts and classify it into four categories (dwelling, food,\nleisure, and mobility), according to the type of activity performed.\nWe further demonstrate a web-based application \u2013 called Social\nSmart Meter \u2013 that implements the proposed pipeline and enables\ndifferent stakeholders to gain an insight into daily energy con-\nsumption behavior, as well as showcase it in case studies involving\nseveral world cities."}
{"Title": "Smart-MD: Neural Paragraph Retrieval of Medical Topics", "Abstract": "We demonstrate Smart-MD, an information retrieval system for\nmedical professionals. The system supports topical queries in the\nform [disease topic], such as [\"lyme disease\", treatments]. In con-\ntrast to document-oriented retrieval systems, Smart-MD retrieves\nrelevant paragraphs and reduces the reading load of a medical doc-\ntor drastically. We recognize diseases and topical aspects with a\nnovel paragraph retrieval method based on bidirectional LSTM neu-\nral networks. We demonstrate Smart-MD on a dataset that contains\n3,469 diseases from the English language part of Wikipedia and\n6,876 distinct medical aspects extracted from Wikipedia headlines."}
{"Title": "A journey from the Physical Web to the Physical Semantic Web", "Abstract": "The Physical Semantic Web (PSW) is a novel paradigm built upon\nthe Google Physical Web (PW) approach and devoted to improve\nthe quality of interactions in the Web of Things. Beacons expose\nsemantic annotations instead of basic identifiers, i.e., machine-\nunderstandable descriptions of physical resources. This enables\nnovel ontology-based object advertisement and discovery and \u2013\nin turn\u2013 advanced user-to-thing and autonomous thing-to-thing\ninteractions.\nThe demo shows the evolution from the PW to the PSW in a\ndiscovery scenario set in a winery, where bottles are equipped\nwith Bluetooth Low Energy beacons and a customer can discover\nthem using her smartphone. The final goal is to prove benefits of\nPSW over basic PW, including: rich semantic-based object annota-\ntion; dynamic annotations exploiting on-board sensors; enhanced\ndiscovery and ranking of nearby objects through semantic match-\nmaking; availability of interactions even without working Internet\ninfrastructure, by means of point-to-point data exchanges."}
{"Title": "EMOFIEL: Mapping Emotions of Relationships in a Story", "Abstract": "We present EMOFIEL, a system that identifies characters and scenes\ninastoryfromafictionalnarrativesummary,generatesappropriate\nscene descriptions, identifies the emotion flow between a given\ndirected pair of story characters in each interaction, and organizes\nthem along the story timeline. These emotions are identified using\ntwo emotion modelling approaches: categorical and dimensional\nemotion models. The generated plots show that in a particular\nscene, two characters can share multiple emotions together with\ndifferent intensity. Furthermore, the directionality of the emotion\ncan be captured as well, depending on which character is more\ndominant in each interaction. EMOFIEL provides a web-based GUI\nthat allows users to query the annotated stories to explore the\nemotion mapping of a given character pair throughout a given\nstory, and to explore scenes for which a certain emotion peaks."}
{"Title": "Minimizing Polarization and Disagreement in Social Networks", "Abstract": "The rise of social media and online social networks has been a\ndisruptive force in society. Opinions are increasingly shaped by\ninteractionsononlinesocialmedia,andsocialphenomenaincluding\ndisagreementandpolarizationarenowtightlywovenintoeveryday\nlife. In this work we initiate the study of the following question:\nGiven n agents, each with its own initial opin-\nion that reflects its core value on a topic, and an\nopinion dynamics model, what is the structure\nofasocialnetworkthatminimizesdisagreement\nand controversy simultaneously?\nThis question is central to recommender systems: should a rec-\nommender system prefer a link suggestion between two online\nusers with similar mindsets in order to keep disagreement low, or\nbetween two users with different opinions in order to expose each\nto the others viewpoint of the world, and decrease overall levels of\npolarization and controversy? Such decisions have an important\nglobal effect on society [ 48 ]. Our contributions include a mathemat-\nical formalization of this question as an optimization problem and\nan exact, time-efficient algorithm. We also prove that there always\nexists a graph with O(n/\u03f5 2 ) edges that is a ( 1 +\u03f5) approximation\nto the optimum. Our formulation is an instance of optimization\nover graph topologies, see also [ 6 , 11 , 45 ]. Furthermore, for a given\ngraph, we show how to optimize the same objective over the agents\u2019\ninnate opinions in polynomial time. Finally, we perform an empir-\nical study of our proposed methods on synthetic and real-world\ndata that verify their value as mining tools to better understand\nthe trade-off between of disagreement and polarization. We find\nthat there is a lot of space to reduce both controversy and disagree-\nment in real-world networks; for instance, on a Reddit network\nwhere users exchange comments on politics, our methods achieve\na reduction in controversy and disagreement of the order 6 . 2 \u00d7 10 4 ."}
{"Title": "Deep Collective Classification in Heterogeneous Information\nNetworks", "Abstract": "Collective classification has attracted considerable attention\nin the last decade, where the labels within a group of in-\nstances are correlated and should be inferred collectively,\ninstead of independently. Conventional approaches on collec-\ntive classification mainly focus on exploiting simple relational\nfeatures (such as count and exists aggregators on neighbor-\ning nodes). However, many real-world applications involve\ncomplex dependencies among the instances, which are ob-\nscure/hidden in the networks. To capture these dependencies\nin collective classification, we need to go beyond simple rela-\ntional features and extract deep dependencies between the\ninstances. In this paper, we study the problem of deep col-\nlective classification in Heterogeneous Information Networks\n(HINs), which involves different types of autocorrelations,\nfrom simple to complex relations, among the instances. Dif-\nferent from conventional autocorrelations, which are given\nexplicitly by the links in the network, complex autocorrela-\ntions are obscure/hidden in HINs, and should be inferred\nfrom existing links in a hierarchical order. This problem is\nhighly challenging due to the multiple types of dependencies\namong the nodes and the complexity of the relational features.\nIn this study, we proposed a deep convolutional collective\nclassification method, called GraphInception, to learn the\ndeep relational features in HINs. The proposed method can\nautomatically generate a hierarchy of relational features with\ndifferent complexities. Extensive experiments on four real-\nworld networks demonstrate that our approach can improve\nthe collective classification performance by considering deep\nrelational features in HINs."}
{"Title": "On Exploring Semantic Meanings of Links\nfor Embedding Social Networks", "Abstract": "There are increasing interests in learning low-dimensional and\ndense node representations from the network structure which is\nusually high-dimensional and sparse. However, most existing meth-\nods fail to consider semantic meanings of links. Different links may\nhave different semantic meanings because the similarities between\ntwonodescanbedifferent,e.g.,twonodessharecommonneighbors\nand two nodes share similar interests which are demonstrated in\nnode-generated content. In this paper, the former type of links are\nreferred to as structure-close links while the latter type are referred\nto as content-close links. These two types of links naturally indicate\nthere are two types of characteristics that nodes expose in a social\nnetwork. Hence, we propose to learn two representations for each\nnode, and render each representation responsible for encoding the\ncorresponding type of node characteristics, which is achieved by\njointly embedding the network structure and inferring the type of\neach link. In the experiments, the proposed method is demonstrated\nto be more effective than five recent methods on four social net-\nworks through applications including visualization, link prediction\nand multi-label classification."}
{"Title": "Mapping the Invocation Structure of Online Political Interaction", "Abstract": "The surge in political information, discourse, and interaction has\nbeen one of the most important developments in social media over\nthe past several years. There is rich structure in the interaction\namong different viewpoints on the ideological spectrum. However,\nwe still have only a limited analytical vocabulary for expressing\nthe ways in which these viewpoints interact.\nIn this paper, we develop network-based methods that operate\non the ways in which users share content; we construct invocation\ngraphs on Web domains showing the extent to which pages from\none domain are invoked by users to reply to posts containing pages\nfrom other domains. When we locate the domains on a political\nspectrum induced from the data, we obtain an embedded graph\nshowing how these interaction links span different distances on\nthe spectrum. The structure of this embedded network, and its\nevolution over time, helps us derive macro-level insights about\nhow political interaction unfolded through 2016, leading up to the\nUS Presidential election. In particular, we find that the domains\ninvoked in replies spanned increasing distances on the spectrum\nover the months approaching the election, and that there was clear\nasymmetry between the left-to-right and right-to-left patterns of\nlinkage."}
{"Title": "Aspect-Aware Latent Factor Model:\nRating Prediction with Ratings and Reviews", "Abstract": "Although latent factor models (e.g., matrix factorization) achieve\ngood accuracy in rating prediction, they suffer from several prob-\nlems including cold-start, non-transparency, and suboptimal recom-\nmendation for local users or items. In this paper, we employ textual\nreview information with ratings to tackle these limitations. Firstly,\nwe apply a proposed aspect-aware topic model (ATM) on the review\ntext to model user preferences and item features from different as-\npects, and estimate the aspect importance of a user towards an item.\nThe aspect importance is then integrated into a novel aspect-aware\nlatent factor model (ALFM), which learns user\u2019s and item\u2019s latent\nfactors based on ratings. In particular, ALFM introduces a weighted\nmatrix to associate those latent factors with the same set of aspects\ndiscovered by ATM, such that the latent factors could be used to\nestimate aspect ratings. Finally, the overall rating is computed via\na linear combination of the aspect ratings, which are weighted by\nthe corresponding aspect importance. To this end, our model could\nalleviate the data sparsity problem and gain good interpretability\nfor recommendation. Besides, an aspect rating is weighted by an\naspectimportance,whichisdependentonthetargeteduser\u2019sprefer-\nences and targeted item\u2019s features. Therefore, it is expected that the\nproposed method can model a user\u2019s preferences on an item more\naccurately for each user-item pair locally. Comprehensive experi-\nmental studies have been conducted on 19 datasets from Amazon\nand Yelp 2017 Challenge dataset. Results show that our method\nachieves significant improvement compared with strong baseline\nmethods, especially for users with only few ratings. Moreover, our\nmodel could interpret the recommendation results in depth."}
{"Title": "Aesthetic-based Clothing Recommendation", "Abstract": "Recently, product images have gained increasing attention in cloth-\ning recommendation since the visual appearance of clothing prod-\nucts has a significant impact on consumers\u2019 decision. Most existing\nmethods rely on conventional features to represent an image, such\nas the visual features extracted by convolutional neural networks\n(CNN features) and the scale-invariant feature transform algorithm\n(SIFT features), color histograms, and so on. Nevertheless, one im-\nportant type of features, the aesthetic features, is seldom considered.\nIt plays a vital role in clothing recommendation since a users\u2019 de-\ncision depends largely on whether the clothing is in line with her\naesthetics, however the conventional image features cannot por-\ntray this directly. To bridge this gap, we propose to introduce the\naesthetic information, which is highly relevant with user prefer-\nence, into clothing recommender systems. To achieve this, we first\npresent the aesthetic features extracted by a pre-trained neural\nnetwork, which is a brain-inspired deep structure trained for the\naesthetic assessment task. Considering that the aesthetic preference\nvariessignificantlyfromusertouserandbytime,wethenproposea\nnew tensor factorization model to incorporate the aesthetic features\nin a personalized manner. We conduct extensive experiments on\nreal-world datasets, which demonstrate that our approach can cap-\nture the aesthetic preference of users and significantly outperform\nseveral state-of-the-art recommendation methods."}
{"Title": "Automated Extractions for Machine Generated Mail", "Abstract": "Mail extraction is a critical task whose objective is to extract valu-\nable data from the content of mail messages. This task is key for\nmany types of applications including re-targeting, mail search, and\nmail summarization, which utilize the important personal data\npieces in mail messages to achieve their objectives. We focus on\nmachine generated traffic, which comprises most of the Web mail\ntraffic today, and use its structured and large-scale repetitive na-\nture to devise a fully automated extraction method. Our solution\nbuilds on an advanced structural clustering technique previously\npresented by some of the authors of this work. The heart of our\nsolution is an offline process that leverages the structural mail-\nspecific characteristics of the clustering, and automatically creates\nextraction rules that are later applied online for each new arriving\nmessage. We provide of a full description of our process, which has\nbeen productized in Yahoo mail backend. We complete our work\nwith large-scale experiments carried over real Yahoo mail traffic,\nand evaluate the performance of our automatic extraction method."}
{"Title": "Bayesian Models for Product Size Recommendations", "Abstract": "Lack of calibrated product sizing in popular categories such as\napparel and shoes leads to customers purchasing incorrect sizes,\nwhich in turn results in high return rates due to fit issues. We\naddress the problem of product size recommendations based on\ncustomer purchase and return data. We propose a novel approach\nbased on Bayesian logit and probit regression models with ordinal\ncategories { Small, Fit, Large } to model size fits as a function\nof the difference between latent sizes of customers and products.\nWe propose posterior computation based on mean-field variational\ninference, leveraging the Polya-Gamma augmentation for the logit\nprior, that results in simple updates, enabling our technique to ef-\nficiently handle large datasets. Our Bayesian approach effectively\ndeals with issues arising from noise and sparsity in the data pro-\nviding robust recommendations. Offline experiments with real-life\nshoe datasets show that our model outperforms the state-of-the-art\nin 5 of 6 datasets. and leads to an improvement of 17-26% in AUC\nover baselines when predicting size fit outcomes."}
{"Title": "Me, My Echo Chamber, and I:\nIntrospection on Social Media Polarization", "Abstract": "Homophily \u2014 our tendency to surround ourselves with others who\nshare our perspectives and opinions about the world \u2014 is both a\npart of human nature and an organizing principle underpinning\nmany of our digital social networks. However, when it comes to\npolitics or culture, homophily can amplify tribal mindsets and pro-\nduce \u201cecho chambers\u201d that degrade the quality, safety, and diversity\nof discourse online. While several studies have empirically proven\nthis point, few have explored how making users aware of the ex-\ntent and nature of their political echo chambers influences their\nsubsequent beliefs and actions. In this paper, we introduce Social\nMirror, a social network visualization tool that enables a sample of\nTwitter users to explore the politically-active parts of their social\nnetwork. We use Social Mirror to recruit Twitter users with a prior\nhistory of political discourse to a randomized experiment where we\nevaluate the effects of different treatments on participants\u2019 i) beliefs\nabout their network connections, ii) the political diversity of who\nthey choose to follow, and iii) the political alignment of the URLs\nthey choose to share. While we see no effects on average political\nalignment of shared URLs, we find that recommending accounts of\nthe opposite political ideology to follow reduces participants\u2019 be-\nliefs in the political homogeneity of their network connections but\nstill enhances their connection diversity one week after treatment.\nConversely, participants who enhance their belief in the political\nhomogeneityoftheirTwitterconnectionshavelessdiversenetwork\nconnections 2-3 weeks after treatment. We explore the implications\nof these disconnects between beliefs and actions on future efforts\nto promote healthier exchanges in our digital public spheres."}
{"Title": "Geographical Feature Extraction for Entities in Location-based\nSocial Networks", "Abstract": "Location-based embedding is a fundamental problem to solve in\nlocation-based social network (LBSN). In this paper, we propose a\ngeographical convolutional neural tensor network (GeoCNTN) as\na generic embedding model. GeoCNTN first takes the raw location\ndata and extracts from it a well-conditioned representation by our\nproposed Geo-CMeans algorithm. We then use a convolutional neu-\nralnetwork(CNN)andanembeddingstructuretoextractindividual\nlatent structural patterns from the preprocessed data. Finally, we\napply a neural tensor network (NTN) to craft the implicitly related\nfeatures we have obtained into a unified geographical feature.\nThe advantages of our GeoCNTN mainly come from its novel\nneural network structure, which intrinsically offers a mechanism\nto extract latent structural features from the geographical data, as\nwell as its wide applicability in various LBSN-related tasks. From\ntwo case studies, i.e. link prediction and entity classification in\nuser-group LBSN, we evaluate the embedding efficacy of our model.\nResults show that GeoCNTN significantly performs better on at\nleast two tasks, with improvement by 9% w.r.t. NDCG and 11% w.r.t.\nF1 score respectively, using the Meetup-USA dataset."}
{"Title": "(Don\u2019t) Mention the War: A Comparison of Wikipedia and\nBritannica Articles on National Histories", "Abstract": "In this paper we present a large-scale quantitative comparison be-\ntween expert- and crowdsourced writing of history by analysing\narticles from the English Wikipedia and Britannica. In order to\nquantify attention to particular periods, we extract mentioned year\nnumbers and utilise them to study historical timelines of nations\nstretched over the last thousand years. By combining this temporal\nanalysis with lexical analysis of both encyclopedic corpora we can\nidentify distinctive historiographic points of view in each ency-\nclopedia. We find that Britannica focuses on social and cultural\nphenomena, e.g. religion, as well as the geographical characteris-\ntics of states, while Wikipedia puts emphasis on political aspects,\nconcentrating on wars and violent conflicts, and events of high\npopularity. Finally, both encyclopedias exhibit characteristics of\nEnglishAcademicprose,withBritannicabeingslightlylessreadable\ncompared to Wikipedia, according to several readability scores."}
{"Title": "On Ridesharing Competition and Accessibility:\nEvidence from Uber, Lyft, and Taxi", "Abstract": "Ridesharing services such as Uber and Lyft have become an im-\nportant part of the Vehicle For Hire (VFH) market, which used to\nbe dominated by taxis. Unfortunately, ridesharing services are not\nrequired to share data like taxi services, which has made it chal-\nlenging to compare the competitive dynamics of these services, or\nassess their impact on cities. In this paper, we comprehensively\ncompare Uber, Lyft, and taxis with respect to key market features\n(supply, demand, price, and wait time) in San Francisco and New\nYork City. Based on point pattern statistics, we develop novel statis-\nticaltechniquestovalidateourmeasurementmethods.Usingspatial\nlag models, we investigate the accessibility of VFH services, and\nfind that transportation infrastructure and socio-economic features\nhave substantial effects on VFH market features."}
{"Title": "What We Read, What We Search:\nMedia Attention and Public Attention Among 193 Countries", "Abstract": "We investigate the alignment of international attention of news\nmedia organizations within 193 countries with the expressed inter-\nnational interests of the public within those same countries from\nMarch 7, 2016 to April 14, 2017. We collect fourteen months of lon-\ngitudinal data of online news from Unfiltered News and web search\nvolume data from Google Trends and build a multiplex network of\nmedia attention and public attention in order to study its structural\nand dynamic properties. Structurally, the media attention and the\npublic attention are both similar and different depending on the\nresolution of the analysis. For example, we find that 63.2% of the\ncountry-specific media and the public pay attention to different\ncountries, but local attention flow patterns, which are measured\nby network motifs, are very similar. We also show that there are\nstrong regional similarities with both media and public attention\nthat is only disrupted by significantly major worldwide incidents\n(e.g., Brexit). Using Granger causality, we show that there are a\nsubstantial number of countries where media attention and pub-\nlic attention are dissimilar by topical interest. Our findings show\nthat the media and public attention toward specific countries are\noften at odds, indicating that the public within these countries may\nbe ignoring their country-specific news outlets and seeking other\nonline sources to address their media needs and desires."}
{"Title": "Human Perceptions of Fairness in Algorithmic Decision Making:\nA Case Study of Criminal Risk Prediction", "Abstract": "As algorithms are increasingly used to make important decisions\nthat affect human lives, ranging from social benefit assignment to\npredicting risk of criminal recidivism, concerns have been raised\naboutthefairnessofalgorithmicdecisionmaking.Mostpriorworks\non algorithmic fairness normatively prescribe how fair decisions\nought to be made. In contrast, here, we descriptively survey users\nfor how they perceive and reason about fairness in algorithmic\ndecision making.\nA key contribution of this work is the framework we propose\nto understand why people perceive certain features as fair or unfair\nto be used in algorithms. Our framework identifies eight properties\nof features, such as relevance, volitionality and reliability, as latent\nconsiderations that inform people\u2019s moral judgments about the\nfairness of feature use in decision-making algorithms. We validate\nour framework through a series of scenario-based surveys with\n576 people. We find that, based on a person\u2019s assessment of the\neight latent properties of a feature in our exemplar scenario, we\ncan accurately (> 85%) predict if the person will judge the use of\nthe feature as fair.\nOur findings have important implications. At a high-level, we\nshow that people\u2019s unfairness concerns are multi-dimensional and\nargue that future studies need to address unfairness concerns be-\nyond discrimination. At a low-level, we find considerable disagree-\nments in people\u2019s fairness judgments. We identify root causes of\nthe disagreements, and note possible pathways to resolve them."}
{"Title": "Political Discourse on Social Media:\nEcho Chambers, Gatekeepers, and the Price of Bipartisanship", "Abstract": "Echo chambers, i.e., situations where one is exposed only to opin-\nions that agree with their own, are an increasing concern for the\npolitical discourse in many democratic countries. This paper stud-\nies the phenomenon of political echo chambers on social media.\nWe identify the two components in the phenomenon: the opinion\nthat is shared, and the \u201cchamber\u201d (i.e., the social network) that\nallows the opinion to \u201cecho\u201d (i.e., be re-shared in the network) \u2013\nand examine closely at how these two components interact. We\ndefine a production and consumption measure for social-media\nusers, which captures the political leaning of the content shared\nand received by them. By comparing the two, we find that Twitter\nusers are, to a large degree, exposed to political opinions that agree\nwith their own. We also find that users who try to bridge the echo\nchambers, by sharing content with diverse leaning, have to pay a\n\u201cprice of bipartisanship\u201d in terms of their network centrality and\ncontentappreciation.Inaddition,westudytheroleof\u201cgatekeepers,\u201d\nusers who consume content with diverse leaning but produce parti-\nsan content (with a single-sided leaning), in the formation of echo\nchambers. Finally, we apply these findings to the task of predicting\npartisans and gatekeepers from social and content features. While\npartisanusersturnoutrelativelyeasytoidentify,gatekeepersprove\nto be more challenging."}
{"Title": "Community Interaction and Conflict on the Web", "Abstract": "Users organize themselves into communities on web platforms.\nThese communities can interact with one another, often leading to\nconflicts and toxic interactions. However, little is known about the\nmechanisms of interactions between communities and how they\nimpact users.\nHere we study intercommunity interactions across 36,000 com-\nmunities on Reddit,examining caseswhere users ofone community\naremobilizedbynegativesentimenttocommentinanothercommu-\nnity. We show that such conflicts tend to be initiated by a handful\nof communities\u2014less than 1% of communities start 74% of conflicts.\nWhile conflicts tend to be initiated by highly active community\nmembers, they are carried out by significantly less active members.\nWe find that conflicts are marked by formation of echo chambers,\nwhere users primarily talk to other users from their own commu-\nnity. In the long-term, conflicts have adverse effects and reduce the\noverall activity of users in the targeted communities.\nOur analysis of user interactions also suggests strategies for mit-\nigating the negative impact of conflicts\u2014such as increasing direct\nengagement between attackers and defenders. Further, we accu-\nratelypredictwhetheraconflictwilloccurbycreatinganovelLSTM\nmodel that combines graph embeddings, user, community, and text\nfeatures. This model can be used to create early-warning systems\nfor community moderators to prevent conflicts. Altogether, this\nwork presents a data-driven view of community interactions and\nconflict, and paves the way towards healthier online communities."}
{"Title": "\u201cYou are no Jack Kennedy\u201d:\nOn Media Selection of Highlights from Presidential Debates", "Abstract": "Political speeches and debates play an important role in shaping\nthe images of politicians, and the public often relies on media out-\nlets to select bits of political communication from a large pool of\nutterances. It is an important research question to understand what\nfactors impact this selection process.\nTo quantitatively explore the selection process, we build a three-\ndecade dataset of presidential debate transcripts and post-debate\ncoverage. We first examine the effect of wording and propose a bi-\nnaryclassificationframeworkthatcontrolsforboththespeakerand\nthe debate situation. We find that crowdworkers can only achieve\nan accuracy of 60% in this task, indicating that media choices are\nnot entirely obvious. Our classifiers outperform crowdworkers on\naverage, mainly in primary debates. We also compare important\nfactorsfromcrowdworkers\u2019free-formexplanationswiththosefrom\ndata-driven methods and find interesting differences. Few crowd-\nworkers mentioned that \u201ccontext matters\u201d, whereas our data show\nthat well-quoted sentences are more distinct from the previous ut-\nterance by the same speaker than less-quoted sentences. Finally, we\nexamine the aggregate effect of media preferences towards different\nwordings to understand the extent of fragmentation among media\noutlets. By analyzing a bipartite graph built from quoting behavior\nin our data, we observe a decreasing trend in bipartisan coverage."}
{"Title": "To Stay or to Leave: Churn Prediction for\nUrban Migrants in the Initial Period", "Abstract": "In China, 260 million people migrate to cities to realize their urban\ndreams. Despite that these migrants play an important role in the\nrapid urbanization process, many of them fail to settle down and\neventually leave the city. The integration process of migrants thus\nraises an important issue for scholars and policymakers.\nIn this paper, we use Shanghai as an example to investigate mi-\ngrants\u2019 behavior in their first weeks and in particular, how their\nbehavior relates to early departure. Our dataset consists of a one-\nmonth complete dataset of 698 telecommunication logs between 54\nmillion users, plus a novel and publicly available housing price data\nfor 18K real estates in Shanghai. We find that migrants who end up\nleaving early tend to neither develop diverse connections in their\nfirst weeks nor move around the city. Their active areas also have\nhigher housing prices than that of staying migrants. We formulate a\nchurn prediction problem to determine whether a migrant is going\nto leave based on her behavior in the first few days. The prediction\nperformance improves as we include data from more days. Inter-\nestingly, when using the same features, the classifier trained from\nonly the first few days is already as good as the classifier trained\nusing full data, suggesting that the performance difference mainly\nlies in the difference between features."}
{"Title": "Parabel: Partitioned Label Trees for Extreme Classification with\nApplication to Dynamic Search Advertising", "Abstract": "This paper develops the Parabel algorithm for extreme multi-label\nlearning where the objective is to learn classifiers that can annotate\neach data point with the most relevant subset of labels from an\nextremely large label set. The state-of-the-art 1-vs-All based DiS-\nMEC and PPDSparse algorithms are the most accurate but can take\nupto months for training and prediction as they learn and apply\nan independent linear classifier per label. Consequently, they do\nnot scale to large datasets with millions of labels. Parabel addresses\nboth limitations by learning a balanced label hierarchy such that:\n(a) the 1-vs-All classifiers in the leaf nodes of the label hierarchy can\nbe trained on a small subset of the training set thereby reducing the\ntraining time to a few hours on a single core of a standard desktop\nand (b) novel points can be classified by traversing the learned\nhierarchy in logarithmic time and applying the 1-vs-All classifiers\npresent in just the leaf thereby reducing the prediction time to\na few milliseconds per test point. This allows Parabel to scale to\ntasks considered infeasible for DiSMEC and PPDSparse such as\npredicting the subset of 7 million Bing queries that might lead to a\nclick on a given ad-landing page for dynamic search advertising.\nExperiments on multiple benchmark datasets revealed that Para-\nbel could be almost as accurate as PPDSparse and DiSMEC while\nbeing upto 1,000x faster at training and upto 40x-10,000x faster at\nprediction. Furthermore, Parabel was demonstrated to significantly\nimprovedynamicsearchadvertisingonBingbymorethandoubling\nthe ad recall and improving the click-through rate by 20%. Source\ncode for Parabel can be downloaded from [1]."}
{"Title": "Content Attention Model for Aspect Based Sentiment Analysis", "Abstract": "Aspect based sentiment classification is a crucial task for sentiment\nanalysis. Recent advances in neural attention models demonstrate\nthat they can be helpful in aspect based sentiment classification\ntasks, which can help identify the focus words in human. How-\never, according to our empirical study, prevalent content attention\nmechanisms proposed for aspect based sentiment classification\nmostly focus on identifying the sentiment words or shifters, with-\nout considering the relevance of such words with respect to the\ngiven aspects in the sentence. Therefore, they are usually insuffi-\ncient for dealing with multi-aspect sentences and the syntactically\ncomplex sentence structures. To solve this problem, we propose a\nnovel content attention based aspect based sentiment classification\nmodel, with two attention enhancing mechanisms: sentence-level\ncontent attention mechanism is capable of capturing the important\ninformation about given aspects from a global perspective, whiles\nthe context attention mechanism is responsible for simultaneously\ntaking the order of the words and their correlations into account, by\nembeddingthemintoaseriesofcustomizedmemories.Experimental\nresultsdemonstratethatourmodeloutperformsthestate-of-the-art,\nin which the proposed mechanisms play a key role."}
{"Title": "Never-Ending Learning for Open-Domain Question Answering\nover Knowledge Bases", "Abstract": "Translating natural language questions to semantic representations\nsuch as SPARQL is a core challenge in open-domain question an-\nswering over knowledge bases (KB-QA). Existing methods rely on\na clear separation between an offline training phase, where a model\nis learned, and an online phase where this model is deployed. Two\nmajor shortcomings of such methods are that (i) they require access\nto a large annotated training set that is not always readily avail-\nable and (ii) they fail on questions from before-unseen domains. To\novercome these limitations, this paper presents NEQA, a continuous\nlearning paradigm for KB-QA. Offline, NEQA automatically learns\ntemplates mapping syntactic structures to semantic ones from a\nsmall number of training question-answer pairs. Once deployed,\ncontinuous learning is triggered on cases where templates are insuf-\nficient. Using a semantic similarity function between questions and\nby judicious invocation of non-expert user feedback, NEQA learns\nnew templates that capture previously-unseen syntactic structures.\nThis way, NEQA gradually extends its template repository. NEQA\nperiodically re-trains its underlying models, allowing it to adapt to\nthe language used after deployment. Our experiments demonstrate\nNEQA\u2019s viability, with steady improvement in answering quality\nover time, and the ability to answer questions from new domains."}
{"Title": "Short-Text Topic Modeling via Non-negative Matrix\nFactorization Enriched with Local Word-Context Correlations", "Abstract": "Being a prevalent form of social communications on the Internet,\nbillions of short texts are generated everyday. Discovering knowl-\nedge from them has gained a lot of interest from both industry and\nacademia. The short texts have a limited contextual information,\nand they are sparse, noisy and ambiguous, and hence, automati-\ncally learning topics from them remains an important challenge. To\ntackle this problem, in this paper, we propose a semantics-assisted\nnon-negative matrix factorization (SeaNMF) model to discover top-\nics for the short texts. It effectively incorporates the word-context\nsemantic correlations into the model, where the semantic relation-\nships between the words and their contexts are learned from the\nskip-gram view of the corpus. The SeaNMF model is solved using\na block coordinate descent algorithm. We also develop a sparse\nvariant of the SeaNMF model which can achieve a better model\ninterpretability. Extensive quantitative evaluations on various real-\nworld short text datasets demonstrate the superior performance of\nthe proposed models over several other state-of-the-art methods\nin terms of topic coherence and classification accuracy. The qual-\nitative semantic analysis demonstrates the interpretability of our\nmodels by discovering meaningful and consistent topics. With a\nsimple formulation and the superior performance, SeaNMF can be\nan effective standard topic model for short texts."}
{"Title": "Are All People Married?\nDetermining Obligatory Attributes in Knowledge Bases", "Abstract": "An attribute is obligatory for a class in a Knowledge Base (KB), if\nall instances of the class have the attribute in the real world. For\nexample, hasBirthDate is an obligatory attribute for the class Person,\nwhile hasSpouse is not. In this paper, we propose a new way to\nmodel incompleteness in KBs. From this model, we derive a method\nto automatically determine obligatory attributes \u2013 using only the\ndata from the KB. Our algorithm can detect such attributes with a\nprecision of up to 90%."}
{"Title": "Socioeconomic Dependencies of Linguistic Patterns\nin Twitter: A Multivariate Analysis ", "Abstract": "Our usage of language is not solely reliant on cognition but is ar-\nguably determined by myriad external factors leading to a global\nvariability of linguistic patterns. This issue, which lies at the core of\nsociolinguistics and is backed by many small-scale studies on face-\nto-face communication, is addressed here by constructing a dataset\ncombining the largest French Twitter corpus to date with detailed\nsocioeconomic maps obtained from national census in France. We\nshow how key linguistic variables measured in individual Twitter\nstreams depend on factors like socioeconomic status, location, time,\nand the social network of individuals. We found that (i) people of\nhigher socioeconomic status, active to a greater degree during the\ndaytime, use a more standard language; (ii) the southern part of\nthe country is more prone to use more standard language than the\nnorthern one, while locally the used variety or dialect is determined\nby the spatial distribution of socioeconomic status; and (iii) individ-\nuals connected in the social network are closer linguistically than\ndisconnected ones, even after the effects of status homophily have\nbeen removed. Our results inform sociolinguistic theory and may\ninspire novel learning methods for the inference of socioeconomic\nstatus of people from the way they tweet."}
{"Title": "Find the Conversation Killers: A Predictive Study of\nThread-ending Posts", "Abstract": "How to improve the quality of conversations in online communities\nhas attracted considerable attention recently. Having engaged, civil,\nand reactive online conversations has a critical effect on the social\nlife of Internet users. In this study, we are particularly interested\nin identifying a post in a multi-party conversation that is unlikely\nto be further replied to, which therefore kills that thread of the\nconversation. For this purpose, we propose a deep learning model\ncalledtheConverNet.ConverNetisattractiveduetoitscapabilityof\nmodelingtheinternalstructureofalongconversationanditsappro-\npriate encoding of the contextual information of the conversation,\nthrough effective integration of attention mechanisms. Empirical\nexperiments on real-world datasets demonstrate the effectiveness\nof the proposed model. For the widely concerned topic, our analysis\nalso offers implications for how to improve the quality and user\nexperience of online conversations, or how to engage users in a\nconversation with a chatbot."}
{"Title": "Semantics and Complexity of GraphQL", "Abstract": "GraphQL is a recently proposed, and increasingly adopted, concep-\ntual framework for providing a new type of data access interface\non the Web. The framework includes a new graph query language\nwhose semantics has been specified informally only. This has pre-\nvented the formal study of the main properties of the language.\nWe embark on the formalization and study of GraphQL. To this\nend, we first formalize the semantics of GraphQL queries based on a\nlabeled-graph data model. Thereafter, we analyze the language and\nshowthatitadmitsreallyefficientevaluationmethods.Inparticular,\nwe prove that the complexity of the GraphQL evaluation problem\nis NL-complete. Moreover, we show that the enumeration problem\ncan be solved with constant delay. This implies that a server can\nanswer a GraphQL query and send the response byte-by-byte while\nspending just a constant amount of time between every byte sent.\nDespitethesepositiveresults,weprovethatthesizeofaGraphQL\nresponse might be prohibitively large for an internet scenario. We\npresent experiments showing that current practical implementa-\ntions suffer from this issue. We provide a solution to cope with this\nproblem by showing that the total size of a GraphQL response can\nbe computed in polynomial time. Our results on polynomial-time\nsize computation plus the constant-delay enumeration can help\ndevelopers to provide more robust GraphQL interfaces on the Web."}
{"Title": "Sentiment Analysis by Capsules \u2217", "Abstract": "In this paper, we propose RNN-Capsule, a capsule model based\non Recurrent Neural Network (RNN) for sentiment analysis. For\na given problem, one capsule is built for each sentiment category\ne.g., \u2018positive\u2019 and \u2018negative\u2019. Each capsule has an attribute, a state,\nand three modules: representation module, probability module, and\nreconstruction module. The attribute of a capsule is the assigned\nsentimentcategory.Givenaninstanceencodedinhiddenvectorsby\na typical RNN, the representation module builds capsule representa-\ntion by the attention mechanism. Based on capsule representation,\nthe probability module computes the capsule\u2019s state probability. A\ncapsule\u2019s state is active if its state probability is the largest among\nall capsules for the given instance, and inactive otherwise. On two\nbenchmark datasets (i.e., Movie Review and Stanford Sentiment\nTreebank) and one proprietary dataset (i.e., Hospital Feedback),\nwe show that RNN-Capsule achieves state-of-the-art performance\non sentiment classification. More importantly, without using any\nlinguistic knowledge, RNN-Capsule is capable of outputting words\nwithsentimenttendenciesreflectingcapsules\u2019attributes.Thewords\nwell reflect the domain specificity of the dataset."}
{"Title": "Modelling Dynamics in Semantic Web Knowledge Graphs\nwith Formal Concept Analysis", "Abstract": "In this paper, we propose a novel data-driven schema for large-\nscale heterogeneous knowledge graphs inspired by Formal Concept\nAnalysis (FCA). We first extract the sets of properties associated\nwith individual entities; these property sets (aka. characteristic sets)\nareannotatedwithcardinalitiesandusedtoinducealatticebasedon\nset-containment relations, forming a natural hierarchical structure\ndescribing the knowledge graph. We then propose an algebra over\nsuchschemalattices,whichallowstocomputediffsbetweenlattices\n(for example, to summarise the changes from one version of a\nknowledge graph to another), to add diffs to lattices (for example, to\nprojectfuturechanges),andsoforth.Whilewearguethatthislattice\nstructure (and associated algebra) may have various applications,\nwe currently focus on the use-case of modelling and predicting the\ndynamic behaviour of knowledge graphs. Along those lines, we\ninstantiate and evaluate our methods for analysing how versions\nof the Wikidata knowledge graph have changed over a period of 11\nweeks. We propose algorithms for constructing the lattice-based\nschema from Wikidata, and evaluate their efficiency and scalability.\nWe then evaluate use of the resulting schema(ta) for predicting how\nthe knowledge graph will evolve in future versions."}
{"Title": "A Structured Approach to Understanding\nRecovery and Relapse in AA", "Abstract": "Alcoholism, also known as Alcohol Use Disorder (AUD), is a se-\nrious problem affecting millions of people worldwide. Recovery\nfrom AUD is known to be challenging and often leads to relapse\nat various points after enrolling in a rehabilitation program such\nas Alcoholics Anonymous (AA). In this work, we take a structured\napproach to understand recovery and relapse from AUD using so-\ncial media data. To do so, we combine linguistic and psychological\nattributes of users with relational features that capture useful struc-\nture in the user interaction network. We evaluate our models on\nAA-attending users extracted from the Twitter social network and\npredict recovery at two different points\u201490 days and 1 year after\nthe user joins AA, respectively. Our experiments reveal that our\nstructured approach is helpful in predicting recovery in these users.\nWe perform extensive quantitative analysis of different groups of\nfeatures and dependencies among them. Our analysis sheds light on\nthe role of each feature group and how they combine to predict re-\ncovery and relapse. Finally, we present a qualitative analysis of the\ndifferent reasons behind users relapsing to AUD. Our models and\nanalysis are helpful in making meaningful predictions in scenarios\nwhere only a subset of features are available and can potentially be\nhelpful in identifying and preventing relapse early."}
{"Title": "Facet Annotation Using Reference Knowledge Bases", "Abstract": "Faceted interfaces are omnipresent on the web to support data ex-\nploration and filtering. A facet is a triple: a domain (e.g., Book ), a\nproperty (e.g., author,lan\u0434ua\u0434e ), and a set of property values (e.g.,\n{Austen,Beauvoir,Coelho,Dostoevsky,Eco,Kerouac,Sskind,.. .},\n{French,En\u0434lish,German,Italian,Portu\u0434uese,Russian,.. .}).\nGiven a property (e.g., lan\u0434ua\u0434e ), selecting one or more of its values\n( En\u0434lish and Italian ) returns the domain entities (of type Book) that\nmatch the given values (the books that are written in English or\nItalian). To implement faceted interfaces in a way that is scalable\nto very large datasets, it is necessary to automate facet extraction.\nPrior work associates a facet domain with a set of homogeneous\nvalues, but does not annotate the facet property. In this paper, we\nannotatethefacetpropertywithapredicatefromareferenceKnowl-\nedge Base ( KB ) so as to maximize the semantic similarity between\nthe property and the predicate. We define semantic similarity in\nterms of three new metrics: specificity, coverage, and frequency. Our\nexperimental evaluation uses the DBpedia and YAGO KB s and\nshows that for the facet annotation problem, we obtain better re-\nsults than a state-of-the-art approach for the annotation of web\ntables as modified to annotate a set of values."}
{"Title": "Matching Natural Language Sentences\nwith Hierarchical Sentence Factorization", "Abstract": "Semantic matching of natural language sentences or identifying\nthe relationship between two sentences is a core research problem\nunderlying many natural language tasks. Depending on whether\ntraining data is available, prior research has proposed both un-\nsupervised distance-based schemes and supervised deep learning\nschemes for sentence matching. However, previous approaches ei-\ntheromitorfailtofullyutilizetheordered,hierarchical,andflexible\nstructures of language objects, as well as the interactions between\nthem. In this paper, we propose Hierarchical Sentence Factorization\u2014\na technique to factorize a sentence into a hierarchical representa-\ntion, with the components at each different scale reordered into a\n\u201cpredicate-argument\u201d form. The proposed sentence factorization\ntechnique leads to the invention of: 1) a new unsupervised distance\nmetricwhich calculatesthe semanticdistancebetweenapairoftext\nsnippets by solving a penalized optimal transport problem while\npreserving the logical relationship of words in the reordered sen-\ntences, and 2) new multi-scale deep learning models for supervised\nsemantic training, based on factorized sentence hierarchies. We\napply our techniques to text-pair similarity estimation and text-pair\nrelationship classification tasks, based on multiple datasets such as\nSTSbenchmark, the Microsoft Research paraphrase identification\n(MSRP) dataset, the SICK dataset, etc. Extensive experiments show\nthat the proposed hierarchical sentence factorization can be used\nto significantly improve the performance of existing unsupervised\ndistance-based metrics as well as multiple supervised deep learning\nmodels based on the convolutional neural network (CNN) and long\nshort-term memory (LSTM)."}
{"Title": "Why Reinvent the Wheel \u2013 Let\u2019s Build Question Answering\nSystems Together", "Abstract": "Modern question answering (QA) systems need to flexibly inte-\ngrate a number of components specialised to fulfil specific tasks\nin a QA pipeline. Key QA tasks include Named Entity Recogni-\ntion and Disambiguation, Relation Extraction, and Query Building.\nSince a number of different software components exist that im-\nplement different strategies for each of these tasks, it is a major\nchallenge to select and combine the most suitable components into\na QA system, given the characteristics of a question. We study\nthis optimisation problem and train classifiers, which take features\nof a question as input and have the goal of optimising the selec-\ntion of QA components based on those features. We then devise a\ngreedy algorithm to identify the pipelines that include the suitable\ncomponents and can effectively answer the given question. We\nimplement this model within Frankenstein, a QA framework able\nto select QA components and compose QA pipelines. We evaluate the effectiveness of the pipelines generated by Frankenstein us-\ning the QALD and LC-QuAD benchmarks. These results not only\nsuggest that Frankenstein precisely solves the QA optimisation\nproblem but also enables the automatic composition of optimised\nQA pipelines, which outperform the static Baseline QA pipeline.\nThanks to this flexible and fully automated pipeline generation pro-\ncess, new QA components can be easily included in Frankenstein\nthus improving the performance of the generated pipelines."}
{"Title": "Weakly-supervised Relation Extraction by\nPa?ern-enhanced Embedding Learning", "Abstract": "Extracting relations from text corpora is an important task with\nwide applications. However, it becomes particularly challenging\nwhen focusing on weakly-supervised relation extraction, that is,\nutilizing a few relation instances (i.e., a pair of entities and their re-\nlation) as seeds to extract from corpora more instances of the same\nrelation. Existing distributional approaches leverage the corpus-\nlevel co-occurrence statistics of entities to predict their relations,\nand require a large number of labeled instances to learn e?ective\nrelation classi?ers. Alternatively, pattern-based approaches per-\nform boostrapping or apply neural networks to model the local\ncontexts, but still rely on a large number of labeled instances to\nbuild reliable models. In this paper, we study the integration of\ndistributional and pattern-based methods in a weakly-supervised\nsetting such that the two kinds of methods can provide complemen-\ntary supervision for each other to build an e?ective, uni?ed model.\nWe propose a novel co-training framework with a distributional\nmodule and a pattern module. During training, the distributional\nmodule helps the pattern module discriminate between the informa-\ntive patterns and other patterns, and the pattern module generates\nsome highly-con?dent instances to improve the distributional mod-\nule. The whole framework can be e?ectively optimized by iterating\nbetween improving the pattern module and updating the distribu-\ntional module. We conduct experiments on two tasks: knowledge\nbase completion with text corpora and corpus-level relation extrac-\ntion. Experimental results prove the e?ectiveness of our framework\nover many competitive baselines."}
{"Title": "Towards Annotating Relational Data on the Web\nwith Language Models", "Abstract": "Tables and structured lists on Web pages are a potential source\nof valuable information, and several methods have been proposed\nto annotate them with semantics that can be leveraged for search,\nquestion answering and information extraction. This paper is con-\ncerned with the specific problem of finding and ranking relations\nfrom a given Knowledge Graph (KG) that hold over pairs of enti-\nties juxtaposed in a table or structured list. The state-of-the-art for\nthis task is to attempt to link the entities mentioned in the table\ncells to objects in the KG and rank the relations that hold for those\nlinked objects. As a result, these methods are hampered by the\nincompleteness and uneven coverage in even the best knowledge\ngraphs available today. The alternative described here does not\nrequire entity linking, relying instead on ranking relations using\ngenerative language models derived from Web-scale corpora. As\nsuch, it can produce quality results even when the entities in the\ntable are missing in the KG. The experimental validation, designed\nto expose the challenges posed by KG incompleteness, shows that\nour approach is robust and effective in practice."}
{"Title": "CESI: Canonicalizing Open Knowledge Bases using\nEmbeddings and Side Information", "Abstract": "Open Information Extraction (OpenIE) methods extract\n(noun phrase, relation phrase, noun phrase) triples from text,\nresulting in the construction of large Open Knowledge Bases\n(Open KBs). The noun phrases (NPs) and relation phrases in\nsuch Open KBs are not canonicalized, leading to the storage\nof redundant and ambiguous facts. Recent research has posed\ncanonicalization of Open KBs as clustering over manually-\ndefined feature spaces. Manual feature engineering is expen-\nsive and often sub-optimal. In order to overcome this chal-\nlenge, we propose Canonicalization using Embeddings and\nSide Information (CESI) \u2013 a novel approach which performs\ncanonicalization over learned embeddings of Open KBs. CESI\nextends recent advances in KB embedding by incorporating\nrelevant NP and relation phrase side information in a prin-\ncipled manner. Through extensive experiments on multiple\nreal-world datasets, we demonstrate CESI\u2019s effectiveness."}
{"Title": "Bid-Limited Targeting", "Abstract": "This paper analyzes a mechanism for selling items in auctions\nin which the auctioneer specifies a cap on the ratio between\nthe maximum and minimum bids that bidders may use in\nthe different auctions. Such a mechanism is widely used in\nonline advertising through the caps that companies impose on\nthe minimum and maximum bid multipliers that advertisers\nmay use in targeting. When bidders\u2019 values are independent\nand identically distributed, using this mechanism results in\nhigher revenue than allowing bidders to condition their bids\non the targeting information in an arbitrary way and also\nalmost always results in higher revenue than not allowing\nbidders to target. Choosing the optimal cap on the ratio\nbetween the maximum bid and the minimum bid can also be\nmore important than introducing additional competition in\nthe auction. However, if bidders\u2019 values are not identically\ndistributed, pure-strategy equilibria may fail to exist."}
{"Title": "Reinforcement Mechanism Design for e-commerce", "Abstract": "We study the problem of allocating impressions to sellers in e-\ncommerce websites, such as Amazon, eBay or Taobao, aiming to\nmaximize the total revenue generated by the platform. We employ a\ngeneral framework of reinforcement mechanism design, which uses\ndeep reinforcement learning to design efficient algorithms, taking\nthe strategic behaviour of the sellers into account. Specifically, we\nmodel the impression allocation problem as a Markov decision\nprocess, where the states encode the history of impressions, prices,\ntransactions and generated revenue and the actions are the possible\nimpression allocations in each round. To tackle the problem of\ncontinuity and high-dimensionality of states and actions, we adopt\nthe ideas of the DDPG algorithm to design an actor-critic policy\ngradient algorithm which takes advantage of the problem domain\nin order to achieve convergence and stability.\nWe evaluate our proposed algorithm, coined IA(GRU), by com-\nparing it against DDPG, as well as several natural heuristics, under\ndifferent rationality models for the sellers - we assume that sellers\nfollow well-known no-regret type strategies which may vary in\ntheir degree of sophistication. We find that IA(GRU) outperforms\nall algorithms in terms of the total revenue."}
{"Title": "Stop the KillFies! Using Deep Learning Models to Identify\nDangerous Selfies", "Abstract": "Selfies have become a prominent medium for self-portrayal on\nsocial media. Unfortunately, certain social media users go to ex-\ntreme lengths to click selfies, which puts their lives at risk. Two\nhundred and sixteen individuals have died since March 2014 until\nJanuary 2018 while trying to click selfies. It is imperative to be able\nto identify dangerous selfies posted on social media platforms to\nbe able to build an intervention for users going to extreme lengths\nfor clicking such selfies. In this work, we propose a convolutional\nneural network based classifier to identify dangerous selfies posted\non social media using only the image (no metadata). We show that\nour proposed approach gives an accuracy of 98% and performs\nbetter than previous methods."}
{"Title": "Iterative Knowledge Extraction from Social Networks", "Abstract": "Knowledge in the world continuously evolves, and ontologies are\nlargely incomplete, especially regarding data belonging to the so-\ncalled long tail. We propose a method for discovering emerging\nknowledge by extracting it from social content. Once initialized by\ndomain experts, the method is capable of finding emerging entities\nby means of a mixed syntactic-semantic method. The method uses\nseeds, i.e. prototypes of emerging entities provided by experts, for\ngenerating candidates; then, it associates candidates to feature vec-\ntors built by using terms occurring in their social content and ranks\nthe candidates by using their distance from the centroid of seeds,\nreturning the top candidates. Our method can run continuously\nor with periodic iterations, using the results as new seeds. In this\npaper we address the following research questions: (1) How does\nreconstructed domain knowledge evolve if the candidates of one\nextraction are recursively used as seeds? (2) How does the recon-\nstructed domain knowledge spread geographically? (3) Can the\nmethod be used to inspect the past, present, and future of knowl-\nedge? (4) Can the method be used to find emerging knowledge?"}
{"Title": "Joint User- and Event- Driven Stable Social Event Organization", "Abstract": "Theproblemofsocialeventorganization(SEO)riseswiththeadvent\nof online web services and plays an important role in helping users\ndiscover new offline events. Existing work on SEO only assumes\nthat different users have different preferences towards different\nevents, ignoring the fact that each event (its organizer) may have a\nseparatepreferencetowardseveryuser.Inthispaper,weinvestigate\njoint user- and event- driven SEO by simultaneously considering\nuser preferences (towards events) and event preferences (towards\nusers). A risen challenging problem is that this joint consideration\nmay suffer instabilities between users and events which are NP-\nhard to handle in SEO. Stability is a desired property that needs to\nbe maintained in SEO, otherwise participants will incline towards\nchanging to other events and trust less the organizer. To the best of\nour knowledge, we are the first to study SEO with both preferences\nfrom users to events and preferences from events to users being\nconsidered. In this work, we formulate the stable social event orga-\nnization (Stable-SEO) problem and prove its NP-hardness. We then\npropose an efficient greedy heuristic algorithm to solve Stable-SEO,\ntaking both user preferences and event preferences into account.\nThe proposed approach is able to find an assignment of a group of\nusers to a set of events for an event organization, in which there is\na minimized number of users involved in user-event pairs such that\nboth the user and event in one such pair will be better off when\nreassigning the user to this event. Our experiments on two real-\nworld datasets demonstrate the strong superiority of our proposed\napproach over existing methods."}
{"Title": "\"Satisfaction with Failure\" or \"Unsatisfied Success\": Investigating\nthe Relationship between Search Success and User Satisfaction", "Abstract": "User satisfaction has been paid much attention to in recent Web\nsearch evaluation studies. Although satisfaction is often considered\nas an important symbol of search success, it doesn\u2019t guarantee suc-\ncess in many cases, especially for complex search task scenarios. In\nthis study, we investigate the differences between user satisfaction\nand search success, and try to adopt the findings to predict search\nsuccess in complex search tasks. To achieve these research goals,\nwe conduct a laboratory study in which search success and user\nsatisfaction are annotated by domain expert assessors and search\nusers, respectively. We find that both \"Satisfaction with Failure\"\nand \"Unsatisfied Success\" cases happen in these search tasks and\ntogether they account for as many as 40.3% of all search sessions.\nThe factors (e.g. document readability and credibility) that lead to\nthe inconsistency of search success and user satisfaction are also\ninvestigated and adopted to predict whether one search task is\nsuccessful. Experimental results show that our proposed prediction\nmethod is effective in predicting search success."}
{"Title": "A Sparse Topic Model for Extracting Aspect-Specific\nSummaries from Online Reviews", "Abstract": "Online reviews have become an inevitable part of a consumer\u2019s\ndecision making process, where the likelihood of purchase not only\ndepends on the product\u2019s overall rating, but also on the description\nof its aspects. Therefore, e-commerce websites such as Amazon\nand Walmart constantly encourage users to write good quality re-\nviews and categorically summarize different facets of the products.\nHowever, despite such attempts, it takes a significant effort to skim\nthrough thousands of reviews and look for answers that address the\nquery of consumers. For example, a gamer might be interested in\nbuying a monitor with fast refresh rates and support for Gsync and\nFreesync technologies, while a photographer might be interested\nin aspects such as color depth and accuracy. To address these chal-\nlenges, in this paper, we propose a generative aspect summarization\nmodel called APSUM that is capable of providing fine-grained sum-\nmaries of online reviews. To overcome the inherent problem of\naspect sparsity, we impose dual constraints: (a) a spike-and-slab\nprior over the document-topic distribution and (b) a linguistic su-\npervision over the word-topic distribution. Using a rigorous set of\nexperiments, we show that the proposed model is capable of out-\nperforming the state-of-the-art aspect summarization model over\na variety of datasets and deliver intuitive fine-grained summaries\nthat could simplify the purchase decisions of consumers."}
{"Title": "Understanding and Predicting Delay in Reciprocal Relations", "Abstract": "Reciprocity in directed networks points to user\u2019s willingness to\nreturn favors in building mutual interactions. High reciprocity has\nbeen widely observed in many directed social media networks such\nas following relations in Twitter and Tumblr. Therefore, reciprocal\nrelations between users are often regarded as a basic mechanism\nto create stable social ties and play a crucial role in the formation\nand evolution of networks. Each reciprocity relation is formed by\ntwo parasocial links in a back-and-forth manner with a time delay.\nHence,understandingthedelaycanhelpusgainbetterinsightsinto\nthe underlying mechanisms of network dynamics. Meanwhile, the\naccurate prediction of delay has practical implications in advancing\na variety of real-world applications such as friend recommendation\nand marketing campaign. For example, by knowing when will users\nfollow back, service providers can focus on the users with a po-\ntential long reciprocal delay for effective targeted marketing. This\npaper presents the initial investigation of the time delay in recipro-\ncal relations. Our study is based on a large-scale directed network\nfrom Tumblr that consists of 62.8 million users and 3.1 billion user\nfollowing relations with a timespan of multiple years (from 31 Oct\n2007 to 24 Jul 2013). We reveal a number of interesting patterns\nabout the delay that motivate the development of a principled learn-\ning model to predict the delay in reciprocal relations. Experimental\nresults on the above mentioned dynamic networks corroborate the\neffectiveness of the proposed delay prediction model."}
{"Title": "Detecting Crowdturfing \u201cAdd to Favorites\u201d Activities in Online\nShopping", "Abstract": "\u201cAdd to Favorites\u201d is a popular function in online shopping sites\nwhich helps users to make a record of potentially interesting items\nfor future purchases. It is usually regarded as a type of explicit\nfeedback signal for item popularity and therefore also adopted as\na ranking signal by many shopping search engines. With the in-\ncreasing usage of crowdsourcing platforms, some malicious online\nsellersalsoorganizecrowdturfngactivitiestoincreasethenumbers\nof \u201cAdd to Favorites\u201d for their items. By this means, they expect\nthe items to gain higher positions in search ranking lists and there-\nfore boost sales. This kind of newly-appeared malicious activity\nproposes challenges to traditional search spam detection eforts\nbecause it involves the participation of many crowd workers who\nare normal online shopping users in most of the times, and these\nactivities are composed of a series of behaviors including search,\nbrowse, click and add to favorites.\nTo shed light on this research question, we are among the frst to\ninvestigate this particular spamming activity by looking into both\nthe task organization information in crowdsourcing platforms and\nthe user behavior information from online shopping sites. With a\ncomprehensive analysis of some ground truth spamming activities\nfrom the perspective of behavior, user and item, we propose a\nfactor graph based model to identify this kind of spamming activity.\nExperimental results based on data collected in practical shopping\nsearch environment show that our model helps detect malicious\n\u201cAdd to Favorites\u201d activities efectively."}
{"Title": "StaQC: A Systematically Mined Question-Code Dataset\nfrom Stack Overflow", "Abstract": "Stack Overflow (SO) has been a great source of natural language\nquestions and their code solutions (i.e., question-code pairs), which\nare critical for many tasks including code retrieval and annotation.\nInmostexistingresearch,question-code pairswerecollected heuris-\ntically and tend to have low quality. In this paper, we investigate\na new problem of systematically mining question-code pairs from\nStack Overflow (in contrast to heuristically collecting them). It is for-\nmulated as predicting whether or not a code snippet is a standalone\nsolution to a question. We propose a novel Bi-View Hierarchical\nNeural Network which can capture both the programming content\nand the textual context of a code snippet (i.e., two views) to make\na prediction. On two manually annotated datasets in Python and\nSQL domain, our framework substantially outperforms heuristic\nmethods with at least 15% higher F 1 and accuracy. Furthermore, we\npresent StaQC ( Sta ck Overflow Q uestion- C ode pairs), the largest\ndataset to date of \u223c148K Python and \u223c120K SQL question-code\npairs, automatically mined from SO using our framework. Under\nvarious case studies, we demonstrate that StaQC can greatly help\ndevelop data-hungry models for associating natural language with\nprogramming language 1 ."}
{"Title": "WWW\u201918 Open Challenge: Financial Opinion Mining and\nQuestion Answering", "Abstract": "The growing maturity of Natural Language Processing (NLP) tech-\nniques and resources is dramatically changing the landscape of\nmany application domains which are dependent on the analysis of\nunstructured data at scale. The finance domain, with its reliance\non the interpretation of multiple unstructured and structured data\nsourcesanditsdemandforfastandcomprehensivedecisionmaking\nis already emerging as a primary ground for the experimentation\nof NLP, Web Mining and Information Retrieval (IR) techniques for\nthe automatic analysis of financial news and opinions online. This\nchallenge focuses on advancing the state-of-the-art of aspect-based\nsentiment analysis and opinion-based Question Answering for the\nfinancial domain."}
{"Title": "Manifold Learning for Rank Aggregation\nShangsong Liang\nKAUST\nThuwal, Saudi Arabia\nshangsong.liang@kaust.edu.sa\nIlya Markov", "Abstract": "We address the task of fusing ranked lists of documents that are\nretrieved in response to a query. Past work on this task of rank\naggregation often assumes that documents in the lists being fused\nare independent and that only the documents that are ranked high\nin many lists are likely to be relevant to a given topic. We propose\nmanifold learningaggregation approaches,ManX and v-ManX, that\nbuild on the cluster hypothesis and exploit inter-document simi-\nlarity information. ManX regularizes document fusion scores, so\nthat documents that appear to be similar within a manifold, receive\nsimilar scores, whereas v-ManX first generates virtual adversarial\ndocuments and then regularizes the fusion scores of both origi-\nnal and virtual adversarial documents. Since aggregation methods\nbuilt on the cluster hypothesis are computationally expensive, we\nadopt an optimization method that uses the top- k documents as\nanchors and considerably reduces the computational complexity\nof manifold-based methods, resulting in two efficient aggregation\napproaches, a-ManX and a-v-ManX. We assess the proposed ap-\nproaches experimentally and show that they significantly outper-\nform the state-of-the-art aggregation approaches, while a-ManX\nand a-v-ManX run faster than ManX, v-ManX, respectively."}
{"Title": "Fine-Grained Analysis of Financial Tweets", "Abstract": "This paper decribes our experimental methods and results in FiQA\n2018 Task 1. There are two subtasks : (1) to predict continuous\nsentiment score between -1 to 1, and (2) to determine which\naspect(s) are related to the content of financial tweets. First, we\npropose a preprocessing procedure for decomposing financial\ntweets. Second, we collect over 334K labeled financial tweets to\nenlarge the scale of the experiments. Third, the sentiment\nprediction task is separated into two steps in this paper, i.e., (1)\nbullish/bearish and (2) sentiment degree. We compare the results\nof the CNN, CRNN and Bi-LSTM models. Besides, we further\ncombine the results of the best models in both steps as the model\nof subtask 1. Finally, we make an investigation of aspects in depth,\nand propose some clues for dealing with the 14 aspects."}
{"Title": "Extracting and Ranking Travel Tips from User-Generated\nReviews", "Abstract": "User-generated reviews are a key driving force behind some\nof the leading websites, such as Amazon, TripAdvisor, and\nYelp. Yet, the proliferation of user reviews in such sites also\nposes an information overload challenge: many items, espe-\ncially popular ones, have a large number of reviews, which\ncannot all be read by the user. In this work, we propose\nto extract short practical tips from user reviews. We focus\non tips for travel attractions extracted from user reviews on\nTripAdvisor. Our method infers a list of templates from a\nsmall gold set of tips and applies them to user reviews to\nextract tip candidates. For each attraction, the associated\ncandidates are then ranked according to their predicted use-\nfulness. Evaluation based on labeling by professional an-\nnotators shows that our method produces high-quality tips,\nwith good coverage of cities and attractions."}
{"Title": "Information Extraction in Illicit Web Domains", "Abstract": "Extracting useful entities and attribute values from illicit\ndomains such as human trafficking is a challenging prob-\nlem with the potential for widespread social impact. Such\ndomains employ atypical language models, have \u2018long tails\u2019\nand suffer from the problem of concept drift. In this pa-\nper, we propose a lightweight, feature-agnostic Information\nExtraction (IE) paradigm specifically designed for such do-\nmains. Our approach uses raw, unlabeled text from an ini-\ntial corpus, and a few (12-120) seed annotations per domain-\nspecific attribute, to learn robust IE models for unobserved\npages and websites. Empirically, we demonstrate that our\napproach can outperform feature-centric Conditional Ran-\ndom Field baselines by over 18% F-Measure on five anno-\ntated sets of real-world human trafficking datasets in both\nlow-supervision and high-supervision settings. We also show\nthat our approach is demonstrably robust to concept drift,\nand can be efficiently bootstrapped even in a serial comput-\ning environment."}
{"Title": "Learning to Extract Events from\nKnowledge Base Revisions", "Abstract": "Broad-coverage knowledge bases (KBs) such as Wikipedia, Free-\nbase, Microsoft\u2019s Satori and Google\u2019s Knowledge Graph contain\nstructured data describing real-world entities. These data sources\nhave become increasingly important for a wide range of intelli-\ngent systems: from information retrieval and question answering,\nto Facebook\u2019s Graph Search, IBM\u2019s Watson, and more. Previous\nwork on learning to populate knowledge bases from text has, for\nthe most part, made the simplifying assumption that facts remain\nconstant over time. But this is inaccurate\u2014we live in a rapidly\nchanging world. Knowledge should not be viewed as a static snap-\nshot, but instead a rapidly evolving set of facts that must change as\nthe world changes.\nIn this paper we demonstrate the feasibility of accurately identi-\nfying entity-transition-events, from real-time news and social me-\ndia text streams, that drive changes to a knowledge base. We use\nWikipedia\u2019s edit history as distant supervision to learn event ex-\ntractors, and evaluate the extractors based on their ability to predict\nonline updates. Our weakly supervised event extractors are able\nto predict 10 KB revisions per month at 0.8 precision. By low-\nering our confidence threshold, we can suggest 34.3 correct edits\nper month at 0.4 precision. 64% of predicted edits were detected\nbefore they were added to Wikipedia. The average lead time of\nour forecasted knowledge revisions over Wikipedia\u2019s editors is 40\ndays, demonstrating the utility of our method for suggesting edits\nthat can be quickly verified and added to the knowledge graph. "}
{"Title": "CoType: Joint Extraction of Typed Entities and Relations\nwith Knowledge Bases", "Abstract": "Extractingentitiesandrelationsfortypesofinterestfromtextisim-\nportant for understanding massive text corpora. Traditionally, sys-\ntems of entity relation extraction have relied on human-annotated\ncorpora for training and adopted an incremental pipeline. Such sys-\ntems require additional human expertise to be ported to a new do-\nmain, and are vulnerable to errors cascading down the pipeline.\nIn this paper, we investigate joint extraction of typed entities and\nrelations with labeled data heuristically obtained from knowledge\nbases ( i.e. , distant supervision). As our algorithm for type label-\ning via distant supervision is context-agnostic, noisy training data\nposes unique challenges for the task. We propose a novel domain-\nindependent framework, called C O T YPE , that runs a data-driven\ntext segmentation algorithm to extract entity mentions, and jointly\nembeds entity mentions, relation mentions, text features and type\nlabelsintotwolow-dimensionalspaces(forentityandrelationmen-\ntions respectively), where, in each space, objects whose types are\nclose will also have similar representations. C O T YPE , then using\nthese learned embeddings, estimates the types of test (unlinkable)\nmentions. We formulate a joint optimization problem to learn em-\nbeddings from text corpora and knowledge bases, adopting a novel\npartial-label loss function for noisy labeled data and introducing\nan object \"translation\" function to capture the cross-constraints of\nentities and relations on each other. Experiments on three public\ndatasets demonstrate the effectiveness of C O T YPE across different\ndomains ( e.g. , news, biomedical), with an average of 25% improve-\nment in F1 score compared to the next best method."}
{"Title": "Modeling Consumer Preferences and Price Sensitivities\nfrom Large-Scale Grocery Shopping Transaction Logs", "Abstract": "In order to match shoppers with desired products and pro-\nvide personalized promotions, whether in online or offline\nshopping worlds, it is critical to model both consumer pref-\nerences and price sensitivities simultaneously. Personalized\npreferences have been thoroughly studied in the field of rec-\nommender systems, though price (and price sensitivity) has\nreceived relatively little attention. At the same time, price\nsensitivity has been richly explored in the area of economics,\nthough typically not in the context of developing scalable,\nworking systems to generate recommendations. In this study,\nwe seek to bridge the gap between large-scale recommender\nsystems and established consumer theories from economics,\nand propose a nested feature-based matrix factorization fra-\nmework to model both preferences and price sensitivities.\nQuantitative and qualitative results indicate the proposed\npersonalized, interpretable and scalable framework is capable\nof providing satisfying recommendations (on two datasets of\ngrocery transactions) and can be applied to obtain economic\ninsights into consumer behavior."}
{"Title": "Do \u201cAlso-Viewed\u201d Products Help User Rating Prediction?", "Abstract": "For online product recommendation engines, learning high-\nquality product embedding that captures various aspects of\nthe product is critical to improving the accuracy of user\nrating prediction. In recent research, in conjunction with\nuser feedback, the appearance of a product as side infor-\nmation has been shown to be helpful for learning product\nembedding. However, since a product has a variety of as-\npects such as functionality and specifications, taking into\naccount only its appearance as side information does not\nsuffice to accurately learn its embedding. In this paper,\nwe propose a matrix co-factorization method that leverages\ninformation hidden in the so-called \u201calso-viewed\u201d products,\ni.e., a list of products that has also been viewed by users\nwho have viewed a target product. \u201cAlso-viewed\u201d prod-\nucts reflect various aspects of a given product that have\nbeen overlooked by visually-aware recommendation meth-\nods proposed in past research. Experiments on multiple\nreal-world datasets demonstrate that our proposed method\noutperforms state-of-the-art baselines in terms of user rat-\ning prediction. We also perform classification on the product\nembedding learned by our method, and compare it with a\nstate-of-the-art baseline to demonstrate the superiority of\nour method in generating high-quality product embedding\nthat better represents the product."}
{"Title": "Predicting Latent Structured Intents\nfrom Shopping Queries", "Abstract": "In online shopping, users usually express their intent through\nsearch queries. However, these queries are often ambiguous.\nFor example, it is more likely (and easier) for users to write\na query like \u201chigh-end bike\u201d than \u201c21 speed carbon frames\njamis or giant road bike\u201d. It is challenging to interpret these\nambiguous queries and thus search result accuracy suffers.\nA user oftentimes needs to go through the frustrating pro-\ncess of refining search queries or self-teaching from possibly\nunstructured information. However, shopping is indeed a\nstructured domain, that is composed of category hierarchy,\nbrands, product lines, features, etc. It would be much bet-\nter if a shopping site could understand users\u2019 intent through\nthis structure, present organized information, and then find\nthe items with the right categories, brands or features.\nIn this paper we study the problem of inferring the la-\ntent intent from unstructured queries and mapping them to\nstructured attributes. We present a novel framework that\njointly learns this knowledge from user consumption behav-\niors and product metadata. We present a hybrid Long Short-\nterm Memory (LSTM) [10] joint model that is accurate and\nrobust, even though user queries are noisy and product cat-\nalog is rapidly growing. Our study is conducted on a large-\nscale dataset from Google Shopping, that is composed of\nmillions of items and user queries along with their click re-\nsponses. Extensive qualitative and quantitative evaluation\nshows that the proposed model is more accurate, concise,\nand robust than multiple possible alternatives. In terms of\ninformation retrieval (IR) performance, our model is able to\nimprove the quality of current Google Shopping production\nsystem, which is a very strong baseline."}
{"Title": "Automated Template Generation for Question Answering\nover Knowledge Graphs", "Abstract": "Templatesareanimportantassetforquestionansweringoverknowl-\nedge graphs, simplifying the semantic parsing of input utterances\nand generating structured queries for interpretable answers. State-\nof-the-art methods rely on hand-crafted templates with limited cov-\nerage. This paper presents QUINT, a system that automatically\nlearns utterance-query templates solely from user questions paired\nwith their answers. Additionally, QUINT is able to harness lan-\nguage compositionality for answering complex questions without\nhaving any templates for the entire question. Experiments with dif-\nferent benchmarks demonstrate the high quality of QUINT."}
{"Title": "A Semantic Graph-Based Approach for Mining Common\nTopics from Multiple Asynchronous Text Streams", "Abstract": "In the age of Web 2.0, a substantial amount of unstructured\ncontent are distributed through multiple text streams in an\nasynchronous fashion, which makes it increasingly difficult\nto glean and distill useful information. An effective way to\nexplore the information in text streams is topic modelling,\nwhich can further facilitate other applications such as search,\ninformation browsing, and pattern mining. In this paper, we\npropose a semantic graph based topic modelling approach\nfor structuring asynchronous text streams. Our model in-\ntegrates topic mining and time synchronization, two core\nmodules for addressing the problem, into a unified model.\nSpecifically, for handling the lexical gap issues, we use global\nsemantic graphs of each timestamp for capturing the hid-\nden interaction among entities from all the text streams.\nFor dealing with the sources asynchronism problem, local\nsemantic graphs are employed to discover similar topics of\ndifferent entities that can be potentially separated by time\ngaps. Our experiment on two real-world datasets shows that\nthe proposed model significantly outperforms the existing\nones."}
{"Title": "Neural Network-based Question Answering over\nKnowledge Graphs on Word and Character Level", "Abstract": "Question Answering (QA) systems over Knowledge Graphs\n(KG) automatically answer natural language questions us-\ning facts contained in a knowledge graph. Simple questions,\nwhich can be answered by the extraction of a single fact,\nconstitute a large part of questions asked on the web but\nstill pose challenges to QA systems, especially when asked\nagainst a large knowledge resource. Existing QA systems\nusually rely on various components each specialised in solv-\ning different sub-tasks of the problem (such as segmenta-\ntion, entity recognition, disambiguation, and relation clas-\nsification etc.). In this work, we follow a quite different\napproach: We train a neural network for answering simple\nquestions in an end-to-end manner, leaving all decisions to\nthe model. It learns to rank subject-predicate pairs to en-\nable the retrieval of relevant facts given a question. The\nnetwork contains a nested word/character-level question en-\ncoder which allows to handle out-of-vocabulary and rare\nword problems while still being able to exploit word-level\nsemantics. Our approach achieves results competitive with\nstate-of-the-art end-to-end approaches that rely on an at-\ntention mechanism."}
{"Title": "Detecting Duplicate Posts in Programming QA\nCommunities via Latent Semantics and Association Rules", "Abstract": "Programming community-based question-answering (PCQA)\nwebsites such as Stack Overflow enable programmers to find\nworking solutions to their questions. Despite detailed post-\ning guidelines, duplicate questions that have been answered\nare frequently created. To tackle this problem, Stack Over-\nflow provides a mechanism for reputable users to manu-\nally mark duplicate questions. This is a laborious effort,\nand leads to many duplicate questions remain undetected.\nExisting duplicate detection methodologies from traditional\ncommunity based question-answering (CQA) websites are\ndifficult to be adopted directly to PCQA, as PCQA posts\noften contain source code which is linguistically very dif-\nferent from natural languages. In this paper, we propose\na methodology designed for the PCQA domain to detect\nduplicate questions. We model the detection as a classifi-\ncation problem over question pairs. To extract features for\nquestion pairs, our methodology leverages continuous word\nvectors from the deep learning literature, topic model fea-\ntures and phrases pairs that co-occur frequently in duplicate\nquestions mined using machine translation systems. These\nfeatures capture semantic similarities between questions and\nproduce a strong performance for duplicate detection. Ex-\nperiments on a range of real-world datasets demonstrate\nthat our method works very well; in some cases over 30%\nimprovement compared to state-of-the-art benchmarks. As\na product of one of the proposed features, the association\nscore feature, we have mined a set of associated phrases\nfrom duplicate questi"}
{"Title": "Exploring Rated Datasets with Rating Maps", "Abstract": "Online rated datasets have become a source for large-scale\npopulation studies for analysts and a means for end-users to\nachieve routine tasks such as finding a book club. Existing\nsystems however only provide limited insights into the opin-\nions of different segments of the rater population. In this\npaper, we develop a framework for finding and exploring\npopulation segments and their opinions. We propose rating\nmaps, a collection of (population segment, rating distribu-\ntion) pairs, where a segment, e.g., h18-29 year old males in\nCAi has a rating distribution in the form of a histogram\nthat aggregates its ratings for a set of items (e.g., movies\nstarring Russel Crowe). We formalize the problem of build-\ning rating maps dynamically given desired input distribu-\ntions. Our problem raises two challenges: (i) the choice of\nan appropriate measure for comparing rating distributions,\nand (ii) the design of efficient algorithms to find segments.\nWe show that the Earth Mover\u2019s Distance (EMD) is well-\nadapted to comparing rating distributions and prove that\nfinding segments whose rating distribution is close to input\nones is NP-complete. We propose an efficient algorithm for\nbuilding Partition Decision Trees and heuristics for combin-\ning the resulting partitions to further improve their quality.\nOur experiments on real and synthetic datasets validate the\nutility of rating maps for both analysts and end-users."}
{"Title": "AttriInfer: Inferring User Attributes in Online Social\nNetworks Using Markov Random Fields", "Abstract": "In the attribute inference problem, we aim to infer users\u2019 private\nattributes (e.g., locations, sexual orientation, and interests) using\ntheir public data in online social networks. State-of-the-art meth-\nods leverage a user\u2019s both public friends and public behaviors (e.g.,\npage likes on Facebook, apps that the user reviewed on Google\nPlay) to infer the user\u2019s private attributes. However, these meth-\nods suffer from two key limitations: 1) suppose we aim to infer a\ncertain attribute for a target user using a training dataset, they only\nleverage the labeled users who have the attribute, while ignoring\nthe label information of users who do not have the attribute; 2) they\nare inefficient because they infer attributes for target users one by\none. As a result, they have limited accuracies and applicability in\nreal-world social networks.\nIn this work, we propose AttriInfer, a new method to infer user\nattributes in online social networks. AttriInfer can leverage both\nfriends and behaviors, as well as the label information of train-\ning users who have an attribute and who do not have the attribute.\nSpecifically, we model a social network as a pairwise Markov Ran-\ndom Field (pMRF). Given a training dataset, which consists of\nsome users who have a certain attribute and some users who do not\nhave a certain attribute, we compute the posterior probability that a\ntarget user has the attribute and use the posterior probability to infer\nattributes. In the basic version of AttriInfer, we use Loopy Belief\nPropagation (LBP) to compute the posterior probability. However,\nLBP is not scalable to very large-scale real-world social networks\nand not guaranteed to converge. Therefore, we further optimize\nLBP to be scalable and guaranteed to converge. We evaluated our\nmethod and compare it with state-of-the-art methods using a real-\nworld Google+ dataset with 5.7M users. Our results demonstrate\nthat our method substantially outperforms state-of-the-art methods\nin terms of both accuracy and efficiency."}
{"Title": "Why We Read Wikipedia", "Abstract": "Wikipedia is one of the most popular sites on the Web, with\nmillions of users relying on it to satisfy a broad range of infor-\nmation needs every day. Although it is crucial to understand\nwhat exactly these needs are in order to be able to meet\nthem, little is currently known about why users visit Wikipe-\ndia. The goal of this paper is to fill this gap by combining a\nsurvey of Wikipedia readers with a log-based analysis of user\nactivity. Based on an initial series of user surveys, we build\na taxonomy of Wikipedia use cases along several dimensions,\ncapturing users\u2019 motivations to visit Wikipedia, the depth of\nknowledge they are seeking, and their knowledge of the topic\nof interest prior to visiting Wikipedia. Then, we quantify\nthe prevalence of these use cases via a large-scale user survey\nconducted on live Wikipedia with almost 30,000 responses.\nOur analyses highlight the variety of factors driving users\nto Wikipedia, such as current events, media coverage of a\ntopic, personal curiosity, work or school assignments, or bore-\ndom. Finally, we match survey responses to the respondents\u2019\ndigital traces in Wikipedia\u2019s server logs, enabling the dis-\ncovery of behavioral patterns associated with specific use\ncases. For instance, we observe long and fast-paced page\nsequences across topics for users who are bored or exploring\nrandomly, whereas those using Wikipedia for work or school\nspend more time on individual articles focused on topics\nsuch as science. Our findings advance our understanding of\nreader motivations and behavior on Wikipedia and can have\nimplications for developers aiming to improve Wikipedia\u2019s\nuser experience, editors striving to cater to their readers\u2019\nneeds, third-party services (such as search engines) providing\naccess to Wikipedia content, and researchers aiming to build\ntools such as recommendation engines."}
{"Title": "An Efficient Approach to Event Detection and Forecasting\nin Dynamic Multivariate Social Media Networks", "Abstract": "Anomalous subgraph detection has been successfully applied\nto event detection in social media. However, the subgraph\ndetection problembecomes challenging when the social me-\ndia network incorporates abundant attributes, which leads\nto a multivariate network. The multivariate characteristic\nmakes most existing methods incapable to tackle this prob-\nlem effectively and efficiently, as it involves joint feature se-\nlection and subgraph detection that has not been well ad-\ndressed in the current literature, especially, in the dynamic\nmultivariate networks in which attributes evolve over time.\nThis paper presents a generic framework, namely dynamic\nmultivariate evolving anomalous subgraphs scanning (DM-\nGraphScan), to addressthis problem in dynamic multivariate\nsocial media networks. We generalize traditional nonpara-\nmetric statistics, and propose a new class of scan statis-\ntic functions for measuring the joint significance of evolving\nsubgraphs and subsets of attributes to indicate the ongo-\ning or forthcoming event in dynamic multivariate networks.\nWe reformulate each scan statistic function as a sequence\nof subproblems with provable guarantees, and then propose\nan efficient approximation algorithm for tackling each sub-\nproblem. This algorithm resorts to the Lagrangian relax-\nation and a dynamic programming based on tree-shaped\npriors. As a case study, we conduct extensive experiments\nto demonstrate the performance of our proposed approach\non two real-world applications (flu outbreak detection, haze\ndetection) in different domains."}
{"Title": "Web-Scale User Modeling for Targeting", "Abstract": "We present the experiences from building a web-scale user\nmodeling platform for optimizing display advertising tar-\ngeting at Yahoo!. The platform described in this paper al-\nlows for per-campaign maximization of conversions repre-\nsenting purchase activities or transactions. Conversions di-\nrectly translate to advertiser\u2019s revenue, and thus provide the\nmost relevant metrics of return on advertising investment.\nWe focus on two major challenges: how to efficiently pro-\ncess histories of billions of users on a daily basis, and how to\nbuild per-campaign conversion models given the extremely\nlow conversion rates (compared to click rates in a traditional\nsetting). We first present mechanisms for building web-scale\nuser profiles in a daily incremental fashion. Second, we show\nhow to reduce the latency through in-memory processing of\nbillions of user records. Finally, we discuss a technique for\nscaling the number of handled campaigns/models by intro-\nducing an efficient labeling technique that allows for sharing\nnegative training examples across multiple campaigns"}
{"Title": "Outage Detection via Real-time Social Stream Analysis:\nLeveraging the Power of Online Complaints", "Abstract": "Over the past couple of years, Netflix has significantly ex-\npanded its online streaming offerings, which now encompass\nmultiple delivery platforms and thousands of titles available\nfor instant view. This paper documents the design and\ndevelopment of an outage detection system for the online\nservices provided by Netflix. Unlike other internal quality-\ncontrol measures used at Netflix, this system uses only pub-\nlicly available information: the tweets, or Twitter posts, that\nmention the word\u201cNetflix,\u201dand has been developed and de-\nployed externally, on servers independent of the Netflix in-\nfrastructure. This paper discussed the system and provides\nassessment of the accuracy of its real-time detection and\nalert mechanisms."}
{"Title": "Optimizing User Exploring Experience in Emerging\nE-Commerce Products", "Abstract": "E-commerce has emerged as a popular channel for Web users\nto conduct transaction over Internet. In e-commerce ser-\nvices, users usually prefer to discover information via query-\ning over category browsing, since the hierarchical structure\nsupported by category browsing can provide them a more\neffective and efficient way to find their interested proper-\nties. However, in many emerging e-commerce services, well-\ndefined hierarchical structures are not always available; more-\nover, in some other e-commerce services, the pre-defined hi-\nerarchical structures are too coarse and less intuitive to dis-\ntinguish properties according to users interests. This will\nlead to very bad user experience. In this paper, to ad-\ndress these problems, we propose a hierarchical clustering\nmethod to build the query taxonomy based on users\u2019 explo-\nration behavior automatically, and further propose an in-\ntuitive and light-weight approach to construct browsing list\nfor each cluster to help users discover interested items. The\nadvantage of our approach is four folded. First, we build\na hierarchical taxonomy automatically, which saves tedious\nhuman effort. Second, we provide a fine-grained structure,\nwhich can help user reach their interested items efficiently.\nThird, our hierarchical structure is derived from users\u2019 inter-\naction logs, and thus is intuitive to users. Fourth, given the\nhierarchical structures, for each cluster, we present both fre-\nquently clicked items and retrieved results of queries in the\ncategory, which provides more intuitive items to users. We\nevaluate our work by applying it to the exploration task of\na real-world e-commerce service, i.e. online shop for smart\nmobile phone\u2019s apps. Experimental results show that our\nclustering algorithm is efficient and effective to assist users\nto discover their interested properties, and further compar-\nisons illustrate that the hierarchical topic browsing performs\nmuch better than existing category browsing approach (i.e.\nAndroid Market mobile apps category) in terms of informa-\ntion exploration."}
{"Title": "FoCUS: Learning to Crawl Web Forums", "Abstract": "In this paper, we present FoCUS (Forum Crawler Under\nSupervision), a supervised web-scale forum crawler. The goal of\nFoCUS is to only trawl relevant forum content from the web with\nminimal overhead. Forum threads contain information content that\nis the target of forum crawlers. Although forums have different\nlayouts or styles and are powered by different forum software\npackages, they always have similar implicit navigation paths\nconnected by specific URL types to lead users from entry pages to\nthread pages. Based on this observation, we reduce the web forum\ncrawling problem to a URL type recognition problem and show\nhow to learn accurate and effective regular expression patterns of\nimplicit navigation paths from an automatically created training\nset using aggregated results from weak page type classifiers.\nRobust page type classifiers can be trained from as few as 5\nannotated forums and applied to a large set of unseen forums. Our\ntest results show that FoCUS achieved over 98% effectiveness\nand 97% coverage on a large set of test forums powered by over\n150 different forum software packages."}
{"Title": "Answering Math Queries With Search Engines", "Abstract": "Conventional search engines such as Bing and Google pro-\nvide a user with a short answer to some queries as well as a\nranked list of documents, in order to better meet her infor-\nmation needs. In this paper we study a class of such queries\nthat we call math. Calculations (e.g. \u201c 12% of 24$ \u201d,\u201csquare\nroot of 120\u201d), unit conversions (e.g. \u201cconvert 10 meter to\nfeet\u201d), and symbolic computations (e.g. \u201cplot x\u02c62+x+1\u201d) are\nexamples of math queries. Among the queries that should\nbe answered, math queries are special because of the infi-\nnite combinations of numbers and symbols, and rather few\nkeywords that form them. Answering math queries must be\ndone through real time computations rather than keyword\nsearches or database look ups.\nThe lack of a formal definition for the entire range of math\nqueries makes it hard to automatically identify them all.\nWe propose a novel approach for recognizing and classifying\nmath queries using large scale search logs, and investigate\nits accuracy through empirical experiments and statistical\nanalysis. It allows us to discover classes of math queries\neven if we do not know their structures in advance. It also\nhelps to identify queries that are not math even though they\nmight look like math queries.\nWe also evaluate the usefulness of math answers based\non the implicit feedback from users. Traditional approaches\nfor evaluating the quality of search results mostly rely on\nthe click information and interpret a click on a link as a\nsign of satisfaction. Answers to math queries do not contain\nlinks, therefore such metrics are not applicable to them. In\nthis paper we describe two evaluation metrics that can be\napplied for math queries, and present the results on a large\ncollection of math queries taken from Bing\u2019s search logs."}
{"Title": "Hierarchical Composable Optimization of Web Pages", "Abstract": "The process of creating modern Web media experiences is chal-\nlenged by the need to adapt the content and presentation choices\nto dynamic real-time fluctuations of user interest across multiple\naudiences. We introduce FAME \u2013 a Framework for Agile Media\nExperiences \u2013 which addresses this scalability problem. FAME al-\nlows media creators to define abstract page models that are sub-\nsequently transformed into real experiences through algorithmic\nexperimentation. FAME\u2019s page models are hierarchically com-\nposed of simple building blocks, mirroring the structure of most\nWeb pages. They are resolved into concrete page instances by\npluggable algorithms which optimize the pages for specific busi-\nness goals. Our framework allows retrieving dynamic content from\nmultiple sources, defining the experimentation\u2019s degrees of free-\ndom, and constraining the algorithmic choices. It offers an ef-\nfective separation of concerns in the media creation process, en-\nabling multiple stakeholders with profoundly different skills to ap-\nply their crafts and perform their duties independently, composing\nand reusing each other\u2019s work in modular ways."}
{"Title": "Delta-Reasoner: a Semantic Web Reasoner for an\nIntelligent Mobile Platform", "Abstract": "To make mobile device applications more intelligent, one\ncan combine the information obtained via device sensors\nwith background knowledge in order to deduce the user\u2019s\ncurrent context, and then use this context to adapt the\napplication\u2019s behaviour to the user\u2019s needs. In this paper\nwe describe Delta-Reasoner, a key component of the Intel-\nligent Mobile Platform (IMP), which was designed to sup-\nport context-aware applications running on mobile devices.\nContext-aware applications and the mobile platform impose\nunusual requirements on the reasoner, which we have met\nby incorporating advanced features such as incremental rea-\nsoning and continuous query evaluation into our reasoner.\nAlthough we have so far been able to conduct only a very\npreliminary performance evaluation, our results are very en-\ncouraging: our reasoner exhibits sub-second response time\non ontologies whose size significantly exceeds the size of the\nontologies used in the IMP."}
{"Title": "Rewriting Null E-Commerce Queries to Recommend\nProducts", "Abstract": "In e-commerce applications product descriptions are often\nconcise. E-Commerce search engines often have to deal with\nqueries that cannot be easily matched to product inventory\nresulting in zero recall or null query situations. Null queries\narise from differences in buyer and seller vocabulary or from\nthe transient nature of products. In this paper, we describe a\nsystem that rewrites null e-commerce queries to find match-\ning products as close to the original query as possible. The\nsystem uses query relaxation to rewrite null queries in order\nto match products. Using eBay as an example of a dynamic\nmarketplace, we show how using temporal feedback that re-\nspects product category structure using the repository of\nexpired products, we improve the quality of recommended\nresults. The system is scalable and can be run in a high vol-\nume setting. We show through our experiments that high\nquality product recommendations for more than 25% of null\nqueries are achievable."}
{"Title": "Towards Expressive Exploratory Search Over\nEntity-Relationship Data", "Abstract": "In this paper we describe a novel approach for exploratory\nsearch over rich entity-relationship data that utilizes a unique\ncombination of expressive, yet intuitive, query language,\nfaceted search, and graph navigation. We describe an ex-\ntended faceted search solution which allows to index, search,\nand browse rich entity-relationship data. We report experi-\nmental results of an evaluation study, using a benchmark of\nseveral of entity-relationship datasets, demonstrating that\nour exploratory approach is both effective and efficient com-\npared to other existing approaches."}
{"Title": "Data Extraction from Web Pages Based\non Structural-Semantic Entropy", "Abstract": "Most of today's web content is designed for human consumption,\nwhich makes it difficult for software tools to access them readily.\nEven web content that is automatically generated from back-end\ndatabases is usually presented without the original structural\ninformation. In this paper, we present an automated information\nextraction algorithm that can extract the relevant attribute-value\npairs from product descriptions across different sites. A notion,\ncalled structural-semantic entropy, is used to locate the data of\ninterest on web pages, which measures the density of occurrence\nof relevant information on the DOM tree representation of web\npages. Our approach is less labor-intensive and insensitive to\nchanges in web-page format. Experimental results on a large\nnumber of real-life web page collections are encouraging and\nconfirm the feasibility of the approach, which has been\nsuccessfully applied to detect false drug advertisements on the\nweb due to its capacity in associating the attributes of records\nwith their respective values."}
{"Title": "Clustering and Load Balancing Optimization for\nRedundant Content Removal", "Abstract": "Removing redundant content is an important data process-\ning operation in search engines and other web applications.\nAn offline approach can be important for reducing the en-\ngine\u2019s cost, but it is challenging to scale such an approach for\na large data set which is updated continuously. This paper\ndiscusses our experience in developing a scalable approach\nwith parallel clustering that detects and removes near dupli-\ncates incrementally when processing billions of web pages.\nIt presents a multidimensional mapping to balance the load\namong multiple machines. It further describes several ap-\nproximation techniques to efficiently manage distributed du-\nplicate groups with transitive relationship. The experimen-\ntal results evaluate the efficiency and accuracy of the incre-\nmental clustering, assess the effectiveness of the multidimen-\nsional mapping, and demonstrate the impact on online cost\nreduction and search quality."}
{"Title": "From Linked Data to Linked Entities: a Migration Path", "Abstract": "Entities have been deserved special attention in the latest years,\nhowever their identification is still troublesome. Existing\napproaches exploit ad hoc services or centralized architectures. In\nthis paper we present a novel approach to recognize naturally\nemerging entity identifiers built on top of Linked Data concepts\nand protocols."}
{"Title": "Cyberbullying Detection:\nA Step Toward a Safer Internet Yard", "Abstract": "As a result of the invention of social networks friendships,\nrelationships and social communications have all gone to a new\nlevel with new definitions. One may have hundreds of friends\nwithout even seeing their faces. Meanwhile, alongside this\ntransition there is increasing evidence that online social\napplications have been used by children and adolescents for\nbullying. State-of-the-art studies in cyberbullying detection have\nmainly focused on the content of the conversations while largely\nignoring the users involved in cyberbullying. We propose that\nincorporation of the users\u2019 information, their characteristics, and\npost-harassing behaviour, for instance, posting a new status in\nanother social network as a reaction to their bullying experience,\nwill improve the accuracy of cyberbullying detection. Cross-\nsystem analyses of the users\u2019 behaviour - monitoring their\nreactions in different online environments - can facilitate this\nprocess and provide information that could lead to more accurate\ndetection of cyberbullying."}
{"Title": "Intelligent Crawling of Web Applications\nfor Web Archiving", "Abstract": "The steady growth of the World Wide Web raises challenges\nregarding the preservation of meaningful Web data. Tools\nused currently by Web archivists blindly crawl and store Web\npages found while crawling, disregarding the kind of Web\nsite currently accessed (which leads to suboptimal crawling\nstrategies) and whatever structured content is contained\nin Web pages (which results in page-level archives whose\ncontent is hard to exploit). We focus in this PhD work\non the crawling and archiving of publicly accessible Web\napplications, especially those of the social Web. A Web\napplication is any application that uses Web standards such\nas HTML and HTTP to publish information on the Web,\naccessible by Web browsers. Examples include Web forums,\nsocial networks, geolocation services, etc. We claim that the\nbest strategy to crawl these applications is to make the Web\ncrawler aware of the kind of application currently processed,\nallowing it to refine the list of URLs to process, and to\nannotate the archive with information about the structure\nof crawled content. We add adaptive characteristics to an\narchival Web crawler: being able to identify when a Web\npage belongs to a given Web application and applying the\nappropriate crawling and content extraction methodology."}
{"Title": "Binary RDF for Scalable Publishing, Exchanging and\nConsumption in the Web of Data", "Abstract": "The Web of Data is increasingly producing large RDF data-\nsets from diverse fields of knowledge, pushing the Web to\na data-to-data cloud. However, traditional RDF represen-\ntations were inspired by a document-centric view, which\nresults in verbose/redundant data, costly to exchange and\npost-process. This article discusses an ongoing doctoral the-\nsis addressing efficient formats for publication, exchange and\nconsumption of RDF on a large scale. First, a binary serial-\nization format for RDF, called HDT, is proposed. Then, we\nfocus on compressed rich-functional structures which take\npart of efficient HDT representation as well as most appli-\ncations performing on huge RDF datasets."}
{"Title": "User-generated Metadata in Audio-visual Collections", "Abstract": "In recent years, crowdsourcing has gained attention as an al-\nternative method for collecting video annotations. An exam-\nple is the internet video labeling game Waisda? launched by\nthe Netherlands Institute for Sound and Vision. The goal of\nthis PhD research is to investigate the value of the user tags\ncollected with this video labeling game. To this end, we ad-\ndress the following four issues. First, we perform a compar-\native analysis between user-generated tags and professional\nannotations in terms of what aspects of videos they describe.\nSecond, we measure how well user tags are suited for frag-\nment retrieval and compare it with fragment search based on\nother sources like transcripts and professional annotations.\nThird, as previous research suggested that user tags pre-\ndominately refer to objects and rarely describe scenes, we\nwill study whether user tags can be successfully exploited\nto generate scene-level descriptions. Finally, we investigate\nhow tag quality can be characterized and potential methods\nto improve it."}
{"Title": "Scalable Search Platform: Improving Pipelined Query\nProcessing for Distributed Full-Text Retrieval", "Abstract": "In theory, term-wise partitioned indexes may provide higher\nthroughput than document-wise partitioned. In practice,\nterm-wise partitioning shows lacking scalability with increas-\ning collection size and intra-query parallelism, which leads\nto long query latency and poor performance at low query\nloads. In our work, we have developed several techniques to\ndeal with these problems. Our current results show a sig-\nnificant improvement over the state-of-the-art approach on\na small distributed IR system, and our next objective is to\nevaluate the scalability of the improved approach on a large\nsystem. In this paper, we describe the relation between our\nwork and the problem of scalability, summarize the results,\nlimitations and challenges of our current work, and outline\ndirections for further research."}
{"Title": "Building Reputation and Trust Using Federated Search\nand Opinion Mining", "Abstract": "The term online reputation addresses trust relationships amongst\nagents in dynamic open systems. These can appear as ratings,\nrecommendations, referrals and feedback. Several reputation\nmodels and rating aggregation algorithms have been proposed.\nHowever, finding a trusted entity on the web is still an issue as all\nreputation systems work individually. The aim of this project is to\nintroduce a global reputation system that aggregates people\u2019s\nopinions from different resources (e.g. e-commerce websites, and\nreview) with the help federated search techniques. A sentiment\nanalysis approach is subsequently used to extract high quality\nopinions and inform how to increase trust in the search result."}
{"Title": "A Semantic Policy Sharing and Adaptation Infrastructure\nfor Pervasive Communities", "Abstract": "Rule based information processing has traditionally been vi-\ntal in many aspects of business, process manufacturing and\ninformation science. The need for rules gets even more mag-\nnified when limitations of ontology development in OWL are\ntaken into account. In conjunction, the potent combination\nof ontology and rule based applications could be the future\nof information processing and knowledge representation on\nthe web. However, semantic rules tend to be very depen-\ndent on multitudes of parameters and context data making\nit less flexible for use in applications where users could ben-\nefit from each other by socially sharing intelligence in the\nform of policies. This work aims to address this issue arising\nin rule based semantic applications in the use cases of smart\nhome communities and privacy aware m-commerce setting\nfor mobile users. In this paper, we propose a semantic pol-\nicy sharing and adaptation infrastructure that enables a se-\nmantic rule created in one set of environmental, physical and\ncontextual settings to be adapted for use in a situation when\nthose settings/parameters/context variables change. The fo-\ncus will mainly be on behavioural policies in the smart home\nuse case and privacy enforcing and data filtering policies in\nthe m-commerce scenario. Finally, we look into the possibil-\nity of making this solution application independent so that\nthe benefits of such a policy adaptation infrastructure could\nbe exploited in other application settings as well."}
{"Title": "A Generic Graph-based Multidimensional\nRecommendation Framework and Its Implementations", "Abstract": "As the volume of information on the Web is explosively growing,\nrecommender systems have become essential tools for helping\nusers to find what they need or prefer. Most existing systems are\ntwo-dimensional in that they only exploit User and Item\ndimensions and perform a typical form of recommendation\n\u2018Recommending Item to User\u2019. Yet, in many applications, the\ncapabilities of dealing with multidimensional information and of\nadapting to various forms of recommendation requests are very\nimportant. In this paper, we take a graph-based approach to\naccomplishing such requirements in recommender systems and\npresent a generic graph-based multidimensional recommendation\nframework. Based on the framework, we propose two\nhomogeneous graph-based and one heterogeneous graph-based\nmultidimensional recommendation methods. We expect our\napproach will be useful for increasing recommendation\nperformance and enabling flexibility of recommender systems so\nthat they can incorporate various user intentions into their\nrecommendation process. We present our research result that we\nhave reached and discuss remaining challenges and future work."}
{"Title": "Semi-Automatic Semantic Moderation of Web Annotations", "Abstract": "Many social media portals are featuring annotation func-\ntionality in order to integrate the end users\u2019 knowledge with\nexisting digital curation processes. This facilitates extending\nexisting metadata about digital resources. However, due to\nvarious levels of annotators\u2019 expertise, the quality of annota-\ntions can vary from excellent to vague. The evaluation and\nmoderation of annotations (be they troll, vague, or helpful)\nhave not been sufficiently analyzed automatically. Avail-\nable approaches mostly attempt to solve the problem by us-\ning distributed moderation systems, which are influenced by\nfactors affecting accuracy (such as imbalance voting). De-\nspite this, we hypothesize that analyzing and exploiting both\ncontent and context dimensions of annotations may assist\nthe automatic moderation process. In this research, we fo-\ncus on leveraging the context and content features of social\nweb annotations for semi-automatic semantic moderation.\nThis paper describes the vision of our research, proposes\nan approach for semi-automatic semantic moderation, intro-\nduces an ongoing effort from which we collect data that can\nserve as a basis for evaluating our assumption, and report\non lessons learned so far."}
{"Title": "Modeling the Flow and Change of Information on the Web", "Abstract": "The proposed PhD work approaches the problem of infor-\nmation flow and change on the Web. To model temporal\ndynamics both of the Web structure and its content, the\nauthor proposes to apply the framework of stochastic graph\ntransformation systems [13]. This framework is currently\nwidely used in software engineering and model checking. A\nquantitative and qualitative evaluation of the framework will\nbe performed during a case study of the short-term temporal\nbehavior of economics news on selected English news web-\nsites and blogs over selected time period."}
{"Title": "Context-Aware Image Semantic Extraction\nin the Social Web", "Abstract": "Media sharing applications such as Panoramio and Flickr\ncontain a huge amount of pictures that need to be organized\nto facilitate browsing and retrieval. Such pictures are often\nsurrounded by a set of metadata or image tags, constitut-\ning the image context. With the advent of the paradigm of\nWeb 2.0 especially the past five years, the concept of im-\nage context has further evolved, allowing users to tag their\nown and other people\u2019s pictures. Focusing on tagging, we\ndistinguish between static and dynamic features. The set of\nstatic features include textual and visual features, as well as\nthe contextual information. Further, we may identify other\nfeatures belonging to the social context as a result of the\nusage within the media sharing applications. Due to their\ndynamic nature, we call these the dynamic set of features.\nIn this work, we assume that every media uploaded contains\nboth static and dynamic features. In addition, a user may\nbe linked with other users with whom he/she shares com-\nmon interests. This has resulted in a new series of challenges\nwithin the research field of semantic understanding. One of\nthe main goals of this work is to address these challenges."}
{"Title": "Augmenting the Web with Accountability", "Abstract": "Given the ubiquity of data on the web, and the lack of us-\nage restriction enforcement mechanisms, stories of personal,\ncreative and other kinds of data misuses are on the rise.\nThere should be both sociological and technological mecha-\nnisms that facilitate accountability on the web that would\nprevent such data misuses. Sociological mechanisms appeal\nto the data consumer\u2019s self-interest in adhering to the data\nprovider\u2019s desires. This involves a system of rewards such\nas recognition and financial incentives, and deterrents such\nas prohibitions by laws for any violations and social pres-\nsure. Bur there is no well-defined technological mechanism\nfor the discovery of accountability or the lack of it on the web.\nAs part of my PhD thesis I propose a solution to this prob-\nlem by designing a web protocol called HTTPA (Accountable\nHTTP). This protocol will enable data consumers and data\nproducers to agree to specific usage restrictions, preserve the\nprovenance of data transferred from a web server to a client\nand back to another web server, and more importantly pro-\nvide a mechanism to derive an \u2018audit trail\u2019 for the data reuse\nwith the help of a trusted intermediary called a \u2018Provenance\nTracker Network\u2019."}
{"Title": "AMBER: Turning Annotations into Knowledge", "Abstract": "WebextractionisthetaskofturningunstructuredHTMLintoknowl-\nedge. Computers are able to generate annotations of unstructured\nHTML, but it is more important to turn those annotations into struc-\ntured knowledge. Unfortunately, the current systems extracting\nknowledge from result pages lack accuracy.\nIn this proposal, we present AMBER , a system fully automated\nturning annotations to structured knowledge from any result page\nof a given domain. AMBER observes basic domain attributes on\na page and leverages repeated occurrences of similar attributes to\ngroup related attributes into records. This contrasts to previous ap-\nproaches that analyze the repeated structure only of the HTML, as\nno domain knowledge is available. Our multi-domain experimental\nevaluation on hundreds of sites demonstrates that AMBER achieves\naccuracy (>98%) comparable to skilled human annotator."}
{"Title": "Chinese News Event 5W1H Semantic Elements Extraction\nfor Event Ontology Population", "Abstract": "To relieve \u201cNews Information Overload\u201d, in this paper, we\npropose a novel approach of 5W1H (who, what, whom, when,\nwhere, how) event semantic elements extraction for Chi-\nnese news event knowledge base construction. The approach\ncomprises a key event identification step, an event semantic\nelements extraction step and an event ontology population\nstep. We first use a machine learning method to identify the\nkey events from Chinese news stories. Then we extract event\n5W1H elements by employing the combination of SRL, NER\ntechnique and rule-based method. At last we populate the\nextracted facts of news events to NOEM, an event ontology\ndesigned specifically for modeling semantic elements and re-\nlations of events. Our experiments on real online news data\nsets show the reasonability and feasibility of our approach."}
{"Title": "Semi-structured Semantic Overlay for Information\nRetrieval in Self-organizing Networks", "Abstract": "As scalability and flexibility have become the critical con-\ncerns in information management systems, self-organizing\nnetworks attract attentions from both research and indus-\ntrial communities. This work proposes a semi-structured\nsemantic overlay for information retrieval in large-scale self-\norganizing networks. With the autonomy to their own re-\nsources, the nodes are organized into a semantic overlay\nhosting topically discriminative communities. For informa-\ntion retrieval within a community, unstructured routing ap-\nproach is employed for the sake of flexibility; While for join-\ning new nodes and routing queries to a distant community, a\nstructured mechanism is designed to save the traffic and time\ncost. Different from the semantic overlay in the literature,\nour proposal has three contributions: 1. we design topic-\nbased indexing to form and maintain the semantic overlay,\nto guarantee both scalability and efficiency; 2. We intro-\nduce unstructured routing approach within the community,\nto allow flexible node joining and leaving; 3. We take advan-\ntage of the interaction among nodes to capture the overlay\nchanges and make corresponding adaption in topic-based in-\ndexing."}
{"Title": "The ERC Webdam on\nFoundations of Web Data Management\n\u2217", "Abstract": "The Webdam ERC grant is a five-year project that started\nin December 2008. The goal is to develop a formal model for\nWeb data management that would open new horizons for the\ndevelopment of the Web in a well-principled way, enhancing\nits functionality, performance, and reliability. Specifically,\nthe goal is to develop a universally accepted formal frame-\nwork for describing complex and flexible interacting Web\napplications featuring notably data exchange, sharing, inte-\ngration, querying, and updating. We also propose to develop\nformal foundations that will enable peers to concurrently\nreason about global data management activities, cooperate in\nsolving specific tasks, and support services with desired qual-\nity of service. Although the proposal addresses fundamental\nissues, its goal is to serve as the basis for future software\ndevelopment for Web data management."}
{"Title": "WAI-ACT: Web Accessibility Now", "Abstract": "The W3C web accessibility standards have now existed for over a\ndecade yet implementation of accessible websites, software, and\nweb technologies is lagging behind this development. This fact is\nlargely due to lack of knowledge and expertise among developers\nand due to fragmentation of web accessibility approaches. It is an\nopportune time to develop authoritative practical guidance and\nharmonized approaches, and to research potential challenges and\nopportunities in future technologies in a collaborative setting. The\nEC-funded WAI-ACT project addresses these needs through use\nof an open cooperation framework that builds on and extends the\nexisting mechanisms of the W3C Web Accessibility Initiative\n(WAI). This paper presents the WAI-ACT project and how it will\ndrive accessibility implementation in advanced web technologies."}
{"Title": "GLOCAL: Event-based Retrieval of Networked Media", "Abstract": "The idea of the European project GLOCAL is to use events as the\ncentral concept for search, organization and combination of\nmultimedia content from various sources. For this purpose\nmethods for event detection and event matching as well as media\nanalysis are developed. Considered events range from private,\nover local, to global events."}
{"Title": "Combining Social Web and BPM for Improving Enterprise\nPerformances: the BPM4People Approach to Social BPM", "Abstract": "Social BPM fuses business process management practices with\nsocial networking applications, with the aim of enhancing the\nenterprise performance by means of a controlled participation of\nexternal stakeholders to process design and enactment. This project-\ncentered demonstration paper proposes a model-driven approach to\nparticipatory and social enactment of business processes. The\napproach consists of defining a specific notation for describing\nSocial BPM behaviors (defined as a BPMN 2.0 extension), a\nmethodology, and a technical framework that allows enterprises to\nimplement social processes as Web applications integrated with\npublic or private Web social networks. The presented work is\nperformed within the BPM4People SME Capacities project."}
{"Title": "Entity Oriented Search and Exploration\nfor Cultural Heritage Collections", "Abstract": "In this paper we describe an entity oriented search and ex-\nploration system that we are developing for the EU Cultura\nproject."}
{"Title": "The Patents Retrieval Prototype in the MOLTO Project", "Abstract": "This paper describes the patents retrieval prototype devel-\noped within the MOLTO project. The prototype aims to\nprovide a multilingual natural language interface for query-\ning the content of patent documents. The developed system\nis focused on the biomedical and pharmaceutical domain\nand includes the translation of the patent claims and ab-\nstracts into English, French and German. Aiming at the\nbest retrieval results of the patent information and text\ncontent, patent documents are preprocessed and semanti-\ncally annotated. Then, the annotations are stored and in-\ndexed in an OWLIM semantic repository, which contains a\npatent specific ontology and others from different domains.\nThe prototype, accessible online at http://molto-patents.\nontotext.com, presents a multilingual natural language in-\nterface to query the retrieval system. In MOLTO, the mul-\ntilingualism of the queries is addressed by means of the GF\nTool, which provides an easy way to build and maintain\ncontrolled language grammars for interlingual translation in\nlimited domains. The abstract representation obtained from\nthe GF is used to retrieve both the matched RDF instances\nand the list of patents semantically related to the user\u2019s\nsearch criteria. The online interface allows to browse the\nretrieved patents and shows on the text the semantic anno-\ntations that explain the reason why any particular patent\nhas matched the user\u2019s criteria."}
{"Title": "End-User-Oriented Telco Mashups: The OMELETTE\nApproach", "Abstract": "With the success of Web 2.0 we are witnessing a growing\nnumber of services and APIs exposed by Telecom, IT and\ncontent providers. Targeting the Web community and, in\nparticular, Web application developers, service providers ex-\npose capabilities of their infrastructures and applications\nin order to open new markets and to reach new customer\ngroups. However, due to the complexity of the underly-\ning technologies, the last step, i.e., the consumption and\nintegration of the offered services, is a non-trivial and time-\nconsuming task that is still a prerogative of expert develop-\ners. Although many approaches to lower the entry barriers\nfor end users exist, little success has been achieved so far.\nIn this paper, we introduce the OMELETTE 1 project and\nshow how it addresses end-user-oriented telco mashup de-\nvelopment. We present the goals of the project, describe\nits contributions, summarize current results, and describe\ncurrent and future work."}
{"Title": "Multilingual Online Generation from Semantic Web\nOntologies", "Abstract": "In this paper we report on our ongoing work in the EU\nproject Multilingual Online Translation (MOLTO), supported\nby the European Union Seventh Framework Programme un-\nder grant agreement FP7-ICT-247914. More specifically, we\npresent work workpackage 8 (WP8): Case Study: Cultural\nHeritage. The objective of the work is to build an ontology-\nbased multilingual application for museum information on\nthe Web. Our approach relies on the innovative idea of\nReason-able View of the Web of linked data applied to the\ndomain of cultural heritage. We have been developing a\nWeb application that uses Semantic Web ontologies for gen-\nerating coherent multilingual natural language descriptions\nabout museum objects. We have been experimenting with\nmuseum data to test our approach and find that it performs\nwell for the examined languages."}
{"Title": "Making Use of Social Media Data in Public Health", "Abstract": "Disease surveillance systems exist to offer an easily accessible\n\"epidemiological snapshot\" on up-to-date summary statistics for\nnumerous infectious diseases. However, these indicator-based\nsystems represent only part of the solution. Experiences show that\nthey fail when confronted with agents that are new emerging like\nthe agents causing the lung disease SARS in 2002. Further, due to\nslow reporting mechanisms, the time until health threats become\nvisible to public health officials can be long. The M-Eco project\nprovides an event-based approach to the early detection of\nemerging health threats. The developed technologies exploit\ncontent from social media and multimedia data as input and\nanalyze it by sophisticated event-detection techniques to identify\npotential threats. Alerts for public health threats are provided to\nthe user in a personalized way."}
{"Title": "SocialSensor: Sensing User Generated Input for Improved\nMedia Discovery and Experience", "Abstract": "SocialSensor will develop a new framework for enabling real-\ntime multimedia indexing and search in the Social Web. The\nproject moves beyond conventional text-based indexing and\nretrieval models by mining and aggregating user inputs and\ncontent over multiple social networking sites. Social Indexing will\nincorporate information about the structure and activity of the\nusers\u2019 social network directly into the multimedia analysis and\nsearch process. Furthermore, it will enhance the multimedia\nconsumption experience by developing novel user-centric media\nvisualization and browsing paradigms. For example, SocialSensor\nwill analyse the dynamic and massive user contributions in order\nto extract unbiased trending topics and events and will use social\nconnections for improved recommendations. To achieve its\nobjectives, SocialSensor introduces the concept of Dynamic\nSocial COntainers (DySCOs), a new layer of online multimedia\ncontent organisation with particular emphasis on the real-time,\nsocial and contextual nature of content and information\nconsumption. Through the proposed DySCOs-centered media\nsearch, SocialSensor will integrate social content mining, search\nand intelligent presentation in a personalized, context and\nnetwork-aware way, based on aggregation and indexing of both\nUGC and multimedia Web content."}
{"Title": "The Multilingual Web", "Abstract": "We report on the MultilingualWeb initiative, a collaboration\nbetween the W3C Internationalization Activity and the European\nCommission, realized as a series of EC-funded projects. We\nreview the outcomes of \u201cMultilingualWeb\u201d, which conducted 4\nworkshops analyzing \u201cgaps\u201d within Web standardization that\ncurrently hinder multilinguality. Gap analysis led to formation of\n\u201cMultilingualWeb-LT\u201d \u2013 project and W3C Working Group with\ncross industry representation that will address priority issues via\nstandardization of interoperability metadata."}
{"Title": "Mobile Web Applications:\nBringing Mobile Apps and Web Together", "Abstract": "The popularity of mobile applications is very high and still\ngrowing rapidly. These applications allow their users to stay\nconnected with a large number of service providers in seamless\nfashion, both for leisure and productivity. But service providers\nsuffer from the high fragmentation of mobile development\nplatforms that force them to develop, maintain and deploy their\napplications in a large number of versions and formats. The\nMobile Web Applications (MobiWebApp [1]) EU project aims to\nbuild on Europe\u2019s strength in mobile technologies to enable\nEuropean research and industry to strengthen its position in Web\ntechnologies to be active and visible on the mobile applications\nmarket."}
{"Title": "The CUBRIK Project", "Abstract": "The Cubrik Project is an Integrated Project of the 7th Frame-\nwork Programme that aims at contributing to the multime-\ndia search domain by opening the architecture of multimedia\nsearch engines to the integration of open source and third\nparty content annotation and query processing components,\nand by exploiting the contribution of humans and commu-\nnities in all the phases of multimedia search, from content\nprocessing to query processing and relevance feedback pro-\ncessing. The CUBRIK presentation will showcase the archi-\ntectural concept and scientific background of the project and\ndemonstrate an initial scenario of human-enhanced content\nand query processing pipeline."}
{"Title": "The webinos Project", "Abstract": "This poster paper describes the webinos project and presents\nthe architecture and security features developed in webinos.\nIt highlights the main objectives and concepts of the project\nand describes the architecture derived to achive the objec-\ntives."}
{"Title": "DIADEM: Domain-centric, Intelligent, Automated Data\nExtraction Methodology\n\u2217", "Abstract": "Search engines are the sinews of the web. These sinews have be-\ncome strained, however: Where the web\u2019s function once was a mix\nof library and yellow pages, it has become the central marketplace\nfor information of almost any kind. We search more and more for\nobjects with specific characteristics, a car with a certain milage, an\naffordable apartment close to a good school, or the latest accessory\nfor our phones. Search engines all too often fail to provide reason-\nable answers, making us sift through dozens of websites with thou-\nsands of offers\u2014never to be sure a better offer isn\u2019t just around the\ncorner. What search engines are missing is understanding of the\nobjects and their attributes published on websites.\nAutomatically identifying and extracting these objects is akin to\nalchemy: transforming unstructured web information into highly\nstructured data with near perfect accuracy. With DIADEM we\npresent a formula for this transformation, but at a price: DIADEM\nidentifies and extracts data from a website with high accuracy. The\nprice is that for this task we need to provide DIADEM with ex-\ntensive knowledge about the ontology and phenomenology of the\ndomain, i.e., about entities (and relations) and about the represen-\ntation of these entities in the textual, structural, and visual language\nof a website of this domain. In this demonstration, we demonstrate\nwith a first prototype of DIADEM that, in contrast to alchemists,\nDIADEM has developed a viable formula."}
{"Title": "Social Media Meta-API: Leveraging the Content\nof Social Networks", "Abstract": "Social Network (SN) environments are the ideal future service\nmarketplaces. It is well known and documented that SN users are\nincreasing at a tremendous pace. Taking advantage of these social\ndynamics as well as the vast volumes, of amateur content\ngenerated every second, is a major step towards creating a\npotentially huge market of services. In this paper, we describe the\nexternal web services that SocIoS project is researching and\ndeveloping, and will support with the Social Media community.\nAiming to support the end users of SNs, to enhance their\ntransactions with more automated ways, and with the advantage\nfor better production and performance in their workflows over\nSNs inputs and content, this work presents the main architecture,\nfunctionality, and benefits per external service. Finally, introduces\nthe end user, into the new era of SNs with business applicability\nand better social transactions over SNs content."}
{"Title": "ARCOMEM - From Collect-All ARchives\nto COmmunity MEMories\n\u2217", "Abstract": "TheARCOMEMproject isabout memoryinstitutions likearchives,\nmuseums and libraries in the age of the Social Web. Social me-\ndia are becoming more and more pervasive in all areas of life.\nARCOMEM\u2019s aim is to help to transform archives into collective\nmemories that are more tightly integrated with their community of\nusers and to exploit Web 2.0 and the wisdom of crowds to make\nWeb archiving a more selective and meaning-based process. AR-\nCOMEM (FP7-IST-270239) is an Integrating Project in the FP7\nprogram of the European Commission, which involves twelve part-\nners from academia, industry and public sector. The project will\nrun from January 1, 2011 to December 31, 2013."}
{"Title": "Plan4All GeoPortal: Web of Spatial Data\nEvangelos Sakkopoulos 1 , Tomas Mildorf 2 , Karel Charvat 3 , Inga Berzina 4 , Kai-U", "Abstract": "Plan4All project contributes on the harmonization of spatial data\nand related metadata in order to make them available through\nWeb across a linked data platform. A prototype of a Web search\nEuropean  spatial  data  portal  is  already  available  at\nhttp://www.plan4all.eu. The key aim is to provide a methodology\nand present best practices towards the standardization of spatial\ndata according to the INSPIRE principles and provide results that\nwould be a reference material for linking data and data\nspecification from the spatial planning point of view. The results\ninclude methodology and implementation of multilingual search\nfor data and common portrayal rules for content providers. These\nare critical services for sharing and understanding spatial data\nacross Europe. Plan4All paradigm shows that a clear applicable\nmethodology for harmonization of spatial data on all different\ntopics of interest can be achieve efficiently. Plan4All shows that it\nis possible to build Pan European Web access, to link spatial data\nand to utilize multilingual metadata providing a roadmap for\nlinked spatial data across and hopefully beyond Europe. The\nproposed demonstration based on Plan4All experience aims to\nshow experience, best practices and methods to achieve data\nharmonization and provision of linked spatial data on the\nWeb."}
{"Title": "Multimedia Search over Integrated Social and Sensor\nNetworks", "Abstract": "This paper presents work in progress within the FP7 EU-\nfunded project SMART to develop a multimedia search en-\ngine over content and information stemming from the phys-\nical world, as derived through visual, acoustic and other\nsensors. Among the unique features of the search engine is\nits ability to respond to social queries, through integrating\nsocial networks with sensor networks. Motivated by this in-\nnovation, the paper presents and discusses the state-of-the-\nart in participatory sensing and other technologies blending\nsocial and sensor networks."}
{"Title": "Tracking Entities in Web Archives: The LAWA Project\n\u2217", "Abstract": "Web-preservation organization like the Internet Archive not\nonly capture the history of born-digital content but also\nreflect the zeitgeist of different time periods over more than\na decade. This longitudinal data is a potential gold mine for\nresearchers like sociologists, politologists, media and market\nanalysts, or experts on intellectual property. The LAWA\nproject (Longitudinal Analytics of Web Archive data) is\ndeveloping an Internet-based experimental testbed for large-\nscale data analytics on Web archive collections. Its emphasis\nis on scalable methods for this specific kind of big-data ana-\nlytics, and software tools for aggregating, querying, mining,\nand analyzing Web contents over long epochs. In this paper,\nwe highlight our research on entity-level analytics in Web\narchive data, which lifts Web analytics from plain text to the\nentity-level by detecting named entities, resolving ambiguous\nnames, extracting temporal facts and visualizing entities over\ntime periods. Our results provide key assets for tracking\nnamed entities in the evolving Web, news, and social media."}
{"Title": "I-SEARCH \u2013 A Multimodal Search Engine based on\nRich Unified Content Description (RUCoD)", "Abstract": "In this paper, we report on work around the I-SEARCH\nEU (FP7 ICT STREP) project whose objective is the de-\nvelopment of a multimodal search engine. We present the\nproject\u2019s objectives, and detail the achieved results, amongst\nwhich a Rich Unified Content Description format."}
{"Title": "Enabling Users to Create Their Own Web-Based Machine\nTranslation Engine", "Abstract": "This paper presents European Union co-funded projects to\nadvance the development and use of machine translation (MT)\nthat will benefit from the possibilities provided by the Web.\nCurrent mass-market and online MT systems are of a general\nnature and perform poorly for smaller languages and domain\nspecific texts. The ICT-PSP Programme project LetsMT!\ndevelops a user-driven machine translation \u201cfactory in the cloud\u201d\nenabling web users to get customized MT that better fits their\nneeds. Harnessing the huge potential of the web together with\nopen statistical machine translation (SMT) technologies LetsMT!\nhas created an innovative online collaborative platform for data\nsharing and building MT. Users can upload their parallel corpora\nto an online repository and generate user-tailored SMT systems\nbased on user selected data. FP7 Programme project ACCURAT\nresearches new methods for accumulating more data from the\nWeb to improve the quality of data-driven machine translation\nsystems. ACCURAT has created techniques and tools to use\ncomparable corpora such as news feeds and multinational web\npages. Although the majority of these texts are not direct\ntranslations, they share a lot of common paragraphs, sentences,\nphrases, terms and named entities in different languages which are\nuseful for machine translation."}
{"Title": "Semantic Evaluation At Large Scale (SEALS)", "Abstract": "This paper describes the main goals and outcomes of the\nEU-funded Framework 7 project entitled Semantic Evalu-\nation at Large Scale (SEALS). The growth and success of\nthe Semantic Web is built upon a wide range of Seman-\ntic technologies from ontology engineering tools through to\nsemantic web service discovery and semantic search. The\nevaluation of such technologies \u2013 and, indeed, assessments\nof their mutual compatibility \u2013 is critical for their sustained\nimprovement and adoption. The SEALS project is creat-\ning an open and sustainable platform on which all aspects\nof an evaluation can be hosted and executed and has been\ndesigned to accommodate most technology types. It is envis-\naged that the platform will become the de facto repository of\ntest datasets and will allow anyone to organise, execute and\nstore the results of technology evaluations free of charge and\nwithout corporate bias. The demonstration will show how\nindividual tools can be prepared for evaluation, uploaded to\nthe platform, evaluated according to some criteria and the\nsubsequent results viewed. In addition, the demonstration\nwill show the flexibility and power of the SEALS Platform\nfor evaluation organisers by highlighting some of the key\ntechnologies used."}
{"Title": "Twitcident: Fighting Fire with Information from\nSocial Web Streams", "Abstract": "In this paper, we present Twitcident, a framework and Web-\nbased system for filtering, searching and analyzing informa-\ntion about real-world incidents or crises. Twitcident con-\nnects to emergency broadcasting services and automatically\nstarts tracking and filtering information from Social Web\nstreams (Twitter) when a new incident occurs. It enriches\nthe semantics of streamed Twitter messages to profile in-\ncidents and to continuously improve and adapt the infor-\nmation filtering to the current temporal context. Faceted\nsearch and analytical tools allow users to retrieve particular\ninformation fragments and overview and analyze the current\nsituation as reported on the Social Web."}
{"Title": "SWiPE: Searching Wikipedia by Example", "Abstract": "A novel method is demonstrated that allows semantic and\nwell-structured knowledge bases (such as DBpedia) to be\neasily queried directly from Wikipedia\u2019s pages. Using Swipe,\nnaive users with no knowledge of RDF triples and sparql\ncan easily query DBpedia with powerful questions such as:\n\u201cWho are the U.S. presidents who took office when they were\n55-year old or younger, during the last 60 years\u201d, or\u201cFind the\ntown in California with less than 10 thousand people\u201d. This\nis accomplished by a novel Search by Example (SBE) ap-\nproach where a user can enter the query conditions directly\non the Infobox of a Wikipedia page. In fact, Swipe activates\nvarious fields of Wikipedia to allow users to enter query con-\nditions, and then uses these conditions to generate equiva-\nlent sparql queries and execute them on DBpedia. Finally,\nSwipe returns the query results in a form that is conducive\nto query refinements and further explorations. Swipe\u2019s SBE\napproach makes semi-structured documents queryable in an\nintuitive and user-friendly way and, through Wikipedia, de-\nlivers the benefits of querying and exploring large knowledge\nbases to all Web users."}
{"Title": "ProFoUnd: Program-analysis\u2013based Form Understanding", "Abstract": "An important feature of web search interfaces are the restric-\ntions enforced on input values \u2013 those reflecting either the\nsemantics of the data or requirements specific to the interface.\nBoth integrity constraints and \u201caccess restrictions\u201d can be\nof great use to web exploration tools. We demonstrate here\na novel technique for discovering constraints that requires\nno form submissions whatsoever. We work via statically\nanalyzing the JavaScript client-side code used to enforce the\nconstraints, when such code is available. We combine custom\nrecognizers for JavaScript functions relevant to constraint\nchecking with a generic program analysis layer. Integrated\nwith a web browser, our system shows the constraints de-\ntected on accessed web forms, and allows a user to see the\ncorresponding JavaScript code fragment."}
{"Title": "A Social Network for Video Annotation and Discovery\nBased on Semantic Profiling", "Abstract": "This paper presents a system for the social annotation and\ndiscovery of videos based on social networks and social knowl-\nedge. The system, developed as a web application, allows\nusers to comment and annotate, manually and automati-\ncally, video frames and scenes enriching their content with\ntags, references to Facebook users and pages and Wikipedia\nresources. These annotations are used to semantically model\nthe interests and the folksonomy of each user and resource\nin the network, and to suggest to users new resources, Face-\nbook friends and videos whose content is related to their\ninterests. A screencast showing an example of these func-\ntionalities is publicly available at:\nhttp://vimeo.com/miccunifi/facetube"}
{"Title": "GovWILD:\nIntegrating Open Government Data for Transparency", "Abstract": "Many government organizations publish a variety of data on\nthe web to enable transparency, foster applications, and to\nsatisfy legal obligations. Data content, format, structure,\nand quality vary widely, even in cases where the data is\npublished using the wide-spread linked data principles. Yet\nwithin this data and their integration lies much value: We\ndemonstrate GovWILD, a web-based prototype that inte-\ngrates and cleanses Open Government Data at a large scale.\nApart from the web-based interface that presents a use case\nof the created dataset at govwild.org, we provide all in-\ntegrated data as a download. This data can be used to\nanswer questions about politicians, companies, and govern-\nment funding."}
{"Title": "FreeQ: An Interactive Query Interface for Freebase\n\u2217", "Abstract": "Freebase is a large-scale open-world database where users\ncollaboratively create and structure content over an open\nplatform. Keyword queries over Freebase are notoriously\nambiguous due to the size and the complexity of the dataset.\nTo this end, novel techniques are required to enable naive\nusers to express their informational needs and retrieve the\ndesired data. FreeQ offers users an interactive interface for\nincremental query construction over a large-scale dataset,\nso that the users can find desired information quickly and\naccurately."}
{"Title": "Querying Socio-spatial Networks on the World-Wide Web", "Abstract": "Many devices, such as cellular smartphones and GPS-based\nnavigation systems, allow users to record their location his-\ntory. The location history data can be analyzed to gener-\nate life patterns\u2014patterns that associate people to places\nthey frequently visit. Accordingly, an SSN is a graph that\nconsists of (1) a social network, (2) a spatial network, and\n(3) life patterns that connect the users of the social network\nto locations, i.e., to geographical entities in the spatial net-\nwork. In this paper we present a system that stores SNN in\na graph-based database management system and provides\na novel query language, namely SSNQL, for querying the\nintegrated data. The system includes a Web-based graphi-\ncal user interface that allows presenting the social network,\npresenting the spatial network and posing SSNQL queries\nover the integrated data. The user interface also depicts the\nstructure of queries for the purpose of debugging and opti-\nmization. Our demonstration presents the management of\nthe integrated data as an SSN and it illustrates the query\nevaluation process in SSNQL."}
{"Title": "Scalable, Flexible and Generic Instant Overview Searc", "Abstract": "The last years there is an increasing interest on providing\nthe top search results while the user types a query letter by\nletter. In this paper we present and demonstrate a family\nof instant search applications which apart from showing in-\nstantly only the top search results, they can show various\nother kinds of precomputed aggregated information. This\nparadigm is more helpful for the end user (in comparison\nto the classic search-as-you-type), since it can combine au-\ntocompletion, search-as-you-type, results clustering, faceted\nsearch, entity mining, etc. Furthermore, apart from being\nhelpful for the end user, it is also beneficial for the server\u2019s\nside. However, the instant provision of such services for\nlarge number of queries, big amounts of precomputed infor-\nmation, and large number of concurrent users is challenging.\nWe demonstrate how this can be achieved using very modest\nhardware. Our approach relies on (a) a partitioned trie-based\nindex that exploits the available main memory and disk, and\n(b) dedicated caching techniques. We report performance re-\nsults over a server running on a modest personal computer\n(with 3 GB main memory) that provides instant services for\nmillions of distinct queries and terabytes of precomputed in-\nformation. Furthermore these services are tolerant to user\ntypos and different word orders."}
{"Title": "WISER: A Web-based Interactive Route Search System\nfor Smartphones", "Abstract": "Many smartphones, nowadays, use GPS to detect the loca-\ntion of the user, and can use the Internet to interact with re-\nmote location-based services. These two capabilities support\nonline navigation that incorporates search. In this demo\nwe presents WISER\u2014a system for Web-based Interactive\nSearch en Route. In the system, users perform route search\nby providing (1) a target location, and (2) search terms that\nspecify types of geographic entities to be visited. The task\nis to find a route that minimizes the travel distance from\nthe initial location of the user to the target, via entities of\nthe specified types. However, planning a route under condi-\ntions of uncertainty requires the system to take into account\nthe possibility that some visited entities will not satisfy the\nsearch requirements, so that the route may need to go via\nseveral entities of the same type. In an interactive search,\nthe user provides feedback regarding her satisfaction with\nentities she visits during the travel, and the system changes\nthe route, in real time, accordingly. The goal is to use the\ninteraction for computing a route that is more effective than\na route that is computed in a non-interactive fashion."}
{"Title": "Automatically Learning Gazetteers from the Deep Web\n\u2217", "Abstract": "Wrapper induction faces a dilemma: To reach web scale, it requires\nautomatically generated examples, but to produce accurate results,\nthese examples must have the quality of human annotations. We re-\nsolve this conflict with AMBER , a system for fully automated data\nextraction from result pages. In contrast to previous approaches,\nAMBER employs domain specific gazetteers to discern basic do-\nmain attributes on a page, and leverages repeated occurrences of\nsimilar attributes to group related attributes into records rather than\nrelying on the noisy structure of the DOM. With this approach AM -\nBER is able to identify records and their attributes with almost per-\nfect accuracy (>98%) on a large sample of websites. To make such\nan approach feasible at scale, AMBER automatically learns domain\ngazetteers from a small seed set. In this demonstration, we show\nhow AMBER uses the repeated structure of records on deep web\nresult pages to learn such gazetteers. This is only possible with a\nhighly accurate extraction system. Depending on its parametriza-\ntion, this learning process runs either fully automatically or with\nhuman interaction. We show how AMBER bootstraps a gazetteer for\nUK locations in 4 iterations: From a small seed sample we achieve\n94.4% accuracy in recognizing UK locations in the 4th iteration."}
{"Title": "FindiLike: Preference Driven Entity Search", "Abstract": "Traditional web search engines enable users to find docu-\nments based on topics. However, in finding entities such as\nrestaurants, hotels and products, traditional search engines\nfail to suffice as users are often interested in finding enti-\nties based on structured attributes such as price and brand\nand unstructured information such as opinions of other web\nusers. In this paper, we showcase a preference driven search\nsystem, that enables users to find entities of interest based\non a set of structured preferences as well as unstructured\nopinion preferences. We demonstrate our system in the con-\ntext of hotel search."}
{"Title": "Partisan Scale", "Abstract": "US Senate is the venue of political debates where the fed-\neral bills are formed and voted. Senators show their sup-\nport/opposition along the bills with their votes. This in-\nformation makes it possible to extract the polarity of the\nsenators. We use signed bipartite graphs for modeling de-\nbates, and we propose an algorithm for partitioning both\nthe senators, and the bills comprising the debate into bi-\nnary opposing camps. Simultaneously, our algorithm scales\nboth the senators and the bills on a univariate scale. Using\nthis scale, a researcher can identify moderate and partisan\nsenators within each camp, and polarizing vs. unifying bills.\nWe applied our algorithm on all the terms of the US Senate\nto the date for longitudinal analysis and developed a web\nbased interactive user interface www.PartisanScale.com\nto visualize the analysis."}
{"Title": "OPAL: A Passe-partout for Web Forms\n\u2217", "Abstract": "Web forms are the interfaces of the deep web. Though modern web\nbrowsers provide facilities to assist in form filling, this assistance\nis limited to prior form fillings or keyword matching.\nAutomatic form understanding enables a broad range of appli-\ncations, including crawlers, meta-search engines, and usability and\naccessibility support for enhanced web browsing. In this demon-\nstration, we use a novel form understanding approach, OPAL , to\nassist in form filling even for complex, previously unknown forms.\nOPAL associates form labels to fields by analyzing structural prop-\nertiesin theHTML encodingand visualfeaturesof thepage render-\ning. OPAL interprets this labeling and classifies the fields according\nto a given domain ontology. The combination of these two prop-\nerties, allows OPAL to deal effectively with many forms outside of\nthe grasp of existing form filling techniques. In the UK real estate\ndomain, OPAL achieves > 99% accuracy in form understanding."}
{"Title": "Round-trip semantics with Sztakipedia and DBpedia\nSpotlight", "Abstract": "We describe a tool kit to support a knowledge-enhancement\ncycle on the Web. In the first step, structured data which\nis extracted from Wikipedia is used to construct automatic\ncontent enhancement engines. Those engines can be used\nto interconnect knowledge in structured and unstructured\ninformation sources on the Web, including Wikipedia it-\nself. Sztakipedia-toolbar is a MediaWiki user script which\nbrings DBpedia Spotlight and other kinds of machine in-\ntelligence into the Wiki editor interface to provide enhance-\nment suggestions to the user. The suggestions offered by the\ntool focus on complementing knowledge and increasing the\navailability of structured data on Wikipedia. This will, in\nturn, increase the available information for the content en-\nhancement engines themselves, completing a virtuous cycle\nof knowledge enhancement.\nA 90 seconds long screencast instroduces the system on\nyoutube: http://www.youtube.com/watch?v=8VW0TrvXpl4.\nFor those who are interested in more details there is an other\n4 minutes long video: http://www.youtube.com/watch?v=\ncLqe-DOqKCM."}
{"Title": "ResEval Mash:\nA Mashup Tool for Advanced Research Evaluation", "Abstract": "In this demonstration, we present ResEval Mash, a mashup\nplatform for research evaluation, i.e., for the assessment of\nthe productivity or quality of researchers, teams, institu-\ntions, journals, and the like \u2013 a topic most of us are ac-\nquainted with. The platform is specifically tailored to the\nneed of sourcing data about scientific publications and re-\nsearchers from the Web, aggregating them, computing met-\nrics (also complex and ad-hoc ones), and visualizing them.\nResEval Mash is a hosted mashup platform with a client-\nside editor and runtime engine, both running inside a com-\nmon web browser. It supports the processing of also large\namounts of data, a feature that is achieved via the sensible\ndistribution of the respective computation steps over client\nand server. Our preliminary user study shows that ResE-\nval Mash indeed has the power to enable domain experts to\ndevelop own mashups (research evaluation metrics); other\nmashup platforms rather support skilled developers. The\nreason for this success is ResEval Mash\u2019s domain-specificity."}
{"Title": "Visual OXPath: Robust Wrapping by Example\n\u2217", "Abstract": "Good examples are hard to find, particularly in wrapper induction:\nPicking even one wrong example can spell disaster by yielding\novergeneralized or overspecialized wrappers. Such wrappers ex-\ntract data with low precision or recall, unless adjusted by human\nexperts at significant cost.\nVisual OXPath is an open-source, visual wrapper induction sys-\ntem that requires minimal examples and eases wrapper refinement:\nOftenitderivestheintendedwrapperfromasingleexamplethrough\nsophisticated heuristics that determine the best set of similar exam-\nples. To ease wrapper refinement, it offers a list of wrappers ranked\nby example similarity and robustness. Visual OXPath offers ex-\ntensive visual feedback for this refinement which can be performed\nwithoutanyknowledgeoftheunderlyingwrapperlanguage. Where\nfurther refinement by a human wrapper is needed, Visual OXPath\nprofits from being based on OXPath, a declarative wrapper lan-\nguage that extends XPath with a thin layer of features necessary for\nextraction and page navigation."}
{"Title": "Adding Wings to Red Bull Media", "Abstract": "The Linked Data movement with the aims of publishing\nand interconnecting machine readable data has originated\nin the last decade. Although the set of (open) data sources\nis rapidly growing, the integration of multimedia in this Web\nof Data is still at a very early stage. This paper describes,\nhow arbitrary video content and metadata can be processed\nto identify meaningful linking partners for video fragments\n- and thus create a web of linked media. The video test-set\nfor our demonstrator is part of the Red Bull Content Pool 1\nand confined to the Cliff Diving domain. The candidate\nset of possible link targets is a combination of a Red Bull\nthesaurus, information about divers from www.redbull.com\nand concepts from DBPedia 2 . The demo includes both a\nsemantic search on videos and video fragments and a player\nfor videos with semantic enhancements."}
{"Title": "Kjing (Mix the knowledge)", "Abstract": "Kjing is a web app that allow to rapidly set a multiscreen\nmulti-device environment and to interact and distribute con-\ntent in realtime. It can be used for museographic, educa-\ntional or conferencing purpose.\nWe propose a demo for the\u201cWorld Wide Web 2012 LYON\n- FRANCE\u201d held in Lyon the 16th to 20th April 2012."}
{"Title": "Interactive Hypervideo Visualization\nfor Browsing Behavior Analysis", "Abstract": "Processing web interaction data is known to be cumber-\nsome and time-consuming. State-of-the-art web tracking\nsystems usually allow replaying user interactions in the form\nof mouse tracks, a video-like visualization scheme, to engage\npractitioners in the analysis process. However, traditional\nonline video inspection has not explored the full capabilities\nof hypermedia and interactive techniques. In this paper,\nwe introduce a web-based tracking tool that generates in-\nteractive visualizations from users\u2019 activity. The system\nunobtrusively collects browser events derived from normal\nusage, offering a unified framework to inspect interaction\ndata in several ways. We compare our approach to related\nwork in the research community as well as in commercial\nsystems, and describe how ours fits in a real-world scenario.\nThis research shows that there is a wide range of applications\nwhere the proposed tool can assist the WWW community."}
{"Title": "Simplifying Friendlist Management", "Abstract": "Online social networks like Facebook allow users to connect,\ncommunicate, and share content. The popularity of these\nservices has lead to an information overload for their users;\nthe task of simply keeping track of different interactions has\nbecome daunting. To reduce this burden, sites like Facebook\nallows the user to group friends into specific lists, known as\nfriendlists, aggregating the interactions and content from\nall friends in each friendlist. While this approach greatly\nreduces the burden on the user, it still forces the user to\ncreate and populate the friendlists themselves and, worse,\nmakes the user responsible for maintaining the membership\nof their friendlists over time.\nWe show that friendlists often have a strong correspon-\ndence to the structure of the social network, implying that\nfriendlists may be automatically inferred by leveraging the\nsocial network structure. We present a demonstration of\nFriendlist Manager, a Facebook application that proposes\nfriendlists to the user based on the structure of their lo-\ncal social network, allows the user to tweak the proposed\nfriendlists, and then automatically creates the friendlists for\nthe user."}
{"Title": "The RaiNewsbook: Browsing Worldwide Multimodal News\nStories by Facts, Entities and Dates", "Abstract": "This paper presents a novel framework for multimodal news\ndata aggregation, retrieval and browsing. News aggregations\nare contextualised within automatically extracted informa-\ntion such as entities (i.e. persons, places and organisations),\ntemporal span, categorical topics, social networks popular-\nity and audience scores. Further resources coming from pro-\nfessional repositories, and related to the aggregation topics,\ncan be accessed as well. The system is accessible through a\nWeb interface supporting interactive navigation and explo-\nration of large-scale collections of news stories at the topic\nand context levels. Users can select news topics and sub-\ntopics interactively, building their personal paths towards\nworldwide events, main characters, dates and contents."}
{"Title": "BabelNetXplorer: A Platform for Multilingual Lexical\nKnowledge Base Access and Exploration", "Abstract": "Knowledge on word meanings and their relations across lan-\nguages is vital for enabling semantic information technolo-\ngies: in fact, the ever increasingly multilingual nature of\nthe Web now calls for the development of methods that are\nboth robust and widely applicable for processing textual in-\nformation in a multitude of languages. In our research, we\napproach this ambitious task by means of BabelNet, a wide-\ncoverage multilingual lexical knowledge base. In this paper\nwe present an Application Programming Interface and a\nGraphical User Interface which, respectively, allow program-\nmatic access and visual exploration of BabelNet. Our contri-\nbution is to provide the research community with easy-to-use\ntools for performing multilingual lexical semantic analysis,\nthereby fostering further research in this direction."}
{"Title": "H 2 RDF: Adaptive Query Processing on RDF Data in the\nCloud.", "Abstract": "In this work we present H 2 RDF, a fully distributed RDF\nstore that combines the MapReduce processing framework\nwith a NoSQL distributed data store. Our system features\ntwo unique characteristics that enable efficient processing\nof both simple and multi-join SPARQL queries on virtu-\nally unlimited number of triples: Join algorithms that exe-\ncute joins according to query selectivity to reduce process-\ning; and adaptive choice among centralized and distributed\n(MapReduce-based) join execution for fast query responses.\nOur system efficiently answers both simple joins and com-\nplex multivariate queries and easily scales to 3 billion triples\nusing a small cluster of 9 worker nodes. H 2 RDF outper-\nforms state-of-the-art distributed solutions in multi-join and\nnonselective queries while achieving comparable performance\nto centralized solutions in selective queries. In this demon-\nstration we showcase the system\u2019s functionality through an\ninteractive GUI. Users will be able to execute predefined or\ncustom-made SPARQL queries on datasets of different sizes,\nusing different join algorithms. Moreover, they can repeat\nall queries utilizing a different number of cluster resources.\nUsing real-time cluster monitoring and detailed statistics,\nparticipants will be able to understand the advantages of\ndifferent execution schemes versus the input data as well as\nthe scalability properties of H 2 RDF over both the data size\nand the available worker resources."}
{"Title": "Paraimpu: a Platform for a Social Web of Things", "Abstract": "The Web of Things is a scenario where potentially billions of\nconnected smart objects communicate using the Web proto-\ncols, HTTP in primis. A Web of Things envisioning and de-\nsign has raised several research issues, from protocols adop-\ntion and communication models to architectural styles and\nsocial aspects facing. In this demo we present the proto-\ntype of a scalable architecture for a large scale social Web of\nThings for smart objects and services, named Paraimpu. It\nis a Web-based platform which allows to add, use, share and\ninter-connect real HTTP-enabled smart objects and \u201dvir-\ntual\u201d things like services on the Web and social networks.\nParaimpu defines and uses few strong abstractions, in order\nto allow mash-ups of heterogeneous things introducing pow-\nerful rules for data adaptation. Adding and inter-connecting\nobjects is supported through user friendly models and fea-\ntures."}
{"Title": "Automated Semantic Tagging of Speech Audio", "Abstract": "The BBC is currently tagging programmes manually, using\nDBpedia as a source of tag identifiers, and a list of sug-\ngested tags extracted from the programme synopsis. These\ntags are then used to help navigation and topic-based search\nof programmes on the BBC website. However, given the\nvery large number of programmes available in the archive,\nmost of them having very little metadata attached to them,\nwe need a way to automatically assign tags to programmes.\nWe describe a framework to do so, using speech recogni-\ntion, text processing and concept tagging techniques. We\ndescribe how this framework was successfully applied to a\nvery large BBC radio archive. We demonstrate an applica-\ntion using automatically extracted tags to aid discovery of\narchive content."}
{"Title": "Baya: Assisted Mashup Development as a Service", "Abstract": "In this demonstration, we describe Baya, an extension of\nYahoo! Pipes that guides and speeds up development by in-\nteractively recommending composition knowledge harvested\nfrom a repository of existing pipes. Composition knowl-\nedge is delivered in the form of reusable mashup patterns,\nwhich are retrieved and ranked on the fly while the devel-\noper models his own pipe (the mashup) and that are auto-\nmatically weaved into his pipe model upon selection. Baya\nmines candidate patterns from pipe models available online\nand thereby leverages on the knowledge of the crowd, i.e.,\nof other developers. Baya is an extension for the Firefox\nbrowser that seamlessly integrates with Pipes. It enhances\nPipes with a powerful new feature for both expert developers\nand beginners, speeding up the former and enabling the lat-\nter. The discovery of composition knowledge is provided as\na service and can easily be extended toward other modeling\nenvironments."}
{"Title": "S2S Architecture and Faceted Browsing Applications", "Abstract": "This demo paper will discuss a search interface framework\ndesigned as part of the Semantic eScience Framework project\nat the Tetherless World Constellation. The search interface\nframework, S2S, was designed to facilitate the construction\nof interactive user interfaces for data catalogs. We use Se-\nmantic Web technologies, including an OWL ontology for\ndescribing the semantics of data services, as well as the se-\nmantics of user interface components. We have applied S2S\nin three different scenarios: (1) the development of a faceted\nbrowse interface integrated with an interactive mapping and\nvisualization tool for biological and chemical oceanographic\ndata, (2) the development of a faceted browser for more\nthan 700,000 open government datasets in over 100 catalogs\nworldwide, and (3) the development of a user interface for\na virtual observatory in the field of solar-terrestrial physics.\nThroughout this paper, we discuss the architecture of the\nS2S framework, focusing on its extensibility and reusability,\nand also review the application scenarios."}
{"Title": "Turning a Web 2.0 Social Network into a Web 3.0,\ndistributed, and secured Social Web Application", "Abstract": "This demonstration presents the process of transforming\na Web 2.0 centralized social network into a Web 3.0, dis-\ntributed, and secured Social application, and what was learnt\nin this process. The initial Web 2.0 Social Network appli-\ncation was written by a group of students over a period of\n4 months in the spring of 2011. It had all the bells and\nwhistles of the well known Social Networks: walls to post\non, circles of friends, etc. The students were very enthusi-\nastic in building their social network, but the chances of it\ngrowing into a large community were close to non-existent\nunless a way could be found to tie it into a bigger social\nnetwork. This is where linked data protected by the Web\nAccess Control Ontology and WebID authentication could\ncome to the rescue. The paper describes this transformation\nprocess, and we will demonstrate the full software version at\nthe conference."}
{"Title": "Adding Fake Facts to Ontologies", "Abstract": "In this paper, we study how artificial facts can be added\nto an RDFS ontology. Artificial facts are an easy way of\nproving the ownership of an ontology: If another ontology\ncontains the artificial fact, it has probably been taken from\nthe original ontology. We show how the ownership of an\nontology can be established with provably tight probability\nbounds, even if only parts of the ontology are being re-used.\nWe explain how artificial facts can be generated in an incon-\nspicuous and minimally disruptive way. Our demo allows\nusers to generate artificial facts and to guess which facts\nwere generated."}
{"Title": "CASIS: A System for Concept-Aware Social Image Search\n\u2217", "Abstract": "Tag-based social image search enables users to formulate queries\nusing keywords. However, as queries are usually very short and\nusers have very different interpretations of a particular tag in an-\nnotating and searching images, the returned images to a tag query\nusually contain a collection of images related to multiple concepts.\nWe demonstrate Casis, a system for concept-aware social image\nsearch. Casis detects tag concepts based on the collective knowl-\nedge embedded in social tagging from the initial results to a query.\nA tag concept is a set of tags highly associated with each other and\ncollectively conveys a semantic meaning. Images to a query are\nthen organized by tag concepts. Casis provides intuitive and in-\nteractive browsing of search results through a tag concept graph,\nwhich visualizes the tags defining each tag concept and their rela-\ntionships within and across concepts. Supporting multiple retrieval\nmethods and multiple concept detection algorithms, Casis offers\nsuperiorsocial image searchexperiencesby choosing themost suit-\nable retrieval methods and concept-aware image organizations."}
{"Title": "In the Mood for Affective Search with Web Stereotypes", "Abstract": "Models of sentiment analysis in text require an understanding of\nwhat kinds of sentiment-bearing language are generally used to\ndescribe specific topics. Thus, fine-grained sentiment analysis\nrequires both a topic lexicon and a sentiment lexicon, and an\naffective mapping between both. For instance, when one speaks\ndisparagingly about a city (like London, say), what aspects of city\ndoes one generally focus on, and what words are used to disparage\nthose aspects? As when we talk about the weather, our language\nobeys certain familiar patterns \u2013 what we might call clich\u00e9s and\nstereotypes \u2013 when we talk about familiar topics. In this paper we\ndescribe the construction of an affective stereotype lexicon, that is,\na lexicon of stereotypes and their most salient affective qualities.\nWe show, via a demonstration system called MOODfinger, how\nthis lexicon can be used to underpin the processes of affective\nquery expansion and summarization in a system for retrieving and\norganizing news content from the Web. Though we adopt a\nsimple bipolar +/- view of sentiment, we show how this stereotype\nlexicon allows users to coin their own nuanced moods on demand."}
{"Title": "Personalized Newscasts and Social Networks: A Prototype\nbuilt over a Flexible Integration Model", "Abstract": "The way we watch television is changing with the introduction of\nattractive Web activities that move users away from TV to other\nmedia. The integration of the cultures of TV and Web is still an\nopen issue. How can we make TV more open? How can we\nenable a possible collaboration of these two different worlds?\nTV-Web convergence is much more than placing a Web browser\ninto a TV set or putting TV content into a Web media player.\nThe NoTube project, funded by the European Community, is\ndemonstrating how an open and general set of tools adaptable to a\nnumber of possible scenarios and allowing a designer to\nimplement the targeted final service with ease can be introduced.\nA prototype based on the NoTube model in which the Smartphone\nis used as secondary screen is presented. The video demonstration\n[11] is available at http://youtu.be/dMM7MH9CZY8."}
{"Title": "An Early Warning System for Unrecognized Drug Side\nEffects Discovery", "Abstract": "Drugs can treat human diseases through chemical interac-\ntions between the ingredients and intended targets in the\nhuman body. However, the ingredients could unexpectedly\ninteract with off-targets, which may cause adverse drug side\neffects. Notifying patients and physicians of potential drug\neffects is an important step in improving healthcare quality\nand delivery. With the increasing popularity of Web 2.0 ap-\nplications, more and more patients start discussing drug side\neffects in many online sources. In this paper, we describe our\nefforts on building UDWarning, a novel early warning sys-\ntem for unrecognized drug side effects discovery based on\nthe text information gathered from the Internet. The sys-\ntem can automatically build a knowledge base for drug side\neffects by integrating the information related to drug side\neffects from different sources. It can also monitor the online\ninformation about drugs and discover possible unrecognized\ndrug side effects. Our demonstration will show that the sys-\ntem has the potentials to expedite the discovery process of\nunrecognized drug side effects and to improve the quality of\nhealthcare."}
{"Title": "Titan: a System for Effective Web Service Discovery", "Abstract": "With the increase of web services and user demand\u2019s diver-\nsity, effective web service discovery is becoming a big chal-\nlenge. Clustering web services would greatly boost the abil-\nity of web service search engine to retrieve relevant ones. In\nthis paper, we propose a web service search engine Titan 1\nwhich contains 15,969 web services crawled from the Inter-\nnet. In Titan, two main technologies, i.e., web service clus-\ntering and tag recommendation, are employed to improve\nthe effectiveness of web service discovery. Specifically, both\nWSDL (Web Service Description Language) documents and\ntags of web services are utilized for clustering, while tag rec-\nommendation is adopted to handle some inherent problems\nof tagging data, e.g., uneven tag distribution and noise tags."}
{"Title": "Deep Answers for Naturally Asked Questions\non the Web of Data", "Abstract": "We present DEANNA, a framework for natural language\nquestion answering over structured knowledge bases. Given\na natural language question, DEANNA translates questions\ninto a structured SPARQL query that can be evaluated over\nknowledge bases such as Yago, Dbpedia, Freebase, or other\nLinked Data sources. DEANNA analyzes questions and\nmaps verbal phrases to relations and noun phrases to ei-\nther individual entities or semantic classes. Importantly, it\njudiciously generates variables for target entities or classes\nto express joins between multiple triple patterns. We lever-\nage the semantic type system for entities and use constraints\nin jointly mapping the constituents of the question to rela-\ntions, classes, and entities. We demonstrate the capabilities\nand interface of DEANNA, which allows advanced users to\ninfluence the translation process and to see how the different\ncomponents interact to produce the final result."}
{"Title": "Associating Structured Records To Text Documents", "Abstract": "Postulate two independently created data sources. The first\ncontains text documents, each discussing one or a small\nnumber of objects. The second is a collection of structured\nrecords, each containing information about the characteris-\ntics of some objects. We present techniques for associating\nstructured records to corresponding text documents and em-\npirical results supporting the proposed techniques."}
{"Title": "Textual and Contextual Patterns\nfor Sentiment Analysis over Microblogs", "Abstract": "Microblog content poses serious challenges to the applicabil-\nity of sentiment analysis, due to its inherent characteristics.\nWe introduce a novel method relying on content-based and\ncontext-based features, guaranteeing high effectiveness and\nrobustness in the settings we are considering. The evalua-\ntion of our methods over a large Twitter data set indicates\nsignificant improvements over the traditional techniques."}
{"Title": "PAC\u2019nPost: A Framework for a Micro-Blogging Social\nNetwork in an Unstructured P2P Network", "Abstract": "We describe a framework for a micro-blogging social net-\nwork implemented in an unstructured peer-to-peer network.\nA micro-blogging social network must provide capabilities\nfor users to (i) publish, (ii) follow and (iii) search. Our\nretrieval mechanism is based on a probably approximately\ncorrect (PAC) search architecture in which a query is sent\nto a fixed number of nodes in the network. In PAC, the\nprobability of attaining a particular accuracy is a function\nof the number of nodes queried (fixed) and the replication\nrate of documents (micro-blog). Publishing a micro-blog\nthen becomes a matter of replicating the micro-blog to the\nrequired number of random nodes without any central co-\nordination. To solve this, we use techniques from the field\nof rumour spreading (gossip protocols) to propagate new\ndocuments. Our document spreading algorithm is designed\nsuch that a document has a very high probability of being\ncopied to only the required number of nodes. Results from\nsimulations performed on networks of 10,000, 100,000 and\n500,000 nodes verify our mathematical models. The frame-\nwork is also applicable for indexing dynamic web pages in\na distributed search engine or for a system which indexes\nnewly created BitTorrents in a de-centralized environment."}
{"Title": "The Impact of Visual Appearance on User Response in\nOnline Display Advertising", "Abstract": "Display advertising has been a significant source of revenue for\npublishers and ad networks in the online advertising ecosystem.\nOne of the main goals in display advertising is to maximize user\nresponse rate for advertising campaigns, such as click through rates\n(CTR) or conversion rates. Although the visual appearance of ads\n(creatives) matters for propensity of user response, there is no pub-\nlished work so far to address this topic via a systematic data-driven\napproach. In this paper we quantitatively study the relationship\nbetween the visual appearance and performance of creatives using\nlarge scale data in the world\u2019s largest display ads exchange sys-\ntem, RightMedia. We designed a set of 43 visual features, some\nof which are novel and some are inspired by related work. We\nextracted these features from real creatives served on RightMedia.\nThen, we present recommendations of visual features that have the\nmost important impact on CTR to the professional designers in or-\nder to optimize their creative design. We believe that the findings\npresented in this paper will be very useful for the online advertis-\ning industry in designing high-performance creatives. We have also\ndesigned and conducted an experiment to evaluate th"}
{"Title": "Impact of Ad Impressions on Dynamic Commercial\nActions: Value Attribution in Marketing Campaigns", "Abstract": "We develop a descriptive method to estimate the impact\nof ad impressions on commercial actions dynamically with-\nout tracking cookies. We analyze 2,885 campaigns for 1,251\nproducts from the Advertising.com ad network. We compare\nour method with A/B testing for 2 campaigns, and with a\npublic synthetic dataset."}
{"Title": "Audience Dynamics of Online Catch Up TV", "Abstract": "This paper studies the demand for TV contents on online catch up\nplatforms, in order to assess how catch up TV offers transform\nTV consumption. We build upon empirical data on French TV\nconsumption in June 2011: a daily monitoring of online audience\non web catch up platforms, and live audience ratings of traditional\nbroadcast TV. We provide three main results: 1) online\nconsumption is more concentrated than off-line audience,\ncontradicting the hypothesis of a long tail effect of catch up TV;\n2) the temporality of replay TV consumption on the web is very\nclose to the live broadcasting of the programs, thus softening\nrather than breaking the synchrony of traditional TV; 3) detailed\ndata on online consumption of news reveals two patterns of\nconsumption (\u201calternative TV ritual\u201d vs. \u201c\u00e0 la carte\u201d)."}
{"Title": "Group Recommendations Via Multi-armed Bandits ", "Abstract": "We study recommendations for persistent groups that repeatedly\nengage in a joint activity. We approach this as a multi-arm ban-\ndit problem. We design a recommendation policy and show it has\nlogarithmic regret. Our analysis also shows that regret depends lin-\nearly on d, the size of the underlying persistent group. We evalu-\nate our policy on movie recommendations over the MovieLens and\nMoviePilot datasets."}
{"Title": "A Revenue Sharing Mechanism for Federated Search and\nAdvertising", "Abstract": "Federated search engines combine search results from two or\nmore (general\u2013purpose or domain\u2013specific) content providers.\nThey enable complex searches (e.g., complete vacation plan-\nning) or more reliable results by allowing users to receive\nhigh quality results from a variety of sources. We propose a\nnew revenue sharing mechanism for federated search engines,\nconsidering different actors involved in the search results\ngeneration (i.e., content providers, advertising providers, hy-\nbrid content+advertising providers, and content integrators).\nWe extend the existing sponsored search auctions by sup-\nporting heterogeneous participants and redistribution of mon-\netary values to the different actors, while maintaining flexi-\nbility in the payment scheme."}
{"Title": "Efficient Multi-View Maintenance\nin the Social Semantic Web", "Abstract": "The Social Semantic Web (SSW) refers to the mix of RDF\ndata in web content, and social network data associated with\nthose who posted that content. Applications to monitor the\nSSW are becoming increasingly popular. For instance, mar-\nketers want to look for semantic patterns relating to the\ncontent of tweets and Facebook posts relating to their prod-\nucts. Such applications allow multiple users to specify pat-\nterns of interest, and monitor them in real-time as new data\ngets added to the web or to a social network. In this paper,\nwe develop the concept of SSW view servers in which all of\nthese types of applications can be simultaneously monitored\nfrom such servers. The patterns of interest are views. We\nshow that a given set of views can be compiled in multiple\npossible ways to take advantage of common substructures,\nand define the concept of an optimal merge. We develop a\nvery fast MultiView algorithm that scalably and efficiently\nmaintains multiple subgraph views. We show that our al-\ngorithm is correct, study its complexity, and experimentally\ndemonstrate that our algorithm can scalably handle updates\nto hundreds of views on real-world SSW databases with up\nto 540M edges."}
{"Title": "BlueFinder: Estimate Where a Beach Photo Was Taken", "Abstract": "This paper describes a system to estimate geographical location-\ns for beach photos. We develop an iterative method that not only\ntrains visual classifiers but also discovers geographical clusters for\nbeach regions. The results show that it is possible to recognize dif-\nferent beaches using visual information with reasonable accuracy,\nand our system works 27 times better than random guess for the\ngeographical localization task."}
{"Title": "News Comments Generation via Mining MicroBlogs", "Abstract": "Microblogging websites such as Twitter and Chinese Sina\nWeibo contain large amounts of microblogs posted by users.\nMany of these microblogs are highly sensitive to the impor-\ntant real-world events and correlated to the news events.\nThus, microblogs from these websites can be collect as com-\nments for the news to reveal the opinions and attitude to-\nwards the news event among large number of users. In this\npaper, we present a framework to automatically collect rel-\nevant microblogs from microblogging websites to generate\ncomments for popular news on news websites."}
{"Title": "MobiMash: End User Development for Mobile Mashups", "Abstract": "The adoption of adequate tools, oriented towards the End\nUser Development (EUD), can promote mobile mashups as\n\u201cdemocratic\u201d tools, able to accommodate the long tail of\nusers\u2019 specific needs. We introduce MobiMash, a novel ap-\nproach and a platform for the construction of mobile mashups,\ncharacterized by a lightweight composition paradigm, mainly\nguided by the notion of visual templates. The composition\nparadigm generates an application schema that is based on\na domain specific language addressing dimensions for data\nintegration and service orchestration, and that guides at run-\ntime the dynamic instantiation of the final mobile app."}
{"Title": "Privacy Management for Online Social Networks", "Abstract": "We introduce a privacy management approach that lever-\nages users\u2019 memory and opinion of their friends to set poli-\ncies for other similar friends. We refer to this new approach\nas Same-As Privacy Management. To demonstrate the effec-\ntiveness of our privacy management improvements, we im-\nplemented a prototype Facebook application and conducted\nan extensive user study. We demonstrated considerable re-\nductions in policy authoring time using Same-As Privacy\nManagement over traditional group based privacy manage-\nment approaches. Finally, we presented user perceptions,\nwhich were very encouraging."}
{"Title": "Fast and Cost-efficient Bid Estimation for Contextual Ads", "Abstract": "We study the problem of estimating the value of a contextual ad im-\npression, andbaseduponwhichanadnetworkbidsonanexchange.\nThe ad impression opportunity would materialize into revenue only\nif the ad network wins the impression and a user clicks on the ads,\nboth as a rare event especially in an open exchange for contextual\nads. Given a low revenue expectation and the elusive nature of\npredicting weak-signal click-through rates, the computational cost\nincurred by bid estimation shall be cautiously justified. We devel-\noped and deployed a novel impression valuation model, which is\nexpected to reduce the computational cost by 95% and hence more\nthan double the profit. Our approach is highly economized through\na fast implementation of kNN regression that primarily leverages\nlow-dimensional sell-side data (user and publisher). We also ad-\ndress the cold-start problem or the exploration vs. exploitation re-\nquirement by Bayesian smoothing using a beta prior, and adapt to\nthe temporal dynamics using an autoregressive model."}
{"Title": "Fast Query Evaluation for Ad Retrieval", "Abstract": "We describe a fast query evaluation method for ad document\nretrieval in online advertising, based upon the classic WAND\nalgorithm. The key idea is to localize per-topic term upper\nbounds into homogeneous ad groups. Our approach is not\nonly theoretically motivated by a topical mixture model; but\nempirically justified by the characteristics of the ad domain,\nthat is, short and semantically focused documents with nat-\nural hierarchy. We report experimental results using artifi-\ncial and real-world query-ad retrieval data, and show that\nthe tighter-bound WAND outperforms the traditional ap-\nproach by 35.4% reduction in number of full evaluations."}
{"Title": "Consento : A Consensus Search Engine for Answering\nSubjective Queries", "Abstract": "Search engines have become an important decision making\ntool today. Decision making queries are often subjective,\nsuch as \u201cbest sedan for family use,\u201d\u201cbest action movies in\n2010,\u201d to name a few. Unfortunately, such queries cannot\nbe answered properly by conventional search systems. In\norder to address this problem, we introduce Consento,\na consensus search engine designed to answer subjective\nqueries. Consento performs subdocument-level indexing\nto more precisely capture semantics from user opinions. We\nalso introduce a new ranking method, or ConsensusRank\nthat counts in online comments referring to an entity as a\nweighted vote to the entity. We validated the framework\nwith an empirical study using the data on movie reviews."}
{"Title": "Good Abandonments in Factoid Queries", "Abstract": "It is often considered that high abandonment rate corre-\nsponds to poor IR system performance. However several\nstudies suggested that there are so called good abandon-\nments, i.e. situations when search engine result page (SERP)\ncontains enough details to satisfy the user information need\nwithout necessity to click on search results. In those papers\nonly editorial metrics of SERP were used, and one cannot\nbe sure that situations marked as good abandonments by as-\nsessors actually imply user satisfaction. In present work we\npropose some real-world evidences for good abandonments\nby calculating correlation between editorial and click met-\nrics."}
{"Title": "Potential Good Abandonment Prediction", "Abstract": "Abandonment rate is one of the most broadly used online\nuser satisfaction metrics. In this paper we discuss the notion\nof potential good abandonment, i.e. queries that may poten-\ntially result in user satisfaction without the need to click on\nsearch results (if search engine result page contains enough\ndetails to satisfy the user information need). We show, that\nwe can train a classifier which is able to distinguish between\npotential good and bad abandonments with rather good re-\nsults compared to our baseline. As a case study we show\nhow to apply these ideas to IR evaluation and introduce a\nnew metric for A/B-testing \u2014 Bad Abandonment Rate."}
{"Title": "Ubiquitous Access Control for SPARQL Endpoints:\nLessons Learned and Future Challenges", "Abstract": "We present and evaluate a context-aware access control frame-\nwork for SPARQL endpoints queried from mobile."}
{"Title": "Mining for Insights in the Search Engine Query Stream", "Abstract": "Search engines record a large amount of metadata each time a\nuser issues a query. While efficiently mining this data can be\nchallenging, the results can be useful in multiple ways, including\nmonitoring search engine performance, improving search rele-\nvance, prioritizing research, and optimizing day-to-day opera-\ntions. In this poster, we describe an approach for mining query log\ndata for actionable insights \u2013 specific query segments (sets of\nqueries) that require attention, and actions that need to be taken to\nimprove the segments. Starting with a set of important metrics, we\nidentify query segments that are \u201cinteresting\u201d with respect to\nthese metrics using a distributed frequent itemset mining algo-\nrithm."}
{"Title": "Developing Domain-Specific Mashup Tools for End Users", "Abstract": "The recent emergence of mashup tools has refueled research\non end user development, i.e., on enabling end users without\nprogramming skills to compose own applications. Yet, simi-\nlar to what happened with analogous promises in web service\ncomposition and business process management, research has\nmostly focused on technology and, as a consequence, has\nfailed its objective. Plain technology (e.g., SOAP/WSDL\nweb services) or simple modeling languages (e.g., Yahoo!\nPipes) don\u2019t convey enough meaning to non-programmers.\nWe propose a domain-specific approach to mashups that\n\u201cspeaks the language of the user\u201d, i.e., that is aware of the\nterminology, concepts, rules, and conventions (the domain)\nthe user is comfortable with. We show what developing a\ndomain-specific mashup tool means, which role the mashup\nmeta-model and the domain model play and how these can\nbe merged into a domain-specific mashup meta-model. We\napply the approach implementing a mashup tool for the re-\nsearch evaluation domain. Our user study confirms that\ndomain-specific mashup tools indeed lower the entry barrier\nto mashup development."}
{"Title": "Discovery and Reuse of Composition Knowledge\nfor Assisted Mashup Development", "Abstract": "Despite the emergence of mashup tools like Yahoo! Pipes or\nJackBe Presto Wires, developing mashups is still non-trivial\nand requires intimate knowledge about the functionality of\nweb APIs and services, their interfaces, parameter settings,\ndata mappings, and so on. We aim to assist the mashup pro-\ncess and to turn it into an interactive co-creation process,\nin which one part of the solution comes from the developer\nand the other part from reusable composition knowledge that\nhas proven successful in the past. We harvest composition\nknowledge from a repository of existing mashup models by\nmining a set of reusable composition patterns, which we then\nuse to interactively provide composition recommendations to\ndevelopers while they model their own mashup. Upon ac-\nceptance of a recommendation, the purposeful design of the\nrespective pattern types allows us to automatically weave\nthe chosen pattern into a partial mashup model, in practice\nperforming a set of modeling actions on behalf of the de-\nveloper. The experimental evaluation of our prototype im-\nplementation demonstrates that it is indeed possible to har-\nvest meaningful, reusable knowledge from existing mashups,\nand that even complex recommendations can be efficiently\nqueried and weaved also inside the client browser."}
{"Title": "Towards Personalized Learning to Rank for\nEpidemic Intelligence Based on Social Media Streams", "Abstract": "In the presence of sudden outbreaks, how can social media\nstreams be used to strengthen surveillance capabilities? In\nMay 2011, Germany reported one of the largest described\noutbreaks of Enterohemorrhagic Escherichia coli (EHEC).\nBy end of June, 47 persons had died. After the detec-\ntion of the outbreak, authorities investigating the cause and\nthe impact in the population were interested in the anal-\nysis of micro-blog data related to the event. Since Thou-\nsands of tweets related to this outbreak were produced every\nday, this task was overwhelming for experts participating\nin the investigation. In this work, we propose a Person-\nalized Tweet Ranking algorithm for Epidemic Intelligence\n(PTR4EI), that provides users a personalized, short list of\ntweets based on the user\u2019s context. PTR4EI is based on a\nlearning to rank framework and exploits as features, comple-\nmentary context information extracted from the social hash-\ntagging behavior in Twitter. Our experimental evaluation\non a dataset, collected in real-time during the EHEC out-\nbreak, shows the superior ranking performance of PTR4EI.\nWe believe our work can serve as a building block for an\nopen early warning system based on Twitter, helping to re-\nalize the vision of Epidemic Intelligence for the Crowd, by\nthe Crowd."}
{"Title": "D2RQ/Update: Updating Relational Data via Virtual RDF", "Abstract": "D2RQ is a popular RDB-to-RDF mapping platform that\nsupports mapping relational databases to RDF and posing\nSPARQL queries to these relational databases. However,\nD2RQ merely provides a read-only RDF view on relational\ndatabases. Thus, we introduce D2RQ/Update\u2014an exten-\nsion of D2RQ to enable executing SPARQL/Update state-\nments on the mapped data, and to facilitate the creation of\na read-write Semantic Web."}
{"Title": "HeterRank: Addressing Information Heterogeneity for\nPersonalized Recommendation in Social Tagging Systems", "Abstract": "A social tagging system provides users an effective way to\ncollaboratively annotate and organize items with their own\ntags. A social tagging system contains heterogenous infor-\nmation like users\u2019 tagging behaviors, social networks, tag se-\nmantics and item profiles. All the heterogenous information\nhelps alleviate the cold start problem due to data sparsity.\nIn this paper, we model a social tagging system as a multi-\ntype graph and propose a graph-based ranking algorithm\ncalled HeterRank for tag recommendation. Experimental\nresults on three publicly available datasets, i.e., CiteULike,\nLast.fm and Delicious prove the effectiveness of HeterRank\nfor tag recommendation with heterogenous information."}
{"Title": "Domain Adaptive Answer Extraction for Discussion Boards", "Abstract": "Answer extraction from discussion boards is an extensively\nstudied problem. Most of the existing work is focused on\nsupervised methods for extracting answers using similarity\nfeatures and forum-specific features. Although this works\nwell for the domain or forum data that it has been trained\non, it is difficult to use the same models for a domain where\nthe vocabulary is different and some forum specific features\nmay not be available. In this poster, we report initial results\nof a domain adaptive answer extractor that performs the ex-\ntraction in two steps: a) an answer recognizer identifies the\nsentences in a post which are likely to be answers, and b)\na domain relevance module determines the domain signifi-\ncance of the identified answer. We use domain independent\nmethodology that can be easily adapted to any given domain\nwith minimum effort."}
{"Title": "Towards Multiple Identity Detection in Social Networks", "Abstract": "In this paper we discuss a piece of work which intends to\nprovide some insights regarding the resolution of the hard\nproblem of multiple identities detection. Based on hypoth-\nesis that each person is unique and identifiable whether in\nits writing style or social behavior, we propose a Framework\nrelying on machine learning models and a deep analysis of\nsocial interactions, towards such detection."}
{"Title": "How Shall We Catch People\u2019s Concerns in Micro-blogging?", "Abstract": "In micro-blogging, people talk about their daily life and change\nminds freely, thus by mining people\u2019s interest in micro-blogging,\nwe will easily perceive the pulse of society. In this paper, we\ncatch what people are caring about in their daily life by\ndiscovering meaningful communities based on probabilistic factor\nmodel (PFM). The proposed solution identifies people\u2019s interest\nfrom their friendship and content information. Therefore, it\nreveals the behaviors of people in micro-blogging naturally.\nExperimental results verify the effectiveness of the proposed\nmodel and show people\u2019s social life vividly."}
{"Title": "Link Prediction via Latent Factor BlockModel", "Abstract": "In this paper we address the problem of link prediction in\nnetworked data, which appears in many applications such\nas social network analysis or recommender systems. Previ-\nous studies either consider latent feature based models but\ndisregarding local structure in the network, or focus exclu-\nsively on capturing local structure of objects based on latent\nblockmodels without coupling with latent characteristics of\nobjects. To combine the benefits of previous work, we pro-\npose a novel model that can incorporate the effects of latent\nfeatures of objects and local structure in the network si-\nmultaneously. To achieve this, we model the relation graph\nas a function of both latent feature factors and latent clus-\nter memberships of objects to collectively discover globally\npredictive intrinsic properties of objects and capture latent\nblock structure in the network to improve prediction perfor-\nmance. Extensive experiments on several real world datasets\nsuggest that our proposed model outperforms the other state\nof the art approaches for link prediction."}
{"Title": "Using Toolbar Data to Understand Yahoo! Answers Usage ", "Abstract": "We use Yahoo! Toolbar data to gain insights into why people\nuse Q&A sites. We look at questions asked on Yahoo! An-\nswers and analyze both the pre-question behavior of users\nas well as their general online behavior. Our results indicate\nthat there is a one-dimensional spectrum of users ranging\nfrom \u201csocial users\u201d to \u201cinformational users\u201d and that web\nsearch and Q&A sites complement each other, rather than\ncompete. Concerning the pre-question behavior, users who\nfirst issue a question-related query are more likely to issue in-\nformational questions, rather than conversational ones, and\nsuch questions are less likely to attract an answer. Finally,\nwe only find weak evidence for topical congruence between\na user\u2019s questions and his web queries."}
{"Title": "SnoopyTagging: Recommending Contextualized Tags to\nIncrease the Quality and Quantity of Meta-Information", "Abstract": "Current mass-collaboration platforms use tags to annotate\nand categorize resources enabling effective search capabil-\nities. However, as tags are freely chosen keywords, the re-\nsulting tag vocabulary is very heterogeneous. Another short-\ncoming of simple tags is that they do not allow for a spec-\nification of context to create meaningful metadata. In this\npaper we present the SnoopyTagging approach which sup-\nports the user in the process of creating contextualized tags\nwhile at the same time decreasing the heterogeneity of the\ntag vocabulary by facilitating intelligent self-learning recom-\nmendation algorithms."}
{"Title": "Comparative Evaluation of JavaScript Frameworks", "Abstract": "For web programmers, it is important to choose the proper\nJavaScript framework that not only serves their current web\nproject needs, but also provides code of high quality and good\nperformance. The scope of this work is to provide a thorough\nquality and performance evaluation of the most popular\nJavaScript frameworks, taking into account well established\nsoftware quality factors and performance tests. The major\noutcome is that we highlight the pros and cons of JavaScript\nframeworks in various areas of interest and signify which and\nwhere are the problematical points of their code, that probably\nneed to be improved in the next versions."}
{"Title": "Getting More RDF Support from Relational Databases", "Abstract": "We introduce the database fragment of RDF, which extends\nthe popular Description Logic fragment, in particular with\nsupport for incomplete information. We then provide novel\nsound and complete saturation- and reformulation-based\ntechniques for answering the Basic Graph Pattern queries\nof SPARQL in this fragment. Notably, we extend the state\nof the art on pushing RDF query processing within robust /\nefficient relational database management systems. Finally,\nwe experimentally compare our query answering techniques\nusing well-established datasets."}
{"Title": "S 2 ORM: Exploiting Syntactic and Semantic Information\nfor Opinion Retrieval", "Abstract": "Opinion retrieval is the task of finding documents that express an\nopinion about a given query. A key challenge in opinion retrieval\nis to capture the query-related opinion score of a document.\nExisting methods rely mainly on the proximity information\nbetween the opinion terms and the query terms to address the key\nchallenge. In this study, we propose to incorporate the syntactic\nand semantic information of terms into a probabilistic language\nmodel in order to capture the query-related opinion score more\naccurately."}
{"Title": "All Our Messages Are Belong to Us: Usable Confidentiality\nin Social Networks", "Abstract": "Current online social networking (OSN) sites pose severe\nrisks to their users\u2019 privacy. Facebook in particular is cap-\nturing more and more of a user\u2019s past activities, sometimes\nstarting from the day of birth. Instead of transiently pass-\ning on information between friends, a user\u2019s data is stored\npersistently and therefore subject to the risk of undesired\ndisclosure. Traditionally, a regular user of a social network\nhas little awareness of her privacy needs in the Web or is not\nready to invest a considerable effort in securing her online\nactivities. Furthermore, the centralised nature of propri-\netary social networking platforms simply does not cater for\nend-to-end privacy protection mechanisms.\nIn this paper, we present a non-disruptive and lightweight\nintegration of a confidentiality mechanism into OSNs. Addi-\ntionally, direct integration of visual security indicators into\nthe OSN UI raise the awareness for (un)protected content\nand thus their own privacy. We present a fully-working pro-\ntotype for Facebook and an initial usability study, showing\nthat, on average, untrained users can be ready to use the\nservice in three minutes."}
{"Title": "Populating Personal Linked Data Caches\nusing Context Models", "Abstract": "The emergence of a Web of Data [3] enables new forms of applica-\ntion that require expressive query access, for which mature, Web-\nscale information retrieval techniques may not be suited. Rather\nthan attempting to deliver expressive query capabilities at Web-\nscale, this paper proposes the use of smaller, pre-populated data\ncaches whose contents are personalized to the needs of an individ-\nual user. We present an approach to a priori population of such\ncaches with Linked Data harvested from the Web, seeded by a sim-\nple context model for each user, which is progressively enriched\nby executing a series of enrichment rules over Linked Data from\nthe Web. Such caches can act as personal data stores supporting\na range of different applications. A comprehensive user evaluation\ndemonstrates that our approach can accurately predict the relevance\nof attributes added to the context model and the execution proba-\nbility of queries based on these attributes, thereby optimizing the\ncache population process."}
{"Title": "Probabilistic Critical Path Identification for Cost-Effective\nMonitoring of Service-based Web Applications", "Abstract": "The critical path of a composite Web application operating in\nvolatile environments, i.e., the execution path in the service\ncomposition with the maximum execution time, should be\nprioritised in cost-effective monitoring as it determines the\nresponse time of the Web application. In volatile operating\nenvironments, the critical path of a Web application is\nprobabilistic. As such, it is important to estimate the criticalities\nof the execution paths, i.e., the probabilities that they are critical,\nto decide which parts of the system to monitor. We propose a\nnovel approach to the identification of Probabilistic Critical Path\nfor Service-based Web Applications (PCP-SWA), which\ncalculates the criticalities of different execution paths in the\ncontext of service composition. We evaluate PCP-SWA\nexperimentally using an example Web application. Compared to\nrandom monitoring, PCP-SWA based monitoring is 55.67% more\ncost-effective on average."}
{"Title": "A Statistical Approach to URL-Based Web Page Clustering", "Abstract": "Most web page classifiers use features from the page content,\nwhich means that it has to be downloaded to be classified.\nWe propose a technique to cluster web pages by means of\ntheir URL exclusively. In contrast to other proposals, we\nanalyse features that are outside the page, hence, we do\nnot need to download a page to classify it. Also, it is non-\nsupervised, requiring little intervention from the user. Fur-\nthermore, we do not need to crawl extensively a site to build\na classifier for that site, but only a small subset of pages. We\nhave performed an experiment over 21 highly visited web-\nsites to evaluate the performance of our classifier, obtaining\ngood precision and recall results."}
{"Title": "Frequent Temporal Social Behavior Search\nin Information Networks", "Abstract": "In current social networking service (SNS) such as Facebook, there\nare diverse kinds of interactions between entity types. One\ncommonly-used activity of SNS users is to track and observe the\nrepresentative social and temporal behaviors of other individuals.\nThis inspires us to propose a new problem of Temporal Social\nBehavior Search (TSBS) from social interactions in an information\nnetwork: given a structural query with associated temporal labels,\nhow to find the subgraph instances satisfying the query structure and\ntemporal requirements? In TSBS, a query can be (a) a topological\nstructure, (b) the partially-assigned individuals on nodes, and/or (c)\nthe temporal sequential labels on edges. The TSBS method consists\nof two parts: offline miningand online matching. to the former mines\nthe temporal subgraph patterns for retrieving representative\nstructures that match the query. Then based on the given query, we\nperform the online structural matching on the mined patterns and\nreturn the top-k resulting subgraphs. Experiments on academic\ndatasets demonstrate the effectiveness of TSBS."}
{"Title": "Social Status and Role Analysis of Palin\u2019s Email Network", "Abstract": "Email usage is pervasive among people from different back-\ngrounds, and email corpus can be an important data source\nto study intricate social structures. Social status and role\nanalysis on a personal email network can help reveal hidden\ninformation. The availability of Sarah Palin\u2019s email corpus\npresents a great opportunity to study social statuses and so-\ncial roles in an email network. However, the email corpus\ndoes not readily lend itself to social network analysis due to\nproblems such as noisy email data, scale in size, and tempo-\nral constraints. In this paper, we report an initial investiga-\ntion of social status and role analysis on Sarah Palin\u2019s email\ncorpus. In particular, we conduct a preliminary study on\nPalin\u2019s social statuses and roles. To the best of our knowl-\nedge, this work is the first exploration of Sarah Palin\u2019s email\ncorpus recently released by the state of Alaska."}
{"Title": "The Affects of Task Difficulty on Medical Searches", "Abstract": "In this paper, we analyze medical searching behavior performed\nby a typical medical searcher. We broadly classify a typical\nmedical searcher as: non-medical professionals or medical\nprofessionals. We use behavioral signals to study how task\ndifficulty affects medical searching behavior. Using simulated\nscenarios, we gathered data from an exploratory survey of 180\nsearch sessions performed by 60 participants. Our research study\nprovides a deep understanding of how task difficulty affects\nmedical search behavior. Non-medical professionals and medical\nprofessionals demonstrate similar search behavior when searching\non an easy task. Longer queries, more time and more incomplete\nsearch sessions are observed for an easy task. However, they\ndemonstrate different results evaluation behavior based on task\ndifficulty."}
{"Title": "Leveraging Interlingual Classification\nto Improve Web Search", "Abstract": "In this paper we address the problem of improving accu-\nracy of web search in a smaller, data-limited search market\n(search language) using behavioral data from a larger, data-\nrich market (assist language). Specifically, we use interlin-\ngual classification to infer the search language query\u2019s intent\nusing the assist language click-through data. We use these\nimproved estimates of query intent, along with the query in-\ntent based on the search language data, to compute features\nthat encode the similarity between a search result (URL)\nand the query. These features are subsequently fed into the\nranking model to improve the relevance ranking of the doc-\numents. Our experimental results on German and French\nlanguages show the effectiveness of using assist language be-\nhavioral data \u2013 especially, when the search language queries\nhave small click-through data."}
{"Title": "Modeling Click-through based Word-pairs\nfor Web Search", "Abstract": "Statistical translation models and latent semantic analysis\n(LSA) are two effective approaches to exploit click-through\ndata for web search ranking. This paper presents two docu-\nment ranking models that combine both approaches by ex-\nplicitly modeling word-pairs. The first model, called Pair-\nModel, is a monolingual ranking model based on word pairs\nthat are derived from click-through data. It maps queries\nand documents into a concept space spanned by these word\npairs. The second model, called Bilingual Paired Topic\nModel (BPTM), uses bilingual word pairs and jointly mod-\nels a bilingual query-document collection. This model maps\nqueries and documents in multiple languages into a lower\ndimensional semantic subspace. Experimental results on\nweb search task show that they significantly outperform the\nstate-of-the-art baseline models, and the best result is ob-\ntained by interpolating PairModel and BPTM"}
{"Title": "Google Image Swirl", "Abstract": "Web image retrieval systems, such as Google or Bing image\nsearch, present search results as a relevance-ordered list. Al-\nthough alternative browsing models (e.g. results as clusters\nor hierarchies) have been proposed in the past, it remains to\nbe seen whether such models can be applied to large-scale\nimage search. This work presents Google Image Swirl, a\nlarge-scale, publicly available, hierarchical image browsing\nsystem by automatically group the search results based on\nvisual and semantic similarity. This paper describes meth-\nods used to build such system and shares the findings from\n2-years worth of user feedback and usage statistics."}
{"Title": "Identifying Sentiments in N-grams", "Abstract": "Our proposal, identifying sentiments in N-grams (ISN), fo-\ncuses on both word order and phrases, and the interdepen-\ndency between specific ratings and corresponding sentiments\nin texts to detect subjective information."}
{"Title": "StormRider: Harnessing \u201cStorm\u201d for Social Networks", "Abstract": "The focus of online social media providers today has shifted\nfrom\u201ccontent generation\u201dtowards finding effective method-\nologies for \u201ccontent storage, retrieval and analysis\u201d in the\npresence of evolving networks. Towards this end, in this pa-\nper we present StormRider, a framework that uses existing\ncloud computing and semantic web technologies to provide\napplication programmers with automated support for these\ntasks, thereby allowing a richer assortment of use cases to\nbe implemented on the underlying evolving social networks."}
{"Title": "Learning from Positive and Unlabeled Amazon Reviews:\nTowards Identifying Trustworthy Reviewers", "Abstract": "On-line marketplaces have been growing in importance over\nthe last few years. In such environments, reviews consist\nthe main reputation mechanism for the available products.\nHence, presenting high quality reviews is crucial in achiev-\ning a high level of customer satisfaction. Towards this direc-\ntion, in this work, we introduce a new dimension of review\nquality, the reviewer\u2019s\u201ctrustfulness\u201d. We assume that volun-\ntary information provided by Amazon reviewers, regarding\nwhether they are the actual buyers of the product, signals\nthe reliability of a review. Based on this information, we\ncharacterize a reviewer as trustworthy (positive instance) or\nof unknown \u201ctrustfulness\u201d (unlabeled instance). Then, we\nbuild models that exploit reviewers\u2019 profile information and\non-line behavior to rank them according to the probability\nof being trustworthy. Our results are very promising, since\nthey provide evidence that our predictive models separate\npositive from unlabeled instances with very high accuracies."}
{"Title": "Treehugger or Petrolhead?", "Abstract": "The Web is a very democratic medium of communication al-\nlowing everyone to express his or her opinion about any type\nof topic. This multitude of voices makes it more and more\nimportant to detect bias and help Internet users understand\nthe background of information sources. Political bias of Web\nsites, articles, or blog posts is hard to identify straightaway.\nManual content analysis conducted by experts is the stan-\ndard way in political and social science to detect this bias.\nIn this paper we present an automated approach relying on\nmethods from information retrieval and corpus statistics to\nidentify biased vocabulary use. As an example, we analyzed\n15 years of parliamentary speeches of the German Bundestag\nand we investigated whether there is bias towards a political\nparty in major national online newspapers and magazines.\nThe results show that bias exists with respect to vocabulary\nuse and it coincides with human judgement."}
{"Title": "Towards Optimizing the Non-Functional Service\nMatchmaking Time", "Abstract": "The Internet is moving fast to a new era where million of\nservices and things will be available. In this way, as there\nwill be many functionally-equivalent services for a specific\nuser task, the service non-functional aspect should be con-\nsidered for filtering and choosing the appropriate services.\nThe related approaches in service discovery mainly concen-\ntrate on exploiting constraint solving techniques for inferring\nif the user non-functional requirements are satisfied by the\nservice nonfunctional capabilities. However, as the match-\nmaking time is proportional to the number of non-functional\nservice descriptions, these approaches fail to fulfill the user\nrequest in a timely manner. To this end, two alternative\ntechniques for improving the non-functional service match-\nmaking time have been developed. The first one is generic as\nit can handle non-functional service specifications contain-\ning n-ary constraints, while the second is only applicable to\nunary-constrained specifications. Both techniques were ex-\nperimentally evaluated. The preliminary evaluation results\nshow that the service matchmaking time is significantly im-\nproved without compromising matchmaking accuracy."}
{"Title": "Measuring Usefulness of Context for Context-Aware\nRanking", "Abstract": "Most of major search engines develop different types of per-\nsonalisation of search results. Personalisation includes deriv-\ning user\u2019s long-term preferences, query disambiguation etc.\nUser sessions provide very powerful tool commonly used for\nthese problems. In this paper we focus on personalisation\nbased on context-aware reranking. We implement a machine\nlearning framework to approach this problem and study im-\nportance of different types of features. We stress that fea-\ntures concerning temporal and context relatedness of queries\nalong with features relied on user\u2019s actions are most impor-\ntant and play crucial role for this type of personalisation."}
{"Title": "TEM: A Novel Perspective to Modeling Content on Microblogs", "Abstract": "In recent times, microblogging sites like Facebook and Twit-\nter have gained a lot of popularity. Millions of users world\nwide have been using these sites to post content that inter-\nests them and also to voice their opinions on several current\nevents. In this paper, we present a novel non-parametric\nprobabilistic model - Temporally driven Theme Event Model\n(TEM) for analyzing the content on microblogs. We also de-\nscribe an online inference procedure for this model that en-\nables its usage on large scale data. Experimentation carried\nout on real world data extracted from Facebook and Twitter\ndemonstrates the efficacy of the proposed approach."}
{"Title": "Using Proximity to Predict Activity in Social Networks", "Abstract": "The structure of a social network contains information useful\nfor predicting its evolution. We show that structural infor-\nmation also helps predict activity. People who are\u201cclose\u201din\nsome sense in a social network are more likely to perform\nsimilar actions than more distant people. We use network\nproximity to capture the degree to which people are \u201cclose\u201d\nto each other. In addition to standard proximity metrics\nused in the link prediction task, such as neighborhood over-\nlap, we introduce new metrics that model different types of\ninteractions that take place between people. We study this\nclaim empirically using data about URL forwarding activity\non the social media sites Digg and Twitter. We show that\nstructural proximity of two users in the follower graph is\nrelated to similarity of their activity, i.e., how many URLs\nthey both forward. We also show that given friends\u2019 ac-\ntivity, knowing their proximity to the user can help better\npredict which URLs the user will forward. We compare the\nperformance of different proximity metrics on the activity\nprediction task and find that metrics that take into account\nthe attention-limited nature of interactions in social media\nlead to substantially better predictions."}
{"Title": "Finding Influential Seed Successors in Social Networks", "Abstract": "In a dynamic social network, nodes can be removed from the net-\nwork for some reasons, and consequently affect the behaviors of\nthe network. In this paper, we tackle the challenge of finding a\nsuccessor node for each removed seed node to maintain the influ-\nence spread in the network. Given a social network and a set of\nseed nodes for influence maximization, the problem is to effec-\ntively choose successors to inherit the jobs of initial influence\npropagation when some seeds are removed from the network. To\ntackle this problem, we present and discuss five neighborhood-\nbased selection heuristics, including degree, degree discount,\noverlapping, community bridge, and community degree. Experi-\nments on DBLP co-authorship network show the effectiveness of\ndevised heuristics."}
{"Title": "Influence Propagation and Maximization\nfor Heterogeneous Social Networks", "Abstract": "Influence propagation and maximization is a well-studied problem\nin social network mining. However, most of the previous works\nfocus only on homogeneous social networks where nodes and links\nare of single type. This work aims at defining information propaga-\ntion for heterogeneous social networks (containing multiple types of\nnodes and links). We propose to consider the individual behaviors of\npersons to model the influence propagation. Person nodes possess\ndifferent influence probabilities to activate their friends according to\ntheir interaction behaviors. The proposed model consists of two\nstages. First, based on the heterogeneous social network, we create a\nhuman-based influence graph where nodes are of human-type and\nlinks carry weights that represent how special the target node is to\nthe source node. Second, we propose two entropy-based heuristics to\nidentify the disseminators in the influence graph to maximize the\ninfluence spread. Experimental results show promising results for\nthe proposed method."}
{"Title": "Dynamic Selection of Activation Targets to\nBoost the Influence Spread in Social Networks", "Abstract": "This paper aims to combine the viral marketing with the idea of\ndirect selling to for influence maximization in a social network. In\ndirect selling, producers can sell the products directly to the con-\nsumers without having to go through a cascade of wholesalers.\nThrough direct selling, it is possible to sell the products in a more\nefficient and economic manner. Motivated by this idea, we pro-\npose a target-selecting independent cascade (TIC) model, in which\nduring influence propagation each active node can give up to at-\ntempt to influence some neighboring nodes, named victims, who\nare hard to affect, and try to activate friends of its friends, termed\ndestinations, who could have higher potential to increase the in-\nfluence spread. The next question to ask is that given a social\nnetwork and a set of seeds for influence propagation under TIC\nmodel, how to effectively select targets (i.e., victims and destina-\ntions) for the attempts of activation during propagation to boost\nthe influence spread. We propose and evaluate three heuristics for\nthe target selection. Experiments show that selecting targets based\non influence probability between nodes have the highest boost of\ninfluence spread."}
{"Title": "Regional Subgraph Discovery in Social Networks", "Abstract": "This paper solves a region-based subgraph discovery problem. We\nare given a social network and some sample nodes which is sup-\nposed to belong to a specific region, and the goal is to obtain a\nsubgraph that contains the sampled nodes with other nodes in the\nsame region. Such regional subgraph discovery can benefit re-\ngion-based applications, including scholar search, friend sugges-\ntion, and viral marketing. To deal with this problem, we assume\nthere is a hidden backbone connecting the query nodes directly or\nindirectly in their region. The idea is that individuals belonging to\nthe same region tend to share similar interests and cultures. By\nmodeling such fact on edge weights, we search the graph to ex-\ntract the regional backbone with respect to the query nodes. Then\nwe can expand the backbone to derive the regional network. Ex-\nperiments on a DBLP co-authorship network show the proposed\nmethod can effectively discover the regional subgraph with high\nprecision scores."}
{"Title": "GPU-Based Minwise Hashing", "Abstract": "Minwise hashing [1] is a standard technique for efficient set simi-\nlarity estimation in the context of search. The recent work of b-bit\nminwise hashing [3] provided asubstantial improvement by storing\nonly the lowest b bits of each hashed value. Both minwise hashing\nand b-bit minwise hashing require an expensive preprocessing step\nfor applying k (e.g., k = 500) permutations on the entire data inor-\nder to compute k minimal values as the hashed data. In this paper,\nwe developed a parallelization scheme using GPUs, which reduced\nthe processing time by a factor of 20 \u223c 80. Reducing the pre-\nprocessing time is highly beneficial in practice, for example, for\nduplicate web page detection (where minwise hashing is a major\nstep in the crawling pipeline) or for increasing the testing speed of\nonline classifiers (when the test data are not preprocessed)."}
{"Title": "CloudSpeller: Query Spelling Correction by Using a\nUnified Hidden Markov Model with Web-scale Resources", "Abstract": "Query spelling correction is an important component of modern\nsearch engines that can help users to express an information need\nmore accurately and thus improve search quality. In this work we\nproposed and implemented an end-to-end speller correction sys-\ntem, namely CloudSpeller. The CloudSpeller system uses a Hidden\nMarkov Model to effectively model major types of spelling errors\nin a unified framework, in which we integrate a large-scale lexicon\nconstructed using Wikipedia, an error model trained from high con-\nfidence correction pairs, and the Microsoft Web N-gram service.\nOur system achieves excellent performance on two search query\nspelling correction datasets, reaching 0.960 and 0.937 F1 scores on\nthe TREC dataset and the MSN dataset respectively."}
{"Title": "Sentiment Classification via Integrating Multiple Feature\nPresentations", "Abstract": "In the bag of words framework, documents are often con-\nverted into vectors according to predefined features together\nwith weighting mechanisms. Since each feature presentation\nhas its character, it is difficult to determine which one should\nbe chosen for a specific domain, especially for the users who\nare not familiar with the domain. This paper explores the\nintegration of various feature presentations to improve the\nclassification accuracy. A general two phases framework is\nproposed. In the first phase, we train multiple base classi-\nfiers with various vector spaces and use these classifiers to\npredict the class of testing samples respectively. In the sec-\nond phase, the previous predicted results are integrated into\nthe ultimate class via stacking with SVM. The experimental\nresults demonstrate the effectiveness of our method."}
{"Title": "Tuning Parameters of the Expected Reciprocal Rank", "Abstract": "There are several popular IR metrics based on an underly-\ning user model. Most of them are parameterized. Usually\nparameters of these metrics are chosen on the basis of gen-\neral considerations and not validated by experiments with\nreal users. Particularly, the parameters of the Expected Re-\nciprocal Rank measure are the normalized parameters of the\nDCG metric, and the latter are chosen in an ad-hoc manner.\nWe suggest two approaches for adjusting parameters of the\nERR model by analyzing real users behaviour: one based\non a controlled experiment and another relying on search\nlog analysis. We show that our approaches generate pa-\nrameters that are largely different from the commonly used\nparameters of the ERR model."}
{"Title": "Conversations Reconstruction in the Social Web", "Abstract": "We propose a socio-semantic approach for building conversations\nfrom social interactions following three steps: (i) content linkage,\n(ii) participants (users) linkage, and (iii) temporal linkage. Prelim-\ninary evaluations on a Twitter dataset show promising and interest-\ning results."}
{"Title": "Secure Querying of Recursive XML Views:\nA Standard XPath-based Technique", "Abstract": "Most state-of-the art approaches for securing XML docu-\nments allow users to access data only through authorized\nviews defined by annotating an XML grammar (e.g. DTD)\nwith a collection of XPath expressions. To prevent im-\nproper disclosure of confidential information, user queries\nposed on these views need to be rewritten into equivalent\nqueries on the underlying documents, which enables us to\navoid the overhead of view materialization and maintenance.\nA major concern here is that XPath query rewriting for\nrecursive XML views is still an open problem. To over-\ncome this problem, some authors have proposed rewriting\napproaches based on the non-standard language, \u201cRegular\nXPath\u201d, which is more expressive than XPath and makes\nrewriting possible under recursion. However, query rewrit-\ning under Regular XPath can be of exponential size as it re-\nlies on automaton model. Most importantly, Regular XPath\nremains a theoretical achievement. Indeed, it is not com-\nmonly used in practice as translation and evaluation tools\nare not available. In this work, we show that query rewriting\nis always possible for recursive XML views using only the ex-\npressive power of the standard XPath. We propose a general\napproach for securely querying of XML data under arbitrary\nsecurity views (recursive or not) and for a significant frag-\nment of XPath. We provide a linear rewriting algorithm\nthat is efficient and scales well."}
{"Title": "GoThere: Travel Suggestions using Geotagged Photos", "Abstract": "We propose a context and preference aware travel guide that sug-\ngests significant tourist destinations to users based on their prefer-\nences and current surrounding context using contextualized user-\ngenerated contents from the social media repository, i.e., Flickr."}
{"Title": "Ad-hoc Ride Sharing Application using Continuous\nSPARQL Queries", "Abstract": "In the existing ride sharing scenario, the ride taker has to cope\nwith uncertainties since the ride giver may be delayed or may not\nshow up due to some exigencies. A solution to this problem is\ndiscussed in this paper. The solution framework is based on\ngathering information from multiple streams such as traffic status\non the ride giver's routes and the ride giver's GPS coordinates.\nAlso, it maintains a list of alternative ride givers so as to almost\nguarantee a ride for the ride taker. This solution uses a SPARQL-\nbased continuous query framework that is capable of sensing fast-\nchanging real-time situation. It also has reasoning capabilities for\nhandling ride taker's preferences. The paper introduces the\nconcept of user-managed windows that is shown to be required\nfor this solution. Finally we show that the performance of the\napplication is enhanced by designing the application with short\nincremental queries."}
{"Title": "Sparse Linear Methods with Side Information\nfor Top-N Recommendations", "Abstract": "This paper focuses on developing effective algorithms that\nutilize side information for top-N recommender systems. A\nset of Sparse Linear Methods with Side information (SSLIM)\nis proposed, that utilize a regularized optimization process\nto learn a sparse item-to-item coefficient matrix based on\nhistorical user-item purchase profiles and side information\nassociated with the items. This coefficient matrix is used\nwithin an item-based recommendation framework to gener-\nate a size-N ranked list of items for a user. Our experimental\nresults demonstrate that SSLIM outperforms other methods\nin effectively utilizing side information and achieving perfor-\nmance improvement."}
{"Title": "Sentiment Analysis amidst Ambiguities in YouTube\nComments on Yoruba Language (Nollywood) Movies", "Abstract": "Nollywood is the second largest movie industry in the world in\nterms of annual movie production. A dominant number of the\nmovies are in Yoruba language spoken by over 20 million people\nacross the globe. The number of Yoruba language movies uploaded\nto YouTube and their corresponding comments is growing\nexponentially. However, YouTube comments made by native\nspeakers on Yoruba movies combine English language, Yoruba\nlanguage, and other commonly used \u201cpidgin\u201d Yoruba language\nwords. Since Yoruba is still a resource constrained language,\nexisting sentiment or subjectivity analysis algorithms have poor\nperformances on YouTube comments made on Yoruba language\nmovies. This is because of the constrained language ambiguities. In\nthis work, we present an automatic sentiment analysis algorithm\nfor YouTube comments on Yoruba language movies. The\nalgorithm uses SentiWordNet thesaurus and a lexicon of\ncommonly used Yoruba language sentiment words and phrases. In\nterms of precision-recall, the algorithm performs more than a state-\nof-the-art sentiment analysis technique by up to 20%."}
{"Title": "C4PS - Colors for Privacy Settings", "Abstract": "The ever increasing popularity of Facebook and other On-\nline Social Networks has left a wealth of personal as well as\nprivate data on the web, aggregated and readily accessible\nfor broad and automatic retrieval. Protection from both\nundesired recipients and harvesting by crawlers is imple-\nmented by access control, manually configured by the user\nand owner of the data. Several studies demonstrate that\ndefault settings cause an unnoticed over-sharing and that\nusers have trouble understanding and configuring adequate\nprivacy settings. We developed an improved interface for\nprivacy settings in Facebook by mainly applying color cod-\ning for different groups, providing easy access to the privacy\nsettings, and applying the principle of common practices.\nUsing a lab study, we show that the new approach increases\nthe usability significantly."}
{"Title": "Extracting Advertising Keywords from URL Strings", "Abstract": "Extracting advertising keywords from web-pages is impor-\ntant in keyword-based online advertising. Previous works\nhave attempted to extract advertising keywords from the\nwhole content of a web-page. However, in some scenarios, it\nis necessary to extract keywords from just the URL string\nitself. In this work, we propose an algorithm for extract-\ning advertising keywords from the URL string alone. Our\nalgorithm has applications in contextual and paid search ad-\nvertising. We evaluate the effectiveness of our algorithm on\npublisher URLs and show that it produces very good quality\nkeywords that are comparable with keywords produced by\npage based extractors."}
{"Title": "Instrumenting a Logic Programming Language to Gather\nProvenance from an Information Extraction Application", "Abstract": "Information extraction (IE) programs for the web consume\nand produce a lot of data. In order to better understand\nthe program output, the developer and user often desire to\nknow the details of how the output was created. Prove-\nnance can be used to learn about the creation of the output.\nWe collect fine\u2013grained provenance by leveraging ongoing\nwork in the IE community to write IE programs in a logic\nprogramming language. The logic programming language\nexposes the semantics of the program, allowing us to gather\nfine\u2013grained provenance during program execution. We dis-\ncuss a case study using a web\u2013based community information\nmanagement system, then present results regarding the per-\nformance of queries over the provenance data gathered by\nour logic program interpreter. Our findings show that it\nis possible to gather useful fine\u2013grained provenance during\nthe execution of a logic based web information extraction\nprogram. Additionally, queries over this provenance infor-\nmation can be performed in a reasonable amount of time."}
{"Title": "Lexical Quality as a Proxy for\nWeb Text Understandability", "Abstract": "We show that a recently introduced lexical quality measure\nis also valid to measure textual Web accessibility. Our mea-\nsure estimates the lexical quality of a site based in the oc-\ncurrence in English Web pages of a large set of words with\nerrors. We first compute the correlation of our measure with\nWeb popularity measures to show that gives independent\ninformation. Second, we carry out a user study using eye\ntracking to prove that the degree of lexical quality of a text\nis related to the degree of understandability of a text, one\nof the factors behind Web accessibility."}
{"Title": "Latent Contextual Indexing of Annotated Documents", "Abstract": "In this paper we propose a simple and flexible framework to\nindex context-annotated documents, e.g., documents with\ntimestamps or georeferences, by contextual topics. A con-\ntextual topic is a distribution over document features with\na particular meaning in the context domain, such as a repet-\nitive event or a geographic phenomenon. Such a framework\nsupports document clustering, labeling, and search, with re-\nspect to contextual knowledge contained in the document\ncollection. To realize the framework, we introduce an ap-\nproach to project documents into a context-feature space.\nThen, dimensionality reduction is used to extract contextual\ntopics in this context-feature space. The topics can then be\nprojected back onto the documents. We demonstrate the\nutility of our approach with a case study on georeferenced\nWikipedia articles."}
{"Title": "APOLLO: A General Framework for Populating Ontology\nwith Named Entities via Random Walks on Graphs", "Abstract": "Automatically populating ontology with named entities ex-\ntracted from the unstructured text has become a key is-\nsue for Semantic Web. This issue naturally consists of two\nsubtasks: (1) for the entity mention whose mapping entity\ndoes not exist in the ontology, attach it to the right cat-\negory in the ontology (i.e., fine-grained named entity clas-\nsification), and (2) for the entity mention whose mapping\nentity is contained in the ontology, link it with its map-\nping real world entity in the ontology (i.e., entity linking).\nPrevious studies only focus on one of the two subtasks.\nThis paper proposes APOLLO, a general weakly supervised\nfrAmework for POpuLating ontoLOgy with named entities.\nAPOLLO leverages the rich semantic knowledge embedded\nin the Wikipedia to resolve this task via random walks on\ngraphs. An experimental study has been conducted to show\nthe effectiveness of APOLLO."}
{"Title": "Multiple Spreaders Affect the Indirect Influence on Twitter", "Abstract": "Most studies on social influence have focused on direct in-\nfluence, while another interesting question can be raised as\nwhether indirect influence exists between two users who\u2019re\nnot directly connected in the network and what affects such\ninfluence. In addition, the theory of complex contagion tells\nus that more spreaders will enhance the indirect influence\nbetween two users. Our observation of intensity of indirect\ninfluence, propagated by n parallel spreaders and quanti-\nfied by retweeting probability on Twitter , shows that com-\nplex contagion is validated globally but is violated locally.\nIn other words, the retweeting probability increases non-\nmonotonically with some local drops."}
{"Title": "Entity based Translation Language Model", "Abstract": "Bridging the lexical gap between the user\u2019s question and\nthe question-answer pairs in Q&A archives has been a ma-\njor challenge for Q&A retrieval. State-of-the-art approaches\naddress this issue by implicitly expanding the queries with\nadditional words using statistical translation models. In this\nwork we extend the lexical word based translation model\nto incorporate semantic concepts. We explore strategies to\nlearn the translation probabilities between words and the\nconcepts using the Q&A archives and Wikipedia. Experi-\nments conducted on a large scale real data from Yahoo An-\nswers! show that the proposed techniques are promising and\nneed further investigation."}
{"Title": "Enabling Accent Resilient Speech based\nInformation Retrieval", "Abstract": "Voice interfaces to browsers and mobile applications are becoming\npopular as typing with touch screens is cumbersome. The main is-\nsue of practical speech based interfaces is how to overcome speech\nrecognition errors. This problem is more severe when the users\nare non-native speakers of English due to differences in pronunci-\nations. In this paper, we describe a novel, intelligent speech inter-\nface design approach for IR tasks that is significantly robust to ac-\ncent variations. Our solution uses phonemic similarity based word\nspreading and semantic information based filtering to boost the ac-\ncuracyofanyASR.WeevaluatedoursolutionwithGoogleVoiceas\nthe ASR for a web question-answering system developed in-house\nand the results are very encouraging."}
{"Title": "Dynamical Information Retrieval Modelling", "Abstract": "The dynamic nature of document relevance is largely ignored\nby traditional Information Retrieval (IR) models, which as-\nsume that scores (relevance) for documents given an in-\nformation need are static. In this paper, we formulate a\ngeneral Dynamical Information Retrieval problem, where\nwe consider retrieval as a stochastic, controllable process.\nThe ranking action continuously controls the retrieval sys-\ntem\u2019s dynamics and an optimal ranking policy is found that\nmaximises the overall users\u2019 satisfaction during each period.\nThrough deriving the posterior probability of the documents\nevolving relevancy from user clicks, we can provide a plug-\nin framework for incorporating a number of click models,\nwhich can be combined with Multi-Armed Bandit theory\nand Portfolio Theory of IR to create a dynamic ranking rule\nthat takes rank bias and click dependency into account. We\nverify the versatility of our algorithms in a number of exper-\niments and demonstrate improved performance over strong\nbaselines and as a result significant performance gains have\nbeen achieved."}
{"Title": "Detecting Dynamic Association among Twitter Topics", "Abstract": "Over the last few years, Twitter is increasingly becoming an\nimportant source of up-to-date topics about what is happening in\nthe world. In this paper, we propose a dynamic topic association\ndetection model to discover relations between Twitter topics, by\nwhich users can gain insights into richer information about topics\nof interest. The proposed model utilizes a time constrained\nmethod to extract event-based spatio-temporal topic association,\nand constructs a dynamic temporal map to represent the obtained\nresult. Experimental results show the improvement of the\nproposed model compared to static spatio-temporal method and\nco-occurrence method."}
{"Title": "Using Community Information to Improve the Precision of\nLink Prediction Methods", "Abstract": "Because network data is often incomplete, researchers con-\nsider the link prediction problem, which asks which non-\nexistent edges in an incomplete network are most likely to\nexist in the complete network. Classical approaches com-\npute the \u2018similarity\u2019 of two nodes, and conclude that highly\nsimilar nodes are most likely to be connected in the com-\nplete network. Here, we consider several such similarity-\nbased measures, but supplement the similarity calculations\nwith community information. We show that for many net-\nworks, the inclusion of community information improves the\naccuracy of similarity-based link prediction methods."}
{"Title": "Open and Decentralized Platform for Visualizing Web\nMash-ups in Augmented and Mirror Worlds", "Abstract": "Augmented reality applications are gaining popularity due to in-\ncreased capabilities of modern mobile devices. However, existing\napplications are tightly integrated with backend services that ex-\npose content using proprietary interfaces. We demonstrate an ar-\nchitecture that allows visualization of web content in augmented\nand mirror world applications, based on open web protocols and\nformats. We describe two clients, one for creating virtual artifacts,\nweb resources that bind together web content with location and a\n3D model, and one that visualizes the virtual artifacts in the mirror\nworld."}
{"Title": "Actualization of Query Suggestions using Query Logs", "Abstract": "In this work we are studying actualization techniques for\nbuilding an up-to-date query suggestions model using query\nlogs. The performance of the proposed actualization algo-\nrithms was estimated by real query flow of the Yandex search\nengine."}
{"Title": "Query Spelling Correction Using Multi-task Learning", "Abstract": "This paper explores the use of online multi-task learning for\nsearch query spelling correction, by effectively transferring\ninformation from different and biased training datasets for\nimproving spelling correction across datasets. Experiments\nwere conducted on three query spelling correction datasets,\nincluding the well-known TREC benchmark data. Our ex-\nperimental results demonstrate that the proposed method\nconsiderably outperforms existing baseline systems in terms\nof accuracy. Importantly, the proposed method is about one-\norder of magnitude faster than baseline systems in terms of\ntraining speed. In contrast to existing methods which typi-\ncally require more than (e.g.,) 50 training passes, our algo-\nrithm can very closely approach the empirical optimum in\naround five passes."}
{"Title": "Incorporating Seasonal Time Series Analysis with Search\nBehavior Information in Sales Forecasting", "Abstract": "We consider the problem of predicting monthly auto sales\nin mainland China. First, we design an algorithm using\nclick-through and query reformulation information to clus-\nter related queries and count their frequencies on monthly-\nbasis. By introducing Exponentially Weighted Moving Av-\nerages (EWMA) model, we measure the seasonal impact on\nthe sales trend. Two features are combined using linear re-\ngression. The experiment shows that our model is effective\nwith high accuracy and outperforms conventional forecast-\ning models."}
{"Title": "Photo-TaPE: User Privacy Preferences in Photo Tagging", "Abstract": "Although they are used to expose pictures on the Web, users\nmay not want to have a link between their identity and pic-\ntures without being able to modify them or control who\naccesses them. Photo tagging \u2014 and more broadly face-\nrecognition algorithms \u2014 often escapes to the users\u2019 control\nand creates links between private situations and their pub-\nlic profile. To address this issue, we designed a geo-location\naided system to let users declare their tagging preferences\ndirectly when the picture is taken. We present Photo-\nTagging Preference Enforcement (Photo-TaPE) a system\nenforcing users tagging preferences without revealing their\nidentity. By improving face-recognition efficiency, Photo-\nTaPE can guarantee the user tagging preferences in 67% of\nthe cases and significantly reduces the processing time of\nface-recognition algorithms."}
{"Title": "Understanding Human Movement Semantics:\nA Point of Interest Based Approach", "Abstract": "The recent availability of human mobility traces has driven a\nnew wave of research on human movement with straightfor-\nward applications in wireless/cellular network. In this paper\nwe revisit the human mobility problem with new assump-\ntions. We believe that human movement is not independent\nof the surrounding locations, i.e. the points of interest that\nthey visit; most of the time people travel with specific goals\nin mind, visit specific points of interest, and frequently re-\nvisit favorite places. Using GPS mobility traces of a large\nnumber of users located across two distinct geographical lo-\ncations we study the correlation between people\u2019s trajecto-\nries and the differently spread points of interest nearby."}
{"Title": "Scalable Multi Stage Clustering of Tagged Micro-Messages", "Abstract": "The growing popularity of microblogging backed by services like\nTwitter, Facebook, Google+ and LinkedIn, raises the challenge of\nclustering short and extremely sparse documents. In this work we\npropose SMSC \u2013 a scalable, accurate and efficient multi stage clus-\ntering algorithm. Our algorithm leverages users practice of adding\ntags to some messages by bootstrapping over virtual non sparse\ndocuments. We experiment on a large corpus of tweets from Twit-\nter, and evaluate results against a gold-standard classification val-\nidated by seven clustering evaluation measures (information theo-\nretic, paired and greedy). Results show that the algorithm presented\nis both accurate and efficient, significantly outperforming other al-\ngorithms. Under reasonable practical assumptions, our algorithm\nscales up sublinearly in time."}
{"Title": "Seeing the Best and Worst of Everything on the Web\nwith a Two-level, Feature-rich Affect Lexicon", "Abstract": "Affect lexica are useful for sentiment analysis because they map\nwords (or senses) onto sentiment ratings. However, few lexica\nexplain their ratings, or provide sufficient feature richness to\nallow a selective \u201cspin\u201d to be placed on a word in context. Since\nan affect lexicon aims to capture the affect of a word or sense in\nits most stereotypical usage, it should be grounded in explicit\nstereotype representations of each word\u2019s most salient properties\nand behaviors. We show here how to acquire a large stereotype\nlexicon from Web content, and further show how to determine\nsentiment ratings for each entry in the lexicon, both at the level of\nproperties and behaviors and at the level of stereotypes. Finally,\nwe show how the properties of a stereotype can be segregated on\ndemand, to place a positive or negative spin on a word in context"}
{"Title": "Unified Classification Model for Geotagging Websites", "Abstract": "The paper presents a novel approach to finding regional\nscopes (geotagging) of websites. It relies on a single binary\nclassification model per region type to perform the multi-\nclass classification and uses a variety of features of different\nnature that have not been yet used together for machine-\nlearning based regional classification of websites. The eval-\nuation demonstrates the advantage of our one model per re-\ngion type method versus the traditional one model per region\napproach."}
{"Title": "Selling Futures Online Advertising Slots via Option\nContracts", "Abstract": "Many online advertising slots are sold through bidding mech-\nanisms by publishers and search engines. Highly affected by\nthe dual force of supply and demand, the prices of advertis-\ning slots vary significantly over time. This then influences\nthe businesses whose major revenues are driven by online\nadvertising, particularly for publishers and search engines.\nTo address the problem, we propose to sell the future ad-\nvertising slots via option contracts (also called ad options).\nThe ad option can give its buyer the right to buy the future\nadvertising slots at a prefixed price. The pricing model of ad\noptions is developed in order to reduce the volatility of the\nincome of publishers or search engines. Our experimental\nresults confirm the validity of ad options and the embedded\nrisk management mechanisms."}
{"Title": "Model News Relatedness through User Comments", "Abstract": "Most of previous work on news relatedness focuses on news article\ntexts. In this paper, we study the benefit of user-generated com-\nments on modeling news relatedness. Comments contain rich text\ninformation which is provided by commenters and rated by read-\ners with thumb-up or thumb-down, but the quality of individual\ncomments varies widely. We compare different ways of capturing\nrelatedness by leveraging both text and user interaction information\nin comments. Our evaluation based on an editorial data set demon-\nstrates that the text information in comments is very effective to\nmodel relatedness while community rating is quite predictive of\nthe comment quality."}
{"Title": "A Data-Driven Sketch of Wikipedia Editors", "Abstract": "Who edits Wikipedia? We attempt to shed light on this question by\nusingaggregatedlogdatafromYahoo!\u2019sbrowsertoolbarinorderto\nanalyze Wikipedians\u2019 editing behavior in the context of their online\nlives beyond Wikipedia. We broadly characterize editors by inves-\ntigating how their online behavior differs from that of other users;\ne.g., we find that Wikipedia editors search more, read more news,\nplay more games, and, perhaps surprisingly, are more immersed in\npop culture. Then we inspect how editors\u2019 general interests relate\nto the articles to which they contribute; e.g., we confirm the intu-\nition that editors show more expertise in their active domains than\naverage users. Our results are relevant as they illuminate novel as-\npects of what has become many Web users\u2019 prevalent source of\ninformation and can help in recruiting new editors."}
{"Title": "A Framework to Represent and Mine Knowledge Evolution\nfrom Wikipedia Revisions", "Abstract": "State-of-the-art knowledge representation in semantic web\nemploys a triple format (subject-relation-object). The lim-\nitation is that it can only represent static information, but\ncannot easily encode revisions of semantic web and knowl-\nedge evolution. In reality, knowledge does not stay still but\nevolves over time. In this paper, we first introduce the con-\ncept of\u201cquintuple representation\u201dby adding two new fields,\nstate and time, where state has two values, either in or out,\nto denote that the referred knowledge takes effective or be-\ncomes expired at the given time. We then discuss a two-\nstep statistical framework to mine knowledge evolution into\nthe proposed quintuple representation. Utilizing extracted\nquintuple properly, it not only can reveal knowledge chang-\ning history but also detect expired information. We evaluate\nthe proposed framework on Wikipedia revisions, as well as,\ncommon web pages currently not in semantic web format."}
{"Title": "Review Spam Detection via Time Series Pattern Discovery", "Abstract": "Online reviews play a crucial role in today\u2019s electronic com-\nmerce. Due to the pervasive spam reviews, customers can\nbe misled to buy low-quality products, while decent stores\ncan be defamed by malicious reviews. We observe that, in\nreality, a great portion (> 90% in the data we study) of the\nreviewers write only one review (singleton review). These\nreviews are so enormous in number that they can almost\ndetermine a store\u2019s rating and impression. However, exist-\ning methods ignore these reviewers. To address this prob-\nlem, we observe that the normal reviewers\u2019 arrival pattern\nis stable and uncorrelated to their rating pattern tempo-\nrally. In contrast, spam attacks are usually bursty and ei-\nther positively or negatively correlated to the rating. Thus,\nwe propose to detect such attacks via unusually correlated\ntemporal patterns. We identify and construct multidimen-\nsional time series based on aggregate statistics, in order to\ndepict and mine such correlation. Experimental results show\nthat the proposed method is effective in detecting singleton\nreview attacks. We discover that singleton review is a signif-\nicant source of spam reviews and largely affects the ratings\nof online stores."}
{"Title": "Combining Classification with Clustering for Web Person\nDisambiguation", "Abstract": "Web Person Disambiguation is often conducted through\nclustering web documents to identify different namesakes for a\ngiven name. This paper presents a new key-phrased clustering\nmethod combined with a second step re-classification to identify\noutliers to improve cluster performance. For document clustering,\nthe hierarchical agglomerative approach is conducted based on\nthe vector space model which uses key phrases as the main\nfeature. Outliers of cluster results are then identified through a\ncentroids-based method. The outliers are then reclassified by the\nSVM classifier into the more appropriate clusters using a key\nphrase-based string kernel model as its feature space. The re-\nclassification uses the clustering result in the first step as its\ntraining data so as to avoid the use of separate training data\nrequired  by  most  classification  algorithms.  Experiments\nconducted on the WePS-2 dataset show that the algorithm based\non key phrases is effective in improving the WPD performance."}
{"Title": "Exploiting Various Implicit Feedback for Collaborative\nFiltering", "Abstract": "So far, many researchers have worked on recommender sys-\ntems using users\u2019 implicit feedback, since it is difficult to\ncollect explicit item preferences in most applications. Ex-\nisting researches generally use a pseudo-rating matrix by\nadding up the number of item consumption; however, this\nna\u00a8\u0131ve approach may not capture user preferences correctly\nin that many other important user activities are ignored. In\nthis paper, we show that users\u2019 diverse implicit feedbacks\ncan be significantly used to improve recommendation accu-\nracy. We classify various users\u2019 behaviors (e.g., search item,\nskip, add to playlist, etc.) into positive or negative feedback\ngroups and construct more accurate pseudo-rating matrix.\nOur preliminary experimental result shows significant po-\ntential of our approach. Also, we bring out a question to\nthe previous approaches, aggregating item usage count into\nratings."}
{"Title": "The Effect of Links on Networked User Engagement", "Abstract": "In the online world, user engagement refers to the phenom-\nena associated with being captivated by a web application\nand wanting to use it longer and frequently. Nowadays,\nmany providers operate multiple content sites, very differ-\nent from each other. Due to their extremely varied content,\nthese are usually studied and optimized separately. How-\never, user engagement should be examined not only within\nindividual sites, but also across sites, that is the entire con-\ntent provider network. In previous work, we investigated\nnetworked user engagement, by defining a global measure of\nengagement that captures the effect that sites have on the\nengagement on other sites within the same browsing ses-\nsion. Here, we look at the effect of links on networked user\nengagement, as these are commonly used by online content\nproviders to increase user engagement."}
{"Title": "Investigating Bias in Traditional Media through Social\nMedia", "Abstract": "It is often the case that traditional media provide coverage\nof a news event on the basis of journalists\u2019 viewpoints - a\nproblem termed in the literature as media bias. On the\nother hand social media have given birth to an alternative\nparadigm of journalism known as \u201ccitizen journalism\u201d. We\ntake advantage of citizen journalism to detect the bias in\ntraditional media and propose a simple model for empirical\nmeasurement of media bias."}
{"Title": "Filtering and Ranking Schemes for Finding Inclusion\nDependencies on the Web", "Abstract": "This paper addresses theproblem offinding inclusiondependencies\non the Web. In our approach, we enumerate pairs of HTML/XML\nelements that possibly represent inclusion dependencies and then\nrank the results for verification. This paper focuses on the chal-\nlenges in the finding and ranking processes."}
{"Title": "Exploiting Shopping and Reviewing Behavior to Re-score\nOnline Evaluations", "Abstract": "Analysis to product reviews has attracted great attention\nfrom both academia and industry. Generally the evaluation\nscores of reviews are used to generate the average scores\nof products and shops for future potential users. However,\nin the real world, there is the inconsistency problem be-\ntween the evaluation scores and review content, and some\ncustomers do not give out fair reviews. In this work, we\nfocus on detecting the credibility of customers by analyzing\nonline shopping and review behaviors, and then we re-score\nthe reviews for products and shops. In the end, we evaluate\nour algorithm based on the real data set from Taobao, the\nbiggest E-commerce site in China."}
{"Title": "Information Cascades in Social Media in Response to a\nCrisis: a Preliminary Model and a Case Study", "Abstract": "The focus of this paper is on demonstrating how a model of\nthe diffusion of actionable information can be used to study\ninformation cascades on Twitter that are in response to an\nactual crisis event, and its concomitant alerts and warning\nmessages from emergency managers. We will: identify the\ntypes of information requested or shared during a crisis situ-\nation; show how messages spread among the users on Twit-\nter including what kinds of information cascades or patterns\nare observed; and note what these patterns tell us about in-\nformation flow and the users. We conclude by noting that\nemergency managers can use this information to either fa-\ncilitate the spreading of accurate information or impede the\nflow of inaccurate or improper messages."}
{"Title": "User Community Reconstruction using Sampled\nMicroblogging Data", "Abstract": "User community recognition in social media services is important\nto identify hot topics or users' interests and concerns in a timely\nway when a disaster has occurred. In microblogging services,\nmany short messages are posted every day and some of them\nrepresent replies or forwarded messages between users. We\nextract such conversational messages to link the users as a user\nnetwork and regard the strongly-connected components in the\nnetwork as indicators of user communities. However, using all of\nthe microblog data for user community extraction is too costly\nand requires too much storage space when decomposing strongly-\nconnected components. In contrast, using sampled data may miss\nsome user connections and thus divide one user community into\npieces. In this paper, we propose a method for user community\nreconstruction using the lexical similarity of the messages and the\nuser\u2019s link information between separate communities."}
{"Title": "Towards Situational Pattern Mining\nfrom Microblogging Activity", "Abstract": "Many  useful  patterns  can  be  derived  from  analyzing\nmicroblogging behavior at different scales (individual and social\ngroup). In this paper, we derive patterns relating to spatio-\ntemporal traffic flow, visit regularity, content and social ties as\nthey relate to an individual\u2019s activities in an urban environment\n(e.g., New York City). We also demonstrate, through an example,\nmethods for reasoning about the activities, locations and group\nstructures that may underlie the microblogging messages in the\naforementioned context of mining situation patterns. These\nindividual and group situational patterns may be very crucial\nwhen planning for disruptions and organized response."}
{"Title": "Mining Conversations of\nGeographically Changing Users", "Abstract": "In recent disaster events, social media has proven to be an effective\ncommunication tool for affected people. The corpus of generated\nmessages contains valuable information about the situation, needs,\nand locations of victims. We propose an approach to extract sig-\nnificant aspects of user discussions to better inform responders and\nenable an appropriate response.\nThe methodology combines location based division of users to-\ngether with standard text mining (term frequency inverse document\nfrequency) to identify important topics of conversation in a dy-\nnamic geographic network. We further suggest that both topics and\nmovement patterns change during a disaster, which requires identi-\nfication of new trends. When applied to an area that has suffered a\ndisaster, this approach can provide \u2018sensemaking\u2019 through insights\ninto where people are located, where they are going and what they\ncommunicate when moving."}
{"Title": "Characterization of Social Media Response to Natural\nDisasters", "Abstract": "Online social networking websites such as Twitter and Face-\nbook often serve a breaking-news role for natural disasters:\nthese websites are among the first ones to mention the news,\nand because they are visited by millions of users regularly\nthe websites also help communicate the news to a large mass\nof people. In this paper, we examine how news about these\ndisasters spreads on the social network. In addition to this,\nwe also examine the countries of the Tweeting users. We\nexamine Twitter logs from the 2010 Philippines typhoon,\nthe 2011 Brazil flood and the 2011 Japan earthquake. We\nfind that although news about the disaster may be initiated\nin multiple places in the social network, it quickly finds a\ncore community that is interested in the disaster, and has\nlittle chance to escape the community via social network\nlinks alone. We also find evidence that the world at large\nexpresses concern about such largescale disasters, and not\njust countries geographically proximate to the epicenter of\nthe disaster. Our analysis has implications for the design of\nfund raising campaigns through social networking websites."}
{"Title": "Rumor Spreading and Inoculation of Nodes in Complex\nNetworks", "Abstract": "Over the Internet or on social networks rumors can spread\nand can affect the society in disaster. The question one\nasks about this phenomenon is that whether these rumors\ncan be suppressed using suitable mechanisms. One of the\npossible solutions is to inoculate a certain fraction of nodes\nagainst rumors. The inoculation can be done randomly or in\ntargeted fashion. In this paper, small world network model\nhas been used to investigate the efficiency of inoculation. It\nhas been found that if average degree of small world network\nis small than both inoculation methods are successful. When\naverage degree is large, neither of these methods are able\nto stop rumor spreading. But if acceptability of rumor is\nreduced along with inoculation, the rumor spreading can be\nstopped even in this case.The proposed hypothesis has been\nverified using simulation experiments."}
{"Title": "Bursty Event Detection from Text Streams for Disaster\nManagement", "Abstract": "In this paper, an approach to automatically identifying bursty\nevents from multiple text streams is presented. We inves-\ntigate the characteristics of bursty terms that appear in\nthe documents generated from text streams, and incorpo-\nrate those characteristics into a term weighting scheme that\ndistinguishes bursty terms from other non-bursty terms. Ex-\nperimental results based on the news corpus show that our\napproach outperforms the existing alternatives in extracting\nbursty terms from multiple text streams. The proposed re-\nsearch is expected to contribute to increasing the situational\nawareness of ongoing events particularly when a natural or\neconomic disaster occurs."}
{"Title": "Automatic Sub-Event Detection in Emergency\nManagement Using Social Media", "Abstract": "Emergency management is about assessing critical situa-\ntions, followed by decision making as a key step. Clearly,\ninformation is crucial in this two-step process. The tech-\nnology of social (multi)media turns out to be an interesting\nsource for collecting information about an emergency situa-\ntion. In particular, situational information can be captured\nin form of pictures, videos, or text messages. The present\npaper investigates the application of multimedia metadata\nto identify the set of sub-events related to an emergency\nsituation. The used metadata is compiled from Flickr and\nYouTube during an emergency situation, where the iden-\ntification of the events relies on clustering. Initial results\npresented in this paper show how social media data can be\nused to detect different sub-events in a critical situation."}
{"Title": "SocialEMIS: Improving Emergency Preparedness through\nCollaboration", "Abstract": "The definition of the contingency plan during the prepared-\nness phase holds a crucial role in emergency management.\nA proper emergency response, indeed, requires the imple-\nmentation of a contingency plan that can be accurate only\nif different people with different skills are involved. The goal\nof this paper is to introduce SocialEMIS, a first prototype\nof a tool that supports the collaborative definition of contin-\ngency plans. Although the current implementation is now\nfocused on the role of the emergency operators, the accuracy\nof the plan will also take advantage of information coming\nfrom the citizens in future releases. Moreover, the contin-\ngency plans defined with SocialEMIS represent a knowledge\nbase for defining other contingency plans."}
{"Title": "Emergency Situation Awareness from Twitter for Crisis\nManagement", "Abstract": "This paper describes ongoing work with the Australian Gov-\nernment to detect, assess, summarise, and report messages\nof interest for crisis coordination published by Twitter. The\ndeveloped platform and client tools, collectively termed the\nEmergency Situation Awareness - Automated Web Text Min-\ning (ESA-AWTM) system, demonstrate how relevant Twit-\nter messages can be identified and utilised to inform the\nsituation awareness of an emergency incident as it unfolds.\nA description of the ESA-AWTM platform is presented\ndetailing how it may be used for real life emergency manage-\nment scenarios. These scenarios are focused on general use\ncases to provide: evidence of pre-incident activity; near-real-\ntime notification of an incident occurring; first-hand reports\nof incident impacts; and gauging the community response\nto an emergency warning. Our tools have recently been de-\nployed in a trial for use by crisis coordinators."}
{"Title": "MECA: Mobile Edge Capture and Analysis Middleware for\nSocial Sensing Applications", "Abstract": "In this paper, we propose and develop MECA, a common\nmiddleware infrastructure for data collection from mobile\ndevices in an efficient, flexible, and scalable manner. It pro-\nvides a high level abstraction of phenomenon such that ap-\nplications can express diverse data needs in a declarative\nfashion. MECA coordinates the data collection and primi-\ntive processing activities, so that data can be shared among\napplications. It addresses the inefficiency issues in the cur-\nrent vertical integration approach. We showcase the benefits\nof MECA by means of a disaster management application."}
{"Title": "The Use of Social Media within the Global Disaster Alert\nand Coordination System (GDACS)", "Abstract": "The Global Disaster Alert and Coordination System (GDACS)\ncollects near real-time hazard information to provide global multi-\nhazard disaster alerting for earthquakes, tsunamis, tropical cyclones,\nfloods and volcanoes. GDACS alerts are based on calculations from\nphysical disaster parameters and used by emergency responders. In\n2011, the Joint Research Centre (JRC) of the European Commission\nstarted exploring if and how social media could be an additional\nvaluable data source for international disaster response. The\nquestion is if awareness of the situation after a disaster could be\nimproved by the use of social media tools and data. In order to\nexplore this, JRC developed a Twitter account and Facebook page\nfor the dissemination of GDACS alerts, a Twitter parser for the\nmonitoring of information and a mobile application for information\nexchange. This paper presents the Twitter parser and the\nintermediate results of the data analysis which shows that the\nparsing of Twitter feeds (so-called tweets) can provide important\ninformation about side effects of disasters, on the perceived impact\nof a hazard and on the reaction of the affected population. The most\nimportant result is that impact information on collapsed buildings\nwere detected through tweets within the first half an hour after an\nearthquake occurred and before any mass media reported the\ncollapse."}
{"Title": "Evaluating the Impact of Incorporating Information from\nSocial Media Streams in Disaster Relief Routing", "Abstract": "In this paper, we describe a model that can be used to evaluate the\nimpact of using imperfect information when routing supplies for\ndisaster relief. Using two objectives, maximizing the population\nsupported, and minimizing response time, we explore the potential\ntradeoffs (e.g. more information, but possibly less accurate) of\nusing information from social media streams to inform routing\nand resource allocation decisions immediately after a disaster."}
{"Title": "Mass and Social Media Corpus Analysis\nafter the 2011 Great East Japan Earthquake", "Abstract": "In this paper, we outline our analysis of mass media and\nsocial media as used for disaster management. We looked\nat the di?erences among multiple sub-corpuses to ?nd rel-\natively unique keywords based on chronologies, geographic\nlocations, or media types. We are currently analyzing a mas-\nsive corpus collected from Internet news sources and Twitter\nafter the Great East Japan Earthquake"}
{"Title": "Social Media and SMS in the Haiti Earthquake", "Abstract": "We describe some first results of an empirical study describing\nhow social media and SMS were used in coordinating\nhumanitarian relief after the Haiti Earthquake in January 2010.\nCurrent information systems for crisis management are\nincreasingly incorporating information obtained from citizens\ntransmitted via social media and SMS. This information proves\nparticularly useful at the aggregate level. However it has led to\nsome problems: information overload and processing difficulties,\nvariable speed of information delivery, managing volunteer\ncommunities, and the high risk of receiving inaccurate or incorrect\ninformation."}
{"Title": "Social Web in Disaster Archives", "Abstract": "Preserving social Web datasets is a crucial part of research\nwork for disaster management based on information from\nsocial media. This paper describes the Michinoku Shinroku-\nden disaster archive project, mainly dedicated to archiving\ndata from the 2011 Great East Japan Earthquake and its\naftermath. Social websites should of course be part of this\narchive. We discuss issues in archiving social websites for\nthe disaster management research communities and intro-\nduce our vision for Michinoku Shinrokuden"}
{"Title": "Extraction of Onomatopoeia Used for Foods from Food\nReviews and Its Application to Restaurant Search", "Abstract": "Onomatopoeia is widely used in food reviews about food or\nrestaurants. In this paper, we propose and evaluate a method to\nextract onomatopoeia including unknown ones automatically from\nfood reviews sites. From the evaluation result, we found that we can\nextract onomatopoeia for specific foods with more than 46 %\nprecision; we find 18 unknown onomatopoeia, i.e. not registered in\nan existing onomatopoeia dictionary, in 62 extracted onomatopoeia.\nIn addition, we propose a system that can present the user with a list\nof onomatopoeia specific to a restaurant she is interested in. The\nevaluation results indicate that an intuitive restaurant search can be\ndone via a list of onomatopo"}
{"Title": "Solution Mining for Specific Contextualised Problems:\nTowards an Approach for Experience Mining", "Abstract": "In this paper we describe the task of automated mining for\nsolutions to highly specific problems. We do so under the\npremise of mapping the split view on context, introduced\nby Br\u00b4 ezillon and Pomerol, onto three different levels of ab-\nstraction of a problem domain. This is done to integrate the\nnotion of activity or focus and its influence on the context\ninto the mining for a solution. We assume that a problem\u2019s\ncontext describes key characteristics to be decisive criteria\nin the mining process to mine successful solutions for it.\nWe further detail on the process of a chain of sub prob-\nlems and their foci adding up to a meta problem solution\nand how this can used to mine for such solutions. Through\na guiding example we introduce basic steps of the solution\nmining process and common aspects we deem interesting to\nbe analysed closer in upcoming research on solution min-\ning. We further examine the possible integration of these\nnewly established outlines for automatic solution mining for\nhighly specific problems into a Seasalt exp , a currently de-\nveloped architecture for explanation-aware extraction and\ncase-based processing of experiences from Internet commu-\nnities. We thereby gained first insights in issues occurring\nwhile trying to integrate automatic solution mining."}
{"Title": "Extraction of Procedural Knowledge from the Web", "Abstract": "User generated Web content includes large amounts of proce-\ndural knowledge (also called how to knowledge). This paper\nis on a comparison of two extraction methods for proce-\ndural knowledge from the Web. Both methods create work-\nflow representations automatically from text with the aim to\nreuse the Web experience by reasoning methods. Two vari-\nants of the workflow extraction process are introduced and\nevaluated by experiments with cooking recipes as a sample\ndomain. The first variant is a term-based approach that in-\ntegrates standard information extraction methods from the\nGATE system. The second variant is a frame-based ap-\nproach that is implemented by means of the SUNDANCE\nsystem. The expert assessment of the extraction results\nclearly shows that the more sophisticated frame-based ap-\nproach outperforms the term-based approach of automated\nworkflow extraction."}
{"Title": "Collecting, Reusing and Executing Private Workflows on\nSocial Network Platforms", "Abstract": "We propose a personal workflow management service as part of a\nsocial network that enables private users to construct personal\nworkflows according to their specific needs and to keep track of\nthe workflow execution. Unlike traditional workflows, such\npersonal workflows aim at supporting processes that contain\npersonal tasks and data. Our proposal includes a process-oriented\ncase-based reasoning approach to support private users to obtain\nan appropriate personal workflow through sharing and reuse of\nrespective experience."}
{"Title": "Contextual Trace-Based Video Recommendations", "Abstract": "People like creating their own videos by mixing various contents.\nMany applications allow us to generate video clips by merging\ndifferent media like videos clips, photos, text and sounds. Some of\nthese applications enable us to combine online content with our own\nresources. Given the large amount of content available, the problem\nis to quickly find content that truly meet our needs. This is when\nrecommender systems come in. In this paper, we propose an\napproach for contextual video recommendations based on a Trace-\nBased Reasoning approach."}
{"Title": "Learning from Users\u2019 Querying Experience on Intranets", "Abstract": "Query recommendation is becoming a common feature of\nweb search engines especially those for Intranets where the\ncontext is more restrictive. This is because of its utility for\nsupporting users to find relevant information in less time by\nusing the most suitable query terms. Selection of queries for\nrecommendation is typically done by mining web documents\nor search logs of previous users. We propose the integration\nof these approaches by combining two models namely the\nconcept hierarchy, typically built from an Intranet\u2019s docu-\nments, and the query flow graph, typically built from search\nlogs. However, we build our concept hierarchy model from\nterms extracted from a subset (training set) of search logs\nsince these are more representative of the user view of the\ndomain than any concepts extracted from the collection. We\nthen continually adapt the model by incorporating query re-\nfinements from another subset (test set) of the user search\nlogs. This process implies learning from or reusing previ-\nous users\u2019 querying experience to recommend queries for a\nnew but similar user query. The adaptation weights are ex-\ntracted from a query flow graph built with the same logs.\nWe evaluated our hybrid model using documents crawled\nfrom the Intranet of an academic institution and its search\nlogs. The hybrid model was then compared to a concept hi-\nerarchy model and query flow graph built from the same col-\nlection and search logs respectively. We also tested various\nstrategies for combining information in the search logs with\nrespect to the frequency of clicked documents after query\nrefinement. Our hybrid model significantly outperformed\nthe concept hierarchy model and query flow graph when\ntested over two different periods of the academic year. We\nintend to further validate our experiments with documents\nand search logs from another institution and devise better\nstrategies for selecting queries for recommendation from the\nhybrid model."}
{"Title": "Exploiting User Profile Information for Answer Ranking in\ncQA", "Abstract": "Answer ranking is very important for cQA services due to\nthe high variance in the quality of answers. Most existing\nworks in this area focus on using various features or em-\nploying machine learning techniques to address this prob-\nlem. Only a few of them noticed and involved user profile\ninformation in this particular task. In this work, we as-\nsume the close relationship between user profile information\nand the quality of their answers under the ground truth\nthat user information records the user behaviors and histo-\nries as a summary. Thus, we exploited the effectiveness of\nthree categories of user profile information, i.e. engagement-\nrelated, authority-related and level-related, on answer rank-\ning in cQA. Different from previous work, we only employed\nthe information which is easy to extract without any limi-\ntations, such as user privacy. Experimental results on Ya-\nhoo! Answers manner questions showed that our system by\nusing the user profile information achieved comparable or\neven better results over the state-of-the-art baseline system.\nMoreover, we found that the picture existence of a user in\ncQA community contributed more than other information\nin the answer ranking task."}
{"Title": "Analyzing and Predicting Question Quality in\nCommunity Question Answering Services", "Abstract": "Users tend to ask and answer questions in community ques-\ntion answering (CQA) services to seek information and share\nknowledge. A corollary is that myriad of questions and an-\nswers appear in CQA service. Accordingly, volumes of stud-\nies have been taken to explore the answer quality so as to\nprovide a preliminary screening for better answers. How-\never, to our knowledge, less attention has so far been paid to\nquestion quality in CQA. Knowing question quality provides\nus with finding and recommending good questions together\nwith identifying bad ones which hinder the CQA service. In\nthis paper, we are conducting two studies to investigate the\nquestion quality issue. The first study analyzes the factors\nof question quality and finds that the interaction between\naskers and topics results in the differences of question qual-\nity. Based on this finding, in the second study we propose\na Mutual Reinforcement-based Label Propagation (MRLP)\nalgorithm to predict question quality. We experiment with\nYahoo! Answers data and the results demonstrate the ef-\nfectiveness of our algorithm in distinguishing high-quality\nquestions from low-quality ones."}
{"Title": "A Classification-based Approach to\nQuestion Routing in Community Question Answering", "Abstract": "Community-based Question and Answering (CQA) services\nhave brought users to a new era of knowledge dissemination\nby allowing users to ask questions and to answer other users\u2019\nquestions. However, due to the fast increasing of posted\nquestions and the lack of an effective way to find interesting\nquestions, there is a serious gap between posted questions\nand potential answerers. This gap may degrade a CQA ser-\nvice\u2019s performance as well as reduce users\u2019 loyalty to the\nsystem. To bridge the gap, we present a new approach to\nQuestion Routing, which aims at routing questions to par-\nticipants who are likely to provide answers. We consider\nthe problem of question routing as a classification task, and\ndevelop a variety of local and global features which capture\ndifferent aspects of questions, users, and their relations. Our\nexperimental results obtained from an evaluation over the\nYahoo! Answers dataset demonstrate high feasibility of ques-\ntion routing. We also perform a systematical comparison on\nhow different types of features contribute to the final results\nand show that question-user relationship features play a key\nrole in improving the overall performance."}
{"Title": "Finding Expert Users in Community Question Answering", "Abstract": "Community Question Answering (CQA) websites provide a\nrapidly growing source of information in many areas. This\nrapid growth, while offering new opportunities, puts forward\nnew challenges. In most CQA implementations there is lit-\ntle effort in directing new questions to the right group of ex-\nperts. This means that experts are not provided with ques-\ntions matching their expertise, and therefore new matching\nquestions may be missed and not receive a proper answer.\nWe focus on finding experts for a newly posted question. We\ninvestigate the suitability of two statistical topic models for\nsolving this issue and compare these methods against more\ntraditional Information Retrieval approaches. We show that\nfor a dataset constructed from the Stackoverflow website,\nthese topic models outperform other methods in retrieving\na candidate set of best experts for a question. We also show\nthat the Segmented Topic Model gives consistently better\nperformance compared to the Latent Dirichlet Allocation\nModel."}
{"Title": "QAque: Faceted Query Expansion Techniques for\nExploratory Search using Community QA Resources", "Abstract": "Recently, query suggestions have become quite useful in web\nsearches. Most provide additional and correct terms based\non the initial query entered by users. However, query sug-\ngestions often recommend queries that differ from the user\u2019s\nsearch intentions due to different contexts. In such cases,\nfaceted query expansions and their usages are quite effi-\ncient. In this paper, we propose faceted query expansion\nmethods using the resources of Community Question An-\nswering (CQA), which is social network service (SNS) that\nshares user knowledge. In a CQA site, users can post ques-\ntions in a suitable category. Others answer them based on\nthe category framework. Thus, the CQA \u201ccategory\u201d makes\na \u201cfacet\u201d of the query expansion. In addition, the time of\nyear when the question was posted plays an important role\nin understanding its context. Thus, such seasonality creates\nanother \u201cfacet\u201d of the query expansion. We implement two-\ndimensional faceted query expansion methods based on the\nresults of the Latent Dirichlet Allocation (LDA) analysis of\nCQA resources. The question articles deriving query expan-\nsion are provided for choosing appropriate terms by users.\nOur sophisticated evaluations using actual and long-term\nCQA resources, such as \u201cYahoo! CHIEBUKURO\u201d demon-\nstrate that most parts of the CQA questions are posted in\nperiodicity and in bursts."}
{"Title": "Socio-semantic Conversational Information Access", "Abstract": "We develop an innovative approach to delivering relevant in-\nformation using a combination of socio-semantic search and\nfiltering approaches. The goal is to facilitate timely and\nrelevant information access through the medium of conver-\nsations by mixing past community specific conversational\nknowledge and web information access to recommend and\nconnect users and information together. Conversational In-\nformation Access is a socio-semantic search and recommen-\ndation activity with the goal to interactively engage people\nin conversations by receiving agent supported recommenda-\ntions. It is useful because people engage in online social dis-\ncussions unlike solitary search; the agent brings in relevant\ninformation as well as identifies relevant users; participants\nprovide feedback during the conversation that the agent uses\nto improve its recommendations."}
{"Title": "Why do you ask this? ", "Abstract": "We use Yahoo! Toolbar data to gain insights into why people\nuse Q&A sites. For this purpose we look at tens of thou-\nsands of questions asked on both Yahoo! Answers and on\nWiki Answers. We analyze both the pre-question behavior\nof users as well as their general online behavior. Using an\nexisting approach (Harper et al.), we classify questions into\n\u201cinformational\u201dvs. \u201cconversational\u201d. Finally, for a subset of\nusers on Yahoo! Answers we also integrate age and gender\ninto our analysis.\nOur results indicate that there is a one-dimensional spec-\ntrum of users ranging from \u201csocial users\u201d to \u201cinformational\nusers\u201d. In terms of demographics, we found that both younger\nand female users are more \u201csocial\u201d on this scale, with older\nand male users being more \u201cinformational\u201d.\nConcerning the pre-question behavior, users who first is-\nsue a question-related query, and especially those who do not\nclick any web results, are more likely to issue informational\nquestions than users who do not search before. Questions\nasked shortly after the registration of a new user on Yahoo!\nAnswers tend to be social and have a lower probability of\nbeing preceded by a web search than other questions.\nFinally, we observed evidence both for and against topical\ncongruence between a user\u2019s questions and his web queries."}
{"Title": "Understanding User Intent in\nCommunity Question Answering", "Abstract": "Community Question Answering (CQA) services, such as\nYahoo! Answers, are specifically designed to address the in-\nnate limitation of Web search engines by helping users obtain\ninformation from a community. Understanding the user in-\ntent of questions would enable a CQA system identify similar\nquestions, find relevant answers, and recommend potential\nanswerers more effectively and efficiently. In this paper, we\npropose to classify questions into three categories according\nto their underlying user intent: subjective, objective, and so-\ncial. In order to identify the user intent of a new question, we\nbuild a predictive model through machine learning based on\nboth text and metadata features. Our investigation reveals\nthat these two types of features are conditionally indepen-\ndent and each of them is sufficient for prediction. Therefore\nthey can be exploited as two views in co-training \u2014 a semi-\nsupervised learning framework \u2014 to make use of a large\namount of unlabelled questions, in addition to the small set\nof manually labelled questions, for enhanced question clas-\nsification. The preliminary experimental results show that\nco-training works significantly better than simply pooling\nthese two types of features together."}
{"Title": "Churn Prediction in New Users of Yahoo! Answers", "Abstract": "One of the important targets of community-based question\nanswering (CQA) services, such as Yahoo! Answers, Quora\nand Baidu Zhidao, is to maintain and even increase the num-\nber of active answerers, that is the users who provide answers\nto open questions. The reasoning is that they are the en-\ngine behind satisfied askers, which is the overall goal behind\nCQA. Yet, this task is not an easy one. Indeed, our empir-\nical observation shows that many users provide just one or\ntwo answers and then leave.\nIn this work we try to detect answerers that are about\nto quit, a task known as churn prediction, but unlike prior\nwork, we focus on new users. To address the task of churn\nprediction in new users, we extract a variety of features to\nmodel the behavior of Yahoo! Answers users over the first\nweek of their activity, including personal information, rate\nof activity, and social interaction with other users. Several\nclassifiers trained on the data show that there is a statisti-\ncally significant signal for discriminating between users who\nare likely to churn and those who are not. A detailed feature\nanalysis shows that the two most important signals are the\ntotal number of answers given by the user, closely related\nto the motivation of the user, and attributes related to the\namount of recognition given to the user, measured in counts\nof best answers, thumbs up and positive responses by the\nasker."}
{"Title": "Email between Private Use and Organizational Purpose", "Abstract": "Emails have become an eminent source of personal and\norganizational information. They are not only used for personal\ncommunication but also for the management of information and the\ncoordination of activities within organizations. Email traffic also\nexhibits the social networks existing in organizations. However, the\ncentral problem, which we still face, is how to tap this rich source\nappropriately. Main problems in this respect are the personal\ncharacter of emails (their privacy) and the mainly unstructured\ncharacter of their contents. Since these two features are essential\nsuccess factors for the use of email they cannot be simply ignored.\nMeanwhile there are various approaches to recover the hidden\ntreasure and make the contained information available to\ninformation and process management. For example, semantic or\nmining technologies play a prominent role in this attempt. The paper\ngives an overview of different strategies to make organizational use\nof emails, also touching the role of privacy."}
{"Title": "Emails as Graph: Relation Discovery in Email Archive", "Abstract": "In this paper, we present an approach for representing an email\narchive in the form of a network, capturing the communication\namong users and relations among the entities extracted from the\ntextual part of the email messages. We showcase the method on\nthe Enron email corpus, from which we extract various entities\nand a social network. The extracted named entities (NE), such as\npeople, email addresses and telephone numbers, are organized in\na graph along with the emails in which they were found. The\nedges in the graph indicate relations between NEs and represent a\nco-occurrence in the same email part, paragraph, sentence or a\ncomposite NE. We study mathematical properties of the graphs so\ncreated and describe our hands-on experience with the processing\nof such structures. Enron Graph corpus contains a few million\nnodes and is large enough for experimenting with various graph-\nquerying techniques, e.g. graph traversal or spread of activation.\nDue to its size, the exploitation of traditional graph processing\nlibraries might be problematic as they keep the whole structure in\nthe memory. We describe our experience with the management of\nsuch data and with the relation discovery among the extracted\nentities. The described experience might be valuable for\npractitioners and highlights several research challenges."}
{"Title": "Interpreting Contact Details out of E-mail Signature Blocks", "Abstract": "This paper describes a fully automated process of address book\nenrichment by means of information extraction in e-mail signature\nblocks. The main issues we tackle are signature block detection,\nnamed entites tagging, mapping with a specific person,\nstandardizing the details and auto-updating of the address book.\nWe adopted a symbolic approach for NLP modules. We describe\nhow the process was designed to handle multiple-type of errors\n(human or computer-driven) while aiming at 100% precision rate.\nLast, we tackle the question of automatic updating confronted to\nusers rights over their own data."}
{"Title": "Context-sensitive Business Process Support\nBased on Emails", "Abstract": "In many companies, a majority of business processes take place\nvia email communication. Large enterprises have the possibility\nto operate enterprise systems for a successful business process\nmanagement. However, these systems are not appropriate for\nSMEs, which are the most common enterprise type in Europe.\nThus, the European research project Commius addresses the\nspecial  needs  of  SMEs  and  characteristics  of  email\ncommunication, namely highly flexibility and unstructuredness.\nCommius turns the existing email-system into a structured process\nmanagement framework. Each incoming email is autonomously\nmatched to the corresponding business process and enhanced by\nproactive annotations. These context-sensitive annotations include\nrecommendations for the most suitable following process steps.\nAn underlying, self-adjusting recommendation model ensures\nmost appropriate recommendations by observing the actual user\nbehavior. This implies that the proposed process course is in no\nway obligatory. To provide a high degree of flexibility, any\ndeviation from the given process structure is allowed."}
{"Title": "Full-text Search in Email Archives using Social Evaluation,\nAttached and Linked Resources", "Abstract": "Emails are important tools for communication and cooperation,\nthey contain large amount of information and connections to\nknowledge and data sources. Because of this, it is very important\nto improve the efficiency of their processing. This paper describes\nan email search system which integrates full-text search with\nsocial search while processing also the attached and linked\nresources.\nThe project described in this paper is still in progress. Due to this\nfact, some proposed parts of the system are not implemented and\nalso not proven yet. The proposed equation for determining the\nsocial importance of an email has also to be tuned during the last\nphases of the development and the evaluation phase.\nThe already implemented part of the system includes content\nextraction from the email messages, attached and linked resources\nand also the textual search and social relation extraction is\nimplemented. The next phase of the development includes tuning\nof the social evaluation and it\u2019s integration with textual search."}
{"Title": "Data Gathering for a Culture Specific Approach in MIR", "Abstract": "In this paper we describe the data gathering work done within a\nlarge research project, CompMusic, which emphasizes a culture\nspecific approach in the automatic description of several world\nmusic repertoires. Currently we are focusing on the Hindustani\n(North India), Carnatic (South India) and Turkish-makam (Turkey)\nmusic traditions. The selection and organization of the data to be\nprocessed for the characterization of each of these traditions is of the\nutmost importance."}
{"Title": "Music Retagging Using Label Propagation and Robust\nPrincipal Component Analysis", "Abstract": "The emergence of social tagging websites such as Last.fm\nhas provided new opportunities for learning computational\nmodels that automatically tag music. Researchers typically\nobtain music tags from the Internet and use them to con-\nstruct machine learning models. Nevertheless, such tags are\nusually noisy and sparse. In this paper, we present a prelim-\ninary study that aims at refining (retagging) social tags by\nexploiting the content similarity between tracks and the se-\nmantic redundancy of the track-tag matrix. The evaluated\nalgorithms include a graph-based label propagation method\nthat is often used in semi-supervised learning and a robust\nprincipal component analysis (PCA) algorithm that has led\nto state-of-the-art results in matrix completion. The results\nindicate that robust PCA with content similarity constraint\nis particularly effective; it improves the robustness of tagging\nagainst three types of synthetic errors and boosts the recall\nrate of music auto-tagging by 7% in a real-world setting."}
{"Title": "Mining Microblogs to Infer Music Artist Similarity and\nCultural Listening Patterns", "Abstract": "This paper aims at leveraging microblogs to address two\nchallenges in music information retrieval (MIR), similarity\nestimation between music artists and inferring typical lis-\ntening patterns at different granularity levels (city, country,\nglobal). From two collections of several million microblogs,\nwhich we gathered over ten months, music-related informa-\ntion is extracted and statistically analyzed. We propose\nand evaluate four co-occurrence-based methods to compute\nartist similarity scores. Moreover, we derive and analyze\nculture-specific music listening patterns to investigate the\ndiversity of listening behavior around the world."}
{"Title": "Melody, Bass Line, and Harmony Representations for\nMusic Version Identification", "Abstract": "In this paper we compare the use of different musical repre-\nsentations for the task of version identification (i.e. retriev-\ning alternative performances of the same musical piece). We\nautomatically compute descriptors representing the melody\nand bass line using a state-of-the-art melody extraction al-\ngorithm, and compare them to a harmony-based descriptor.\nThe similarity of descriptor sequences is computed using a\ndynamic programming algorithm based on nonlinear time\nseries analysis which has been successfully used for version\nidentification with harmony descriptors. After evaluating\nthe accuracy of individual descriptors, we assess whether\nperformance can be improved by descriptor fusion, for which\nwe apply a classification approach, comparing different clas-\nsification algorithms. We show that both melody and bass\nline descriptors carry useful information for version identifi-\ncation, and that combining them increases version detection\naccuracy. Whilst harmony remains the most reliable musi-\ncal representation for version identification, we demonstrate\nhow in some cases performance can be improved by combin-\ning it with melody and bass line descriptions. Finally, we\nidentify some of the limitations of the proposed descriptor\nfusion approach, and discuss directions for future research."}
{"Title": "Power-Law Distribution in Encoded MFCC Frames of\nSpeech, Music, and Environmental Sound Signals", "Abstract": "Many sound-related applications use Mel-Frequency Cep-\nstral Coefficients (MFCC) to describe audio timbral content.\nMost of the research efforts dealing with MFCCs have been\nfocused on the study of different classification and clustering\nalgorithms, the use of complementary audio descriptors, or\nthe effect of different distance measures. The goal of this\npaper is to focus on the statistical properties of the MFCC\ndescriptor itself. For that purpose, we use a simple encoding\nprocess that maps a short-time MFCC vector to a dictio-\nnary of binary code-words. We study and characterize the\nrank-frequency distribution of such MFCC code-words, con-\nsidering speech, music, and environmental sound sources.\nWe show that, regardless of the sound source, MFCC code-\nwords follow a shifted power-law distribution. This implies\nthat there are a few code-words that occur very frequently\nand many that happen rarely. We also observe that the inner\nstructure of the most frequent code-words has characteris-\ntic patterns. For instance, close MFCC coefficients tend to\nhave similar quantization values in the case of music signals.\nFinally, we study the rank-frequency distributions of individ-\nual music recordings and show that they present the same\ntype of heavy-tailed distribution as found in the large-scale\ndatabases. This fact is exploited in two supervised semantic\ninference tasks: genre and instrument classification. In par-\nticular, we obtain similar classification results as the ones\nobtained by considering all frames in the recordings by just\nusing 50 (properly selected) frames. Beyond this particular\nexample, we believe that the fact that MFCC frames follow\na power-law distribution could potentially have important\nimplications for future audio-based applications."}
{"Title": "Creating a Large-Scale Searchable Digital Collection from\nPrinted Music Materials", "Abstract": "In this paper we present our work towards developing a large-\nscale web application for digitizing, recognizing (via optical\nmusic recognition), correcting, displaying, and searching printed\nmusic texts. We present the results of a recently completed\nprototype implementation of our workflow process, from\ndocument capture to presentation on the web. We discuss a\nnumber of lessons learned from this prototype. Finally, we present\nsome open-source Web 2.0 tools developed to provide essential\ninfrastructure components for making searchable printed music\ncollections available online. Our hope is that these experiences\nand tools will help in creating next-generation globally accessible\ndigital music libraries."}
{"Title": "The Million Song Dataset Challenge", "Abstract": "We introduce the Million Song Dataset Challenge: a large-\nscale, personalized music recommendation challenge, where\nthe goal is to predict the songs that a user will listen to,\ngiven both the user\u2019s listening history and full information\n(including meta-data and content analysis) for all songs. We\nexplain the taste profile data, our goals and design choices\nin creating the challenge, and present baseline results using\nsimple, off-the-shelf recommendation algorithms."}
{"Title": "Towards Minimal Test Collections for Evaluation\nof Audio Music Similarity and Retrieval", "Abstract": "Reliable evaluation of Information Retrieval systems requires\nlarge amounts of relevance judgments. Making these annotations\nis quite complex and tedious for many Music Information\nRetrieval tasks, so performing such evaluations requires too much\neffort. A low-cost alternative is the application of Minimal Test\nCollection algorithms, which offer quite reliable results while\nsignificantly reducing the annotation effort. The idea is to\nincrementally select what documents to judge so that we can\ncompute estimates of the effectiveness differences between\nsystems with a certain degree of confidence. In this paper we\nshow a first approach towards its application to the evaluation of\nthe Audio Music Similarity and Retrieval task, run by the annual\nMIREX evaluation campaign. An analysis with the MIREX 2011\ndata shows that the judging effort can be reduced to about 35% to\nobtain results with 95% confidence."}
{"Title": "Combining Usage and Content in an Online Music\nRecommendation System for Music in the Long-Tail", "Abstract": "In this paper we propose a hybrid music recommender sys-\ntem, which combines usage and content data. We describe\nan online evaluation experiment performed in real time on a\ncommercial music web site, specialised in content from the\nvery long tail of music content. We compare it against two\nstand-alone recommenders, the first system based on usage\nand the second one based on content data. The results show\nthat the proposed hybrid recommender shows advantages\nwith respect to usage- and content-based systems, namely,\nhigher user absolute acceptance rate, higher user activity\nrate and higher user loyalty."}
{"Title": "Adapting Similarity on the MagnaTagATune Database:\nEffects of Model and Feature Choices", "Abstract": "Predicting user\u2019s tastes on music has become crucial for a\ncompetitive music recommendation systems, and perceived\nsimilarity plays an influential role in this. MIR currently\nturns towards making recommendation systems adaptive to\nuser preferences and context. Here, we consider the par-\nticular task of adapting music similarity measures to user\nvoting data. This work builds on and responds to previ-\nous publications based on the MagnaTagATune dataset. We\nhave reproduced the similarity dataset presented by Stober\nand N\u00fcrnberger at AMR 2011 to enable a comparison of\napproaches. On this dataset, we compare their two-level\napproach, defining similarity measures on individual facets\nand combining them in a linear model, to the Metric Learn-\ning to Rank (MLR) algorithm. MLR adapts a similarity\nmeasure that operates directly on low-level features to the\nuser data. We compare the different algorithms, features\nand parameter spaces with regards to minimising constraint\nviolations. Furthermore, the effectiveness of the MLR algo-\nrithm in generalising to unknown data is evaluated on this\ndataset. We also explore the effects of feature choice. Here,\nwe find that the binary genre data shows little correlation\nwith the similarity data, but combined with audio features\nit clearly improves generalisation."}
{"Title": "User Profile Integration Made Easy\u2014Model-Driven\nExtraction and Transformation of Social Network Schemas", "Abstract": "User profile integration from multiple social networks is in-\ndispensable for gaining a comprehensive view on users. Al-\nthough current social networks provide access to user profile\ndata via dedicated apis, they fail to provide accurate schema\ninformation, which aggravates the integration of user pro-\nfiles, and not least the adaptation of applications in the face\nof schema evolution. To alleviate these problems, this pa-\nper presents, firstly, a semi-automatic approach to extract\nschema information from instance data. Secondly, transfor-\nmations of the derived schemas to different technical spaces\nare utilized, thereby allowing, amongst other benefits, the\napplication of established integration tools and methods.\nFinally, as a case study, schemas are derived for Facebook,\nGoogle+, and LinkedIn. The resulting schemas are analyzed\n(i) for completeness and correctness according to the docu-\nmentation, and (ii) for semantic overlaps and heterogeneities\namongst each other, building the basis for future user profile\nintegration."}
{"Title": "Multi-application Profile Updates Propagation: a Semantic\nLayer to improve Mapping between Applications", "Abstract": "In the field of multi-application personalization, several tech-\nniques have been proposed to support user modeling for\nuser data management across different applications. Many\nof them are based on data reconciliation techniques often\nimplying the concepts of static ontologies and generic user\ndata models. None of them have sufficiently investigated\ntwo main issues related to user modeling: (1) profile defini-\ntion in order to allow every application to build their own\nview of users while promoting the sharing of these profiles\nand (2) profile evolution over time in order to avoid data\ninconsistency and the subsequent loss of income for web-site\nusers and companies.\nIn this paper, we conduct work and propose separated so-\nlutions for every issue. We propose a flexible user modeling\nsystem, not imposing any fixed user model whom different\napplications should conform to, but based on the concept of\nmapping among applications (and mapping functions among\ntheir user attributes). We focus in particular on the manage-\nment of user profile data propagation, as a way to reduce the\namount of inconsistent user profile information over several\napplications.\nA second goal of this paper is to illustrate, in this context,\nthe benefit obtained by the integration of a Semantic Layer\nthat can help application designers to automatically identify\npotential user attribute mappings between applications.\nThis paper so illustrates a work-in-progress work where\ntwo complementary approaches are integrated to improve\na main goal: managing multi-application user profiles in a\nsemi-automatic manner."}
{"Title": "Personalised Placement in Networked Video", "Abstract": "Personalised video can be achieved by inserting objects into a\nvideo play-out according to the viewer's profile. Content which\nhas been authored and produced for general broadcast can take on\nadditional commercial service features when personalised either\nfor individual viewers or for groups of viewers participating in\nentertainment, training, gaming or informational activities.\nAlthough several scenarios and use-cases can be envisaged, we\nare focussed on the application of personalised product\nplacement. Targeted advertising and product placement are\ncurrently garnering intense interest in the commercial networked\nmedia industries. Personalisation of product placement is a\nrelevant and timely service for next generation online marketing\nand advertising and for many other revenue generating interactive\nservices.\nThis paper discusses the acquisition and insertion of media\nobjects into a TV video play-out stream where the objects are\ndetermined by the profile of the viewer. The technology is based\non MPEG-4 standards using object based video and MPEG-7 for\nmetadata. No proprietary technology or protocol is proposed. To\ntrade the objects into the video play-out, a Software-as-a-Service\nbrokerage platform based on intelligent agent technology is\nadopted. Agencies, libraries and service providers are represented\nin a commercial negotiation to facilitate the contractual selection\nand usage of objects to be inserted into the video play-out."}
{"Title": "A User Profile Modelling Using Social Annotations:\nA Survey", "Abstract": "As social networks are growing in terms of the number of users,\nresources and interactions; the user may be lost or unable to find\nuseful information. Social elements could avoid this disorientation\nlike the social annotations (tags) which become more and more\npopular and contribute to avoid the disorientation of the user.\nRepresenting a user based on these social annotations has showed\ntheir utility in reflecting an accurate user profile which could be\nused for a recommendation purpose. In this paper, we give a state\nof the art of characteristics of social user and techniques which\nmodel and update a tag-based profile. We show how to treat social\nannotations and the utility of modelling tag-based profiles for\nrecommendation purposes."}
{"Title": "Towards an Interoperable Device Profile Containing Rich\nUser Constraints", "Abstract": "Currently, multimedia documents can be accessed at anytime and\nanywhere with a wide variety of mobile devices, e.g., laptops,\nsmartphones, tablets. Obviously, platforms heterogeneity, user\u2019s\npreferences and context variations require documents adaptation\naccording to execution constraints, e.g., audio contents may not be\nplayed while a user is participating at a meeting. Current context\nmodeling languages do not handle such a real life user constraints.\nThey generally list multiple information values that are interpreted\nby adaptation processes in order to deduce implicitly such high-\nlevel constraints. This paper overcomes this limitation by\nproposing a novel context modeling approach based on services\nwhere context information are linked according to explicit high-\nlevel constraints. In order to validate our proposal, we have used\nSemantic Web technologies by specifying RDF profiles and\nexperiment their usage on several platforms."}
{"Title": "From Network Mining to Large Scale Business Networks", "Abstract": "The vision of Large Scale Network Analysis (LSNA) states\non large amounts of network data, which are produced by\nsocial media applications like Facebook, Twitter, and the\ncompetitive domain of biological networks as well as their\nneeds for network data extraction and analysis. That raises\ndata management challenges which are addressed by biolog-\nical, data mining and linked (web) data management com-\nmunities. So far, mainly these domains were considered\nwhen identifying research topics and measuring approaches\nand progress. We argue that an important domain, the\nBusiness Network Management (BNM), representing busi-\nness and (technical) integration data, implicitely linked and\navailable in enterprises, has been neglected. Not only do en-\nterprises need visibilities into their business networks, they\nneed ad-hoc analysis capabilities on them.\nIn this paper, we introduce BNM as domain, which comes\nwith large scale network data. We discuss how linked busi-\nness data can be made explicit by what we called Network\nMining (NM) from dynamic, heterogeneous enterprise envi-\nronments to combine it to a (cross-) enterprise linked busi-\nness data network and state on its different facets w.r.t large\nnetwork analysis and highlight challenges and opportunities."}
{"Title": "Role-Dynamics: Fast Mining of Large Dynamic Networks", "Abstract": "To understand the structural dynamics of a large-scale so-\ncial, biological or technological network, it may be useful\nto discover behavioral roles representing the main connec-\ntivity patterns present over time. In this paper, we pro-\npose a scalable non-parametric approach to automatically\nlearn the structural dynamics of the network and individual\nnodes. Roles may represent structural or behavioral pat-\nterns such as the center of a star, peripheral nodes, or bridge\nnodes that connect different communities. Our novel ap-\nproach learns the appropriate structural\u201crole\u201ddynamics for\nany arbitrary network and tracks the changes over time. In\nparticular, we uncover the specific global network dynamics\nand the local node dynamics of a technological, communi-\ncation, and social network. We identify interesting node\nand network patterns such as stationary and non-stationary\nroles, spikes/steps in role-memberships (perhaps indicating\nanomalies), increasing/decreasing role trends, among many\nothers. Our results indicate that the nodes in each of these\nnetworks have distinct connectivity patterns that are non-\nstationary and evolve considerably over time. Overall, the\nexperiments demonstrate the effectiveness of our approach\nfor fast mining and tracking of the dynamics in large net-\nworks. Furthermore, the dynamic structural representation\nprovides a basis for building more sophisticated models and\ntools that are fast for exploring large dynamic networks."}
{"Title": "Harnessing User Library Statistics for Research Evaluation\nand Knowledge Domain Visualization", "Abstract": "Social reference management systems provide a wealth of in-\nformation that can be used for the analysis of science. In this\npaper, we examine whether user library statistics can pro-\nduce meaningful results with regards to science evaluation\nand knowledge domain visualization. We are conducting\ntwo empirical studies, using a sample of library data from\nMendeley, the world\u2019s largest social reference management\nsystem. Based on the occurrence of references in users\u2019 li-\nbraries, we perform a large-scale impact factor analysis and\nan exploratory co-readership analysis. Our preliminary find-\nings indicate that the analysis of user library statistics can\nproduce accurate, timely, and content-rich results. We find\nthat there is a significant relationship between the impact\nfactor and the occurrence of references in libraries. Using\na knowledge domain visualization based on co-occurrence\nmeasures, we are able to identify two areas of topics within\nthe emerging field of technology-enhanced learning."}
{"Title": "MenuMiner: Revealing the Information Architecture of\nLarge Web Sites by Analyzing Maximal Cliques", "Abstract": "The foundation of almost all web sites' information architecture is\na hierarchical content organization. Thus information architects put\nmuch effort in designing taxonomies that structure the content in a\ncomprehensible and sound way. The taxonomies are obvious to\nhuman users from the site's system of main and sub menus. But\ncurrent methods of web structure mining are not able to extract\nthese central aspects of the information architecture. This is\nbecause they cannot interpret the visual encoding to recognize\nmenus and their rank as humans do. In this paper we show that a\nweb site's main navigation system can not only be distinguished by\nvisual features but also by certain structural characteristics of the\nHTML tree and the web graph. We have developed a reliable and\nscalable solution that solves the problem of extracting menus for\nmining the information architecture. The novel MenuMiner-\nalgorithm allows retrieving the original content organization of\nlarge-scale web sites. These data are very valuable for many\napplications, e.g. the presentation of search results. In an\nexperiment we applied the method for finding site boundaries\nwithin a large domain. The evaluation showed that the method\nreliably delivers menus and site boundaries where other current\napproaches fail."}
{"Title": "Large Scale Microblog Mining Using Distributed MB-LDA", "Abstract": "In the information explosion era, large scale data processing and\nmining is a hot issue. As microblog grows more popular,\nmicroblog services have become information provider on a web\nscale, so researches on microblog begin to focus more on its\ncontent mining than solely user\u2019s relationship analysis before.\nAlthough traditional text mining methods have been studied well,\nno algorithm is designed specially for microblog data, which\ncontain structured information on social network besides plain\ntext. In this paper, we introduce a novel probabilistic generative\nmodel MicroBlog-Latent Dirichlet Allocation (MB-LDA), which\ntakes both contactor relevance relation and document relevance\nrelation into consideration to improve topic mining in microblogs.\nThrough Gibbs sampling for approximate inference of our model,\nMB-LDA can discover not only the topics of microblogs, but also\nthe topics focused by contactors. When faced with large datasets,\ntraditional techniques on single node become less practical within\nlimited resources. So we present distributed MB-LDA in\nMapReduce framework in order to process large scale microblogs\nwith high scalability. Furthermore, we apply a performance model\nto optimize the execution time by tuning the number of mappers\nand reducers. Experimental results on actual dataset show MB-\nLDA outperforms the baseline of LDA and distributed MB-LDA\noffers an effective solution to topic mining for large scale\nmicroblogs."}
{"Title": "k-Centralities: Local Approximations of\nGlobal Measures Based on Shortest Paths", "Abstract": "A lot of centrality measures have been developed to analyze\ndifferent aspects of importance. Some of the most popular\ncentrality measures (e.g. betweenness centrality, closeness\ncentrality) are based on the calculation of shortest paths. This\ncharacteristic limits the applicability of these measures for larger\nnetworks. In this article we elaborate on the idea of bounded-\ndistance shortest paths calculations. We claim criteria for k-\ncentrality measures and we introduce one algorithm for\ncalculating both betweenness and closeness based centralities. We\nalso present normalizations for these measures. We show that k-\ncentrality  measures  are  good  approximations  for  the\ncorresponding centrality measures by achieving a tremendous\ngain of calculation time and also having linear calculation\ncomplexity \u0398(n) for networks with constant average degree. This\nallows researchers to approximate centrality measures based on\nshortest paths for networks with millions of nodes or with high\nfrequency in dynamically changing networks."}
{"Title": "Building a Role Search Engine for Social Media", "Abstract": "A social role is a set of characteristics that describe the be-\nhavior of individuals and their interactions between them\nwithin a social context. In this paper, we describe the ar-\nchitecture of a search engine for detecting roles in a social\nnetwork. Our approach, based on indexed clusters, gives the\nuser the possibility to define the roles interactively during\na search session and retrieve the users for that role in mil-\nliseconds. We found that role selection strategies based on\nselecting people deviating from the average standards pro-\nvides flexible query expressions and high quality results."}
{"Title": "Wikidata: A New Platform for Collaborative\nData Collection", "Abstract": "This year, Wikimedia starts to build a new platform for the collaborative acquisition and maintenance of\nstructured data: Wikidata. Wikidata's prime purpose is to be used within the other Wikimedia projects, like\nWikipedia, to provide well-maintained, high-quality data. The nature and requirements of the Wikimedia\nprojects require to develop a few novel, or at least unusual features for Wikidata: Wikidata will be a\nsecondary database, i.e. instead of containing facts it will contain references for facts. It will be fully\ninternationalized. It will contain inconsistent and contradictory facts, in order to represent the diversity of\nknowledge about a given entity."}
{"Title": "User Assistance for Collaborative Knowledge Construction", "Abstract": "In this paper, we study tools for providing assistance to users\nin distributed spaces. More precisely, we focus on the activ-\nity of collaborative construction of knowledge, supported by\na network of distributed semantic wikis. Assisting the users\nin such an activity is made necessary mainly by two factors:\nthe inherent complexity of the tools supporting that activ-\nity, and the collaborative nature of the activity, involving\nmany interactions between users. In this paper we focus on\nthe second aspect. For this, we propose to build an assis-\ntance tool based on users interaction traces. This tool will\nprovide a contextualized assistance by leveraging the valu-\nable knowledge contained in traces. We discuss the issue of\nassistance in our context and we show the different types of\nassistance that we intend to provide through three scenarios.\nWe highlight research questions raised by this preliminary\nstudy."}
{"Title": "Knowledge Continuous Integration Process (K-CIP)", "Abstract": "Social semantic web creates read/write spaces where users\nand smart agents collaborate to produce knowledge readable\nby humans and machines. An important issue concerns the\nontology evolution and evaluation in man-machine collabo-\nration. How to perform a change on ontologies in a social\nsemantic space that currently uses these ontologies through\nrequests ? In this paper, we propose to implement a contin-\nuous knowledge integration process named K-CIP. We take\nadvantage of man-machine collaboration to transform feed-\nback of people into tests. This paper presents how K-CIP\ncan be deployed to allow fruitful man-machine collaboration\nin the context of the WikiTaaable system."}
{"Title": "Linking Justifications in the Collaborative Semantic Web\nApplications", "Abstract": "Collaborative Semantic Web applications produce ever chang-\ning interlinked Semantic Web data. Applications that uti-\nlize these data to obtain their results should provide ex-\nplanations about how the results are obtained in order to\nensure the effectiveness and increase the user acceptance of\nthese applications. Justifications providing meta informa-\ntion about why a conclusion has been reached enable genera-\ntion of such explanations. We present an encoding approach\nfor justifications in a distributed environment focusing on\nthe collaborative platforms. We discuss the usefulness of\nlinking justifications across the Web. We introduce a vocab-\nulary for encoding justifications in a distributed environment\nand provide examples of our encoding approach."}
{"Title": "Synchronizing Semantic Stores with Commutative\nReplicated Data Types", "Abstract": "Social semantic web technologies led to huge amounts of\ndata and information being available. The production of\nknowledge from this information is challenging, and ma-\njor efforts, like DBpedia, has been done to make it reality.\nLinked data provides interconnection between this informa-\ntion, extending the scope of the knowledge production.\nThe knowledge construction between decentralized sources\nin the web follows a co-evolution scheme, where knowledge\nis generated collaboratively and continuously. Sources are\nalso autonomous, meaning that they can use and publish\nonly the information they want.\nThe updating of sources with this criteria is intimately\nrelated with the problem of synchronization, and the consis-\ntency between all the replicas managed.\nRecently, a new family of algorithms called Commutative\nReplicated Data Types have emerged for ensuring eventual\nconsistency in highly dynamic environments. In this paper,\nwe define SU-Set, a CRDT for RDF-Graph that supports\nSPARQL Update 1.1 operations."}
{"Title": "Building Consensus via a Semantic Web Collaborative\nSpace", "Abstract": "In this paper we outline the design and implementation of\nthe eDialogos Consensus process and platform to support\nwide-scale collaborative decision making. We present the\ndesign space and choices made and perform a conceptual\nalignement of the domains this space entails, based on the\nuse of the eDialogos Consensus ontology as a crystalliza-\ntion point for platform design and implementation as well\nas interoperability with existing solutions. We also present a\nmetric for calculating agreement on the issues under debate\nin the platform, incorporating argumentation structure and\nuser feedback."}
{"Title": "Improving Wikipedia with Dbpedia", "Abstract": "DBpedia is the semantic mirror of Wikipedia. DBpedia\nextracts information from Wikipedia and stores it in a se-\nmantic knowledge base. This semantic feature allows com-\nplex semantic queries, which could infer new relations that\nare missing in Wikipedia. This is an interesting source\nof knowledge to increase Wikipedia content. But, what\nis the best way to add these new relations following the\nWikipedia conventions? In this paper, we propose a path\nindexing algorithm (PIA) which takes the resulting set of a\nDBPedia query and discovers the best representative path\nin Wikipedia. We evaluate the algorithm with real data sets\nfrom DBpedia."}
{"Title": "Man-Machine Collaboration to Acquire Cooking Adaptation\nKnowledge for the TAAABLE Case-Based Reasoning\nSystem", "Abstract": "This paper shows how humans and machines can better\ncollaborate to acquire adaptation knowledge (AK) in the\nframework of a case-based reasoning (CBR) system whose\nknowledge is encoded in a semantic wiki. Automatic pro-\ncesses like the CBR reasoning process itself, or specific tools\nfor acquiring AK are integrated as wiki extensions. These\ntools and processes are combined on purpose to collect AK.\nUsers are at the center of our approach, as they are in a clas-\nsical wiki, but they will now benefit from automatic tools for\nhelping them to feed the wiki. In particular, the CBR sys-\ntem, which is currently only a consumer for the knowledge\nencoded in the semantic wiki, will also be used for producing\nknowledge for the wiki. A use case in the domain of cooking\nis given to exemplify the man-machine collaboration."}
{"Title": "Community: Issues, Definitions, and Operationalization\non the Web", "Abstract": "This paper addresses the concepts of community and online\ncommunity and discusses the physical, functional, and symbolic\ncharacteristics of a community that have formed the basis for\ntraditional definitions. It applies a four-dimensional perspective of\nspace and place (i.e., shape, structure, context, and experience) as\na framework for refining the definition of traditional offline\ncommunities and for developing a definition of online\ncommunities that can be effectively operationalized. The methods\nand quantitative measures of social network analysis are proposed\nas appropriate tools for investigating the nature and function of\ncommunities because they can be used to quantify the typically\nsubjective  social  phenomena  generally  associated  with\ncommunities."}
{"Title": "Business Session \u201cSocial Media and News\u201d", "Abstract": "The workshop also includes a business section that will focus on\naspects of Social Media in the News domain. Panelists with\nexpertise in innovation management, news provision, journalism\nand market developments will discuss some of the challenges of,\nand opportunities for, the news sector with regards to Social\nMedia. This part of the workshop is organised and brought to you\nby the SocialSensor project."}
{"Title": "Graph Embedding on Spheres and its Application\nto Visualization of Information Diffusion Data", "Abstract": "We address the problem of visualizing structure of undi-\nrected graphs that have a value associated with each node\ninto a K-dimensional Euclidean space in such a way that 1)\nthe length of the point vector in this space is equal to the\nvalue assigned to the node and 2) nodes that are connected\nare placed as close as possible to each other in the space\nand nodes not connected are placed as far apart as possible\nfrom each other. The problem is reduced to K-dimensional\nspherical embedding with a proper objective function. The\nexisting spherical embedding method can handle only a bi-\npartite graph and cannot be used for this purpose. The other\ngraph embedding methods, e.g., multi-dimensional scaling,\nspring force embedding methods, etc., cannot handle the\nvalue constraint and thus are not applicable, either. We\npropose a very efficient algorithm based on a power iteration\nthat employs the double-centering operations. We apply the\nmethod to visualize the information diffusion process over a\nsocial network by assigning the node activation time to the\nnode value, and compare the results with the other visu-\nalization methods. The results applied to four real world\nnetworks indicate that the proposed method can visualize\nthe diffusion dynamics which the other methods cannot and\nthe role of important nodes, e.g. mediator, more naturally\nthan the other methods."}
{"Title": "A Predictive Model for the Temporal Dynamics of\nInformation Diffusion in Online Social Networks", "Abstract": "Today, online social networks have become powerful tools\nfor the spread of information. They facilitate the rapid and\nlarge-scale propagation of content and the consequences of\nan information \u2013 whether it is favorable or not to some-\none, false or true \u2013 can then take considerable proportions.\nTherefore it is essential to provide means to analyze the\nphenomenon of information dissemination in such networks.\nMany recent studies have addressed the modeling of the\nprocess of information diffusion, from a topological point\nof view and in a theoretical perspective, but we still know\nlittle about the factors involved in it. With the assumption\nthat the dynamics of the spreading process at the macro-\nscopic level is explained by interactions at microscopic level\nbetween pairs of users and the topology of their intercon-\nnections, we propose a practical solution which aims to pre-\ndict the temporal dynamics of diffusion in social networks.\nOur approach is based on machine learning techniques and\nthe inference of time-dependent diffusion probabilities from\na multidimensional analysis of individual behaviors. Ex-\nperimental results on a real dataset extracted from Twitter\nshow the interest and effectiveness of the proposed approach\nas well as interesting recommendations for future investiga-\ntion."}
{"Title": "Targeting Online Communities to Maximise Information\nDiffusion", "Abstract": "In recent years, many companies have started to utilise on-\nline social communities as a means of communicating with\nand targeting their employees and customers. Such online\ncommunities include discussion fora which are driven by the\nconversational activity of users. For example, users may re-\nspond to certain ideas as a result of the influence of their\nneighbours in the underlying social network. We analyse\nsuch influence to target communities rather than individual\nactors because information is usually shared with the com-\nmunity and not just with individual users. In this paper,\nwe study information diffusion across communities and ar-\ngue that some communities are more suitable for maximising\nspread than others. In order to achieve this, we develop a set\nof novel measures for cross-community influence, and show\nthat it outperforms other targeting strategies on 51 weeks of\ndata of the largest Irish online discussion system, Boards.ie."}
{"Title": "Identifying Communicator Roles in Twitter", "Abstract": "Twitter has redefined the way social activities can be coordinated;\nused for mobilizing people during natural disasters, studying\nhealth epidemics, and recently, as a communication platform\nduring social and political change. As a large scale system, the\nvolume of data transmitted per day presents Twitter users with a\nproblem: how can valuable content be distilled from the back\nchatter, how can the providers of valuable information be\npromoted, and ultimately how can influential individuals be\nidentified?\nTo tackle this, we have developed a model based upon the Twitter\nmessage exchange which enables us to analyze conversations\naround specific topics and identify key players in a conversation.\nA working implementation of the model helps categorize Twitter\nusers by specific roles based on their dynamic communication\nbehavior rather than an analysis of their static friendship network.\nThis provides a method of identifying users who are potentially\nproducers or distributers of valuable knowledge"}
{"Title": "File Diffusion in a Dynamic Peer-to-peer Network", "Abstract": "Many studies have been made on diffusion in the field of\nepidemiology, and in the last few years, the development of\nsocial networking has induced new types of diffusion. In this\npaper, we focus on file diffusion on a peer-to-peer dynamic\nnetwork using eDonkey protocol. On this network, we ob-\nserve a linear behavior of the actual file diffusion. This result\nis interesting, because most diffusion models exhibit expo-\nnential behaviors. In this paper, we propose a new model\nof diffusion, based on the SI (Susceptible / Infected) model,\nwhich produces results close to the linear behavior of the ob-\nserved diffusion. We then justify the linearity of this model,\nand we study its behavior in more details."}
{"Title": "Community Cores in Evolving Networks", "Abstract": "Community structure is a key property of complex networks.\nMany algorithms have been proposed to automatically de-\ntect communities in static networks but few studies have\nconsidered the detection and tracking of communities in an\nevolving network. Tracking the evolution of a given commu-\nnity over time requires a clustering algorithm that produces\nstable clusters. However, most community detection algo-\nrithms are very unstable and therefore unusable for evolving\nnetworks. In this paper, we apply the methodology proposed\nin [14] to detect what we call community cores in evolving\nnetworks. We show that cores are much more stable than\n\u201dclassical\u201d communities and that we can overcome the dis-\nadvantages of the stabilized methods."}
{"Title": "Watch me Playing, I am a Professional:\na First Study on Video Game Live Streaming", "Abstract": "\u201cElectronic-sport\u201d(E-Sport) is now established as a new en-\ntertainment genre. More and more players enjoy stream-\ning their games, which attract even more viewers. In fact,\nin a recent social study, casual players were found to pre-\nfer watching professional gamers rather than playing the\ngame themselves. Within this context, advertising provides\na significant source of revenue to the professional players,\nthe casters (displaying other people\u2019s games) and the game\nstreaming platforms. For this paper, we crawled, during\nmore than 100 days, the most popular among such special-\nized platforms: Twitch.tv. Thanks to these gigabytes of\ndata, we propose a first characterization of a new Web com-\nmunity, and we show, among other results, that the number\nof viewers of a streaming session evolves in a predictable way,\nthat audience peaks of a game are explainable and that a\nCondorcet method can be used to sensibly rank the stream-\ners by popularity. Last but not least, we hope that this\npaper will bring to light the study of E-Sport and its grow-\ning community. They indeed deserve the attention of indus-\ntrial partners (for the large amount of money involved) and\nresearchers (for interesting problems in social network dy-\nnamics, personalized recommendation, sentiment analysis,\netc.)."}
{"Title": "Supervised Rank Aggregation Approach for Link\nPrediction in Complex Networks", "Abstract": "In this paper we propose a new topological approach for link\nprediction in dynamic complex networks. The proposed ap-\nproach applies a supervised rank aggregation method. This\nfunctions as follows: first we rank the list of unlinked nodes\nin a network at instant t according to different topological\nmeasures (nodes characteristics aggregation, nodes neigh-\nborhood based measures, distance based measures, etc). Each\nmeasure provides its own rank. Observing the network at\ninstant t + 1 where some new links appear, we weight each\ntopological measure according to its performances in pre-\ndicting these observed new links. These learned weights are\nthen used in a modified version of classical computational\nsocial choice algorithms (such as Borda, Kemeny, etc) in or-\nder to have a model for predicting new links. We show the\neffectiveness of this approach through different experimen-\ntations applied to co-authorship networks extracted from\nthe DBLP bibliographical database. Results we obtain, are\nalso compared with the outcome of classical supervised ma-\nchine learning based link prediction approaches applied to\nthe same datasets."}
{"Title": "Predicting Information Diffusion on Social Networks with\nPartial Knowledge", "Abstract": "Models of information diffusion and propagation over large\nsocial media usually rely on a Close World Assumption: in-\nformation can only propagate onto the network relational\nstructure, it cannot come from external sources, the network\nstructure is supposed fully known by the model. These as-\nsumptions are nonrealistic for many propagation processes\nextracted from Social Websites. We address the problem of\npredicting information propagation when the network diffu-\nsion structure is unknown and without making any closed\nworld assumption. Instead of modeling a diffusion process,\nwe propose to directly predict the final propagation state of\nthe information over a whole user set. We describe a gen-\neral model, able to learn predicting which users are the most\nlikely to be contaminated by the information knowing an ini-\ntial state of the network. Different instances are proposed\nand evaluated on artificial datasets."}
{"Title": "Collective Attention and the Dynamics of Group Deals", "Abstract": "We present a study of the group purchasing behavior of daily\ndeals in Groupon and LivingSocial and formulate a predic-\ntive dynamic model of collective attention for group buying\nbehavior. Using large data sets from both Groupon and\nLivingSocial we show how the model is able to predict the\nsuccess of group deals as a function of time. We find that\nGroupon deals are easier to predict accurately earlier in the\ndeal lifecycle than LivingSocial deals due to the total number\nof deal purchases saturating quicker. One possible explana-\ntion for this is that the incentive to socially propagate a deal\nis based on an individual threshold in LivingSocial, whereas\nin Groupon it is based on a collective threshold which is\nreached very early. Furthermore, the personal benefit of\npropagating a deal is greater in LivingSocial."}
{"Title": "Social Networking Trends and Dynamics Detection\nvia a Cloud-based Framework Design", "Abstract": "Social networking media generate huge content streams, which\nleverage, both academia and developers efforts in providing\nunbiased, powerful indications of users\u2019 opinion and interests.\nHere, we present Cloud4Trends, a framework for collecting and\nanalyzing user generated content through microblogging and\nblogging applications, both separately and jointly, focused on\ncertain geographical areas, towards the identification of the most\nsignificant topics using trend analysis techniques. The cloud\ncomputing paradigm appears to offer a significant benefit in order\nto make such applications viable considering that the massive data\nsizes produced daily impose the need of a scalable and powerful\ninfrastructure. Cloud4Trends constitutes an efficient Cloud-based\napproach in order to solve the online trend tracking problem based\non Web 2.0 sources. A detailed system architecture model is also\nproposed, which is largely based on a set of service modules\ndeveloped within the VENUS-C research project to facilitate the\ndeployment of research applications on Cloud infrastructures."}
{"Title": "Effects of the Recession on Public Mood in the UK", "Abstract": "Large scale analysis of social media content allows for real\ntime discovery of macro-scale patterns in public opinion and\nsentiment. In this paper we analyse a collection of 484 mil-\nlion tweets generated by more than 9.8 million users from the\nUnited Kingdom over the past 31 months, a period marked\nby economic downturn and some social tensions. Our find-\nings, besides corroborating our choice of method for the de-\ntection of public mood, also present intriguing patterns that\ncan be explained in terms of events and social changes. On\nthe one hand, the time series we obtain show that periodic\nevents such as Christmas and Halloween evoke similar mood\npatterns every year. On the other hand, we see that a sig-\nnificant increase in negative mood indicators coincide with\nthe announcement of the cuts to public spending by the gov-\nernment, and that this effect is still lasting. We also detect\nevents such as the riots of summer 2011, as well as a pos-\nsible calming effect coinciding with the run up to the royal\nwedding."}
{"Title": "Improving News Ranking by Community Tweets", "Abstract": "Users frequently express their information needs by means of\nshort and general queries that are difficult for ranking algo-\nrithms to interpret correctly. However, users\u2019 social contexts\ncan offer important additional information about their infor-\nmation needs which can be leveraged by ranking algorithms\nto provide augmented, personalized results. Existing meth-\nods mostly rely on users\u2019 individual behavioral data such as\nclickstream and log data, but as a result suffer from data\nsparsity and privacy issues. Here, we propose a Community\nTweets Voting Model (CTVM) to re-rank Google and Yahoo\nnews search results on the basis of open, large-scale Twitter\ncommunity data. Experimental results show that CTVM\noutperforms baseline rankings from Google and Yahoo for\ncertain online communities. We propose an application sce-\nnario of CTVM and provide an agenda for further research."}
{"Title": "TwitterEcho - A Distributed Focused Crawler to Support\nOpen Research with Twitter Data", "Abstract": "Modern social network analysis relies on vast quantities of\ndata to infer new knowledge about human relations and\ncommunication. In this paper we describe TwitterEcho, an\nopen source Twitter crawler for supporting this kind of re-\nsearch, which is characterized by a modular distributed ar-\nchitecture. Our crawler enables researchers to continuously\ncollect data from particular user communities, while respect-\ning Twitter\u2019s imposed limits. We present the core modules\nof the crawling server, some of which were specifically de-\nsigned to focus the crawl on the Portuguese Twittosphere.\nAdditional modules can be easily implemented, thus chang-\ning the focus to a different community. Our evaluation of\nthe system shows high crawling performance and coverage."}
{"Title": "\"Making Sense of it All\": An Attempt to Aid Journalists\nin Analysing and Filtering User Generated Content", "Abstract": "This position paper explores how journalists can embrace new\nways of content provision and authoring, by aggregating and\nanalyzing content gathered from Social Media. Current challenges\nin the news media industry are reviewed and a new system for\ncapturing emerging knowledge from Social Media is described.\nNovel features that assist professional journalists in processing\nsheer amounts of Social Media information are presented with a\nreference to the technical requirements of the system. First\nimplementation steps are also discussed, particularly focusing in\nevent detection and user influence identification."}
{"Title": "Demo: X3DOM \u2013 Declarative (X)3D in HTML5", "Abstract": "In this demo description we present X3DOM, which is an\nopen source framework and runtime system to support the\nongoing discussion in the Web3D and W3C communities\nhow an integration of HTML5 and declarative 3D graph-\nics can look like by including X3D elements as part of the\nHTML5 DOM tree. The goal here is to have a live X3D\nscene-graph integrated into the HTML DOM, which allows\nmanipulating the 3D content by only adding, removing, or\nchanging the corresponding DOM elements. No specific plu-\ngin or plugin interface, such as the X3D-specific SAI, is re-\nquired. X3DOM also supports CSS integration as well as\nstandard HTML events like \u201conclick\u201d on 3D objects."}
{"Title": "Demo: XML3D \u2013 Interactive 3D Graphics for the Web", "Abstract": "XML3D is a extension to HTML5 to support interactive 3D\ngraphics in the Browser. XML3D is designed to integrate\nand interoperate well with ubiquitous W3C standards such\nas DOM, CSS, and others. Embedding XML3D into this\nfamily of standards brings many benefits: It allows millions\nof exiting web programmers to directly apply their existing\nknowledge also to interactive 3D graphics. The XML3D\nproposal and its implementations serve as a demonstrator\nplatform for the W3C Community Group \u201dDeclarative 3D\nfor the Web Architecture\u201d. In this demo description, we\npresent a selection of features of XML3D, that demonstrate\nthe advantages of a web-based declarative approach to 3d\ngraphics."}
{"Title": "Towards Declarative 3D in Web Architecture", "Abstract": "The recent WebGL integration in major web browser has open the\nway to many 3D applications as well as high-level libraries\ntargeting 3D content developers. While most of these libraries\nprovide solid grounds for interoperable 3D on web browsers, one\nmight wonder if their use could not be simplified both in terms of\nprocessing overhead and 3D description syntax; looking beyond\nthese issues, if there is room for a declarative 3D language for\nweb architecture, its features should be well defined to ensure its\nsuccess. In this paper, we review some use cases, some existing\ntechnologies and some drawback of existing tools in order to\nderive some requirement for the upcoming declarative 3D\nlanguage for the HTML ecosystem."}
{"Title": "Writing Effective Use Cases for the Declarative 3D\nfor the Web Architecture", "Abstract": "In this paper we present a guide for writing use cases for the\nDeclarative 3D for the Web Architecture, use cases where\nembedding 3D data in HTML using declarative approach\nprovides significant benefit. We list components of a use\ncase which we believe are essential when writing use cases\nand then we walk through a simple use case example. We\nbelieve that thanks to properly described use cases, it will\nbe much easier to deduce different required dimensions for\nthe Dec3D specification."}
{"Title": "Declarative 3D Use-Cases for galleries and marketplaces", "Abstract": "In this paper, we discuss possible Declarative 3D Use-Cases and\nRequirements. The origin of this position paper is a set of users\u2019\nrequirements, wants and needs collected as the initial part of a\nresearch project. We focused our investigations on the analysis of\nexisting implementations of galleries/marketplaces of 3D objects.\nOur project is currently in its initial phase, therefore we cannot go\nfurther in the analysis of derived technical requirements and in the\ndescription of the design phase. However, some of the technical\nrequirements can be directly available and understandable from\nthe descriptions of the use cases, others can be topics for\ndiscussion for the workshop itself."}
{"Title": "Declarative 3D Approaches for Distributed Web-based\nScientific Visualization Services", "Abstract": "Recent developments in the area of efficient web-service ar-\nchitectures and the requirement to provide applications not\njust for a small expert group lead to new approaches in\nthe field of web-based (scientific) visualization. The just\nemerging support for GPU-supported and therefore high-\nperformance 2D and 3D graphics in modern web-client im-\nplementations and standards provide new application envi-\nronments, which are especially interesting for the demands\nof scientific visualization solutions. Thus, in this paper we\npresent a web application deployment architecture that aims\nat supporting decision making processes more efficiently. We\nalso show that current approaches in the field of declarative\n3D techniques are useful for client-side rendering as well as\nfor a large number of processing and visualization aspects."}
{"Title": "Publishing the Greatest Common Denominator", "Abstract": "This paper presents our experiences creating X3DOM web\npublications of models and environments for four different\ndomains:  a  Structure  and  Form  Curriculum,  Scientific\nVisualization, Homepage Designs, and Building Visualization.\nThrough a series of six case studies across these domains, we\nhighlight several tools and content pipelines using ISO Extensible\n3D (X3D) and describe our experience publishing this content to\nX3DOM. We detail our lessons learned through these diverse use\ncases and show that a greatest common denominator exists. We\nhope these features and lessons contribute to the development of\nnative Web3D graphics that leverage the standard scene graph and\nweb-browser event systems such as the DOM."}
{"Title": "Towards Networked Linked Data-Driven Web3D\nApplications", "Abstract": "The Web of Data has grown to a size of several billion triples\nand provides human interface opportunities and challenges\nbeyond those of the traditional Web. Although Linked Data\nis now generated at a fast pace and very large scale, we ob-\nserve that browsing and visualisation of Linked Data is still\nin its infancy. In parallel to the enormous boost in Linked\nData, recent work on integrating 3D graphics capabilities\ninto the W3C technology stack provides fresh momentum\nfor an effort to extend the Web with 3D content and tech-\nnologies. In light of the timid uptake and consumption of\nLinked Data by non-technical audiences, we make the case\nfor Web3D-based user interfaces to the Web of Data and\naim at promoting synergistic research in the Web3D and\nWeb of Data communities. To that end, we describe a sce-\nnario that requires the combination of Linked Data from\ngeospatial and encyclopedic data sources and the transfor-\nmation of the combined Linked Data into a format amenable\nto Web3D rendering. Based on the scenario, we derive high-\nlevel requirements and propose a Linked Data-driven design\npattern based on REST architecture principles that satisfies\nthe requirements. Our prototypical implementation shows\nthat a combination of current Web technologies is sufficient\nto implement distributed application that ultimately arrive\nat Web3D renderings of Linked Data. Based on the exper-\niments, we identify and discuss areas which require further\nresearch."}
{"Title": "JavaScript library for audio/video timeline representation", "Abstract": "This short paper is a description of a JavaScript library developed\nfor temporal media (audio or video) navigation and segmentation\nrepresentation, using the HMTL5 specification."}
{"Title": "ACE: An Adaptive CSS Engine\nfor Web Pages and Web-based Applications", "Abstract": "ACE is a system that tailors web interfaces to the users\u2019\nbehavior without requiring end user intervention. By lever-\naging implicit interactions (e.g., tracking mouse or touch\nevents), the visual appearance of page elements is subtly\nmodified in an unsupervised and incremental manner. Such\npage elements (accessed by means of CSS selectors) and their\nalterable parts (defined as CSS properties) are both specified\nby the webmaster via JSON notation. ACE remembers the\nadapted styles for a given user, and consequently reapplies\nthem when the user returns to the website, being also able\nto populate them to other non-browsed pages that share a\nsimilar structure."}
{"Title": "WebCL for Hardware-Accelerated Web Applications", "Abstract": "Mobile devices, such as smartphones and tablets, now run full\nfeature browsers capable of handling rich media and web content.\nThe emergence of HTML5 makes the browser an ever more\nattractive platform for application developers. In addition,\nimprovements in JavaScript engines are further shrinking the\nperformance gap between native applications, typically written in\nC and C++, and web apps, those written in web-based\ntechnologies (HTML, CSS and JavaScript). However there is still\none area where native applications can show significantly better\nperformance: compute-intensive functions, such as complex\nimage and audio processing algorithms, are considered beyond the\nreach of JavaScript. This work looks at removing this last\ndeficiency from the web application platform. Specifically we\nshow how high-performance compute capabilities of multi-core\nCPUs and programmable GPUs can be made accessible to web\napplications and then discuss the standardization of this\ntechnology and its implementation for a mobile browser."}
{"Title": "WikiNext, a JavaScript Semantic Wiki", "Abstract": "In this position paper we present WikiNext 1 , a semantic wiki we\nhave been developing for eight months, written in JavaScript,\nfrom database to client code. It uses the HTTP/WebSocket\nNodeJS server, several frameworks such as NowJS for\ndistributing JavaScript objects between server and client code or\nMongoDB for persistence. WikiNext proposes a new approach\nto deal with classical problems like data storage and\nrepresentation (both for semantic data and CMS data), working\nwith semantics, including and developing small applications\nwithin the wiki, sharing objects between client code running in\nthe browser and server code, mixing HTTP asynchronous\ncommunication means with synchronous ones like web sockets,\nexploit original HTML5 features and finally use an event based\nprogrammatic style on the server side with an dedicated micro\nHTTP server."}
{"Title": "The DataTank: an Open Data adapter with semantic output", "Abstract": "The idea of Open Data states that by making data sets freely\navailable on the Internet, data owners can benefit from a\nhuge community. In this paper, we extend The DataTank\nframework, a data adapter for publishing local Open Data\nas a Web API, to produce semantic output. A data set and\nits content are identified by a unique URI, exploiting the\nREST interface and differentiating between IR and NIR.\nAlso, a data set can be requested as RDF in multiple nota-\ntions. Furthermore, ontology information can be added to\na data model, thus creating machine-understandable RDF.\nThis ontology information is made externally reusable and\nchangeable. The DataTank now produces semantic output,\nwhile the original architecture remains unchanged."}
{"Title": "The Information Workbench as a Self-Service Platform\nfor Developing Linked Data Applications", "Abstract": "The Information Workbench is a self-service platform for\ndeveloping Linked Data applications in the enterprise. Tar-\ngeting the full life-cycle of Linked Data applications, it facil-\nitates the integration and processing of Linked Data follow-\ning a Data-as-a-Service paradigm. UI development is based\non Semantic Wiki technologies, combined with a large set\nof predefined widgets for data access, navigation and ex-\nploration, visualization, analytics, as well as data mashups\nwith external data sources. In this paper, we present how\nthe Information Workbench can be used to rapidly build\nindustrial-strength Linked Data applications."}
{"Title": "LDIF - A Framework for Large-Scale Linked Data\nIntegration", "Abstract": "While the Web of Linked Data grows rapidly, the develop-\nment of Linked Data applications is still cumbersome and\nhampered due to the lack of software libraries for accessing,\nintegrating and cleansing Linked Data from the Web. In\norder to make it easier to develop Linked Data applications,\nwe provide the LDIF - Linked Data Integration Framework.\nLDIF can be used as a component within Linked Data ap-\nplications to gather Linked Data from the Web and to trans-\nlate the gathered data into a clean local target representa-\ntion while keeping track of data provenance. LDIF provides\na Linked Data crawler as well as components for accessing\nSPARQL endpoints and remote RDF dumps. It provides\nan expressive mapping language for translating data from\nthe various vocabularies that are used on the Web to a con-\nsistent, local target vocabulary. LDIF includes an identity\nresolution component which discovers URI aliases in the in-\nput data and replaces them with a single target URI based\non flexible, user-provided matching heuristics. For prove-\nnance tracking, the LDIF framework employs the Named\nGraphs data model. LDIF contains a data quality assess-\nment and a data fusion module which allow Web data to be\nfiltered according to different data quality assessment poli-\ncies and provide for fusing Web data using different conflict\nresolution methods. In order to deal with use cases of differ-\nent sizes, we provide an in-memory implementation of the\nLDIF framework as well as an RDF-store-backed implemen-\ntation and a Hadoop implementation that can be deployed\non Amazon EC2."}
{"Title": "A JavaScript RDF store and application library for linked\ndata client applications", "Abstract": "In this paper we present a pure JavaScript implementation of\nan RDF store supporting the SPARQL query language that\ncan be executed in modern browsers as well as in server\nside JavaScript platforms. We also present a declarative\nJavaScript library, built on top of the store, that makes\nit possible to build rich web clients combining the power\nof structured linked data, lightweight RDF notations like\nJSON-LD and the SPARQL query language with the dy-\nnamic nature of the DOM event model to provide a simple\ndevelopment framework appealing to general web develop-\ners with little prior knowledge of the semantic web stack of\ntechnologies."}
{"Title": "Enriching the web with CSS Filters", "Abstract": "Filters in the web world are not a novelty, as they were\nalready available for SVG content. The ability of applying\nfilters on various SVG elements provided the developers with\nsophisticated rendered effects. With CSS Filters, it will be\nto be possible to style HTML elements by using filter ef\u00ad\nfects. By default, the CSS Filters offers a good selection\nof predefined filters (like blur, sepia, grayscale) which allow\ndevelopers to quickly make their HTML application more\nawesome. For the ones who want more control on how a\nfilter should apply to a HTML element, CSS Shaders are a\nway as they provide the flexibility and expressivity needed\nto created arbitrary effects. This paper will look at how\nCSS Filters and CSS Shaders works and how can be used to\ncreate unique HTML content."}
{"Title": "Enabling on-the-fly Video Shot Detection on YouTube", "Abstract": "Video shot detection is the processor-intensive task of split-\nting a video into continuous shots, with hard or soft cuts as\nthe boundaries. In this paper, we present a client-side on-\nthe-fly approach to this challenge based on modern HTML5-\nenabled Web APIs. We show how video shot detection can\nbe seamlessly embedded into video platforms like YouTube\nusing browser extensions. Once a video has been split into\nshots, shot-based video navigation gets enabled and more\nfine-grained playing statistics can be created."}
{"Title": "Let Google Index Your Media Fragments", "Abstract": "Current multimedia applications in Web 2.0 have generated\na massive amount of multimedia resources, but most search\nresults for multimedia resources still focus on the whole re-\nsource level. Media fragments expose the inside content of\nmultimedia resources for annotations, but they are yet fully\nexplored and indexed by major search engines. W3C has\npublished Media Fragment 1.0 as a standard way to describe\nmedia fragments on the Web. In this proposal, we make use\nof Google\u2019s Ajax Application Crawler to index media frag-\nments represented by Media Fragment URIs. Each media\nfragment with related annotations will have an individual\nsnapshot page, which could be indexed by the crawler. Ini-\ntial evaluation has shown that the snapshot pages are suc-\ncessfully fetched by Googlebot and we are expecting more\nmedia fragments to be indexed using this method, so that\nthe search for multimedia resources would be more efficient."}
{"Title": "Visualizing Large Image Datasets in 3D Using WebGL and\nMedia Fragments", "Abstract": "The recent standardization of WebGL opened new possi-\nbilities for graphically-intensive web-based applications. In\nthis paper, we show how we can interactively visualize very\nlarge texture datasets (in the order of gigapixels) on arbi-\ntrary 3D geometry using WebGL and JavaScript. Our re-\nsults show that real-time performance can be achieved on\ncurrent-generation hardware and browsers."}
{"Title": "API Blender: A Uniform Interface to Social Platform APIs", "Abstract": "With the growing success of the social Web, most Web devel-\nopers have to interact with at least one social Web platform,\nwhich implies studying the related API specifications. These\nare often only informally described, may contain errors, lack\nharmonization, and generally speaking make the developer\u2019s\nwork difficult. Most attempts to solve this problem, propos-\ning formal description languages for Web service APIs, have\nhad limited success outside of B2B applications; we believe it\nis due to their top-down nature. In addition, a programmer\ndealing with one or several of these APIs has to deal with a\nnumber of related tasks such as data integration, requests\nchaining, or policy management, that are cumbersome to\nimplement. Inspired by the SPORE project, we present\nAPI Blender, an open-source solution to describe, interact\nwith, and integrate the most common social Web APIs. In\nthis perspective, we first introduce two new lightweight de-\nscription formats for requests and services and demonstrate\ntheir relevance with respect to current platform APIs. We\npresent our Python implementation of API Blender and its\nfeatures regarding authentication, policy management and\nmulti-platform data integration."}
{"Title": "Better Web Development with WebKit Remote Debugging", "Abstract": "The WebKit Remote Debugging API can be used to build custom\ntools, such as Web Development IDEs to aid in web design and\ndevelopment. In this presentation, source code walkthroughs and\ndemos are presented to highlight the power of this API and show\nhow it can be used in one\u2019s own tools - 1) Pausing the debugger\nwhen an uncaught exception is thrown, 2) Inspecting the computed\nstyle of a node that is visually selected by the user. Developers are\nencouraged to create their own tools based on this API."}
{"Title": "How to Run your Favorite Language in Web Browsers", "Abstract": "This paper is a concise guide for developers who want to\nport an existing language to Web browsers, about what to\ndo and what not to. It is based on the various experiments\nthat have been led in the OCaml language community. In\nparticular, it exhibits how reusing the underlying virtual\nmachine and bytecode associated to the language can come\nof great help in this task."}
{"Title": "Client-server Web applications with Ocsigen", "Abstract": "The Ocsigen framework offers a new way to develop sophis-\nticated client-server Web applications. It makes it possible\nto write as a single program both the server and client sides\nof a Web application, thus simplifying a lot communications\nand data transfers, and avoiding code duplications. It also\nproposes a wide set of high level concepts to program tra-\nditional Web interactions in a very concise way while mix-\ning them seamlessly with client side features. The use of a\npowerful type system improves a lot the reliability of pro-\ngrams, reducing debugging time, and making the code easier\nto maintain."}
{"Title": "The Kasabi Information Marketplace", "Abstract": "Publishing and consuming structured data on the Web is\nbecoming more and more common across domains as var-\nied as the public sector, the media, cultural institutions, the\nmanufacturing industry or retailers. Kasabi, an online data\nmarket based on linked data principles, offers data publish-\ners an easy way to publish, link and monetise data, while\ngiving developers of data-centric applications access to this\ndata in different formats and through a number of different\ninterfaces. This short paper introduces Kasabi and gives an\noverview of its capabilities and the functionality it offers to\ndevelopers through its APIs."}
{"Title": "Decoupling Content Management", "Abstract": "Traditional content management systems (CMS) are following a\nmonolithic architecture. If they wouldn't, one could exchange UI\ncomponents much easier and CMS developers could share many\nof the front-end components, also divide the effort and multiply\nquality. Our approach to decouple content management is to use\nRDFa enhanced templating and Backbone.js as a mid-layer to\ninstantiate a rich editor and other interactive widgets when the\nuser would like to edit the content. We show components and\nprototypes in development."}
{"Title": "A Mobile Learning Scenario improvement for HST Inquiry\nBased learning", "Abstract": "We investigate how the three technologies, social media, mobile /\npervasive learning and semantic web, may enhance Inquiry-Based\nScience Teaching (IBST) approaches and digital literacy. IBST\nmay be defined by engaging students in: i) authentic and problem-\nbased activities, ii) experimental procedures, iii) self regulated\nlearning  sequences,  iv)  discursive  argumentation  and\ncommunication with peers. We analyzed the benefits of each\ntechnology and their combination. From an existing IBST\nlearning scenario, we propose an innovative one based on the\nconvergence of the three following technologies: social media,\nmobile / pervasive learning and semantic web."}
{"Title": "Actualizing Progressive Learning - Discovering Resolute\nParadigm In Social World", "Abstract": "Extended cognition is now a reality with rise of social web. Smart\ndevices and emerging collective intelligence is aiding us to take\npart in tasks much bigger than we can naturally handle. In the age\nof digital natives, learning is the most affected process by this\nphenomenon, and has created a void in this space to rethink the\nmodel to suit the generation of web. Current educational model is\nnot future proof as its creating more autonomous problem solvers,\nwhile future demands high caliber people to collaborate on\ninterdisciplinary problems with potential global impact. Primary\nmotto of this learning model is to develop critical thinking and\ncontinuous learning among individuals. Can such process be\nengineered in the first place? If ones goal is to attain the formal\nderivatives, what are the possible ways to realize it? Current paper\ndiscusses along with the generic web learning trends, a model\nbased on Rhizomatic learning and contextual relations generated\nfrom similarity sets of social networks. This unique approach\nemphasizes more on distance among the similar sets to promote\nmaximum diversity in the learning flows. Also leverages our\nearlier work, a feedback framework designed to judge diverse\nfacets of a personality from interactions on the web."}
{"Title": "Web Technologies and Tertiary IT Education: A Case\nStudy", "Abstract": "The theme of the workshop on Emerging Web Technologies:\nFacing the Future of Education potentially covers a broad\nspectrum: schools, universities and other educational institutions;\nacademic disciplines; pedagogy; administration; innovation; and,\nmost importantly, the people involved, viz. students, teachers,\nadministrators and, at a remove, parents and society at large. It is\ntherefore necessary to define at the outset how much of this\nspectrum one wants to cover. This paper reports on efforts and\nexperience of a group of academics in computing in a university\nwho have used the Web as an emerging technology, from 1996\nonwards, specifically in terms of how it affected the delivery of\neducation at tertiary level. The paper describes initiatives that led\nto changes in curricula, pedagogy including assessment strategies\nand techniques, and administration of students and courses. In\ninnovation terms, the initiatives started at individual levels and\ngathered momentum before the developments were adopted at a\ncollective level. On the way, there were several lessons to be\nlearnt. In chronological order, creation of a faculty Web site led\nto insights in introducing new subjects at undergraduate level and\nonline delivery of educational material undertaken by a few\nacademics, the \u2018innovators\u2019. That generated pressure on others to\nemulate the effort, some of whom became \u2018early adopters\u2019,\nleading to demands from them for easy procedures to meet the\ndemands from both students and administration. The overall\nexperience over a few years forced the recognition that\nundergraduate students were ill-prepared to analyse and\nunderstand the implications of the emerging technologies. The\noutcome was the introduction of a specially designed postgraduate\nprogramme in IT with Web Engineering as a specialisation.\nPedagogically,  the  emphasis  shifted  from  quizzes  and\nexaminations to project work over a semester, at times continuing\non to another semester. New technologies were introduced as\nassignments, short projects and group projects. Teaching and\nlearning strategies dealt with practical matters such as creating\nWeb sites and Web applications, performance analysis, security\nand social and legal issues. Finally, students were encouraged to\n\u2018innovate\u2019 using newer technologies, in a fairly well-directed\nmanner by the academics. The paper reports on these initiatives\nresulting from the Web technologies and contrasts them with the\nlatest developments where student cohorts have started to create\ncontent themselves. In addition to the lessons learnt, a tentative\nconclusion is that the initiative to use emerging technologies in\nfurthering education may henceforth be led more by students\nunless specific strategies are devised by the academic world."}
{"Title": "The Influence of Teacher Created Metadata in\nOnline Resource Exchanges", "Abstract": "Online resource exchanges offer a different paradigm for how\nteachers find and select resources. As more teachers choose to go\nonline to gather resources, questions remain about what factors\ninfluence their selection of resources. Using decision heuristics\ntheory as a lens, we created a hierarchical linear model from the\ndataset of an online teacher resource exchange for a national\nteaching organization. Our specific focus was to discover what\nteacher generated resource metadata predicts number of\ndownloads. Based on our findings, there is little support to suggest\nthat the majority of users rely on a simple heuristic. We also\nfound that a high number of low ratings predict more downloads\nthan a resource with a low number of high ratings. This seemingly\nruns counterintuitive to the idea that more low ratings would\ndissuade teachers from looking at a resource."}
{"Title": "From kinetic energy to climate change: Design of\ntechnology to link school science to personal energy\nconsumption", "Abstract": "Energy sustainability is prevalent in political and popular rhetoric\nand yet energy consumption is rising. Teenagers are an important\ncategory of future energy consumers, but little is known of their\nconceptions about energy, energy saving, and energy related\nproblems. We report on a study with a group of teenagers that\nexplored their conceptions about energy and their skills in finding\ninformation about their personal energy consumption. In this\npaper we focus our discussion on the challenges in using Web\ntechnologies to support learning about complex real world issues\nlike energy consumption. An initial analysis of the data indicates\nthat teenagers struggle to grasp the complexity of problems\naround energy and to search for information on examples of their\npersonal energy consumption. We highlight that, to fulfil the\nlearning potential of Web technologies, educators must provide\nsupport both in terms of learners\u2019 initial conceptual understanding\nof the learning topic and the process of searching for information.\nMotivation is also critical if learners are to engage in the time\nintense process of searching for information and creating content.\nThese findings have implications for the design of technology\nenhanced learning experiences that build on young people\u2019s Web\ntechnology skills to support learning about complex real world\nissues."}
{"Title": "Generic Gaze Interaction Events for Web Browsers", "Abstract": "In the last decade much research has been conducted on an-\nalyzing human eye and gaze movements using eye tracking\ntechnology, not only in the fields of neuroscience, psychol-\nogy and marketing, but also in the field of human computer\ninteraction. However, no flexbile framework exists to inte-\ngrate eye tracking directly into web applications to easily\ncreate and test new interaction concepts. We have created\na JavaScript library based on the latest HTML5 Web tech-\nnology and the jQuery library to close this gap. Facilitated\nby HTML5 WebSocket, the browser directly receives gaze\ninput samples from an eye tracker, generating events that\nare similar to those of a mouse input device. Events like\ngazeOver/-Out or fixationStart/-End can be attached to any\nHTML element in the DOM tree. New custom events de-\nrived from the eye tracking data, e.g. blink or read, can\neasily be added. Using this library we have successfully\nimplemented a number of Web applications, allowing the\nusers to interact with their eyes. This paper also describes\nour gaze enabled Web-based eLearning environment. Our\nJavaScript library is used within the eLearning environment\nto capture and interpret eye gaze events for the purpose to\nsupport users in the acquisition of new knowledge."}
{"Title": "Modeling the Web : Paradigm changes and strategic scenarios", "Abstract": "In order to understand the Web  1 , we need to adopt an approach that should be as\nmuch as possible unambiguous. We believe that Informatics is the discipline of Information 2\noffering the concepts 3 for modelling Information production, transformation and consumption\nin complex, heterogeneous organisations consisting of autonomous communicating entities\ncalled agents. In the following we will argue that the two currently emerging paradigm shifts\nin Informatics: Interaction versus algorithms  4 ; and Services versus programs  5 are\nnecessary and perhaps sufficient for understanding most of the current Web phenomena, and\nfor forecasting future evolutions in Governance control but also in other subareas of Web\nScience, particularly Web Science education. At the same time, this paper shows the\nintentions of the Montpellier team to launch a Web Science curriculum based on these\nemerging paradigms for serving in the best way the trilogy of Web Science goals: understand\nwhat the Web is, engineer its future and ensure its social benefit."}
{"Title": "Challenges for Master Programs on Human-Computer\nInteraction (HCI): experience report of the M2IHM", "Abstract": "There are an increasing number of undergraduate programs in\nComputer Science that now includes teaching of Human-\nComputer Interaction (HCI). The occurrence of HCI courses in\nundergraduate programs is essential to present concepts (e.g.\nusability, accessibility, User eXperience) and techniques (e.g.\nprototyping, user interface evaluation) necessary for designing\nand developing user-centered interactive systems. However, the\nnumber of hours devoted to HCI teaching in graduate levels is\nbarely enough to make of students proper usability professionals.\nFor this very purpose, several graduate courses devoted to HCI\nhave been created in the last years, in particularly across the\nUnited States and Europe. In this position paper we report the\nexperience of creation of a master 2 program on Human-\nComputer Interaction. We present the structure and the contents\nof the M2IHM and, in particular, the place that occupies the\nteaching of Web technology in this program. We also discuss the\nevolution that occurred in the last 10 years with the M2IHM in\norder to cope with the evolution of technology and the market.\nSince September 2011, the master 2 IHM is now included in a\ntwo years master program on HCI. As no information is available\nabout its drawbacks and advantages we don\u2019t report on this two\nyears program here."}
{"Title": "Semantic Web Game Based Learning:\nAn I18n approach with Greek Dbpedia", "Abstract": "Recent web advances and progress in technology enhanced\npedagogies have outlined the important role of gaming within\neducation. The advent of Semantic Web and Linked Data, as\nwell as, the availability of initiatives and infrastructures like\nthat of DBpedia have facilitated the incorporation of gaming\nwithin educational knowledge bases. This paper presents a\nweb game that uses datasets derived from Greek DBpedia.\nIt is consisted of several different quiz types and it is used\nfor educational purposes in native speakers. The application\nhas a preliminary evaluation in primary education settings\nin north Greece. The evaluation results indicated its poten-\ntial in the learning process."}
{"Title": "Assembling and Applying an Education Graph\nbased on Learning Resources in Universities", "Abstract": "This paper introduces the notion of the education graph,\na conceptual representation of the resources and intercon-\nnections at the heart of the learning process. We present\nour latest work on the Talis Aspire family of products that,\nthrough the use of Linked Data principles and technologies,\nenables the assembly and application of a rich education\ngraph based on learning resources used in tens of UK uni-\nversities. Techniques for entity extraction and reconciliation\nacross data sources are presented, in addition to descriptions\nof recommendation generation from portions of this educa-\ntion graph."}
{"Title": "Semantic CMS and Wikis as Platforms for Linked Learning", "Abstract": "Although interoperability has always been a priority in e-learning,\nconventional Learning Management Systems are mostly geared\ntowards the Standards for Learning Objects exchange and the\nintegration among systems. The contingency for integration with\nother web applications and data is hardly foreseen. This prevents\nthem, nowadays, from being flexible to adapt to the Linked Data\nstandards emergence and the advent of Semantic Web in general,\nunless they radically change orientation. In contrast, Wikis,\nfollowed by Content Management Systems, proved to be more\nversatile in complying with the Semantic Web and Linked Data\nstandards. These advancements, together with their modular\narchitecture, turn Wikis and CMSs into a decent choice for\nmodern e-learning solutions. MediaWiki and Drupal were\ncustomized and deployed in the Aristotle University of\nThessaloniki to assess their potential in exposing the University\u2019s\nlearning resources on the Web of Linked Data, in accordance with\nthe Linked Universities Initiative. On the occasion of these two\ndeployments, a thorough comparison of their platforms\u2019 potentials\nto function as Learning Management Systems took place and is\npresented on this paper."}
{"Title": "LOD.CS.UNIPA Project:\nan experience of LOD at the University of Palermo", "Abstract": "This paper describes the LOD.CS.UNI.PA Project and its main\ngoal, the transformation process of data already available on the\nweb site of the Computer Science curricula web site at the\nUniversity of Palermo into data ready to be connected to the LOD.\nSince 1997 information about bachelor and master degrees in\nComputer Science at the University of Palermo has been\npublished on the web, and provides a reference point for students,\nteachers and researchers who have easy access to the information\nthey require. However, the users of the web are now changing;\ndata cannot be published only for human comprehension but\nintelligent devices also need access to web data and above all they\nneed to understand them. In 2006 Tim Berners Lee presented a\nstar rating system for the data available on the web. Following his\nfive star classification, the aim of the work described in this paper\nis to raise the level of data concerning degrees in Computer\nScience at Palermo University, from one star (data available on\nthe web in whatever format), to five stars (data connected to other\nLOD datasets). So far the data has been transformed to a four star\nlevel in which each item has a URL that can be dereferenced, but\nthe final objective of this project is to reach the five star level."}
{"Title": "Common vs. Expert knowledge:\nmaking the Semantic Web an educational model", "Abstract": "Model based learning uses models in order to generate\neducational resources and adapt learning paths to learners and\ntheir context. Many domain models are published on the Web\nthrough linked data, thus providing a collective knowledge base\nthat can be reused in the educational domain. However, for these\nmodels to be usable in an educational context, it should be\npossible to predict the learning context in which they can be used.\nA typical indicator of the usability of learning objects is their\ndifficulty. Predicting the difficulty of an assessment item depends\nboth on the construct, i.e., what is assessed, and on the form of the\nitem. In this paper, we present several experiments that can\nsupport the prediction of the difficulty of an assessment item\ngenerated from a linked data source and the difficulty of the\nunderlying item construct. We analyze the results of a test carried\nout with choice items (such as multiple choice questions),\ntogether with a Web mining approach, in order to provide\nindicators that the factual knowledge is common knowledge or\nexpert knowledge in a particular population. Our objective is to\nannotate the semantic models and increase their reusability in an\neducational context."}
{"Title": "Semantic Web and Linked Learning to\nSupport Workplace Learning", "Abstract": "In the last few years, the Social Web has offered new affordances\nfor how learning is conceptualized and supported. Supporting\nworkplace learning, however, faces specific challenges, some in\nparticular due to its informal, contextual and social nature. The in-\nformal nature of workplace learning requires knowledge workers\nto be supported in their self-regulatory learning processes, whilst\nthe social side draws attention to the role of collective in those\nprocesses. To address these challenges, in this paper we present\nLearn-B, a workplace learning environment. We also present how\nwe developed and applied a common ontological foundation for\nthe integration of our proposed learning services and existing\ntools in this environment."}
{"Title": "Aggregating Digital Traces into a Semantic-enriched Data\nCloud for Informal Learning", "Abstract": "Modern informal learning models require linking experiences in\nthe training environments with experiences in the real-world.\nHowever, data about real-world experiences is notoriously hard to\ncollect. Social spaces bring new opportunities to tackle this\nchallenge, supplying digital traces where people talk about their\nreal-world experiences, which can become valuable resource,\nespecially in ill-defined domains which embed multiple\ninterpretations. The paper presents a unique approach to aggregate\ncontent from social spaces into a semantic-rich data cloud to\nfacilitate informal learning in ill-defined domains. This pioneers a\nnew way to exploit digital traces about real-world experiences as\nauthentic examples in informal learning contexts. The research\neffort to date allows us to make some observations about potential\nof technology and the overall approach, as well as to draw a\nroadmap with issues for further consideration."}
{"Title": "Technical Evaluation of The mEducator 3.0 Linked Data-\nbased Environment for Sharing Medical Educational\nResources", "Abstract": "mEducator 3.0 is a content sharing approach for medical\neducation,  based  on  Linked  Data  principles.  Through\nstandardization, it enables sharing and discovery of medical\ninformation. Overall the mEducator project seeks to address the\nfollowing two different approaches, mEducator 2.0, based on web\n2.0 and ad-hoc Application Programmers Interfaces (APIs), and\nmEducator 3.0, which builds upon a collection of Semantic Web\nServices that federate existing sources of medical and Technology\nEnhanced Learning (TEL) data. The semantic mEducator 3.0\napproach It has a number of different instantiations, allowing\nflexibility and choice. At present these comprise of a standalone\nsocial  web-based  instantiation  (MetaMorphosis+)  and\ninstantiations integrated with Drupal, Moodle and OpenLabyrinth\nsystems. This paper presents the evaluation results of the\nmEducator 3.0 Linked Data based environment for sharing\nmedical educational resources and focuses on metadata\nenrichment, conformance to the requirements and technical\nperformance (of the MetaMorphosis+ and Drupal instantiations)."}
{"Title": "SemUNIT \u2013 French UNT and Linked Data", "Abstract": "Over the past 15 years, the explosion in the number of learn-\ning materials available on the Web has raised the problem of\ntheir sharing. For several years, learning resources are anno-\ntated with metadata to ease this sharing. On the other hand,\nSemantic Web and Linked Data approach provide tools to\npublish metadata in a standardized way, allowing data to\nbe shared and reused across applications, enterprises, and\ncommunity boundaries.\nIn this paper, we describe the SemUnit project, initiated\nby french higher education institutions. This project aims\nat taking advantages of Semantic Web and Linked Data to\nimprove e-learning services for a wide set of french higher\neducation institutions. We present, firstly, the ontology de-\nsigned to support the project: an OWL ontology taking into\naccount the semantics of LOM elements. Afterwards, we\npresent our architecture and some semantic services 1 ."}
{"Title": "Thinking Semantic Wikis as Learning Object Repositories", "Abstract": "Wikis have been extensively adopted in educational contexts\nbecoming repositories of potentially reusable, self-contained\nlearning units (commonly known as learning objects).\nUnfortunately, wikis lack of the packaging and metadata\nfacilities that are needed to enable effective reuse of these units.\nIn this article we propose an approach to use semantic wikis as\nlearning object repositories, we discuss its challenges and its\npotential, and we present a conceptual model and its\nimplementation based on the Semantic MediaWiki engine."}
{"Title": "Semi-Automatic Generation of Quizzes and\nLearning Artifacts from Linked Data", "Abstract": "In this position paper, we illustrate how Linked Data can\nbe effectively used in a Technology-enhanced Learning sce-\nnario. Specifically, we aim at using structured data to semi-\nautomatically generate artifacts to support learning delivery\nand assessment: natural language facts, Q&A systems and\nquizzes, also used with a gaming flavour, can be creatively\ngenerated to help teachers and learners to support and im-\nprove the learning path. Moreover, those artifacts can in\nturn be published on the Web as Linked Data, thus directly\ncontributing to make the Web a global data space also for\nlearning purposes."}
{"Title": "Exploiting the Web of Data to provide descriptions of ICT\ntools: a preliminary report about SEEK-AT-WD", "Abstract": "In order to support educators when selecting ICT tools, sev-\neral educational organizations provide ICT tool registries\nwhose functionality is limited by the data they contain.\nThese registries could reduce the cost of creating and up-\ndating their datasets obtaining ICT tool descriptions from\nthe Web of Data. However, some problems hinder the ed-\nucational consumption of these descriptions: they are not\ndescribed using an appropriate vocabulary for this specific\ndomain and registries do not discriminate which tools are\nuseful for education. SEEK-AT-WD is proposed as an in-\nfrastructure that overcomes these problems and consumes\nICT tool descriptions from the Web of Data, publishing\nthem back once they are related to educational concepts.\nThis paper discusses the challenges found when developing\nSEEK-AT-WD, and how they were solved to take advantage\nof data from the Web in the educational applications."}
{"Title": "Information Theoretic Tools for Social Media", "Abstract": "Information theory provides a powerful set of tools for dis-\ncovering relationships among variables with minimal assump-\ntions. Social media platforms provide a rich source of in-\nformation than can include temporal, spatial, textual, and\nnetwork information. What are the interesting information\ntheoretic measures for social media and how can we estimate\nthese quantities? I will discuss how measures like informa-\ntion transfer can be used to quantify how predictive some\nvariables are, e.g., how well one user\u2019s activity can predict\nanother\u2019s. I will also discuss techniques for estimating en-\ntropies even when the data are sparse, as is the case for\nspatio-temporal events, or very high-dimensional, as is the\ncase for textual information."}
{"Title": "Alleviating Data Sparsity for Twitter Sentiment Analysis", "Abstract": "Twitter has brought much attention recently as a hot research topic\nin the domain of sentiment analysis. Training sentiment classifiers\nfrom tweets data often faces the data sparsity problem partly due to\nthe large variety of short and irregular forms introduced to tweets\nbecause of the 140-character limit. In this work we propose using\ntwo different sets of features to alleviate the data sparseness prob-\nlem. One is the semantic feature set where we extract semantically\nhidden concepts from tweets and then incorporate them into classi-\nfier training through interpolation. Another is the sentiment-topic\nfeature set where we extract latent topics and the associated topic\nsentimentfromtweets, thenaugmenttheoriginalfeaturespacewith\nthese sentiment-topics. Experimental results on the Stanford Twit-\nter Sentiment Dataset show that both feature sets outperform the\nbaseline model using unigrams only. Moreover, using semantic\nfeatures rivals the previously reported best result. Using sentiment-\ntopicfeaturesachieves86.3%sentimentclassificationaccuracy, which\noutperforms existing approaches."}
{"Title": "Making Sense of Microposts at Scientific Conferences", "Abstract": "Twitter is being widely used at scientific conferences. Fol-\nlowing the microblogging stream, however, adds to the cog-\nnitive load of a conference participant. Therefore, there is a\nneed for means of extracting the most important topics from\na Twitter stream. This demo paper presents an adaptable\nsystem for detecting trends based on Twitter, and shows\nhow it can be used within the setting of a conference. Fol-\nlowing the cues of visual analytics, we use visualizations to\nshow both the temporal evolution of topics, and the relations\nbetween different topics."}
{"Title": "Extracting Unambiguous Keywords from\nMicroposts Using Web and Query Logs Data", "Abstract": "In the recent years, a new form of content type has become\nubiquitous in the web. These are small and noisy text snip-\npets, created by users of social networks such as Twitter and\nFacebook. The full interpretation of those microposts by\nmachines impose tremendous challenges, since they strongly\nrely on context. In this paper we propose a task which is\nmuch simpler than full interpretation of microposts: we aim\nto build classification systems to detect keywords that un-\nambiguously refer to a single dominant concept, even when\ntaken out of context. For example, in the context of this\ntask, apple would be classified as ambiguous whereas mi-\ncrosoft would not. The contribution of this work is twofold.\nFirst, we formalize this novel classification task that can be\ndirectly applied for extracting information from microposts.\nSecond, we show how high precision classifiers for this prob-\nlem can be built out of Web data and search engine logs,\ncombining traditional information retrieval metrics, such as\ninverted document frequency, and new ones derived from\nsearch query logs. Finally, we have proposed and evaluated\nrelevant applications for these classifiers, which were able\nto meet precision \u2265 72% and recall \u2265 56% on unambigu-\nous keyword extraction from microposts. We also compare\nthose results with closely related systems, none of which\ncould outperform those numbers."}
{"Title": "Visualizing Contextual and Dynamic Features of\nMicropost Streams", "Abstract": "Visual techniques provide an intuitive way of making sense of the\nlarge amounts of microposts available from social media sources,\nparticularly in the case of emerging topics of interest to a global\naudience, which often raise controversy among key stakeholders.\nMicropost streams are context-dependent and highly dynamic in\nnature. We describe a visual analytics platform to handle high-\nvolume micropost streams from multiple social media channels.\nFor each post we extract key contextual features such as location,\ntopic and sentiment, and subsequently render the resulting multi-\ndimensional information space using a suite of coordinated views\nthat support a variety of complex information seeking behaviors.\nWe also describe three new visualization techniques that extend\nthe original platform to account for the dynamic nature of micro-\npost streams through dynamic topography information landscapes,\nnews flow diagrams and longitudinal cross-media analyses."}
{"Title": "What makes a tweet relevant for a topic?", "Abstract": "Users who rely on microblogging search (MS) engines to find\nrelevant microposts for their queries usually follow their in-\nterests and rationale when deciding whether a retrieved post\nis of interest to them or not. While today\u2019s MS engines\ncommonly rely on keyword-based retrieval strategies, we in-\nvestigate if there exist additional micropost characteristics\nthat are more predictive of a post\u2019s relevance and interest-\ningness than its keyword-based similarity with the query. In\nthis paper, we experiment with a corpus of Twitter messages\nand investigate sixteen features along two dimensions: topic-\ndependent and topic-independent features. Our in-depth\nanalysis compares the importance of the different types of\nfeatures and reveals that semantic features and therefore an\nunderstanding of the semantic meaning of the tweets plays\na major role in determining the relevance of a tweet with\nrespect to a query. We evaluate our findings in a relevance\nclassification experiment and show that by combining differ-\nent features, we can achieve a precision and recall of more\nthan 35% and 45% respectively."}
{"Title": "Exploiting Twitter\u2019s Collective Knowledge for Music\nRecommendations", "Abstract": "Twitter is the largest source of public opinion and also con-\ntains a vast amount of information about its users\u2019 music\nfavors or listening behaviour. However, this source has not\nbeen exploited for the recommendation of music yet. In this\npaper, we present how Twitter can be facilitated for the cre-\nation of a data set upon which music recommendations can\nbe computed. The data set is based on microposts which\nwere automatically generated by music player software or\nposted by users and may also contain further information\nabout audio tracks."}
{"Title": "Understanding co-evolution of social and content\nnetworks on Twitter", "Abstract": "Social media has become an integral part of today\u2019s web and\nallows users to share content and socialize. Understanding\nthe factors that influence how users evolve over time - for ex-\nample how their social network and their contents co-evolve -\nis an issue of both theoretical and practical relevance. This\npaper sets out to study the temporal co-evolution of con-\ntent and social networks on Twitter and bi-directional in-\nfluences between them by using multilevel time series re-\ngression models. Our findings suggest that on Twitter so-\ncial networks have a strong influence on content networks\nover time, and that social network properties, such as users\u2019\nnumber of followers, strongly influence how active and in-\nformative users are. While our investigations are limited to\none small dataset obtained from Twitter, our analysis opens\nup a path towards more systematic studies of network co-\nevolution on platforms such as Twitter or Facebook. Our\nresults are relevant for researchers and social media hosts\ninterested in understanding how content-related and social\nactivities of social media users evolve over time and which\nfactors impact their co-evolution."}
{"Title": "When social bots attack: Modeling susceptibility of users\nin online social networks", "Abstract": "Social bots are automatic or semi-automatic computer pro-\ngrams that mimic humans and/or human behavior in online\nsocial networks. Social bots can attack users (targets) in on-\nline social networks to pursue a variety of latent goals, such\nas to spread information or to influence targets. Without\na deep understanding of the nature of such attacks or the\nsusceptibility of users, the potential of social media as an\ninstrument for facilitating discourse or democratic processes\nis in jeopardy. In this paper, we study data from the So-\ncial Bot Challenge 2011 - an experiment conducted by the\nWebEcologyProject during 2011 - in which three teams im-\nplemented a number of social bots that aimed to influence\nuser behavior on Twitter. Using this data, we aim to de-\nvelop models to (i) identify susceptible users among a set\nof targets and (ii) predict users\u2019 level of susceptibility. We\nexplore the predictiveness of three different groups of fea-\ntures (network, behavioral and linguistic features) for these\ntasks. Our results suggest that susceptible users tend to use\nTwitter for a conversational purpose and tend to be more\nopen and social since they communicate with many different\nusers, use more social words and show more affection than\nnon-susceptible users."}
{"Title": "Knowledge Discovery in distributed Social Web sharing\nactivities", "Abstract": "Taking into consideration the steady shift towards informa-\ntion digitisation, an increasing number of approaches are\ntargeting the unification of the user\u2019s digital \u201cPersonal In-\nformation Sphere\u201dto increase user awareness, provide single-\npoint management, and enable context-driven recommenda-\ntion. The Personal Information Sphere refers to both con-\nventional information such as semi/structured information\non the user\u2019s personal devices and online accounts, but also\nin the form of more abstract personal information such as\na user\u2019s presence and activities. Online activities consti-\ntute a rich source for mining this type of personal infor-\nmation, since they are usually the only means by which a\ntypical user consciously puts effort into sharing their activi-\nties. In view of this opportunity, we present an approach to\nextract implicit presence knowledge embedded in multiple\nstreams of heterogeneous online posts. Semantic Web tech-\nnologies are applied on top of syntactic analysis to extract\nand map entities onto a personal knowledge base, itself in-\ntegrated within the wider context of the Semantic Web. For\nthe purpose, we introduce the DLPO ontology\u2014a concise\nontology that captures all facets of dynamic personal infor-\nmation shared through online posts, as well their various\nderived links to personal and global semantic data clouds.\nBased on this conceptualisation, we outline the information\nextraction techniques targeted by our approach and present\nan as yet theoretical use-case to substantiate it."}
{"Title": "Small talk in the Digital Age: Making Sense of Phatic Posts", "Abstract": "This paper presents some practical implications of a theoretical\nweb desktop analysis and addresses microposts in the Social Web\ncontextual sense and their role contributing diverse information to\nthe Web as part of informal and semi-formal communication and\nsocial activities on Social Networking Sites (SNS). We reflect\nupon and present the most pervasive and relevant socio-\ncommunication function of an online presence on microposts and\nsocial networks: the phatic communication function. Although\nsome theorists such as Malinowski say these microposts have no\npractical information value, we argue that they have semantic and\nsocial value for the interlocutors, determined by socio-\ntechnological and cultural factors such as online presence and\nsocial awareness. We investigate and offer new implications for\nemerging social and communication dynamics formed around\nmicroposts, what we call here \u201cphatic posts\u201d. We suggest that\napparently trivial uses and features of SNS actually play an\nimportant role in setting the social and informational context of\nthe rest of the conversation - a \u201cphatic\u201d function - and thus that\nthese phatic posts are key to the success of SNS."}
{"Title": "Social Media \u2013 are they underpinned by social or\ninterest-based interactions?", "Abstract": "On many social media and user\u2013generated content sites,\nusers can not only upload content but also create links with\nother users to follow their activities. It is interesting to ask\nwhether the resulting user\u2013user Followers\u2019 Network is based\nmore on social ties, or shared interests in similar content.\nThis paper reports our preliminary progress in answering\nthis question using around five years of data from social\nvideo\u2013sharing site vimeo.\nMany links in the Followers\u2019 Network are between users\nwho do not have any videos in common, which would im-\nply the network is not interest\u2013based, but rather has a so-\ncial character. However, the Followers\u2019 Network also exhibits\nproperties unlike other social networks, for instance, cluster-\ning co\u2013efficient is low, links are frequently not reciprocated,\nand users form links across vast geographical distances. In\naddition, analysis of the relationship strength, calculated as\nthe number of commonly liked videos, people who follow\neach other and share some \u201clikes\u2019\u2019 have more video likes in\ncommon than the general population. We conclude by spec-\nulating on the reasons for these differences and proposals for\nfurther work."}
{"Title": "Topological Trends of Internet Content Providers", "Abstract": "The Internet is constantly changing, and its hierarchy was\nrecently shown to become flatter. Recent studies of inter-\ndomain traffic showed that large content providers drive this\nchange by bypassing tier-1 networks and reaching closer to\ntheir users, enabling them to save transit costs and reduce\nreliance of transit networks as new services are being de-\nployed, and traffic shaping is becoming increasingly popular.\nIn this paper we take a first look at the evolving connec-\ntivity of large content provider networks, from a topological\npoint of view of the autonomous systems (AS) graph. We\nperform a 5-year longitudinal study of the topological trends\nof large content providers, by analyzing several large content\nproviders and comparing these trends to those observed for\nlarge tier-1 networks. We study trends in the connectivity\nof the networks, neighbor diversity and geographical spread,\ntheir hierarchy, the adoption of IXPs as a convenient method\nfor peering, and their centrality. Our observations indicate\nthat content providers gradually increase and diversify their\nconnectivity, enabling them to improve their centrality in the\nInternet, while tier-1 networks lose dominance over time."}
{"Title": "On Social Community Networks\nThe Cost Sharing Problem", "Abstract": "Wireless social community networks (WSCNs) is an emerg-\ning technology that operate in the unlicensed spectrum and\nhave been created as an alternative to cellular wireless net-\nworks for providing low-cost, high speed wireless data access\nin urban areas. WSCNs is an upcoming idea that is start-\ning to gain attention amongst the civilian Internet users. By\nusing special WiFi routers that are provided by a social com-\nmunity network provider (SCNP), users can effectively share\ntheir connection with the neighborhood in return for some\nmonthly monetary benefits. However, deployment maps of\nexisting WSCNs reflect their slow progress in capturing the\nWiFi router market. In this paper, we look at a router\ndesign and cost sharing problem in WSCNs to improve de-\nployment. We devise a simple to implement, successful 1 ,\nbudget-balanced, ex-post efficient, and individually rational 2\nauction-based mechanism that generates the optimal num-\nber of features a router should have and allocates costs to\nresidential users in proportion to the feature benefits they\nreceive. Our problem is important to a new-entrant SCNP\nwhen it wants to design its multi-feature routers with the\ngoal to popularize them and increase their deployment in\na residential locality. Our proposed mechanism accounts\nfor heterogeneous user preferences towards different router\nfeatures and comes up with the optimal (feature-set, user\ncosts) router blueprint that satisfies each user in a locality,\nin turn motivating them to buy routers and thereby improve\ndeployment"}
{"Title": "Predicting Human Contacts in Mobile Social Networks\nusing Supervised Learning", "Abstract": "Having access to human contact traces has allowed researchers\nto study and understand how people contact each other in\ndifferent social settings. However, most of the existing hu-\nman contact traces are limited in the number of deployed\nBluetooth sensors. In most experiments, there are two types\nof participants, the ordinary ones who carry cellphones and\na specially selected group who additionally carry sensors.\nAlthough the contacts between any pair of participants are\nknown when at least one of them carry a sensor, the contacts\nbetween any pair of participants are \u201chidden\u201d when both of\nthem carry their cellphones. In this paper, we employ two\nwell-known supervised classifiers for predicting hidden con-\ntacts among participants who carry their cellphones. The\nperformance results of our supervised classifiers show the ap-\nplicability of using machine learning algorithms for contact\nprediction task. The results also show that a small subset\nof features such as number of common neighbors and total\noverlap time play essential roles in forming human contacts.\nFinally, we show that contacts of nodes with high centralities\nare more predictable than nodes with low centralities."}
{"Title": "Quality Distributed Community Formation for Data Delivery\nin Pocket Switched Networks", "Abstract": "In this paper we look at ways of detecting groups of strongly\nrelated devices called communities which are present in mo-\nbile Pocket Switched Networks (PSNs). We use existing\nmethods to detect communities which leverage repeated hu-\nman movement patterns and \u201cfamiliar strangers\u201d within a\nnumber of real PSNs extracted from the CRAWDAD repos-\nitory. By using different community detection techniques we\nattempt to show that there is a correlation between commu-\nnity size and compactness and inter-community membership\nof devices with increased data delivery. Finally our find-\nings are implemented in a prototype protocol called Qual-\nity which creates larger communities with increased inter-\ncommunity membership distributively."}
{"Title": "Publish-Subscribe Systems via Gossip:\na Study based on Complex Networks", "Abstract": "This paper analyzes the adoption of unstructured P2P over-\nlay networks to build publish-subscribe systems. We con-\nsider a very simple distributed communication protocol, ba-\nsed on gossip and on the local knowledge each node has\nabout subscriptions made by its neighbours. A mathemati-\ncal analysis is provided to estimate the number of nodes re-\nceiving the event. These outcomes are compared to those ob-\ntained via simulation. Results show even when the amount\nof subscribers represents a very small (yet non-negligible)\nportion of network nodes, by tuning the gossip probability\nthe event can percolate through the overlay."}
{"Title": "The Dynamic Network Notation:\nHarnessing Network Effects in PaaS-Ecosystems", "Abstract": "Web applications complement the Platform-as-a-Service (PaaS)\nvalue by satisfying widespread and rapidly changing consumer\nrequirements within limited time and budget. Successful PaaS\nproviders excel in governing their market performance by\nleveraging complex network effects, which implicitly control\nPaaS-ecosystems. There is currently no methodically sound and\neasy to use tool available to business analysts and software\nengineers of PaaS-offerings that addresses challenges and\nopportunities in launching and governing such highly dynamic\nnetworks. In this paper, we capture network behavior through\nelements of complex system and control theory. Our dynamic\nnetwork notation (DYNO) builds upon these theories. In more\ndetail, DYNO models PaaS offerings with a focus on identifying\nand shaping network effects towards a sufficient user-base and an\noptimized portfolio of Web applications, all while maintaining a\nhigh quality of service."}
{"Title": "Distributed Assessment of the Closeness Centrality\nRanking in Complex Networks", "Abstract": "We propose a method for the Distributed Assessment of the\nCloseness CEntrality Ranking (DACCER) in complex net-\nworks. DACCER computes centrality based only on local-\nized information restricted to a given neighborhood around\neach node, thus not requiring full knowledge of the network\ntopology. We show that the node centrality ranking com-\nputed by DACCER is highly correlated with the node rank-\ning based on the traditional closeness centrality, which re-\nquires high computational costs and full knowledge of the\nnetwork topology. This outcome is quite useful given the\nvast potential applicability of closeness centrality, which is\nseldom applied to large-scale networks due to its high com-\nputational costs. Results indicate that DACCER is simple,\nyet efficient, in assessing node centrality while allowing a\ndistributed implementation that contributes to its perfor-\nmance. This also contributes to the practical applicability of\nDACCER in the analysis of large-scale complex networks, as\nwe show using in our experimental evaluation both synthet-\nically generated networks and traces of real-world networks\nof different kinds and scales."}
{"Title": "Robustness of Centrality Measures against Link Weight\nQuantization in Social Network Analysis", "Abstract": "Research on social network analysis has been actively pursued.\nIn social network analysis, individuals are represented as nodes\nin a graph and social ties among them are represented as links,\nand the graph is therefore analyzed to provide an understanding\nof complex social phenomena that involve interactions among a\nlarge number of people. However, graphs used for social network\nanalyses generally contain several errors since it is not easy to ac-\ncurately and completely identify individuals in a society or social\nties among them. For instance, unweighted graphs or graphs with\nquantized link weights are used for conventional social network\nanalyses since the existence and strengths of social ties are gener-\nally known from the results of questionnaires. In this paper, we\nstudy, through simulations of graphs used for social network anal-\nyses, the effects of link weight quantization on the conventional\ncentrality measures (degree, betweenness, closeness, and eigenvec-\ntor centralities). Consequently, we show that (1) the effect of link\nweight quantization on the centrality measures are not significant\nto infer the most important node in the graph, (2) conversely, 5\u2013\n8 quantization levels are necessary for determining both the most\ncentral node and broad-range node rankings, and (3) graphs with\nhigh skewness of their degree distribution and/or with high corre-\nlation between node degree and link weights are robust against link\nweight quantization."}
{"Title": "Build Your Own Music Recommender by Modeling Internet\nRadio Streams", "Abstract": "In the Internet music scene, where recommendation technology is\nkey for navigating huge collections, large market players enjoy a\nconsiderable advantage. Accessing a wider pool of user feedback\nleads to an increasingly more accurate analysis of user tastes, effec-\ntively creating a \u201crich get richer\u201d effect. This work aims at signifi-\ncantly lowering the entry barrier for creating music recommenders,\nthrough a paradigm coupling a public data source and a new collab-\norative filtering (CF) model. We claim that Internet radio stations\nform a readily available resource of abundant fresh human signals\non music through their playlists, which are essentially cohesive sets\nof related tracks.\nIn a way, our models rely on the knowledge of a diverse group of\nexperts in lieu of the commonly used wisdom of crowds. Over sev-\neral weeks, we aggregated publicly available playlists of thousands\nof Internet radio stations, resulting in a dataset encompassing mil-\nlions of plays, and hundreds of thousands of tracks and artists. This\nprovides the large scale ground data necessary to mitigate the cold\nstart problem of new items at both mature and emerging services.\nFurthermore, we developed a new probabilistic CF model, tai-\nlored to the Internet radio resource. The success of the model was\nempirically validated on the collected dataset. Moreover, we tested\nthe model at a cross-source transfer learning manner \u2013 the same\nmodel trained on the Internet radio data was used to predict be-\nhavior of Yahoo! Music users. This demonstrates the ability to\ntap the Internet radio signals in other music recommendation se-\ntups. Based on encouraging empirical results, our hope is that the\nproposed paradigm will make quality music recommendation ac-\ncessible to all interested parties in the community."}
{"Title": "Using Control Theory for Stable and Efficient\nRecommender Systems", "Abstract": "The aim of a web-based recommender system is to pro-\nvide highly accurate and up-to-date recommendations to\nits users; in practice, it will hope to retain its users over\ntime. However, this raises unique challenges. To achieve\ncomplex goals such as keeping the recommender model up-\nto-date over time, we need to consider a number of exter-\nnal requirements. Generally, these requirements arise from\nthe physical nature of the system, for instance the available\ncomputational resources. Ideally, we would like to design\na system that does not deviate from the required outcome.\nModeling such a system over time requires to describe the\ninternal dynamics as a combination of the underlying rec-\nommender model and the its users\u2019 behavior. We propose to\nsolve this problem by applying the principles of modern con-\ntrol theory\u2014a powerful set of tools to deal with dynamical\nsystems\u2014to construct and maintain a stable and robust rec-\nommender system for dynamically evolving environments.\nIn particular, we introduce a design principle by focusing\non the dynamic relationship between the recommender sys-\ntem\u2019s performance and the number of new training samples\nthe system requires. This enables us to automate the control\nother external factors such as the system\u2019s update frequency.\nWe show that, by using a Proportional-Integral-Derivative\ncontroller, a recommender system is able to automatically\nand accurately estimate the required input to keep the out-\nput close to a pre-defined requirements. Our experiments\non a standard rating dataset show that, by using a feedback\nloop between system performance and training, the trade-\noff between the effectiveness and efficiency of the system can\nbe well maintained. We close by discussing the widespread\napplicability of our approach to a variety of scenarios that\nrecommender systems face."}
{"Title": "An Exploration of Improving Collaborative\nRecommender Systems via User-Item Subgroups", "Abstract": "Collaborative filtering (CF) is one of the most successful\nrecommendation approaches. It typically associates a user\nwith a group of like-minded users based on their preferences\nover all the items, and recommends to the user those items\nenjoyed by others in the group. However we find that two\nusers with similar tastes on one item subset may have to-\ntally different tastes on another set. In other words, there\nexist many user-item subgroups each consisting of a subset\nof items and a group of like-minded users on these items.\nIt is more natural to make preference predictions for a user\nvia the correlated subgroups than the entire user-item ma-\ntrix. In this paper, to find meaningful subgroups, we for-\nmulate the Multiclass Co-Clustering (MCoC) problem and\npropose an effective solution to it. Then we propose an\nunified framework to extend the traditional CF algorithms\nby utilizing the subgroups information for improving their\ntop-N recommendation performance. Our approach can be\nseen as an extension of traditional clustering CF models.\nSystematic experiments on three real world data sets have\ndemonstrated the effectiveness of our proposed approach."}
{"Title": "How Far Can Client-Only Solutions Go\nfor Mobile Browser Speed?", "Abstract": "Mobile browser is known to be slow because of the bottleneck in\nresource loading. Client-only solutions to improve resource load-\ning are attractive because they are immediately deployable, scala-\nble, and secure. We present the first publicly known treatment of\nclient-only solutions to understand how much they can improve\nmobile browser speed without infrastructure support. Leveraging\nan unprecedented set of web usage data collected from 24 iPhone\nusers continuously over one year, we examine the three funda-\nmental, orthogonal approaches a client-only solution can take:\ncaching, prefetching, and speculative loading. Speculative load-\ning, as is firstly proposed and studied in this work, predicts and\nspeculatively loads the subresources needed to open a webpage\nonce its URL is given. We show that while caching and prefetch-\ning are highly limited for mobile browsing, speculative loading\ncan be significantly more effective. Empirically, we show that\nclient-only solutions can improve the browser speed by about 1.4\nsecond on average for websites visited by the 24 iPhone users. We\nalso report the design, realization, and evaluation of speculative\nloading in a WebKit-based browser called Tempo. On average,\nTempo can reduce browser delay by 1 second (~20%)."}
{"Title": "Who Killed My Battery:\nAnalyzing Mobile Browser Energy Consumption", "Abstract": "Despite the growing popularity of mobile web browsing, the energy\nconsumed by a phone browser while surfing the web is poorly un-\nderstood. We present an infrastructure for measuring the precise\nenergy used by a mobile browser to render web pages. We then\nmeasure the energy needed to render financial, e-commerce, email,\nblogging, news and social networking sites. Our tools are suffi-\nciently precise to measure the energy needed to render individual\nweb elements, such as cascade style sheets (CSS), Javascript, im-\nages, and plug-in objects. Our results show that for popular sites,\ndownloading and parsing cascade style sheets and Javascript con-\nsumes a significant fraction of the total energy needed to render the\npage. Using the data we collected we make concrete recommen-\ndations on how to design web pages so as to minimize the energy\nneeded to render the page. As an example, by modifying scripts on\nthe Wikipedia mobile site we reduced by 30% the energy needed to\ndownload and render Wikipedia pages with no change to the user\nexperience. We conclude by estimating the point at which offload-\ning browser computations to a remote proxy can save energy on the\nphone."}
{"Title": "Periodic Transfers in Mobile Applications:\nNetwork-wide Origin, Impact, and Optimization", "Abstract": "Cellular networks employ a specific radio resource management\npolicy distinguishing them from wired and Wi-Fi networks. A\nlack of awareness of this important mechanism potentially leads\nto resource-inefficient mobile applications. We perform the first\nnetwork-wide, large-scale investigation of a particular type of ap-\nplication traffic pattern called periodic transfers where a handset\nperiodically exchanges some data with a remote server every t sec-\nonds. Using packet traces containing 1.5 billion packets collected\nfrom a commercial cellular carrier, we found that periodic transfers\nare very prevalent in today\u2019s smartphone traffic. However, they\nare extremely resource-inefficient for both the network and end-\nuser devices even though they predominantly generate very little\ntraffic. This somewhat counter-intuitive behavior is a direct con-\nsequence of the adverse interaction between such periodic trans-\nfer patterns and the cellular network radio resource management\npolicy. For example, for popular smartphone applications such as\nFacebook, periodic transfers account for only 1.7% of the overall\ntraffic volume but contribute to 30% of the total handset radio\nenergy consumption. We found periodic transfers are generated\nfor various reasons such as keep-alive, polling, and user behavior\nmeasurements. We further investigate the potential of various\ntraffic shaping and resource control algorithms. Depending on\ntheir traffic patterns, applications exhibit disparate responses to op-\ntimization strategies. Jointly using several strategies with moderate\naggressiveness can eliminate almost all energy impact of periodic\ntransfers for popular applications such as Facebook and Pandora"}
{"Title": "Understanding and Combating Link Farming\nin the Twitter Social Network\nSaptarshi Ghosh\nIIT Kharagpur, India\nBimal Viswanath\nMPI-SWS, Germany\nFarsha", "Abstract": "Recently, Twitter has emerged as a popular platform for\ndiscovering real-time information on the Web, such as news\nstories and people\u2019s reaction to them. Like the Web, Twitter\nhas become a target for link farming, where users, especially\nspammers, try to acquire large numbers of follower links in\nthe social network. Acquiring followers not only increases\nthe size of a user\u2019s direct audience, but also contributes to\nthe perceived influence of the user, which in turn impacts\nthe ranking of the user\u2019s tweets by search engines.\nIn this paper, we first investigate link farming in the Twit-\nter network and then explore mechanisms to discourage the\nactivity. To this end, we conducted a detailed analysis of\nlinks acquired by over 40,000 spammer accounts suspended\nby Twitter. We find that link farming is wide spread and\nthat a majority of spammers\u2019 links are farmed from a small\nfraction of Twitter users, the social capitalists, who are\nthemselves seeking to amass social capital and links by fol-\nlowing back anyone who follows them. Our findings shed\nlight on the social dynamics that are at the root of the link\nfarming problem in Twitter network and they have impor-\ntant implications for future designs of link spam defenses. In\nparticular, we show that a simple user ranking scheme that\npenalizes users for connecting to spammers can effectively\naddress the problem by disincentivizing users from linking\nwith other users simply to gain influence."}
{"Title": "Analyzing Spammers\u2019 Social Networks for Fun and Profit", "Abstract": "In this paper, we perform an empirical analysis of the cy-\nber criminal ecosystem on Twitter. Essentially, through\nanalyzing inner social relationships in the criminal ac-\ncount community, we find that criminal accounts tend to\nbe socially connected, forming a small-world network. We\nalso find that criminal hubs, sitting in the center of the so-\ncial graph, are more inclined to follow criminal accounts.\nThrough analyzing outer social relationships between\ncriminal accounts and their social friends outside the crim-\ninal account community, we reveal three categories of ac-\ncounts that have close friendships with criminal accounts.\nThrough these analyses, we provide a novel and effective\ncriminal account inference algorithm by exploiting criminal\naccounts\u2019 social relationships and semantic coordinations."}
{"Title": "Branded with a Scarlet \u201cC\u201d:\nCheaters in a Gaming Social Network", "Abstract": "Online gaming is a multi-billion dollar industry that en-\ntertains a large, global population. One unfortunate phe-\nnomenon, however, poisons the competition and the fun:\ncheating. The costs of cheating span from industry-supported\nexpenditures to detect and limit cheating, to victims\u2019 mon-\netary losses due to cyber crime.\nThis paper studies cheaters in the Steam Community, an\nonline social network built on top of the world\u2019s dominant\ndigital game delivery platform. We collected information\nabout more than 12 million gamers connected in a global\nsocial network, of which more than 700 thousand have their\nprofiles flagged as cheaters. We also collected in-game in-\nteraction data of over 10 thousand players from a popular\nmultiplayer gaming server. We show that cheaters are well\nembedded in the social and interaction networks: their net-\nwork position is largely indistinguishable from that of fair\nplayers. We observe that the cheating behavior appears to\nspread through a social mechanism: the presence and the\nnumber of cheater friends of a fair player is correlated with\nthe likelihood of her becoming a cheater in the future. Also,\nwe observe that there is a social penalty involved with being\nlabeled as a cheater: cheaters are likely to switch to more re-\nstrictive privacy settings once they are tagged and they lose\nmore friends than fair players. Finally, we observe that the\nnumber of cheaters is not correlated with the geographical,\nreal-world population density, or with the local popularity\nof the Steam Community."}
{"Title": "Risk-Aware Revenue Maximization in Display Advertising", "Abstract": "Display advertising is the graphical advertising on the World\nWide Web (WWW) that appears next to content on web\npages, instant messaging (IM) applications, email, etc. Over\nthe past decade, display ads have evolved from simple ban-\nner and pop-up ads to include various combinations of text,\nimages, audio, video, and animations. As a market seg-\nment, display continues to show substantial growth poten-\ntial, as evidenced by companies such as Microsoft, Yahoo,\nand Google actively vying for market share. As a sales pro-\ncess, display ads are typically sold in packages, the result of\nnegotiations between sales and advertising agents.\nA key component to any successful business model in dis-\nplay advertising is sound pricing. Main objectives for on-line\npublishers (e.g. Amazon, YouTube, CNN) are maximizing\nrevenue while managing their available inventory appropri-\nately, and pricing must reflect these considerations.\nThis paper addresses the problem of maximizing revenue\nby adjusting prices of display inventory. We cast this as\nan inventory allocation problem. Our formal objective (a)\nmaximizes revenue using (b) iterative price adjustments in\nthe direction of the gradient of an appropriately constructed\nLagrangian relaxation. We show that our optimization ap-\nproach drives the revenue towards local maximum under\nmild conditions on the properties of the (unknown) demand\ncurve.\nThe major unknown for optimizing revenue in display en-\nvironment is how the demand for display ads changes to\nprices, the classical demand curve. This we address directly,\nby way of a factorial pricing experiment. This enables us\nto estimate the gradient of the revenue function with re-\nspect to inventory prices. Overall, the result is a principled,\nrisk-aware, and empirically efficient methodology.\nThis paper is based on research undertaken on behalf of\none of Google\u2019s clients."}
{"Title": "Targeting Converters for New Campaigns Through Factor\nModels", "Abstract": "In performance based display advertising, campaign effec-\ntiveness is often measured in terms of conversions that rep-\nresent some desired user actions like purchases and product\ninformation requests on advertisers\u2019 website. Hence, iden-\ntifying and targeting potential converters is of vital impor-\ntance to boost campaign performance. This is often accom-\nplished by marketers who define the user base of campaigns\nbased on behavioral, demographic, search, social, purchase,\nand other characteristics. Such a process is manual and sub-\njective, it often fails to utilize the full potential of targeting.\nIn this paper we show that by using past converted users\nof campaigns and campaign meta-data (e.g., ad creatives,\nlanding pages), we can combine disparate user information\nin a principled way to effectively and automatically target\nconverters for new/existing campaigns. At the heart of our\napproach is a factor model that estimates the affinity of each\nuser feature to a campaign using historical conversion data.\nIn fact, our approach allows building a conversion model for\na brand new campaign through campaign meta-data alone,\nand hence targets potential converters even before the cam-\npaign is run. Through extensive experiments, we show the\nsuperiority of our factor model approach relative to sev-\neral other baselines. Moreover, we show that the perfor-\nmance of our approach at the beginning of a campaign\u2019s life\nis typically better than the other models even when they\nare trained using all conversion data after the campaign has\ncompleted. This clearly shows the importance and value of\nusing historical campaign data in constructing an effective\naudience selection strategy for display advertising."}
{"Title": "How Effective is Targeted Advertising?", "Abstract": "Advertisers are demanding more accurate estimates of the\nimpact of targeted advertisements, yet no study proposes\nan appropriate methodology to analyze the effectiveness of\na targeted advertising campaign, and there is a dearth of\nempirical evidence on the effectiveness of targeted advertis-\ning as a whole. The targeted population is more likely to\nconvert from advertising so the response lift between the tar-\ngeted and untargeted group to the advertising is likely an\noverestimate of the impact of targeted advertising. We pro-\npose a difference-in-differences estimator to account for this\nselection bias by decomposing the impact of targeting into\nselection bias and treatment effects components. Using sev-\neral large-scale online advertising campaigns, we test the ef-\nfectiveness of targeted advertising on brand-related searches\nand clickthrough rates. We find that the treatment effect\non the targeted group is about twice as large for brand-\nrelated searches, but naively estimating this effect without\ntaking into account selection bias leads to an overestima-\ntion of the lift from targeting on brand-related searches by\nalmost 1,000%."}
{"Title": "Compressed Data Structures for Annotated Web Search", "Abstract": "Entity relationship search at Web scale depends on adding\ndozens of entity annotations to each of billions of crawled\npages and indexing the annotations at rates comparable to\nregular text indexing. Even small entity search benchmarks\nfrom TREC and INEX suggest that the entity catalog sup-\nport thousands of entity types and tens to hundreds of mil-\nlions of entities. The above targets raise many challenges,\nmajor ones being the design of highly compressed data struc-\ntures in RAM for spotting and disambiguating entity men-\ntions, and highly compressed disk-based annotation indices.\nThese data structures cannot be readily built upon stan-\ndard inverted indices. Here we present a Web scale entity\nannotator and annotation index. Using a new workload-\nsensitive compressed multilevel map, we fit statistical dis-\nambiguation models for millions of entities within 1.15GB\nof RAM, and spend about 0.6 core-milliseconds per disam-\nbiguation. In contrast, DBPedia Spotlight spends 158 mil-\nliseconds, Wikipedia Miner spends 21 milliseconds, and Ze-\nmanta spends 9.5 milliseconds. Our annotation indices use\nideas from vertical databases to reduce storage by 30%. On\n40\u00d78 cores with 40\u00d73 disk spindles, we can annotate and\nindex, in about a day, a billion Web pages with two million\nentities and 200,000 types from Wikipedia. Index decom-\npression and scan speed are comparable to MG4J."}
{"Title": "The SemSets Model for Ad-hoc Semantic List Search", "Abstract": "The amount of semantic data on the web has been growing rapidly\nin recent years. One of the key challenges triggered by this growth\nis the ad-hoc querying, i.e., the ability to retrieve answers from se-\nmantic resources using natural language queries. This facilitates\ninteraction with semantic resources for the users so they can bene-\nfit from the knowledge covered by semantic data without the com-\nplexities of semantic query languages. In this paper, we focus on\nsemantic queries, where the aim is to retrieve objects belonging to\na set of semantically related entities. An example of such an ad-\nhoc type query is \"Apollo astronauts who walked on the Moon\". In\norder to address the task, we propose the SemSets retrieval model\nthat exploits and combines traditional document-based information\nretrieval, link structure of the semantic data and entity membership\nin semantic sets, in order to provide the answers. The novelty of the\napproach lies in the utilization of semantic sets, i.e., groups of se-\nmantically related entities. We propose two approaches to identify\nsuch semantic sets from the knowledge bases; the first one requires\ninvolvement of an expert user knowledgeable of the data set struc-\nture, the second one is fully automatic and provides results that are\ncomparable with those delivered by the expert users. As demon-\nstrated in the experimental evaluation, the proposed model has the\nstate-of-the-art performance on the SemSearch2011 data set, which\nhas been designed especially for the semantic list search evaluation."}
{"Title": "Heterogeneous Web Data Search\nUsing Relevance-based On The Fly Data Integration", "Abstract": "Searching over heterogeneous structured data on the Web\nis challenging due to vocabulary and structure mismatches\namong different data sources. In this paper, we study two\nexisting strategies and present a new approach to integrate\nadditional data sources into the search process. The first\nstrategy relies on data integration to mediate mismatches\nthrough upfront computation of mappings, based on which\nqueries are rewritten to fit individual sources. The other\nextreme is keyword search, which does not require any up-\nfront investment, but ignores structure information. Build-\ning on these strategies, we present a hybrid approach, which\ncombines the advantages of both. Our approach does not\nrequire any upfront data integration, but also leverages the\nfine grained structure of the underlying data. For a struc-\ntured query adhering to the vocabulary of just one source,\nthe so-called seed query, we construct an entity relevance\nmodel (ERM), which captures the content and the struc-\nture of the seed query results. This ERM is then aligned\non the fly with keyword search results retrieved from other\nsources and also used to rank these results. The outcome of\nour experiments using large-scale real-world data sets sug-\ngests that data integration leads to higher search effective-\nness compared to keyword search and that our new hybrid\napproach consistently exceeds both strategies."}
{"Title": "TailGate: Handling Long-Tail Content with a Little Help\nfrom Friends", "Abstract": "Distributing long-tail content is an inherently difficult task\ndue to the low amortization of bandwidth transfer costs as\nsuch content has limited number of views. Two recent trends\nare making this problem harder. First, the increasing pop-\nularity of user-generated content (UGC) and online social\nnetworks (OSNs) create and reinforce such popularity dis-\ntributions. Second, the recent trend of geo-replicating con-\ntent across multiple PoPs spread around the world, done for\nimproving quality of experience (QoE) for users and for re-\ndundancy reasons, can lead to unnecessary bandwidth costs.\nWe build TailGate, a system that exploits social relation-\nships, regularities in read access patterns, and time-zone dif-\nferences to efficiently and selectively distribute long-tail con-\ntent across PoPs. We evaluate TailGate using large traces\nfrom an OSN and show that it can decrease WAN bandwidth\ncosts by as much as 80% as well as reduce latency, improv-\ning QoE. We deploy TailGate on PlanetLab and show that\neven in the case when imprecise social information is avail-\nable, TailGate can still decrease the latency for accessing\nlong-tail YouTube videos by a factor of 2."}
{"Title": "DOHA: Scalable Real-time Web Applications through\nAdaptive Concurrent Execution", "Abstract": "Browsers have become mature execution platforms enabling\nweb applications to rival their desktop counterparts. An\nimportant class of such applications is interactive multime-\ndia: games, animations, and interactive visualizations. Un-\nlike many early web applications, these applications are la-\ntency sensitive and processing (CPU and graphics) inten-\nsive. When demands exceed available resources, application\nquality (e.g., frame rate) diminishes because it is hard to\nbalance timeliness and utilization. The quality of ambitious\nweb applications is also limited by single-threaded execution\nprevalent in the Web. Applications need to scale their qual-\nity, and thereby scale processing load, based on the resources\nthat are available. We refer to this as scalable quality.\nDOHA is an execution layer written entirely in JavaScript\nto enable scalable quality in web applications. DOHA favors\nimportant computations with more influence over quality\nbased on hints from application-specific adaptation policies.\nTo utilize widely available multi-core resources, DOHA aug-\nments HTML5 web workers with mechanisms to facilitate\nstate management and load-balancing. We evaluate DOHA\nwith an award-winning web-based game. When resources\nare limited, the modified game has better timing and over-\nall quality. More importantly, quality scales linearly with a\nsmall number of cores and the game is playable in challeng-\ning scenarios that are beyond the scope of the original game."}
{"Title": "Surviving a Search Engine Overload", "Abstract": "Search engines are an essential component of the web, but\ntheir web crawling agents can impose a significant burden on\nheavily loaded web servers. Unfortunately, blocking or de-\nferring web crawler requests is not a viable solution due to\neconomic consequences. We conduct a quantitative mea-\nsurement study on the impact and cost of web crawling\nagents, seeking optimization points for this class of request.\nBased on our measurements, we present a practical caching\napproach for mitigating search engine overload, and imple-\nment the two-level cache scheme on a very busy web server.\nOur experimental results show that the proposed caching\nframework can effectively reduce the impact of search en-\ngine overload on service quality."}
{"Title": "Semi-Supervised Correction of Biased Comment Ratings", "Abstract": "In many instances, offensive comments on the internet at-\ntract a disproportionate number of positive ratings from\nhighly biased users. This results in an undesirable scenario\nwhere these offensive comments are the top rated ones. In\nthis paper, we develop semi-supervised learning techniques\nto correct the bias in user ratings of comments. Our scheme\nuses a small number of comment labels in conjunction with\nuser rating information to iteratively compute user bias and\nunbiased ratings for unlabeled comments. We show that the\nrunning time of each iteration is linear in the number of\nratings, and the system converges to a unique fixed point.\nTo select the comments to label, we devise an active learn-\ning algorithm based on empirical risk minimization. Our\nactive learning method incrementally updates the risk for\nneighboring comments each time a comment is labeled, and\nthus can easily scale to large comment datasets. On real-life\ncomments from Yahoo! News, our semi-supervised and ac-\ntive learning algorithms achieve higher accuracy than simple\nbaselines, with few labeled examples."}
{"Title": "Spotting Fake Reviewer Groups in Consumer Reviews", "Abstract": "Opinionated social media such as product reviews are now widely\nused by individuals and organizations for their decision making.\nHowever, due to the reason of profit or fame, people try to game\nthe system by opinion spamming (e.g., writing fake reviews) to\npromote or demote some target products. For reviews to reflect\ngenuine user experiences and opinions, such spam reviews should\nbe detected. Prior works on opinion spam focused on detecting\nfake reviews and individual fake reviewers. However, a fake\nreviewer group (a group of reviewers who work collaboratively to\nwrite fake reviews) is even more damaging as they can take total\ncontrol of the sentiment on the target product due to its size. This\npaper studies spam detection in the collaborative setting, i.e., to\ndiscover fake reviewer groups. The proposed method first uses a\nfrequent itemset mining method to find a set of candidate groups.\nIt then uses several behavioral models derived from the collusion\nphenomenon among fake reviewers and relation models based on\nthe relationships among groups, individual reviewers, and\nproducts they reviewed to detect fake reviewer groups.\nAdditionally, we also built a labeled dataset of fake reviewer\ngroups. Although labeling individual fake reviews and reviewers\nis very hard, to our surprise labeling fake reviewer groups is much\neasier. We also note that the proposed technique departs from the\ntraditional supervised learning approach for spam detection\nbecause of the inherent nature of our problem which makes the\nclassic supervised learning approach less effective. Experimental\nresults show that the proposed method outperforms multiple\nstrong  baselines  including  the  state-of-the-art  sup"}
{"Title": "Estimating the Prevalence of Deception\nin Online Review Communities", "Abstract": "Consumers\u2019 purchase decisions are increasingly influenced\nby user-generated online reviews [3]. Accordingly, there has\nbeen growing concern about the potential for posting decep-\ntive opinion spam\u2014fictitious reviews that have been deliber-\nately written to sound authentic, to deceive the reader [15].\nBut while this practice has received considerable public at-\ntention and concern, relatively little is known about the ac-\ntual prevalence, or rate, of deception in online review com-\nmunities, and less still about the factors that influence it.\nWe propose a generative model of deception which, in con-\njunction with a deception classifier [15], we use to explore the\nprevalence of deception in six popular online review commu-\nnities: Expedia, Hotels.com, Orbitz, Priceline, TripAdvisor,\nand Yelp. We additionally propose a theoretical model of\nonline reviews based on economic signaling theory [18], in\nwhich consumer reviews diminish the inherent information\nasymmetry between consumers and producers, by acting as a\nsignal to a product\u2019s true, unknown quality. We find that de-\nceptive opinion spam is a growing problem overall, but with\ndifferent growth rates across communities. These rates, we\nargue, are driven by the different signaling costs associated\nwith deception for each review community, e.g., posting re-\nquirements. When measures are taken to increase signaling\ncost, e.g., filtering reviews written by first-time reviewers,\ndeception prevalence is effectively reduced."}
{"Title": "Musubi: Disintermediated Interactiv", "Abstract": "This paper presents Musubi, a mobile social application\nplatform that enables users to share any data type in real-\ntime feeds created by any application on the phone. Musubi\nis unique in providing a disintermediated service to end\nusers; all communication is supported using public key en-\ncryption thus leaking no user information to a third party.\nDespite the heavy use of cryptography to provide user au-\nthentication and access control, users found Musubi simple\nto use. We embed key exchange within familiar friending\nactions, and allow users to interact with any friend in their\naddress books without requiring them to join a common net-\nwork a priori. Our feed abstraction allows users to easily ex-\nercise access control. All data reside on the phone, granting\nusers the freedom to apply applications of their choice.\nIn addition to disintermediating personal messaging, we\nhave created an application platform to support multi-party\nsoftware with the same respect for personal data. The So-\ncialKit library we created on top of Musubi\u2019s trusted com-\nmunication protocol facilitates the development of multi-\nparty applications and integrates with Musubi to provide\na compelling group application experience. SocialKit allows\ndevelopers to make social, interactive, privacy-honoring ap-\nplications without needing to host their own servers."}
{"Title": "Economics of BitTorrent Communities", "Abstract": "Over the years, private file-sharing communities built on the\nBitTorrent protocol have developed their own policies and\nmechanisms for motivating members to share content and\ncontribute resources. By requiring members to maintain\na minimum ratio between uploads and downloads, private\ncommunities effectively establish credit systems, and with\nthem full-fledged economies. We report on a half-year-long\nmeasurement study of DIME \u2013 a community for sharing live\nconcert recordings \u2013 that sheds light on the economic forces\naffecting users in such communities. A key observation is\nthat while the download of files is priced only according to\nthe size of the file, the rate of return for seeding new files is\nsignificantly greater than for seeding old files. We find via\na natural experiment that users react to such differences in\nresale value by preferentially consuming older files during a\n\u2018free leech\u2019 period. We consider implications of these finding\non a user\u2019s ability to earn credits and meet ratio enforce-\nments, focusing in particular on the relationship between\nvisitation frequency and wealth and on low bandwidth users.\nWe then share details from an interview with DIME mod-\nerators, which highlights the goals of the community based\non which we make suggestions for possible improvement."}
{"Title": "A Habit Mining Approach for Discovering Similar Mobile\nUsers", "Abstract": "Discovering similar users with respect to their habits plays\nan important role in a wide range of applications, such as\ncollaborative filtering for recommendation, user segmenta-\ntion for market analysis, etc. Recently, the progressing abil-\nity to sense user contexts of smart mobile devices makes it\npossible to discover mobile users with similar habits by min-\ning their habits from their mobile devices. However, though\nsome researchers have proposed effective methods for mining\nuser habits such as behavior pattern mining, how to lever-\nage the mined results for discovering similar users remains\nless explored. To this end, we propose a novel approach\nfor conquering the sparseness of behavior pattern space and\nthus make it possible to discover similar mobile users with\nrespect to their habits by leveraging behavior pattern min-\ning. To be specific, first, we normalize the raw context log of\neach user by transforming the location-based context data\nand user interaction records to more general representation-\ns. Second, we take advantage of a constraint-based Bayesian\nMatrix Factorization model for extracting the latent com-\nmon habits among behavior patterns and then transforming\nbehavior pattern vectors to the vectors of mined common\nhabits which are in a much more dense space. The experi-\nments conducted on real data sets show that our approach\noutperforms three baselines in terms of the effectiveness of\ndiscovering similar mobile users with respect to their habits."}
{"Title": "YouTube Around the World:\nGeographic Popularity of Videos", "Abstract": "One of the most popular user activities on the Web is watch-\ning videos. Services like YouTube, Vimeo, and Hulu host\nand stream millions of videos, providing content that is on\npar with TV [21]. While some of this content is popular\nall over the globe, some videos might be only watched in a\nconfined, local region.\nIn this work we study the relationship between popular-\nity and locality of online YouTube videos. We investigate\nwhether YouTube videos exhibit geographic locality of in-\nterest, with views arising from a confined spatial area rather\nthan from a global one. Our analysis is done on a corpus of\nmore than 20 millions YouTube videos, uploaded over one\nyear from different regions. We find that about 50% of the\nvideos have more than 70% of their views in a single region.\nBy relating locality to viralness we show that social sharing\ngenerally widens the geographic reach of a video. If, how-\never, a video cannot carry its social impulse over to other\nmeans of discovery, it gets stuck in a more confined geo-\ngraphic region. Finally, we analyze how the geographic prop-\nerties of a video\u2019s views evolve on a daily basis during its life-\ntime, providing new insights on how the geographic reach of\na video changes as its popularity peaks and then fades away.\nOur results demonstrate how, despite the global nature of\nthe Web, online video consumption appears constrained by\ngeographic locality of interest: this has a potential impact\non a wide range of systems and applications, spanning from\ndelivery networks to recommendation and discovery engines,\nproviding new directions for future research."}
{"Title": "Dynamical Classes of Collective Attention in Twitter", "Abstract": "Micro-blogging systems such as Twitter expose digital traces\nof social discourse with an unprecedented degree of resolu-\ntion of individual behaviors. They offer an opportunity to\ninvestigate how a large-scale social system responds to ex-\nogenous or endogenous stimuli, and to disentangle the tem-\nporal, spatial and topical aspects of users\u2019 activity. Here we\nfocus on spikes of collective attention in Twitter, and specif-\nically on peaks in the popularity of hashtags. Users employ\nhashtags as a form of social annotation, to define a shared\ncontext for a specific event, topic, or meme. We analyze a\nlarge-scale record of Twitter activity and find that the evolu-\ntion of hashtag popularity over time defines discrete classes\nof hashtags. We link these dynamical classes to the events\nthe hashtags represent and use text mining techniques to\nprovide a semantic characterization of the hashtag classes.\nMoreover, we track the propagation of hashtags in the Twit-\nter social network and find that epidemic spreading plays a\nminor role in hashtag popularity, which is mostly driven by\nexogenous factors."}
{"Title": "We Know What @You #Tag:\nDoes the Dual Role Affect Hashtag Adoption?", "Abstract": "Researchers and social observers have both believed that\nhashtags, as a new type of organizational objects of informa-\ntion, play a dual role in online microblogging communities\n(e.g., Twitter). On one hand, a hashtag serves as a book-\nmark of content, which links tweets with similar topics; on\nthe other hand, a hashtag serves as the symbol of a com-\nmunity membership, which bridges a virtual community of\nusers. Are the real users aware of this dual role of hash-\ntags? Is the dual role affecting their behavior of adopting\na hashtag? Is hashtag adoption predictable? We take the\ninitiative to investigate and quantify the effects of the dual\nrole on hashtag adoption. We propose comprehensive mea-\nsures to quantify the major factors of how a user selects con-\ntent tags as well as joins communities. Experiments using\nlarge scale Twitter datasets prove the effectiveness of the\ndual role, where both the content measures and the com-\nmunity measures significantly correlate to hashtag adoption\non Twitter. With these measures as features, a machine\nlearning model can effectively predict the future adoption of\nhashtags that a user has never used before."}
{"Title": "Factorizing YAGO", "Abstract": "Vast amounts of structured information have been published\nin the Semantic Web\u2019s Linked Open Data (LOD) cloud and\ntheir size is still growing rapidly. Yet, access to this infor-\nmation via reasoning and querying is sometimes difficult,\ndue to LOD\u2019s size, partial data inconsistencies and inherent\nnoisiness. Machine Learning offers an alternative approach\nto exploiting LOD\u2019s data with the advantages that Machine\nLearning algorithms are typically robust to both noise and\ndata inconsistencies and are able to efficiently utilize non-\ndeterministic dependencies in the data. From a Machine\nLearning point of view, LOD is challenging due to its re-\nlational nature and its scale. Here, we present an efficient\napproach to relational learning on LOD data, based on the\nfactorization of a sparse tensor that scales to data consisting\nof millions of entities, hundreds of relations and billions of\nknown facts. Furthermore, we show how ontological knowl-\nedge can be incorporated in the factorization to improve\nlearning results and how computation can be distributed\nacross multiple nodes. We demonstrate that our approach\nis able to factorize the YAGO 2 core ontology and glob-\nally predict statements for this large knowledge base using\na single dual-core desktop computer. Furthermore, we show\nexperimentally that our approach achieves good results in\nseveral relational learning tasks that are relevant to Linked\nData. Once a factorization has been computed, our model is\nable to predict efficiently, and without any additional train-\ning, the likelihood of any of the 4.3 \u00b7 10 14 possible triples in\nthe YAGO 2 core ontology."}
{"Title": "Semantic Navigation on the Web of Data: Specification of\nRoutes, Web Fragments and Actions", "Abstract": "The massive semantic data sources linked in the Web of Data\ngive new meaning to old features like navigation; introduce\nnew challenges like semantic specification of Web fragments;\nand make it possible to specify actions relying on semantic\ndata. In this paper we introduce a declarative language to\nface these challenges. Based on navigational features, it is\ndesigned to specify fragments of the Web of Data and actions\nto be performed based on these data. We implement it in\na centralized fashion, and show its power and performance.\nFinally, we explore the same ideas in a distributed setting,\nshowing their feasibility, potentialities and challenges."}
{"Title": "Understanding Web Images by Object Relation Network", "Abstract": "This paper presents an automatic method for understand-\ning and interpreting the semantics of unannotated web im-\nages. We observe that the relations between objects in an\nimage carry important semantics about the image. To cap-\nture and describe such semantics, we propose Object Rela-\ntion Network (ORN), a graph model representing the most\nprobable meaning of the objects and their relations in an im-\nage. Guided and constrained by an ontology, ORN transfers\nthe rich semantics in the ontology to image objects and the\nrelations between them, while maintaining semantic consis-\ntency (e.g., a soccer player can kick a soccer ball, but cannot\nride it). We present an automatic system which takes a raw\nimage as input and creates an ORN based on image visual\nappearance and the guide ontology. We demonstrate various\nuseful web applications enabled by ORNs, such as automatic\nimage tagging, automatic image description generation, and\nimage search by image."}
{"Title": "Investigating the Distribution of Password Choices", "Abstract": "The distribution of passwords chosen by users has impli-\ncations for site security, password-handling algorithms and\neven how users are permitted to select passwords. Using\npassword lists from four different web sites, we investigate if\nZipf\u2019s law is a good description of the frequency with which\npasswords are chosen. We use a number of standard statis-\ntics, which measure the security of password distributions,\nto see if modelling the data using a simple distribution is\neffective. We then consider how much the password dis-\ntributions from each site have in common, using password\ncracking as a metric. This shows that these distributions\nhave enough high-frequency passwords in common to pro-\nvide effective speed-ups for cracking passwords. Finally, as\nan alternative to a deterministic banned list, we will show\nhow to stochastically shape the distribution of passwords,\nby occasionally asking users to choose a different password."}
{"Title": "Is this App Safe? A Large Scale Study on Application\nPermissions and Risk Signals", "Abstract": "Third-party applications (apps) drive the attractiveness of\nweb and mobile application platforms. Many of these plat-\nforms adopt a decentralized control strategy, relying on ex-\nplicit user consent for granting permissions that the apps\nrequest. Users have to rely primarily on community rat-\nings as the signals to identify the potentially harmful and\ninappropriate apps even though community ratings typi-\ncally reflect opinions about perceived functionality or perfor-\nmance rather than about risks. With the arrival of HTML5\nweb apps, such user-consent permission systems will become\nmore widespread. We study the effectiveness of user-consent\npermission systems through a large scale data collection of\nFacebook apps, Chrome extensions and Android apps.\nOur analysis confirms that the current forms of commu-\nnity ratings used in app markets today are not reliable in-\ndicators of privacy risks of an app. We find some evidence\nindicating attempts to mislead or entice users into grant-\ning permissions: free applications and applications with ma-\nture content request more permissions than is typical;\u201clook-\nalike\u201d applications which have names similar to popular ap-\nplications also request more permissions than is typical. We\nalso find that across all three platforms popular applications\nrequest more permissions than average."}
{"Title": "SessionJuggler: Secure Web Login From an Untrusted\nTerminal Using Session Hijacking", "Abstract": "We use modern features of web browsers to develop a secure login\nsystem from an untrusted terminal. The system, called Session\nJuggler, requires no server-side changes and no special software\non the terminal beyond a modern web browser. This important\nproperty makes adoption much easier than with previous proposals.\nWith Session Juggler users never enter their long term credential\non the untrusted terminal. Instead, users log in to a web site using\na smartphone app and then transfer the entire session, including\ncookies and all other session state, to the untrusted terminal. We\nshowthatSessionJuggler worksonalltheAlexatop100sitesexcept\neight. Of those eight, five failures were due to the site enforcing IP\nsession binding. We also show that Session Juggler works flawlessly\nwith Facebook connect. Beyond login, Session Juggler also provides\na secure logout mechanism where the trusted phone is used to kill\nthe session. To validate the session juggling concept we conducted\na number of web site surveys that are of independent interest. First,\nwe survey how web sites bind a session token to a specific device and\nshow that most use fairly basic techniques that are easily defeated.\nSecond, we survey how web sites handle logout and show that many\npopular sites surprisingly do not properly handle logout requests."}
{"Title": "Using Content and Interactions for Discovering\nCommunities in Social Networks", "Abstract": "In recent years, social networking sites have not only enabled\npeople to connect with each other using social links but have\nalso allowed them to share, communicate and interact over\ndiverse geographical regions. Social network provide a rich\nsource of heterogeneous data which can be exploited to dis-\ncover previously unknown relationships and interests among\ngroups of people. In this paper, we address the problem of\ndiscovering topically meaningful communities from a social\nnetwork. We assume that a persons\u2019 membership in a com-\nmunity is conditioned on its social relationship, the type of\ninteraction and the information communicated with other\nmembers of that community. We propose generative models\nthat can discover communities based on the discussed topics,\ninteraction types and the social connections among people.\nIn our models a person can belong to multiple communities\nand a community can participate in multiple topics. This\nallows us to discover both community interests and user in-\nterests based on the information and linked associations. We\ndemonstrate the effectiveness of our model on two real word\ndata sets and show that it performs better than existing\ncommunity discovery models."}
{"Title": "Community Detection in Incomplete Information Networks", "Abstract": "With the recent advances in information networks, the prob-\nlem of community detection has attracted much attention in\nthe last decade. While network community detection has\nbeen ubiquitous, the task of collecting complete network\ndata remains challenging in many real-world applications.\nUsually the collected network is incomplete with most of\nthe edges missing. Commonly, in such networks, all nodes\nwith attributes are available while only the edges within a\nfew local regions of the network can be observed. In this\npaper, we study the problem of detecting communities in\nincomplete information networks with missing edges. We\nfirst learn a distance metric to reproduce the link-based dis-\ntance between nodes from the observed edges in the local\ninformation regions. We then use the learned distance met-\nric to estimate the distance between any pair of nodes in\nthe network. A hierarchical clustering approach is proposed\nto detect communities within the incomplete information\nnetworks. Empirical studies on real-world information net-\nworks demonstrate that our proposed method can effectively\ndetect community structures within incomplete information\nnetworks"}
{"Title": "QUBE: a Quick algorithm for Updating BEtweenness\ncentrality", "Abstract": "The betweenness centrality of a vertex in a graph is a mea-\nsure for the participation of the vertex in the shortest paths\nin the graph. The Betweenness centrality is widely used in\nnetwork analyses. Especially in a social network, the re-\ncursive computation of the betweenness centralities of ver-\ntices is performed for the community detection and finding\nthe influential user in the network. Since a social network\ngraph is frequently updated, it is necessary to update the be-\ntweenness centrality efficiently. When a graph is changed,\nthe betweenness centralities of all the vertices should be re-\ncomputed from scratch using all the vertices in the graph.\nTo the best of our knowledge, this is the first work that\nproposes an efficient algorithm which handles the update\nof the betweenness centralities of vertices in a graph. In\nthis paper, we propose a method that efficiently reduces the\nsearch space by finding a candidate set of vertices whose be-\ntweenness centralities can be updated and computes their\nbetweenness centeralities using candidate vertices only. As\nthe cost of calculating the betweenness centrality mainly de-\npends on the number of vertices to be considered, the pro-\nposed algorithm significantly reduces the cost of calculation.\nThe proposed algorithm allows the transformation of an ex-\nisting algorithm which does not consider the graph update.\nExperimental results on large real datasets show that the\nproposed algorithm speeds up the existing algorithm 2 to\n2418 times depending on the dataset."}
{"Title": "On Revenue in the Generalized Second Price Auction", "Abstract": "The Generalized Second Price (GSP) auction is the primary\nauction used for selling sponsored search advertisements. In\nthis paper we consider the revenue of this auction at equi-\nlibrium. We prove that if agent values are drawn from\nidentical regular distributions, then the GSP auction paired\nwith an appropriate reserve price generates a constant frac-\ntion (1/6th) of the optimal revenue.\nIn the full-information game, we show that at any Nash\nequilibrium of the GSP auction obtains at least half of the\nrevenue of the VCG mechanism excluding the payment of a\nsingle participant. This bound holds also with any reserve\nprice, and is tight.\nFinally, we consider the tradeoff between maximizing rev-\nenue and social welfare. We introduce a natural convexity\nassumption on the click-through rates and show that it im-\nplies that the revenue-maximizing equilibrium of GSP in the\nfull information model will necessarily be envy-free. In par-\nticular, it is always possible to maximize revenue and social\nwelfare simultaneously when click-through rates are convex.\nWithout this convexity assumption, however, we demon-\nstrate that revenue may be maximized at a non-envy-free\nequilibrium that generates a socially inefficient allocation."}
{"Title": "Handling Forecast Errors\nWhile Bidding for Display Advertising", "Abstract": "Most of the online advertising today is sold via an auction,\nwhich requires the advertiser to respond with a valid bid\nwithin a fraction of a second. As such, most advertisers\nemploy bidding agents to submit bids on their behalf. The\narchitecture of such agents typically has (1) an offline opti-\nmization phase which incorporates the bidder\u2019s knowledge\nabout the market and (2) an online bidding strategy which\nsimply executes the offline strategy. The online strategy\nis typically highly dependent on both supply and expected\nprice distributions, both of which are forecast using tradi-\ntional machine learning methods. In this work we investi-\ngate the optimum strategy of the bidding agent when faced\nwith incorrect forecasts. At a high level, the agent can in-\nvest resources in improving the forecasts, or can tighten the\nloop between successive offline optimization cycles in order\nto detect errors more quickly. We show analytically that\nthe latter strategy, while simple, is extremely effective in\ndealing with forecast errors, and confirm this finding with\nexperimental evaluations."}
{"Title": "Optimizing Budget Allocation Among\nChannels and Influencers", "Abstract": "Brands and agencies use marketing as a tool to influence cus-\ntomers. One of the major decisions in a marketing plan deals\nwith the allocation of a given budget among media channels\nin order to maximize the impact on a set of potential cus-\ntomers. A similar situation occurs in a social network, where\na marketing budget needs to be distributed among a set of\npotential influencers in a way that provides high-impact.\nWe introduce several probabilistic models to capture the\nabove scenarios. The common setting of these models con-\nsists of a bipartite graph of source and target nodes. The ob-\njective is to allocate a fixed budget among the source nodes\nto maximize the expected number of influenced target nodes.\nThe concrete way in which source nodes influence target\nnodes depends on the underlying model. We primarily con-\nsider two models: a source-side influence model, in which a\nsource node that is allocated a budget of k makes k indepen-\ndent trials to influence each of its neighboring target nodes,\nand a target-side influence model, in which a target node be-\ncomes influenced according to a specified rule that depends\non the overall budget allocated to its neighbors. Our main\nresults are an optimal (1 \u2212 1/e)-approximation algorithm\nfor the source-side model, and several inapproximability re-\nsults for the target-side model, establishing that influence\nmaximization in the latter model is provably harder."}
{"Title": "Structured Query Suggestion for Specialization and\nParallel Movement: Effect on Search Behaviors", "Abstract": "Query suggestion, which enables the user to revise a query with a\nsingle click, has become one of the most fundamental features of\nWeb search engines. However, it is often difficult for the user to\nchoose from a list of query suggestions, and to understand the re-\nlation between an input query and suggested ones. In this paper,\nwe propose a new method to present query suggestions to the user,\nwhich has been designed to help two popular query reformulation\nactions, namely, specialization (e.g. from \u201cnikon\u201d to \u201cnikon cam-\nera\u201d ) and parallel movement (e.g. from \u201cnikon camera\u201d to \u201ccanon\ncamera\u201d). Using a query log collected from a popular commercial\nWeb search engine, our prototype called SParQS classifies query\nsuggestions intoautomatically generated categories andgenerates a\nlabel for each category. Moreover, SParQSpresents some new enti-\nties asalternatives to the original query (e.g. \u201ccanon\u201d in response to\nthe query \u201cnikon\u201d), together with their query suggestions classified\nin the same way as the original query\u2019s suggestions. We conducted\na task-based user study to compare SParQS with a traditional \u201cflat\nlist\u201d query suggestion interface. Our results show that the SParQS\ninterface enables subjects to search more successfully than the flat\nlist case, even though query suggestions presented were exactly the\nsame in thetwo interfaces. In addition, thesubjects found the query\nsuggestions more helpful when they were presented in the SParQS\ninterface rather than in a flat list."}
{"Title": "Partitioned Multi-Indexing: Bringing Order to Social Search", "Abstract": "To answer search queries on a social network rich with user-\ngenerated content, it is desirable to give a higher ranking\nto content that is closer to the individual issuing the query.\nQueries occur at nodes in the network, documents are also\ncreated by nodes in the same network, and the goal is to find\nthe document that matches the query and is closest in net-\nwork distance to the node issuing the query. In this paper,\nwe present the \u201cPartitioned Multi-Indexing\u201d scheme, which\nprovides an approximate solution to this problem. With m\nlinks in the network, after an offline\n\u02dc\nO(m) pre-processing\ntime, our scheme allows for social index operations (i.e., so-\ncial search queries, as well as insertion and deletion of words\ninto and from a document at any node), all in time\n\u02dc\nO(1).\nFurther, our scheme can be implemented on open source dis-\ntributed streaming systems such as Yahoo! S4 or Twitter\u2019s\nStorm so that every social index operation takes\n\u02dc\nO(1) pro-\ncessing time and network queries in the worst case, and just\ntwo network queries in the common case where the reverse\nindex corresponding to the query keyword is much smaller\nthan the memory available at any distributed compute node.\nBuilding on Das Sarma et al.\u2019s approximate distance or-\nacle, the worst-case approximation ratio of our scheme is\n\u02dc\nO(1) for undirected networks. Our simulations on the social\nnetwork Twitter as well as synthetic networks show that\nin practice, the approximation ratio is actually close to 1\nfor both directed and undirected networks. We believe that\nthis work is the first demonstration of the feasibility of social\nsearch with real-time text updates at large scales."}
{"Title": "Unsupervised Extraction of Template Structure in Web\nSearch Queries", "Abstract": "Web search queries are an encoding of the user\u2019s search intent and\nextracting structured information from them can facilitate central\nsearch engine operations like improving the ranking of search re-\nsults and advertisements. Not surprisingly, this area has attracted\na lot of attention in the research community in the last few years.\nThe problem is, however, made challenging by the fact that search\nqueries tend to be extremely succinct; a condensation of user search\nneeds to the bare-minimum set of keywords. In this paper we con-\nsider the problem of extracting, with no manual intervention, the\nhidden structure behind the observed search queries in a domain:\nthe origins of the constituent keywords as well as the manner the in-\ndividual keywords are assembled together. We formalize important\nproperties of the problem and then give a principled solution based\non generative models that satisfies these properties. Using manually\nlabeled data we show that the query templates extracted by our so-\nlution are superior to those discovered by strong baseline methods.\nThe query templates extracted by our approach have potential\nuses in many search engine tasks; query answering, advertisement\nmatching and targeting, to name a few. In this paper we study\none such task, estimating Query-Advertisability, and empirically\ndemonstrate that using extracted template information can improve\nperformance over and above the current state-of-the-art."}
{"Title": "Multi-Objective Ranking of Comments on Web", "Abstract": "With the explosion of information on any topic, the need\nfor ranking is becoming very critical. Ranking typically de-\npends on several aspects. Products, for example, have sev-\neral aspects like price, recency, rating, etc. Product ranking\nhas to bring the \u201cbest\u201d product which is recent and highly\nrated. Hence ranking has to satisfy multiple objectives. In\nthis paper, we explore multi-objective ranking of comments\nusing Hodge decomposition. While Hodge decomposition\nproduces a globally consistent ranking, a globally inconsis-\ntent component is also present. We propose an active learn-\ning strategy for the reduction of this component. Finally,\nwe develop techniques for online Hodge decomposition. We\nexperimentally validate the ideas presented in this paper."}
{"Title": "Care to Comment?\nRecommendations for Commenting on News Stories", "Abstract": "Many websites provide commenting facilities for users to ex-\npress their opinions or sentiments with regards to content\nitems, such as, videos, news stories, blog posts, etc. Pre-\nvious studies have shown that user comments contain valu-\nable information that can provide insight on Web documents\nand may be utilized for various tasks. This work presents a\nmodel that predicts, for a given user, suitable news stories\nfor commenting. The model achieves encouraging results\nregarding the ability to connect users with stories they are\nlikely to comment on. This provides grounds for personal-\nized recommendations of stories to users who may want to\ntake part in their discussion. We combine a content-based\napproach with a collaborative-filtering approach (utilizing\nusers\u2019 co-commenting patterns) in a latent factor modeling\nframework. We experiment with several variations of the\nmodel\u2019s loss function in order to adjust it to the problem\ndomain. We evaluate the results on two datasets and show\nthat employing co-commenting patterns improves upon us-\ning content features alone, even with as few as two available\ncomments per story. Finally, we try to incorporate avail-\nable social network data into the model. Interestingly, the\nsocial data does not lead to substantial performance gains,\nsuggesting that the value of social data for this task is quite\nnegligible."}
{"Title": "Leveraging User Comments for Aesthetic Aware\nImage Search Reranking", "Abstract": "The increasing number of images available online has created\na growing need for efficient ways to search for relevant con-\ntent. Text-based query search is the most common approach\nto retrieve images from the Web. In this approach, the sim-\nilarity between the input query and the metadata of images\nis used to find relevant information. However, as the amount\nof available images grows, the number of relevant images also\nincreases, all of them sharing very similar metadata but dif-\nfering in other visual characteristics. This paper studies the\ninfluence of visual aesthetic quality in search results as a\ncomplementary attribute to relevance. By considering aes-\nthetics, a new ranking parameter is introduced aimed at\nimproving the quality at the top ranks when large amounts\nof relevant results exist. Two strategies for aesthetic rating\ninference are proposed: one based on visual content, another\nbased on the analysis of user comments to detect opinions\nabout the quality of images. The results of a user study with\n58 participants show that the comment-based aesthetic pre-\ndictor outperforms the visual content-based strategy, and\nreveals that aesthetic-aware rankings are preferred by users\nsearching for photographs on the Web."}
{"Title": "LINDEN: Linking Named Entities with Knowledge Base via\nSemantic Knowledge", "Abstract": "Integrating the extracted facts with an existing knowledge\nbase has raised an urgent need to address the problem of\nentity linking. Specifically, entity linking is the task to link\nthe entity mention in text with the corresponding real world\nentity in the existing knowledge base. However, this task is\nchallenging due to name ambiguity, textual inconsistency,\nand lack of world knowledge in the knowledge base. Sev-\neral methods have been proposed to tackle this problem,\nbut they are largely based on the co-occurrence statistics of\nterms between the text around the entity mention and the\ndocument associated with the entity. In this paper, we pro-\npose LINDEN 1 , a novel framework to link named entities in\ntext with a knowledge base unifying Wikipedia and Word-\nNet, by leveraging the rich semantic knowledge embedded in\nthe Wikipedia and the taxonomy of the knowledge base. We\nextensively evaluate the performance of our proposed LIN-\nDEN over two public data sets and empirical results show\nthat LINDEN significantly outperforms the state-of-the-art\nmethods in terms of accuracy."}
{"Title": "Cross-lingual Knowledge Linking Across\nWiki Knowledge Bases", "Abstract": "Wikipedia becomes one of the largest knowledge bases on\nthe Web. It has attracted 513 million page views per day in\nJanuary 2012. However, one critical issue for Wikipedia is\nthat articles in different language are very unbalanced. For\nexample, the number of articles on Wikipedia in English has\nreached 3.8 million, while the number of Chinese articles is\nstill less than half million and there are only 217 thousand\ncross-lingual links between articles of the two languages. On\nthe other hand, there are more than 3.9 million Chinese Wi-\nki articles on Baidu Baike and Hudong.com, two popular\nencyclopedias in Chinese. One important question is how to\nlink the knowledge entries distributed in different knowledge\nbases. This will immensely enrich the information in the on-\nline knowledge bases and benefit many applications. In this\npaper, we study the problem of cross-lingual knowledge link-\ning and present a linkage factor graph model. Features are\ndefined according to some interesting observations. Exper-\niments on the Wikipedia data set show that our approach\ncan achieve a high precision of 85.8% with a recall of 88.1%.\nThe approach found 202,141 new cross-lingual links between\nEnglish Wikipedia and Baidu Baike."}
{"Title": "ZenCrowd: Leveraging Probabilistic Reasoning and\nCrowdsourcing Techniques for Large-Scale Entity Linking", "Abstract": "We tackle the problem of entity linking for large collections\nof online pages; Our system, ZenCrowd, identifies entities\nfrom natural language text using state of the art techniques\nand automatically connects them to the Linked Open Data\ncloud. We show how one can take advantage of human in-\ntelligence to improve the quality of the links by dynamically\ngenerating micro-tasks on an online crowdsourcing platform.\nWe develop a probabilistic framework to make sensible deci-\nsions about candidate links and to identify unreliable human\nworkers. We evaluate ZenCrowd in a real deployment and\nshow how a combination of both probabilistic reasoning and\ncrowdsourcing techniques can significantly improve the qual-\nity of the links, while limiting the amount of work performed\nby the crowd."}
{"Title": "A Flexible Generative Model for Preference Aggregation", "Abstract": "Many areas of study, such as information retrieval, collabo-\nrative filtering, and social choice face the preference aggre-\ngation problem, in which multiple preferences over objects\nmust be combined into a consensus ranking. Preferences\nover items can be expressed in a variety of forms, which\nmakes the aggregation problem difficult. In this work we\nformulate a flexible probabilistic model over pairwise com-\nparisons that can accommodate all these forms. Inference\nin the model is very fast, making it applicable to problems\nwith hundreds of thousands of preferences. Experiments on\nbenchmark datasets demonstrate superior performance to\nexisting methods."}
{"Title": "Evaluating the Effectiveness of Search Task Trails", "Abstract": "In this paper, we introduce \u201ctask trail\u201d as a new concept to\nunderstand user search behaviors. We define task to be an\natomic user information need. Web search logs have been\nstudied mainly at session or query level where users may\nsubmit several queries within one task and handle several\ntasks within one session. Although previous studies have\naddressed the problem of task identification, little is known\nabout the advantage of using task over session and query\nfor search applications. In this paper, we conduct exten-\nsive analyses and comparisons to evaluate the effectiveness\nof task trails in three search applications: determining user\nsatisfaction, predicting user search interests, and query sug-\ngestion. Experiments are conducted on large scale datasets\nfrom a commercial search engine. Experimental results show\nthat: (1) Sessions and queries are not as precise as tasks in\ndetermining user satisfaction. (2) Task trails provide higher\nweb page utilities to users than other sources. (3) Tasks rep-\nresent atomic user information needs, and therefore can pre-\nserve topic similarity between query pairs. (4) Task-based\nquery suggestion can provide complementary results to other\nmodels. The findings in this paper verify the need to extract\ntask trails from web search logs and suggest potential appli-\ncations in search and recommendation systems."}
{"Title": "Evaluation with Informational and Navigational Intents", "Abstract": "Given an ambiguous or underspecified query, search result diver-\nsification aims at accomodating different user intents within a sin-\ngle \u201centry-point\u201d result page. However, some intents are informa-\ntional, for which many relevant pages may help, while others are\nnavigational, for which only one web page is required. We propose\nnew evaluation metrics for search result diversification that consid-\ners this distinction, as well as a simple method for comparing the\nintuitiveness of a given pair of metrics quantitatively. Our main\nexperimental findings are: (a) In terms of discriminative power\nwhich reflects statistical reliability, the proposed metrics, DIN?-\nnDCG and P+Q?, are comparable to intent recall and D?-nDCG,\nand possibly superior to \u03b1-nDCG; (b) In terms of preference agree-\nment with intent recall, P+Q? is superior to other diversity metrics\nand therefore may be the most intuitive as a metric that emphasises\ndiversity; and (c) In terms of preference agreement with effective\nprecision, DIN?-nDCG is superior to other diversity metrics and\ntherefore may be the most intuitive as a metric that emphasises\nrelevance. Moreover, DIN?-nDCG may be the most intuitive as\na metric that considers both diversity and relevance. In addition,\nwe demonstrate that the randomised Tukey\u2019s Honestly Significant\nDifferences test that takes the entire set of available runs into ac-\ncount is substantially more conservative than the paired bootstrap\ntest that only considers one run pair at a time, and therefore rec-\nommend the former approach for significance testing when a set of\nruns is available for evaluation."}
{"Title": "Information Transfer in Social Media", "Abstract": "Recent research has explored the increasingly important role\nof social media by examining the dynamics of individual and\ngroup behavior, characterizing patterns of information dif-\nfusion, and identifying influential individuals. In this paper\nwe suggest a measure of causal relationships between nodes\nbased on the information\u2013theoretic notion of transfer en-\ntropy, or information transfer. This theoretically grounded\nmeasure is based on dynamic information, captures fine\u2013\ngrain notions of influence, and admits a natural, predictive\ninterpretation. Networks inferred by transfer entropy can\ndiffer significantly from static friendship networks because\nmost friendship links are not useful for predicting future\ndynamics. We demonstrate through analysis of synthetic\nand real\u2013world data that transfer entropy reveals meaning-\nful hidden network structures. In addition to altering our\nnotion of who is influential, transfer entropy allows us to\ndifferentiate between weak influence over large groups and\nstrong influence over small groups."}
{"Title": "The Role of Social Networks in Information Diffusion", "Abstract": "Online social networking technologies enable individuals to\nsimultaneously share information with any number of peers.\nQuantifying the causal effect of these mediums on the dis-\nsemination of information requires not only identification\nof who influences whom, but also of whether individuals\nwould still propagate information in the absence of social sig-\nnals about that information. We examine the role of social\nnetworks in online information diffusion with a large-scale\nfield experiment that randomizes exposure to signals about\nfriends\u2019 information sharing among 253 million subjects in\nsitu. Those who are exposed are significantly more likely to\nspread information, and do so sooner than those who are\nnot exposed. We further examine the relative role of strong\nand weak ties in information propagation. We show that,\nalthough stronger ties are individually more influential, it\nis the more abundant weak ties who are responsible for the\npropagation of novel information. This suggests that weak\nties may play a more dominant role in the dissemination of\ninformation online than currently believed."}
{"Title": "Recommendations to Boost Content Spread in Social\nNetworks", "Abstract": "Content sharing in social networks is a powerful mechanism\nfor discovering content on the Internet. The degree to which\ncontent is disseminated within the network depends on the\nconnectivity relationships among network nodes. Existing\nschemes for recommending connections in social networks\nare based on the number of common neighbors, similarity\nof user profiles, etc. However, such similarity-based connec-\ntions do not consider the amount of content discovered.\nIn this paper, we propose novel algorithms for recommend-\ning connections that boost content propagation in a social\nnetwork without compromising on the relevance of the rec-\nommendations. Unlike existing work on influence propaga-\ntion, in our environment, we are looking for edges instead\nof nodes, with a bound on the number of incident edges\nper node. We show that the content spread function is not\nsubmodular, and develop approximation algorithms for com-\nputing a near-optimal set of edges. Through experiments on\nreal-world social graphs such as Flickr and Twitter, we show\nthat our approximation algorithms achieve content spreads\nthat are as much as 90 times higher compared to existing\nheuristics for recommending connections."}
{"Title": "Implementing Optimal Outcomes in Social Computing:\nA Game-Theoretic Approach", "Abstract": "In many social computing applications such as online Q&A\nforums, the best contribution for each task receives some\nhigh reward, while all remaining contributions receive an\nidentical, lower reward irrespective of their actual qualities.\nSuppose a mechanism designer (site owner) wishes to opti-\nmize an objective that is some function of the number and\nqualities of received contributions. When potential contribu-\ntors are strategic agents, who decide whether to contribute or\nnot to selfishly maximize their own utilities, is such a \u201cbest\ncontribution\u201d mechanism, M B , adequate to implement an\noutcome that is optimal for the mechanism designer?\nWe first show that in settings where a contribution\u2019s value\nis determined primarily by an agent\u2019s expertise, and agents\nonly strategically choose whether to contribute or not, con-\ntests can implement optimal outcomes: for any reasonable\nobjective, the rewards for the best and remaining contribu-\ntions in M B can always be chosen so that the outcome in the\nunique symmetric equilibrium of M B maximizes the mech-\nanism designer\u2019s utility. We also show how the mechanism\ndesigner can learn these optimal rewards when she does not\nknow the parameters of the agents\u2019 utilities, as might be the\ncase in practice. We next consider settings where a contri-\nbution\u2019s value depends on both the contributor\u2019s expertise\nas well as her effort, and agents endogenously choose how\nmuch effort to exert in addition to deciding whether to con-\ntribute. Here, we show that optimal outcomes can never be\nimplemented by contests if the system can rank the qualities\nof contributions perfectly. However, if there is noise in the\ncontributions\u2019 rankings, then the mechanism designer can\nagain induce agents to follow strategies that maximize his\nutility. Thus imperfect rankings can actually help achieve\nimplementability of optimal outcomes when effort is endoge-\nnous and influences quality."}
{"Title": "Lattice Games and the Economics of Aggregators", "Abstract": "We model the strategic decisions of web sites in content markets,\nwhere sites may reduce user search cost by aggregating content.\nExample aggregations includepolitical news, technology, andother\nniche-topic websites. We model this market scenario as an exten-\nsive form game of complete information, where sites choose a set\nof content to aggregate and users associate with sites that are near-\nest to their interests.\nThus, our scenario is a location game in which sites choose to\naggregate content at a certain point in user-preference space, and\nour choice of distance metric, Jacquard distance, induces a lattice\nstructure on the game. We provide two variants of this scenario:\none where users associate with the first site to enter amongst sites\nof equal distances, and a second where users choose uniformly\nbetween sites at equal distances. We show that subgame perfect\nNash equilibria exist for both games. While it appears to be com-\nputationally hard to compute equilibria in both games, we show a\npolynomial-time satisficing strategy called Frontier Descent for the\nfirst game. A satisficing strategy is not a best response, but ensures\nthat earlier sites will have positive profits, assuming all subsequent\nsites also have positive profits. By contrast, we show that the sec-\nond game has no satisficing solution."}
{"Title": "Strategic Formation of Credit Networks", "Abstract": "Creditnetworks areanabstractionformodeling trustbetweenagents\nin a network. Agents who do not directly trust each other can trans-\nact through exchange of IOUs (obligations) along a chain of trust\nin the network. Credit networks are robust to intrusion, can enable\ntransactions between strangers in exchange economies, and have\nthe liquidity to support a high rate of transactions. We study the\nformation of such networks when agents strategically decide how\nmuch credit to extend each other. When each agent trusts a fixed\nset of other agents, and transacts directly only with those it trusts,\nthe formation game is a potential game and all Nash equilibria are\nsocial optima. Moreover, theNashequilibriaof thisgameareequiv-\nalent in a very strong sense: the sequences of transactions that can\nbe supported from each equilibrium credit network are identical.\nWhen we allow transactions over longer paths, the game may not\nadmit a Nash equilibrium, and even when it does, the price of anar-\nchy may be unbounded. Hence, we study two special cases. First,\nwhen agents have a shared belief about the trustworthiness of each\nagent, the networks formed in equilibrium have a star-like structure.\nThough the price of anarchy is unbounded, myopic best response\nquickly converges to a social optimum. Similar star-like structures\nare found in equilibria of heuristic strategies found via simulation.\nIn addition, we simulate a second case where agents may have vary-\ning information about each others\u2019 trustworthiness based on their\ndistance in a social network. Empirical game analysis of these sce-\nnarios suggests that star structures arise only when defaults are rela-\ntively rare, and otherwise, credit tends to be issued over short social\ndistances conforming to the locality of information."}
{"Title": "Beyond Dwell Time: Estimating Document Relevance from\nCursor Movements and other Post-click Searcher Behavior", "Abstract": "Result clickthrough statistics and dwell time on clicked results have\nbeen shown valuable for inferring search result relevance, but the\ninterpretation of these signals can vary substantially for different\ntasks and users. This paper shows that that post-click searcher be-\nhavior, such as cursor movement and scrolling, provides additional\nclues for better estimating document relevance. To this end, we\nidentify patterns of examination and interaction behavior that cor-\nrespond to viewing a relevant or non-relevant document, and design\na new Post-Click Behavior (PCB) model to capture these patterns.\nTo our knowledge, PCB is the first to successfully incorporate post-\nclick searcher interactions such as cursor movements and scrolling\non a landing page for estimating document relevance. We evaluate\nPCB on a dataset collected from a controlled user study that con-\ntains interactions gathered from hundreds of unique queries, result\nclicks, and page examinations. The experimental results show that\nPCB is significantly more effective than using page dwell time in-\nformation alone, both for estimating the explicit judgments of each\nuser, and for re-ranking the results using the estimated relevance"}
{"Title": "Joint Relevance and Freshness Learning From\nClickthroughs for News Search", "Abstract": "In contrast to traditional Web search, where topical rel-\nevance is often the main selection criterion, news search\nis characterized by the increased importance of freshness.\nHowever, the estimation of relevance and freshness, and es-\npecially the relative importance of these two aspects, are\nhighly specific to the query and the time when the query\nwas issued. In this work, we propose a unified framework\nfor modeling the topical relevance and freshness, as well as\ntheir relative importance, based on click logs. We use click\nstatistics and content analysis techniques to define a set of\ntemporal features, which predict the right mix of freshness\nand relevance for a given query. Experimental results on\nboth historical click data and editorial judgments demon-\nstrate the effectiveness of the proposed approach."}
{"Title": "Active Objects: Actions for Entity-Centric Search", "Abstract": "We introduce an entity-centric search experience, called Active\nObjects, in which entity-bearing queries are paired with actions\nthat can be performed on the entities. For example, given a query\nfor a specific flashlight, we aim to present actions such as reading\nreviews, watching demo videos, and finding the best price online.\nIn an annotation study conducted over a random sample of user\nquery sessions, we found that a large proportion of queries in\nquery logs involve actions on entities, calling for an automatic\napproach to identifying relevant actions for entity-bearing queries.\nIn this paper, we pose the problem of finding actions that can be\nperformed on entities as the problem of probabilistic inference in\na graphical model that captures how an entity bearing query is\ngenerated. We design models of increasing complexity that\ncapture latent factors such as entity type and intended actions that\ndetermine how a user writes a query in a search box, and the URL\nthat they click on. Given a large collection of real-world queries\nand clicks from a commercial search engine, the models are\nlearned efficiently through maximum likelihood estimation using\nan EM algorithm. Given a new query, probabilistic inference\nenables recommendation of a set of pertinent actions and hosts.\nWe propose an evaluation methodology for measuring the\nrelevance of our recommended actions, and show empirical\nevidence of the quality and the diversity of the discovered actions."}
{"Title": "Modeling and Predicting Behavioral Dynamics on the Web", "Abstract": "User behavior on the Web changes over time. For exam-\nple, the queries that people issue to search engines, and the\nunderlying informational goals behind the queries vary over\ntime. In this paper, we examine how to model and predict\nthis temporal user behavior. We develop a temporal mod-\neling framework adapted from physics and signal processing\nthat can be used to predict time-varying user behavior us-\ning smoothing and trends. We also explore other dynamics\nof Web behaviors, such as the detection of periodicities and\nsurprises. We develop a learning procedure that can be used\nto construct models of users\u2019 activities based on features of\ncurrent and historical behaviors. The results of experiments\nindicate that by using our framework to predict user behav-\nior, we can achieve significant improvements in prediction\ncompared to baseline models that weight historical evidence\nthe same for all queries. We also develop a novel learning\nalgorithm that explicitly learns when to apply a given pre-\ndiction model among a set of such models. Our improved\ntemporal modeling of user behavior can be used to enhance\nquery suggestions, crawling policies, and result ranking."}
{"Title": "Are Web Users Really Markovian?", "Abstract": "User modeling on the Web has rested on the fundamental assump-\ntion of Markovian behavior \u2014 a user\u2019s next action depends only\non her current state, and not the history leading up to the current\nstate. This forms the underpinning of PageRank web ranking, as\nwell as a number of techniques for targeting advertising to users.\nIn this work we examine the validity of this assumption, using data\nfrom a number of Web settings. Our main result invokes statisti-\ncal order estimation tests for Markov chains to establish that Web\nusers are not, in fact, Markovian. We study the extent to which the\nMarkovian assumption is invalid, and derive a number of avenues\nfor further research."}
{"Title": "Human Wayfinding in Information Networks", "Abstract": "Navigating information spaces is an essential part of our everyday\nlives, and in order to design efficient and user-friendly information\nsystems, it is important to understand how humans navigate and\nfind the information they are looking for. We perform a large-scale\nstudy of human wayfinding, in which, given a network of links be-\ntween the concepts of Wikipedia, people play a game of finding a\nshort path from a given start to a given target concept by following\nhyperlinks. What distinguishes our setup from other studies of hu-\nman Web-browsing behavior is that in our case people navigate a\ngraph of connections between concepts, and that the exact goal of\nthe navigation is known ahead of time. We study more than 30,000\ngoal-directed human search paths and identify strategies people use\nwhen navigating information spaces. We find that human wayfind-\ning, while mostly very efficient, differs from shortest paths in char-\nacteristic ways. Most subjects navigate through high-degree hubs\nin the early phase, while their search is guided by content features\nthereafter. We also observe a trade-off between simplicity and ef-\nficiency: conceptually simple solutions are more common but tend\nto be less efficient than more complex ones. Finally, we consider\nthe task of predicting the target a user is trying to reach. We de-\nsign a model and an efficient learning algorithm. Such predictive\nmodels of human wayfinding can be applied in intelligent browsing\ninterfaces."}
{"Title": "Counting Beyond a Yottabyte, or how SPARQL 1.1\nProperty Paths will Prevent Adoption of the Standard", "Abstract": "SPARQL \u2013the standard query language for querying RDF\u2013 pro-\nvides only limited navigational functionalities, although these fea-\ntures are of fundamental importance for graph data formats such as\nRDF. This has led the W3C to include the property path feature in\nthe upcoming version of the standard, SPARQL 1.1.\nWetestedseveralimplementationsofSPARQL1.1handlingprop-\nerty path queries, and we observed that their evaluation methods\nfor this class of queries have a poor performance even in some\nvery simple scenarios. To formally explain this fact, we conduct\na theoretical study of the computational complexity of property\npaths evaluation. Our results imply that the poor performance of\nthe tested implementations is not a problem of these particular sys-\ntems, but of the specification itself. In fact, we show that any im-\nplementation that adheres to the SPARQL 1.1 specification (as of\nNovember 2011) is doomed to show the same behavior, the key\nissue being the need for counting solutions imposed by the current\nspecification. Weprovideseveralintractabilityresults, thattogether\nwith our empirical results, provide strong evidence against the cur-\nrent semantics of SPARQL 1.1 property paths. Finally, we put our\nresults in perspective, and propose a natural alternative semantics\nwith tractable evaluation, that we think may lead to a wide adoption\nof the language by practitioners, developers and theoreticians."}
{"Title": "Template-based Question Answering over RDF Data", "Abstract": "As an increasing amount of RDF data is published as Linked\nData, intuitive ways of accessing this data become more\nand more important. Question answering approaches have\nbeen proposed as a good compromise between intuitiveness\nand expressivity. Most question answering systems trans-\nlate questions into triples which are matched against the\nRDF data to retrieve an answer, typically relying on some\nsimilarity metric. However, in many cases, triples do not\nrepresent a faithful representation of the semantic structure\nof the natural language question, with the result that more\nexpressive queries can not be answered. To circumvent this\nproblem, we present a novel approach that relies on a parse\nof the question to produce a SPARQL template that directly\nmirrors the internal structure of the question. This template\nis then instantiated using statistical entity identification and\npredicate detection. We show that this approach is compet-\nitive and discuss cases of questions that can be answered\nwith our approach but not with competing approaches."}
{"Title": "On Directly Mapping Relational Databases to RDF and\nOWL", "Abstract": "Mapping relational databases to RDF is a fundamental problem\nfor the development of the Semantic Web. We present a solution,\ninspired by draft methods defined by the W3C where relational\ndatabases are directly mapped to RDF and OWL. Given a relational\ndatabase schema and its integrity constraints, this direct mapping\nproduces an OWL ontology, which, provides the basis for generat-\ning RDF instances. The semantics of this mapping is defined using\nDatalog. Two fundamental properties are information preservation\nand query preservation. We prove that our mapping satisfies both\nconditions, even for relational databases that contain null values.\nWe also consider two desirable properties: monotonicity and se-\nmantics preservation. We prove that our mapping is monotone and\nalso prove that no monotone mapping, including ours, is semantic\npreserving. We realize that monotonicity is an obstacle for seman-\ntic preservation and thus present a non-monotone direct mapping\nthat is semantics preserving."}
{"Title": "Practical End-to-End Web Content Integrity", "Abstract": "Widespread growth of open wireless hotspots has made it easy to\ncarry out man-in-the-middle attacks and impersonate web sites. Al-\nthough HTTPS can be used to prevent such attacks, its universal\nadoption is hindered by its performance cost and its inability to\nleverage caching at intermediate servers (such as CDN servers and\ncaching proxies) while maintaining end-to-end security.\nTo complement HTTPS, we revive an old idea from SHTTP, a\nprotocol that offers end-to-end web integrity without confidential-\nity. We name the protocol HTTPi and give it an efficient design that\nis easy to deploy for today\u2019s web. In particular, we tackle several\npreviously-unidentified challenges, such as supporting progressive\npage loading on the client\u2019s browser, handling mixed content, and\ndefining access control policies among HTTP, HTTPi, and HTTPS\ncontent from the same domain. Our prototyping and evaluation\nexperience show that HTTPi incurs negligible performance over-\nhead over HTTP, can leverage existing web infrastructure such as\nCDNs or caching proxies without any modifications to them, and\ncan make many of the mixed-content problems in existing HTTPS\nweb sites easily go away. Based on this experience, we advocate\nbrowser and web server vendors to adopt HTTPi."}
{"Title": "Serf and Turf: Crowdturfing for Fun and Profit", "Abstract": "Popular Internet services in recent years have shown that remark-\nable things can be achieved by harnessing the power of the masses\nusing crowd-sourcing systems. However, crowd-sourcing systems\ncan also pose a real challenge to existing security mechanisms de-\nployed to protect Internet services. Many of these security tech-\nniques rely on the assumption that malicious activity is generated\nautomatically by automated programs. Thus they would perform\npoorly or be easily bypassed when attacks are generated by real\nusersworking inacrowd-sourcing system. Through measurements,\nwe have found surprising evidence showing that not only do mali-\ncious crowd-sourcing systems exist, but they are rapidly growing\nin both user base and total revenue. We describe in this paper a sig-\nnificant effort to study and understand these crowdturfing systems\nin today\u2019s Internet. We use detailed crawls to extract data about the\nsize and operational structure of these crowdturfing systems. We\nanalyze details of campaigns offered and performed in these sites,\nand evaluate their end-to-end effectiveness by running active, be-\nnign campaigns of our own. Finally, we study and compare the\nsource of workers on crowdturfing sites in different countries. Our\nresults suggest that campaigns on these systems are highly effec-\ntive at reaching users, and their continuing growth poses a concrete\nthreat to online communities both in the US and elsewhere."}
{"Title": "Actions Speak as Loud as Words:\nPredicting Relationships from Social Behavior Data", "Abstract": "In recent years, new studies concentrating on analyzing user\npersonality and finding credible content in social media have\nbecome quite popular. Most such work augments features\nfrom textual content with features representing the user\u2019s\nsocial ties and the tie strength. Social ties are crucial in un-\nderstanding the network the people are a part of. However,\ntextual content is extremely useful in understanding topics\ndiscussed and the personality of the individual. We bring\na new dimension to this type of analysis with methods to\ncompute the type of ties individuals have and the strength\nof the ties in each dimension. We present a new genre of be-\nhavioral features that are able to capture the \u201cfunction\u201d of\na specific relationship without the help of textual features.\nOur novel features are based on the statistical properties of\ncommunication patterns between individuals such as reci-\nprocity, assortativity, attention and latency. We introduce a\nnew methodology for determining how such features can be\ncompared to textual features, and show, using Twitter data,\nthat our features can be used to capture contextual informa-\ntion present in textual features very accurately. Conversely,\nwe also demonstrate how textual features can be used to\ndetermine social attributes related to an individual."}
{"Title": "Echoes of Power: Language Effects and Power Differences\nin Social Interaction", "Abstract": "Understanding social interaction within groups is key to analyz-\ning online communities. Most current work focuses on structural\nproperties: who talks to whom, and how such interactions form\nlarger network structures. The interactions themselves, however,\ngenerally take place in the form of natural language \u2014 either spo-\nken or written \u2014 and one could reasonably suppose that signals\nmanifested in language might also provide information about roles,\nstatus, and other aspects of the group\u2019s dynamics. To date, how-\never, finding domain-independent language-based signals has been\na challenge.\nHere, we show that in group discussions, power differentials be-\ntween participants are subtly revealed by how much one individual\nimmediately echoes the linguistic style of the person they are re-\nsponding to. Starting from this observation, we propose an anal-\nysis framework based on linguistic coordination that can be used\nto shed light on power relationships and that works consistently\nacross multiple types of power \u2014 including a more \u201cstatic\u201d form\nof power based on status differences, and a more \u201csituational\u201d form\nof power in which one individual experiences a type of dependence\non another. Using this framework, we study how conversational\nbehavior can reveal power relationships in two very different set-\ntings: discussions among Wikipedians and arguments before the\nU. S. Supreme Court."}
{"Title": "Bimodal Invitation-Navigation Fair Bets Model\nfor Authority Identification in a Social Network", "Abstract": "We consider the problem of identifying the most respected,\nauthoritative members of a large-scale online social network\n(OSN) by constructing a global ranked list of its members.\nThe problem is distinct from the problem of identifying in-\nfluencers: we are interested in identifying members who are\ninfluential in the real world, even when not necessarily so\non the OSN. We focus on two sources for information about\nuser authority: (a) invitations to connect, which are usually\nsent to people whom the inviter respects, and (b) members\u2019\nbrowsing behavior, as profiles of more important people are\nviewed more often than others\u2019. We construct two directed\ngraphs over the same set of nodes (representing member\nprofiles): the invitation graph and the navigation graph re-\nspectively. We show that the standard PageRank algorithm,\na baseline in web page ranking, is not effective in people\nranking, and develop a social capital based model, called\nthe fair bets model, as a viable solution. We then propose\na novel approach, called bimodal fair bets, for combining\ninformation from two (or more) endorsement graphs drawn\nfrom the same OSN, by simultaneously using the authority\nscores of nodes in one graph to inform the other, and vice\nversa, in a mutually reinforcing fashion. We evaluate the\nranking results on the LinkedIn social network using this\nmodel, where members who have Wikipedia profiles are as-\nsumed to be authoritative. Experimental results show that\nour approach outperforms the baseline approach by a large\nmargin."}
{"Title": "Targeted Disambiguation of Ad-hoc, Homogeneous Sets of\nNamed Entities", "Abstract": "In many entity extraction applications, the entities to be\nrecognized are constrained to be from a list of \u201ctarget en-\ntities\u201d. In many cases, these target entities are (i) ad-hoc,\ni.e., do not exist in a knowledge base and (ii) homogeneous\n(e.g., all the entities are IT companies). We study the fol-\nlowing novel disambiguation problem in this unique setting:\ngiven the candidate mentions of all the target entities, deter-\nmine which ones are true mentions of a target entity. Prior\ntechniques only consider target entities present in a knowl-\nedge base and/or having a rich set of attributes. In this pa-\nper, we develop novel techniques that require no knowledge\nabout the entities except their names. Our main insight is to\nleverage the homogeneity constraint and disambiguate the\ncandidate mentions collectively across all documents. We\npropose a graph-based model, called MentionRank, for that\npurpose. Furthermore, if additional knowledge is available\nfor some or all of the entities, our model can leverage it to\nfurther improve quality. Our experiments demonstrate the\neffectiveness of our model. To the best of our knowledge,\nthis is the first work on targeted entity disambiguation for\nad-hoc entities."}
{"Title": "Collective Context-Aware Topic Models for Entity\nDisambiguation", "Abstract": "A crucial step in adding structure to unstructured data is to identify\nreferences to entities and disambiguate them. Such disambiguated\nreferencescanhelpenhancereadabilityanddrawsimilaritiesacross\ndifferent pieces of running text in an automated fashion. Previous\nresearch has tackled this problem by first forming a catalog of en-\ntities from a knowledge base, such as Wikipedia, and then using\nthis catalog to disambiguate references in unseen text. However,\nmost of the previously proposed models either do not use all text in\nthe knowledge base, potentially missing out on discriminative fea-\ntures, or do not exploit word-entity proximity to learn high-quality\ncatalogs. In this work, we propose topic models that keep track of\nthe context of every word in the knowledge base; so that words ap-\npearing within the same context as an entity are more likely to be\nassociated with that entity. Thus, our topic models utilize all text\npresent in the knowledge base and help learn high-quality catalogs.\nOur models also learn groups of co-occurring entities thus enabling\ncollective disambiguation. Unlike most previous topic models, our\nmodels are non-parametric and do not require the user to specify\nthe exact number of groups present in the knowledge base. In ex-\nperiments performed on an extract of Wikipedia containing almost\n60,000references, ourmodelsoutperformSVM-basedbaselinesby\nas much as 18% in terms of disambiguation accuracy translating to\nan increment of almost 11,000 correctly disambiguated references."}
{"Title": "Document Hierarchies from Text and Links", "Abstract": "Hierarchical taxonomies provide a multi-level view of large doc-\nument collections, allowing users to rapidly drill down to fine-\ngrained distinctions in topics of interest. We show that automat-\nically induced taxonomies can be made more robust by combining\ntext with relational links. The underlying mechanism is a Bayesian\ngenerative model in which a latent hierarchical structure explains\nthe observed data \u2014 thus, finding hierarchical groups of documents\nwith similar word distributions and dense network connections. As\na nonparametric Bayesian model, our approach does not require\npre-specification of the branching factor at each non-terminal, but\nfinds the appropriate level of detail directly from the data. Unlike\nmany prior latent space models of network structure, the complex-\nity of our approach does not grow quadratically in the number of\ndocuments, enabling application to networks with more than ten\nthousand nodes. Experimental results on hypertext and citation\nnetwork corpora demonstrate the advantages of our hierarchical,\nmultimodal approach."}
{"Title": "Mining Photo-sharing Websites to Study\nEcological Phenomena", "Abstract": "The popularity of social media websites like Flickr and Twitter has\ncreated enormous collections of user-generated content online. La-\ntent in these content collections are observations of the world: each\nphoto is a visual snapshot of what the world looked like at a par-\nticular point in time and space, for example, while each tweet is\na textual expression of the state of a person and his or her envi-\nronment. Aggregating these observations across millions of social\nsharing users could lead to new techniques for large-scale moni-\ntoring of the state of the world and how it is changing over time.\nIn this paper we step towards that goal, showing that by analyzing\nthe tags and image features of geo-tagged, time-stamped photos we\ncan measure and quantify the occurrence of ecological phenomena\nincluding ground snow cover, snow fall and vegetation density. We\ncompare several techniques for dealing with the large degree of\nnoise in the dataset, and show how machine learning can be used to\nreduce errors caused by misleading tags and ambiguous visual con-\ntent. We evaluate the accuracy of these techniques by comparing to\nground truth data collected both by surface stations and by Earth-\nobserving satellites. Besides the immediate application to ecology,\nour study gives insight into how to accurately crowd-source other\ntypes of information from large, noisy social sharing datasets."}
{"Title": "Learning from the Past:\nAnswering New Questions with Past Answers", "Abstract": "Community-based Question Answering sites, such as Yahoo!\nAnswers or Baidu Zhidao, allow users to get answers to com-\nplex, detailed and personal questions from other users. How-\never, since answering a question depends on the ability and\nwillingness of users to address the asker\u2019s needs, a significant\nfraction of the questions remain unanswered. We measured\nthat in Yahoo! Answers, this fraction represents 15% of all\nincoming English questions. At the same time, we discov-\nered that around 25% of questions in certain categories are\nrecurrent, at least at the question-title level, over a period\nof one year.\nWe attempt to reduce the rate of unanswered questions in\nYahoo! Answers by reusing the large repository of past re-\nsolved questions, openly available on the site. More specifi-\ncally, we estimate the probability whether certain new ques-\ntions can be satisfactorily answered by a best answer from\nthe past, using a statistical model specifically trained for\nthis task. We leverage concepts and methods from query-\nperformance prediction and natural language processing in\norder to extract a wide range of features for our model. The\nkey challenge here is to achieve a level of quality similar to\nthe one provided by the best human answerers.\nWe evaluated our algorithm on offline data extracted from\nYahoo! Answers, but more interestingly, also on online data\nby using three \u201clive\u201d answering robots that automatically\nprovide past answers to new questions when a certain degree\nof confidence is reached. We report the success rate of these\nrobots in three active Yahoo! Answers categories in terms of\nboth accuracy, coverage and askers\u2019 satisfaction. This work\npresents a first attempt, to the best of our knowledge, of\nautomatic question answering to questions of social nature,\nby reusing past answers of high quality."}
{"Title": "Discovering Geographical Topics In The Twitter Stream", "Abstract": "Micro-blogging services have become indispensable com-\nmunication tools for online users for disseminating break-\ning news, eyewitness accounts, individual expression, and\nprotest groups. Recently, Twitter, along with other on-\nline social networking services such as Foursquare, Gowalla,\nFacebook and Yelp, have started supporting location ser-\nvices in their messages, either explicitly, by letting users\nchoose their places, or implicitly, by enabling geo-tagging,\nwhich is to associate messages with latitudes and longitudes.\nThis functionality allows researchers to address an exciting\nset of questions: 1) How is information created and shared\nacross geographical locations, 2) How do spatial and linguis-\ntic characteristics of people vary across regions, and 3) How\nto model human mobility. Although many attempts have\nbeen made for tackling these problems, previous methods\nare either complicated to be implemented or oversimplified\nthat cannot yield reasonable performance.\nIt is a challenge task to discover topics and identify users\u2019\ninterests from these geo-tagged messages due to the sheer\namount of data and diversity of language variations used on\nthese location sharing services. In this paper we focus on\nTwitter and present an algorithm by modeling diversity in\ntweets based on topical diversity, geographical diversity, and\nan interest distribution of the user. Furthermore, we take\nthe Markovian nature of a user\u2019s location into account. Our\nmodel exploits sparse factorial coding of the attributes, thus\nallowing us to deal with a large and diverse set of covariates\nefficiently. Our approach is vital for applications such as\nuser profiling, content recommendation and topic tracking.\nWe show high accuracy in location estimation based on our\nmodel. Moreover, the algorithm identifies interesting topics\nbased on location and language."}
{"Title": "Declarative Platform for Data Sourcing Games", "Abstract": "Harnessing a crowd of users for the collection of mass data\n(data sourcing) has recently become a wide-spread practice.\nOne effective technique is based on games as a tool that at-\ntracts the crowd to contribute useful facts. We focus here\non the data management layer of such games, and observe\nthat the development of this layer involves challenges such\nas dealing with probabilistic data, combined with recursive\nmanipulation of this data. These challenges are difficult to\naddress using current declarative data management frame-\nworks, and we thus propose here a novel such framework,\nand demonstrate its usefulness in expressing different as-\npects in the data management of Trivia-like games. We have\nimplemented a system prototype with our novel data man-\nagement framework at its core, and we highlight key issues\nin the system design, as well as our experimentations that\nindicate the usefulness and scalability of the approach."}
{"Title": "Information Integration Over Time in Unreliable and\nUncertain Environments", "Abstract": "Often an interesting true value such as a stock price, sports\nscore, or current temperature is only available via the obser-\nvations of noisy and potentially conflicting sources. Several\ntechniques have been proposed to reconcile these conflicts\nby computing a weighted consensus based on source relia-\nbilities, but these techniques focus on static values. When\nthe real-world entity evolves over time, the noisy sources\ncan delay, or even miss, reporting some of the real-world up-\ndates. This temporal aspect introduces two key challenges\nfor consensus-based approaches: (i) due to delays, the map-\nping between a source\u2019s noisy observation and the real-world\nupdate it observes is unknown, and (ii) missed updates may\ntranslate to missing values for the consensus problem, even\nif the mapping is known.\nTo overcome these challenges, we propose a formal ap-\nproach that models the history of updates of the real-world\nentity as a hidden semi-Markovian process (HSMM). The\nnoisy sources are modeled as observations of the hidden\nstate, but the mapping between a hidden state (i.e. real-\nworld update) and the observation (i.e. source value) is un-\nknown. We propose algorithms based on Gibbs Sampling\nand EM to jointly infer both the history of real-world up-\ndates as well as the unknown mapping between them and the\nsource values. We demonstrate using experiments on real-\nworld datasets how our history-based techniques improve\nupon history-agnostic consensus-based approaches."}
{"Title": "SAFE Extensibility for Data-Driven Web Applications", "Abstract": "This paper presents a novel method for enabling fast devel-\nopment and easy customization of interactive data-intensive\nweb applications. Our approach is based on a high-level\nhierarchical programming model that results in both a very\nclean semantics of the application while at the same time cre-\nating well-defined interfaces for customization of application\ncomponents. A prototypical implementation of a conference\nmanagement system shows the efficacy of our approach."}
{"Title": "On the Analysis of Cascading Style Sheets", "Abstract": "Developing and maintaining cascading style sheets (CSS) is\nan important issue to web developers as they suffer from the\nlack of rigorous methods. Most existing means rely on val-\nidators that check syntactic rules, and on runtime debuggers\nthat check the behavior of a CSS style sheet on a particular\ndocument instance. However, the aim of most style sheets is\nto be applied to an entire set of documents, usually defined\nby some schema. To this end, a CSS style sheet is usually\nwritten w.r.t. a given schema. While usual debugging tools\nhelp reducing the number of bugs, they do not ultimately\nallow to prove properties over the whole set of documents to\nwhich the style sheet is intended to be applied.\nWe propose a novel approach to fill this lack. We intro-\nduce ideas borrowed from the fields of logic and compile-time\nverification for the analysis of CSS style sheets. We present\nan original tool based on recent advances in tree logics. The\ntool is capable of statically detecting a wide range of errors\n(such as empty CSS selectors and semantically equivalent\nselectors), as well as proving properties related to sets of\ndocuments (such as coverage of styling information), in the\npresence or absence of schema information. This new tool\ncan be used in addition to existing runtime debuggers to\nensure a higher level of quality of CSS style sheets."}
{"Title": "Extracting Client-side Web Application Code", "Abstract": "The web application domain is one of the fastest growing\nand most wide-spread application domains today. By uti-\nlizing fast, modern web browsers and advanced scripting\ntechniques, web developers are developing highly interactive\napplications that can, in terms of user-experience and re-\nsponsiveness, compete with standard desktop applications.\nA web application is composed of two equally important\nparts: the server-side and the client-side. The client-side\nacts as a user-interface to the application, and can be viewed\nas a collection of behaviors. Similar behaviors are often\nused in a large number of applications, and facilitating their\nreuse offers considerable benefits. However, due to client-\nside specifics, such as multi-language implementation and\nextreme dynamicity, identifying and extracting code respon-\nsible for a certain behavior is difficult. In this paper we\npresent a semi-automatic method for extracting client-side\nweb application code implementing a certain behavior. We\nshow how by analyzing the execution of a usage scenario,\ncode responsible for a certain behavior can be identified,\nhow dependencies between different parts of the application\ncan be tracked, and how in the end only the code responsi-\nble for a certain behavior can be extracted. Our evaluation\nshows that the method is capable of extracting stand-alone\nbehaviors, while achieving considerable savings in terms of\ncode size and application performance."}
{"Title": "OPAL: Automated Form Understanding for the Deep Web", "Abstract": "Forms are our gates to the web. They enable us to access the deep\ncontent of web sites. Automatic form understanding unlocks this\ncontent for applications ranging from crawlers to meta-search en-\ngines and is essential for improving usability and accessibility of\nthe web. Form understanding has received surprisingly little at-\ntention other than as component in specific applications such as\ncrawlers. No comprehensive approach to form understanding exists\nand previous works disagree even in the definition of the problem.\nIn this paper, we present OPAL , the first comprehensive approach\nto form understanding. We identify form labeling and form inter-\npretation as the two main tasks involved in form understanding. On\nboth problems OPAL pushes the state of the art: For form labeling,\nit combines signals from the text, structure, and visual rendering\nof a web page, yielding robust characterisations of common design\npatterns. In extensive experiments on the ICQ and TEL-8 bench-\nmarks and a set of 200 modern web forms OPAL outperforms previ-\nous approaches by a significant margin. For form interpretation, we\nintroduce a template language to describe frequent form patterns.\nThese two parts of OPAL combined yield form understanding with\nnear perfect accuracy (> 98%)."}
{"Title": "Online Team Formation in Social Networks", "Abstract": "We study the problem of online team formation. We con-\nsider a setting in which people possess different skills and\ncompatibility among potential team members is modeled by\na social network. A sequence of tasks arrives in an online\nfashion, and each task requires a specific set of skills. The\ngoal is to form a new team upon arrival of each task, so that\n(i) each team possesses all skills required by the task, (ii)\neach team has small communication overhead, and (iii) the\nworkload of performing the tasks is balanced among people\nin the fairest possible way.\nWe propose efficient algorithms that address all these re-\nquirements: our algorithms form teams that always sat-\nisfy the required skills, provide approximation guarantees\nwith respect to team communication overhead, and they are\nonline-competitive with respect to load balancing. Experi-\nments performed on collaboration networks among film ac-\ntors and scientists, confirm that our algorithms are success-\nful at balancing these conflicting requirements.\nThis is the first paper that simultaneously addresses all\nthese aspects. Previous work has either focused on minimiz-\ning coordination for a single task or balancing the workload\nneglecting coordination costs."}
{"Title": "Understanding Task-Driven Information Flow\nin Collaborative Networks", "Abstract": "Collaborative networks are a special type of social network\nformed by members who collectively achieve specific goals,\nsuch as fixing software bugs and resolving customers\u2019 prob-\nlems. In such networks, information flow among members\nis driven by the tasks assigned to the network, and by the\nexpertise of its members to complete those tasks. In this\nwork, we analyze real-life collaborative networks to under-\nstand their common characteristics and how information\nis routed in these networks. Our study shows that col-\nlaborative networks exhibit significantly different proper-\nties compared with other complex networks. Collaborative\nnetworks have truncated power-law node degree distribu-\ntions and other organizational constraints. Furthermore, the\nnumber of steps along which information is routed follows\na truncated power-law distribution. Based on these obser-\nvations, we developed a network model that can generate\nsynthetic collaborative networks subject to certain struc-\nture constraints. Moreover, we developed a routing model\nthat emulates task-driven information routing conducted by\nhuman beings in a collaborative network. Together, these\ntwo models can be used to study the efficiency of informa-\ntion routing for different types of collaborative networks \u2013\na problem that is important in practice yet difficult to solve\nwithout the method proposed in this paper."}
{"Title": "New Objective Functions for Social Collaborative Filtering", "Abstract": "This paper examines the problem of social collaborative fil-\ntering (CF) to recommend items of interest to users in a so-\ncial network setting. Unlike standard CF algorithms using\nrelatively simple user and item features, recommendation in\nsocial networks poses the more complex problem of learn-\ning user preferences from a rich and complex set of user\nprofile and interaction information. Many existing social\nCF methods have extended traditional CF matrix factor-\nization, but have overlooked important aspects germane to\nthe social setting. We propose a unified framework for so-\ncial CF matrix factorization by introducing novel objective\nfunctions for training. Our new objective functions have\nthree key features that address main drawbacks of existing\napproaches: (a) we fully exploit feature-based user similar-\nity, (b) we permit direct learning of user-to-user information\ndiffusion, and (c) we leverage co-preference (dis)agreement\nbetween two users to learn restricted areas of common inter-\nest. We evaluate these new social CF objectives, comparing\nthem to each other and to a variety of (social) CF baselines,\nand analyze user behavior on live user trials in a custom-\ndeveloped Facebook App involving data collected over five\nmonths from over 100 App users and their 37,000+ friends."}
{"Title": "Micropinion Generation: An Unsupervised Approach to\nGenerating Ultra-Concise Summaries of Opinions", "Abstract": "This paper presents a new unsupervised approach to gen-\nerating ultra-concise summaries of opinions. We formulate\nthe problem of generating such a micropinion summary as\nan optimization problem, where we seek a set of concise and\nnon-redundant phrases that are readable and represent key\nopinions in text. We measure representativeness based on a\nmodified mutual information function and model readability\nwith an n-gram language model. We propose some heuris-\ntic algorithms to efficiently solve this optimization problem.\nEvaluation results show that our unsupervised approach out-\nperforms other state of the art summarization methods and\nthe generated summaries are informative and readable."}
{"Title": "Mr. LDA: A Flexible Large Scale Topic Modeling Package\nusing Variational Inference in MapReduce", "Abstract": "Latent Dirichlet Allocation (LDA) is a popular topic modeling tech-\nnique for exploring document collections. Because of the increasing\nprevalence of large datasets, there is a need to improve the scal-\nability of inference for LDA. In this paper, we introduce a novel\nand flexible large scale topic modeling package in MapReduce (Mr.\nLDA). As opposed to other techniques which use Gibbs sampling,\nour proposed framework uses variational inference, which easily fits\ninto a distributed environment. More importantly, this variational\nimplementation, unlike highly tuned and specialized implementa-\ntions based on Gibbs sampling, is easily extensible. We demonstrate\ntwo extensions of the models possible with this scalable framework:\ninformed priors to guide topic discovery and extracting topics from\na multilingual corpus. We compare the scalability of Mr. LDA\nagainst Mahout, an existing large scale topic modeling package. Mr.\nLDA out-performs Mahout both in execution speed and held-out\nlikelihood."}
{"Title": "Trains of Thought: Generating Information Maps", "Abstract": "When information is abundant, it becomes increasingly diffi-\ncult to fit nuggets of knowledge into a sigle coherent picture.\nComplex stories spaghetti into branches, side stories, and in-\ntertwining narratives. In order to explore these stories, one\nneeds a map to navigate unfamiliar territory. We propose\na methodology for creating structured summaries of infor-\nmation, which we call metro maps. Our proposed algorithm\ngenerates a concise structured set of documents which max-\nimizes coverage of salient pieces of information. Most im-\nportantly, metro maps explicitly show the relations among\nretrieved pieces in a way that captures story development.\nWe first formalize characteristics of good maps and formu-\nlate their construction as an optimization problem. Then\nwe provide efficient methods with theoretical guarantees for\ngenerating maps. Finally, we integrate user interaction into\nour framework, allowing users to alter the maps to better\nreflect their interests. Pilot user studies with a real-world\ndataset demonstrate that the method is able to produce\nmaps which help users acquire knowledge efficiently."}
{"Title": "Learning Causality for News Events Prediction", "Abstract": "The problem we tackle in this work is, given a present news\nevent, to generate a plausible future event that can be caused\nby the given event. We present a new methodology for mod-\neling and predicting such future news events using machine\nlearning and data mining techniques. Our Pundit algorithm\ngeneralizes examples of causality pairs to infer a causality\npredictor. To obtain precise labeled causality examples, we\nmine 150 years of news articles, and apply semantic nat-\nural language modeling techniques to titles containing cer-\ntain predefined causality patterns. For generalization, the\nmodel uses a vast amount of world knowledge ontologies\nmined from LinkedData, containing 200 datasets with ap-\nproximately 20 billion relations. Empirical evaluation on\nreal news articles shows that our Pundit algorithm reaches\na human-level performance."}
{"Title": "Your Two Weeks of Fame and your Grandmother\u2019s", "Abstract": "Did celebrity last longer in 1929, 1992 or 2009? We in-\nvestigate the phenomenon of fame by mining a collection\nof news articles that spans the twentieth century, and also\nperform a side study on a collection of blog posts from the\nlast 10 years. By analyzing mentions of personal names, we\nmeasure each person\u2019s time in the spotlight, and watch the\ndistribution change from a century ago to a year ago. We\nexpected to find a trend of decreasing durations of fame as\nnews cycles accelerated and attention spans became shorter.\nInstead, we find a remarkable consistency through most of\nthe period we study. Through a century of rapid technologi-\ncal and societal change, through the appearance of Twitter,\ncommunication satellites and the Internet, we do not observe\na significant change in typical duration of celebrity. We also\nstudy the most famous of the famous, and find different re-\nsults depending on our method for measuring duration of\nfame. With a method that may be thought of as measuring\na spike of attention around a single narrow news story, we\nsee the same result as before: stories last as long now as they\ndid in 1930. A second method, which may be thought of as\nmeasuring the duration of public interest in a person, indi-\ncates that famous people\u2019s presence in the news is becoming\nlonger rather than shorter, an effect most likely driven by\nthe wider distribution and higher volume of media in mod-\nern times. Similar studies have been done with much shorter\ntimescales specifically in the context of information spread-\ning on Twitter and similar social networking site. However,\nto the best of our knowledge, this is the first massive scale\nstudy of this nature that spans over a century of archived\ndata, thereby allowing us to track changes across decades."}
{"Title": "A Unified Approach to Learning Task-Specific Bit Vector\nRepresentations for Fast Nearest Neighbor Search", "Abstract": "Fast nearest neighbor search is necessary for a variety of large scale\nweb applications such as information retrieval, nearest neighbor\nclassification and nearest neighbor regression. Recently a number\nof machine learning algorithms have been proposed for represent-\ning the data to be searched as (short) bit vectors and then using\nhashing to do rapid search. These algorithms have been limited in\ntheir applicability in that they are suited for only one type of task \u2013\ne.g. Spectral Hashing learns bit vector representations for retrieval,\nbut not say, classification. In this paper we present a unified ap-\nproach to learning bit vector representations for many applications\nthat use nearest neighbor search. The main contribution is a sin-\ngle learning algorithm that can be customized to learn a bit vector\nrepresentation suited for the task at hand. This broadens the useful-\nness of bit vector representations to tasks beyond just conventional\nretrieval.\nWe propose a learning-to-rank formulation to learn the bit vec-\ntor representation of the data. LambdaRank algorithm is used for\nlearning a function that computes a task-specific bit vector from an\ninput data vector. Our approach outperforms state-of-the-art near-\nest neighbor methods on a number of real world text and image\nclassification and retrieval datasets. It is scalable and learns a 32-\nbit representation on 1.46 million training cases in two days"}
{"Title": "Lightweight Automatic Face Annotation in Media Pages", "Abstract": "Labeling human faces in images contained in Web media\nstories enables enriching the user experience offered by me-\ndia sites. We propose a lightweight framework for automatic\nimage annotation that exploits named entities mentioned in\nthe article to significantly boost the accuracy of face recog-\nnition. While previous works in the area labor to train com-\nprehensive offline visual models for a pre-defined universe of\ncandidates, our approach models the people mentioned in a\ngiven story on the fly, using a standard Web image search\nengine as an image sampling mechanism. We overcome mul-\ntiple sources of noise introduced by this ad-hoc process, to\nbuild a fast and robust end-to-end system from off-the-shelf\nerror-prone text analysis and machine vision components. In\nexperiments conducted on approximately 900 faces depicted\nin 500 stories from a major celebrity news website, we were\nable to correctly label 81.5% of the faces while mislabeling\n14.8% of them."}
{"Title": "Distributed Graph Pattern Matching", "Abstract": "Graph simulation has been adopted for pattern matching\nto reduce the complexity and capture the need of novel ap-\nplications. With the rapid development of the Web and\nsocial networks, data is typically distributed over multiple\nmachines. Hence a natural question raised is how to evalu-\nate graph simulation on distributed data. To our knowledge,\nno such distributed algorithms are in place yet. This paper\nsettles this question by providing evaluation algorithms and\noptimizations for graph simulation in a distributed setting.\n(1) We study the impacts of components and data locali-\nty on the evaluation of graph simulation. (2) We give an\nanalysis of a large class of distributed algorithms, captured\nby a message-passing model, for graph simulation. We also\nidentify three complexity measures: visit times, makespan\nand data shipment, for analyzing the distributed algorithm-\ns, and show that these measures are essentially controversial\nwith each other. (3) We propose distributed algorithms and\noptimization techniques that exploit the properties of graph\nsimulation and the analyses of distributed algorithms. (4)\nWe experimentally verify the effectiveness and efficiency of\nthese algorithms, using both real-life and synthetic data."}
{"Title": "Towards Network-aware Service Composition in the Cloud", "Abstract": "Service-Oriented Computing (SOC) enables the composition\nof loosely coupled services provided with varying Quality of\nService (QoS) levels. Selecting a (near-)optimal set of ser-\nvices for a composition in terms of QoS is crucial when many\nfunctionally equivalent services are available. With the ad-\nvent of Cloud Computing, both the number of such services\nand their distribution across the network are rising rapidly,\nincreasing the impact of the network on the QoS of such\ncompositions. Despite this, current approaches do not dif-\nferentiate between the QoS of services themselves and the\nQoS of the network. Therefore, the computed latency differs\nsubstantially from the actual latency, resulting in subopti-\nmal QoS for service compositions in the cloud. Thus, we\npropose a network-aware approach that handles the QoS of\nservices and the QoS of the network independently. First,\nwe build a network model in order to estimate the network\nlatency between arbitrary services and potential users. Our\nselection algorithm then leverages this model to find com-\npositions that will result in a low latency given an employed\nexecution policy. In our evaluation, we show that our ap-\nproach efficiently computes compositions with much lower\nlatency than current approaches."}
{"Title": "Towards Robust Service Compositions in the Context of\nFunctionally Diverse Services", "Abstract": "Web service composition provides a means of customized\nand flexible integration of service functionalities. Quality-\nof-Service (QoS) optimization algorithms select services in\norder to adapt workflows to the non-functional requirements\nof the user. With increasing number of services in a work-\nflow, previous approaches fail to achieve a sufficient relia-\nbility. Moreover, expensive ad-hoc replanning is required to\ndeal with service failures. The major problem with such se-\nquential application of planning and replanning is that it ig-\nnores the potential costs during the initial planning and they\nconsequently are hidden from the decision maker. Our ba-\nsic idea to overcome this substantial problem is to compute\na QoS optimized selection of service clusters that includes\na sufficient number of backup services for each service em-\nployed. To support the human decision maker in the service\nselection task, our approach considers the possible repair\ncosts directly in the initial composition. On the basis of a\nmulti-objective approach and using a suitable service selec-\ntion interface, the decision maker can select compositions in\nline with his/her personal risk preferences."}
{"Title": "CloudGenius: Decision Support for Web Server Cloud\nMigration", "Abstract": "Cloud computing is the latest computing paradigm that de-\nlivers hardware and software resources as virtualized ser-\nvices in which users are free from the burden of worry-\ning about the low-level system administration details. Mi-\ngrating Web applications to Cloud services and integrat-\ning Cloud services into existing computing infrastructures\nis non-trivial. It leads to new challenges that often require\ninnovation of paradigms and practices at all levels: techni-\ncal, cultural, legal, regulatory, and social. The key problem\nin mapping Web applications to virtualized Cloud services\nis selecting the best and compatible mix of software images\n(e.g., Web server image) and infrastructure services to en-\nsure that Quality of Service (QoS) targets of an applica-\ntion are achieved. The fact that, when selecting Cloud ser-\nvices, engineers must consider heterogeneous sets of criteria\nand complex dependencies between infrastructure services\nand software images, which are impossible to resolve man-\nually, is a critical issue. To overcome these challenges, we\npresent a framework (called CloudGenius) which automates\nthe decision-making process based on a model and factors\nspecifically for Web server migration to the Cloud. Cloud-\nGenius leverages a well known multi-criteria decision making\ntechnique, called Analytic Hierarchy Process, to automate\nthe selection process based on a model, factors, and QoS pa-\nrameters related to an application. An example application\ndemonstrates the applicability of the theoretical CloudGe-\nnius approach. Moreover, we present an implementation of\nCloudGenius that has been validated through experiments."}
{"Title": "Max Algorithms in Crowdsourcing Environments", "Abstract": "Our work investigates the problem of retrieving the maxi-\nmum item from a set in crowdsourcing environments. We\nfirst develop parameterized families of max algorithms, that\ntake as input a set of items and output an item from the\nset that is believed to be the maximum. Such max algo-\nrithms could, for instance, select the best Facebook profile\nthat matches a given person or the best photo that describes\na given restaurant. Then, we propose strategies that select\nappropriate max algorithm parameters. Our framework sup-\nports various human error and cost models and we consider\nmany of them for our experiments. We evaluate under many\nmetrics, both analytically and via simulations, the tradeoff\nbetween three quantities: (1) quality, (2) monetary cost,\nand (3) execution time. Also, we provide insights on the\neffectiveness of the strategies in selecting appropriate max\nalgorithm parameters and guidelines for choosing max algo-\nrithms and strategies for each application."}
{"Title": "Crowdsourcing with Endogenous Entry", "Abstract": "We investigate the design of mechanisms to incentivize\nhigh quality outcomes in crowdsourcing environments with\nstrategic agents, when entry is an endogenous, strategic\nchoice. Modeling endogenous entry in crowdsourcing mar-\nkets is important because there is a nonzero cost to making\na contribution of any quality which can be avoided by not\nparticipating, and indeed many sites based on crowdsourced\ncontent do not have adequate participation. We use a mech-\nanism with monotone, rank-based, rewards in a model where\nagents strategically make participation and quality choices\nto capture a wide variety of crowdsourcing environments,\nranging from conventional crowdsourcing contests with mon-\netary rewards such as TopCoder, to crowdsourced content\nas in online Q&A forums.\nWe begin by explicitly constructing the unique mixed-\nstrategy equilibrium for such monotone rank-order mech-\nanisms, and use the participation probability and distribu-\ntion of qualities from this construction to address the ques-\ntion of designing incentives for two kinds of rewards that\narise in the context of crowdsourcing. We first show that\nfor attention rewards that arise in the crowdsourced content\nsetting, the entire equilibrium distribution and therefore ev-\nery increasing statistic including the maximum and average\nquality (accounting for participation), improves when the\nrewards for every rank but the last are as high as possible.\nIn particular, when the cost of producing the lowest possi-\nble quality content is low, the optimal mechanism displays\nall but the poorest contribution. We next investigate how\nto allocate rewards in settings where there is a fixed total\nreward that can be arbitrarily distributed amongst partic-\nipants, as in crowdsourcing contests. Unlike models with\nexogenous entry, here the expected number of participants\ncan be increased by subsidizing entry, which could poten-\ntially improve the expected value of the best contribution.\nHowever, we show that subsidizing entry does not improve\nthe expected quality of the best contribution, although it\nmay improve the expected quality of the average contribu-\ntion. In fact, we show that free entry is dominated by taxing\nentry\u2014 making all entrants pay a small fee, which is rebated\nto the winner along with whatever rewards were already as-\nsigned, can improve the quality of the best contribution over\na winner-take-all contest with no taxes."}
{"Title": "Answering Search Queries with CrowdSearcher", "Abstract": "Web users are increasingly relying on social interaction to\ncomplete and validate the results of their search activities. While\nsearch systems are superior machines to get world-wide\ninformation, the opinions collected within friends and expert/local\ncommunities can ultimately determine our decisions: human\ncuriosity and creativity is often capable of going much beyond the\ncapabilities of search systems in scouting \u201cinteresting\u201d results, or\nsuggesting new, unexpected search directions. Such personalized\ninteraction occurs in most times aside of the search systems and\nprocesses, possibly instrumented and mediated by a social\nnetwork; when such interaction is completed and users resort to\nthe use of search systems, they do it through new queries, loosely\nrelated to the previous search or to the social interaction.\nIn this paper we propose CrowdSearcher, a novel search paradigm\nthat embodies crowds as first-class sources for the information\nseeking process. CrowdSearcher aims at filling the gap between\ngeneralized search systems, which operate upon world-wide\ninformation - including facts and recommendations as crawled\nand indexed by computerized systems \u2013 with social systems,\ncapable of interacting with real people, in real time, to capture\ntheir opinions, suggestions, emotions. The technical contribution\nof this paper is the discussion of a model and architecture for\nintegrating computerized search with human interaction, by\nshowing how search systems can drive and encapsulate social\nsystems. In particular we show how social platforms, such as\nFacebook, LinkedIn and Twitter, can be used for crowdsourcing\nsearch-related tasks; we demonstrate our approach with several\nprototypes and we report on our experiment upon real user\ncommunities."}
{"Title": "Vertex Collocation Profiles: Subgraph Counting for Link\nAnalysis and Prediction", "Abstract": "We introduce the concept of a vertex collocation profile\n(VCP) for the purpose of topological link analysis and pre-\ndiction. VCPs provide nearly complete information about\nthe surrounding local structure of embedded vertex pairs.\nThe VCP approach offers a new tool for domain experts\nto understand the underlying growth mechanisms in their\nnetworks and to analyze link formation mechanisms in the\nappropriate sociological, biological, physical, or other con-\ntext. The same resolution that gives VCP its analytical\npower also enables it to perform well when used in super-\nvised models to discriminate potential new links. We first\ndevelop the theory, mathematics, and algorithms underly-\ning VCPs. Then we demonstrate VCP methods performing\nlink prediction competitively with unsupervised and super-\nvised methods across several different network families. We\nconclude with timing results that introduce the comparative\nperformance of several existing algorithms and the practica-\nbility of VCP computations on large networks."}
{"Title": "Framework and Algorithms for Network Bucket Testing", "Abstract": "Bucket testing, also known as split testing, A/B testing, or\n0/1 testing, is a widely used method for evaluating users\u2019\nsatisfaction with new features, products, or services. In or-\nder not to expose the whole user base to the new service,\nthe mean user satisfaction rate is estimated by exposing the\nservice only to a few uniformly chosen random users. In\na recent work, Backstrom and Kleinberg, defined the no-\ntion of network bucket testing for social services. In this\ncontext, users\u2019 interactions are only valid for measurement\nif some minimal number of their friends are also given the\nservice. The goal is to estimate the mean user satisfaction\nrate while providing the service to the least number of users.\nThis constraint makes uniform sampling, which is optimal\nfor the traditional case, grossly inefficient.\nIn this paper we introduce a simple general framework for\ndesigning and evaluating sampling techniques for network\nbucket testing. The framework is constructed in a way that\nsampling algorithms are only required to generate sets of\nusers to which the service should be provided. Given an\nalgorithm, the framework produces an unbiased user satis-\nfaction rate estimator and a corresponding variance bound\nfor any network and any user satisfaction function. Further-\nmore, we present several simple sampling algorithms that are\nevaluated using both synthetic and real social networks. Our\nexperiments corroborate the theoretical results and demon-\nstrate the effectiveness of the proposed framework and algo-\nrithms."}
{"Title": "Winner Takes All:\nCompeting Viruses or Ideas on fair-play Networks", "Abstract": "Given two competing products (or memes, or viruses etc.)\nspreading over a given network, can we predict what will\nhappen at the end, that is, which product will \u2019win\u2019, in terms\nof highest market share? One may na\u00a8\u0131vely expect that the\nbetter product (stronger virus) will just have a larger foot-\nprint, proportional to the quality ratio of the products (or\nstrength ratio of the viruses). However, we prove the sur-\nprising result that, under realistic conditions, for any graph\ntopology, the stronger virus completely wipes-out the weaker\none, thus not merely \u2018winning\u2019 but \u2018taking it all\u2019. In addi-\ntion to the proofs, we also demonstrate our result with sim-\nulations over diverse, real graph topologies, including the\nsocial-contact graph of the city of Portland OR (about 31\nmillion edges and 1 million nodes) and internet AS router\ngraphs. Finally, we also provide real data about competing\nproducts from Google-Insights, like Facebook-Myspace, and\nwe show again that they agree with our analysis."}
{"Title": "A Dual-Mode User Interface for Accessing 3D Content\non the World Wide Web", "Abstract": "The Web evolved from a text-based system to the current\nrich and interactive medium that supports images, 2D graph-\nics, audio and video. The major media type that is still\nmissing is 3D graphics. Although various approaches have\nbeen proposed (most notably VRML/X3D), they have not\nbeen widely adopted. One reason for the limited acceptance\nis the lack of 3D interaction techniques that are optimal\nfor the hypertext-based Web interface. We present a novel\nstrategy for accessing integrated information spaces, where\nhypertext and 3D graphics data are simultaneously avail-\nable and linked. We introduce a user interface that has\ntwo modes between which a user can switch anytime: the\ndriven by simple hypertext-based interactions \u201ddon\u2019t-make-\nme-think\u201dmode, where a 3D scene is embedded in hypertext\nand the more immersive 3D \u201dtake-me-to-the-Wonderland\u201d\nmode, which immerses the hypertextual annotations into the\n3D scene. A user study is presented, which characterizes the\nuser interface in terms of its efficiency and usability."}
{"Title": "Exploiting Single-User Web Applications for Shared\nEditing - A Generic Transformation Approach", "Abstract": "In the light of the Web 2.0 movement, web-based collabo-\nration tools such as Google Docs have become mainstream\nand in the meantime serve millions of users. Apart from es-\ntablished collaborative web applications, numerous web ed-\nitors lack multi-user support even though they are suitable\nfor collaborative work. Enhancing these single-user editors\nwith shared editing capabilities is a costly endeavor since the\nimplementation of a collaboration infrastructure (accommo-\ndating conflict resolution, document synchronization, etc.)\nis required. In this paper, we present a generic transforma-\ntion approach capable of converting single-user web editors\ninto multi-user editors. Since our approach only requires\nthe configuration of a generic collaboration infrastructure\n(GCI), the effort to inject shared editing support is signif-\nicantly reduced in contrast to conventional implementation\napproaches neglecting reuse. We also report on experimen-\ntal results of a user study showing that converted editors\nmeet user requirements with respect to software and col-\nlaboration qualities. Moreover, we define the characteristics\nthat editors must adhere to in order to leverage the GCI."}
{"Title": "\u201cIt\u2019s Simply Integral to What I do\u201d:\nEnquiries into how the Web is Weaved into Everyday Life", "Abstract": "This paper presents findings from a field study of 24 individuals\nwho kept diaries of their web use, across device and location, for\na period of four days. Our focus was on how the web was used for\nnon-work purposes, with a view to understanding how this is\nintertwined with everyday life. While our initial aim was to\nupdate existing frameworks of \u2018web activities\u2019, such as those\ndescribed by Sellen et al. [25] and Kellar et al. [14], our data lead\nus to suggest that the notion of \u2018web activity\u2019 is only partially\nuseful for an analytic understanding of what it is that people do\nwhen they go online. Instead, our analysis leads us to present five\nmodes of web use, which can be used to frame and enrich\ninterpretations of \u2018activity\u2019. These are respite, orienting,\nopportunistic use, purposeful use and lean-back internet. We then\nconsider two properties of the web that enable it to be tailored to\nthese different modes, persistence and temporality, and close by\nsuggesting ways of drawing upon these qualities in order to\ninform design."}
{"Title": "Opinion Analysis on CAW 2.0 Datasets", "Abstract": "In this paper, we aim to classify non-domain-specific web articles\ninto three types: factual, opinion positive and opinion negative, on\nthe Web 2.0 corpus provided by the CAW 2.0 workshop. We\ninvestigate the fields in these Web 2.0 articles and extract features\nfrom their contents for developing the classifier. On the one hand,\nto fully utilize the characteristics of Web 2.0 data, the metadata of\nthe articles are extracted as features for the classifier. On the\nother hand, words, word sequences, and linguistic cues are also\nextracted to represent the contents of the articles. An SVM\nclassifier is applied here. Features extracted from the fields\navailable in one dataset are compared with those available in all\ndatasets. Moreover, to extract the sentence structures and\ngrammatical relations as features, a parser is applied on the strings\nof body contents. Experiment results show that metadata are\nuseful in the classification process, and despite of the evaluating\nscore, the helpfulness field and the author field are two most\nuseful features. As the training data released by CAW 2.0\nworkshop  are  categorized  into  recommended  and  not\nrecommended, accuracy 0.985, f-score 0.992 for retrieving\nrecommended comments and f-score 0.871 for retrieving not\nrecommended comments are achieved by the proposed SVM\nclassifier using features extracted from both metadata and\ncontents of comments. Then a method for mapping our results to\nthe requested answers of CAW 2.0 workshop is proposed to\ngenerate the testing results in the opinion analysis track."}
{"Title": "News Article Extraction\nwith Template-Independent Wrapper", "Abstract": "We consider the problem of template-independent news\nextraction. The state-of-the-art news extraction method is based\non template-level wrapper induction, which has two serious\nlimitations. 1) It cannot correctly extract pages belonging to an\nunseen template until the wrapper for that template has been\ngenerated. 2) It is costly to maintain up-to-date wrappers for\nhundreds of websites, because any change of a template may\nlead to the invalidation of the corresponding wrapper. In this\npaper we formalize news extraction as a machine learning\nproblem and learn a template-independent wrapper using a very\nsmall number of labeled news pages from a single site. Novel\nfeatures dedicated to news titles and bodies are developed\nrespectively. Correlations between the news title and the news\nbody are exploited. Our template-independent wrapper can\nextract news pages from different sites regardless of templates.\nIn experiments, a wrapper is learned from 40 pages from a\nsingle news site. It achieved 98.1% accuracy over 3,973 news\npages from 12 news sites."}
{"Title": "A Dynamic Bayesian Network Click Model\nfor Web Search Ranking", "Abstract": "As with any application of machine learning, web search\nranking requires labeled data. The labels usually come in\nthe form of relevance assessments made by editors. Click\nlogs can also provide an important source of implicit feed-\nback and can be used as a cheap proxy for editorial labels.\nThe main difficulty however comes from the so called posi-\ntion bias \u2014 urls appearing in lower positions are less likely\nto be clicked even if they are relevant. In this paper, we\npropose a Dynamic Bayesian Network which aims at pro-\nviding us with unbiased estimation of the relevance from\nthe click logs. Experiments show that the proposed click\nmodel outperforms other existing click models in predicting\nboth click-through rate and relevance."}
{"Title": "Click Chain Model in Web Search", "Abstract": "Given a terabyte click log, can we build an efficient and ef-\nfective click model? It is commonly believed that web search\nclick logs are a gold mine for search business, because they\nreflect users\u2019 preference over web documents presented by\nthe search engine. Click models provide a principled ap-\nproach to inferring user-perceived relevance of web docu-\nments, which can be leveraged in numerous applications in\nsearch businesses. Due to the huge volume of click data,\nscalability is a must.\nWe present the click chain model (CCM), which is based\non a solid, Bayesian framework. It is both scalable and in-\ncremental, perfectly meeting the computational challenges\nimposed by the voluminous click logs that constantly grow.\nWe conduct an extensive experimental study on a data set\ncontaining 8.8 million query sessions obtained in July 2008\nfrom a commercial search engine. CCM consistently outper-\nforms two state-of-the-art competitors in a number of met-\nrics, with over 9.7% better log-likelihood, over 6.2% better\nclick perplexity and much more robust (up to 30%) predic-\ntion of the first and the last clicked position."}
{"Title": "Spatio-Temporal Models for Estimating Click-through Rate", "Abstract": "We propose novel spatio-temporal models to estimate click-\nthrough rates in the context of content recommendation. We\ntrack article CTR at a fixed location over time through a dy-\nnamic Gamma-Poisson model and combine information from\ncorrelated locations through dynamic linear regressions, sig-\nnificantly improving on per-location model. Our models ad-\njust for user fatigue through an exponential tilt to the first-\nview CTR (probability of click on first article exposure) that\nis based only on user-specific repeat-exposure features. We\nillustrate our approach on data obtained from a module (To-\nday Module) published regularly on Yahoo! Front Page and\ndemonstrate significant improvement over commonly used\nbaseline methods. Large scale simulation experiments to\nstudy the performance of our models under different sce-\nnarios provide encouraging results. Throughout, all model-\ning assumptions are validated via rigorous exploratory data\nanalysis."}
{"Title": "Fast Dynamic Reranking in Large Graphs", "Abstract": "In this paper we consider the problem of re-ranking search\nresults by incorporating user feedback. We present a graph\ntheoretic measure for discriminating irrelevant results from\nrelevant results using a few labeled examples provided by\nthe user. The key intuition is that nodes relatively closer\n(in graph topology) to the relevant nodes than the irrele-\nvant nodes are more likely to be relevant. We present a\nsimple sampling algorithm to evaluate this measure at spe-\ncific nodes of interest, and an efficient branch and bound\nalgorithm to compute the top k nodes from the entire graph\nunder this measure. On quantifiable prediction tasks the\nintroduced measure outperforms other diffusion-based prox-\nimity measures which take only the positive relevance feed-\nback into account. On the Entity-Relation graph built from\nthe authors and papers of the entire DBLP citation corpus\n(1.4 million nodes and 2.2 million edges) our branch and\nbound algorithm takes about 1.5 seconds to retrieve the top\n10 nodes w.r.t. this measure with 10 labeled nodes."}
{"Title": "Estimating the ImpressionRank of Web Pages", "Abstract": "The ImpressionRank of a web page (or, more generally, of\na web site) is the number of times users viewed the page\nwhile browsing search results. ImpressionRank captures the\nvisibility of pages and sites in search engines and is thus an\nimportant measure, which is of interest to web site owners,\ncompetitors, market analysts, and end users.\nAll previous approaches to estimating the ImpressionRank\nof a page rely on privileged access to private data sources,\nlike the search engine\u2019s query log. In this paper we present\nthe first external algorithm for estimating the Impression-\nRank of a web page. This algorithm relies on access to three\npublic data sources: the search engine, the query suggestion\nservice of the search engine, and the web. In addition, the\nalgorithm is local and uses modest resources. It can therefore\nbe used by almost any party to estimate the ImpressionRank\nof any page on any search engine.\nEn route to estimating the ImpressionRank of a page, our\nalgorithm solves a novel variant of the keyword extraction\nproblem: it finds the most popular search keywords that\ndrive impressions of a page.\nEmpirical analysis of the algorithm on the Google and Ya-\nhoo! search engines indicates that it is accurate and provides\ninteresting insights about sites and search queries."}
{"Title": "Learning to Recognize Reliable Users and Content in\nSocial Media with Coupled Mutual Reinforcement", "Abstract": "Community Question Answering (CQA) has emerged as a\npopular forum for users to pose questions for other users to\nanswer. Over the last few years, CQA portals such as Naver\nand Yahoo! Answers have exploded in popularity, and now\nprovide a viable alternative to general purpose Web search.\nAt the same time, the answers to past questions submit-\nted in CQA sites comprise a valuable knowledge repository\nwhich could be a gold mine for information retrieval and\nautomatic question answering. Unfortunately, the quality\nof the submitted questions and answers varies widely - in-\ncreasingly so that a large fraction of the content is not usable\nfor answering queries. Previous approaches for retrieving\nrelevant and high quality content have been proposed, but\nthey require large amounts of manually labeled data \u2013 which\nlimits the applicability of the supervised approaches to new\nsites and domains. In this paper we address this problem\nby developing a semi-supervised coupled mutual reinforce-\nment framework for simultaneously calculating content qual-\nity and user reputation, that requires relatively few labeled\nexamples to initialize the training process. Results of a large\nscale evaluation demonstrate that our methods are more ef-\nfective than previous approaches for finding high-quality an-\nswers, questions, and users. More importantly, our quality\nestimation significantly improves the accuracy of search over\nCQA archives over the state-of-the-art methods."}
{"Title": "Detecting the Origin of Text Segments Efficiently", "Abstract": "In the origin detection problem an algorithm is given a set\nS of documents, ordered by creation time, and a query doc-\nument D. It needs to output for every consecutive sequence\nof k alphanumeric terms in D the earliest document in S\nin which the sequence appeared (if such a document exists).\nAlgorithms for the origin detection problem can, for exam-\nple, be used to detect the\u201corigin\u201dof text segments in D and\nthus to detect novel content in D. They can also find the\ndocument from which the author of D has copied the most\n(or show that D is mostly original.)\nWe propose novel algorithms for this problem and evaluate\nthem together with a large number of previously published\nalgorithms. Our results show that (1) detecting the origin of\ntext segments efficiently can be done with very high accuracy\neven when the space used is less than 1% of the size of\nthe documents in S, (2) the precision degrades smoothly\nwith the amount of available space, (3) various estimation\ntechniques can be used to increase the performance of the\nalgorithms."}
{"Title": "Enhancing Diversity, Coverage and Balance for\nSummarization through Structure Learning", "Abstract": "Document summarization plays an increasingly important\nrole with the exponential growth of documents on the Web.\nMany supervised and unsupervised approaches have been\nproposed to generate summaries from documents. However,\nthese approaches seldom simultaneously consider summary\ndiversity, coverage, and balance issues which to a large ex-\ntent determine the quality of summaries. In this paper, we\nconsider extract-based summarization emphasizing the fol-\nlowing three requirements: 1) diversity in summarization,\nwhich seeks to reduce redundancy among sentences in the\nsummary; 2) sufficient coverage, which focuses on avoiding\nthe loss of the document\u2019s main information when gener-\nating the summary; and 3) balance, which demands that\ndifferent aspects of the document need to have about the\nsame relative importance in the summary. We formulate the\nextract-based summarization problem as learning a mapping\nfrom a set of sentences of a given document to a subset of\nthe sentences that satisfies the above three requirements.\nThe mapping is learned by incorporating several constraints\nin a structure learning framework, and we explore the graph\nstructure of the output variables and employ structural SVM\nfor solving the resulted optimization problem. Experiments\non the DUC2001 data sets demonstrate significant perfor-\nmance improvements in terms of F 1 and ROUGE metrics."}
{"Title": "Efficient Overlap and Content Reuse Detection in Blogs\nand Online News Articles", "Abstract": "The use of blogs to track and comment on real world (po-\nlitical, news, entertainment) events is growing. Similarly, as\nmore individuals start relying on the Web as their primary\ninformation source and as more traditional media outlets try\nreaching consumers through alternative venues, the number\nof news sites on the Web is also continuously increasing.\nContent-reuse, whether in the form of extensive quotations\nor content borrowing across media outlets, is very common in\nblogs and news entries outlets tracking the same real-world\nevent. Knowledge about which web entries re-use content\nfrom which others can be an effective asset when organiz-\ning these entries for presentation. On the other hand, this\nknowledge is not cheap to acquire: considering the size of\nthe related space web entries, it is essential that the tech-\nniques developed for identifying re-use are fast and scalable.\nFurthermore, the dynamic nature of blog and news entries\nnecessitates incremental processing for reuse detection. In\nthis paper, we develop a novel qSign algorithm that effi-\nciently and effectively analyze the blogosphere for quotation\nand reuse identification. Experiment results show that with\nqSign processing time gains from 10X to 100X are possi-\nble while maintaining reuse detection rates of upto 90%.\nFurthermore, processing time gains can be pushed multiple\norders of magnitude (from 100X to 1000X) for 70% recall."}
{"Title": "Latent Space Domain Transfer between High Dimensional\nOverlapping Distributions", "Abstract": "Transferring knowledge from one domain to another is chal-\nlenging due to a number of reasons. Since both conditional\nand marginal distribution of the training data and test data\nare non-identical, model trained in one domain, when di-\nrectly applied to a different domain, is usually low in ac-\ncuracy. For many applications with large feature sets, such\nas text document, sequence data, medical data, image data\nof different resolutions, etc. two domains usually do not\ncontain exactly the same features, thus introducing large\nnumbers of\u201cmissing values\u201dwhen considered over the union\nof features from both domains. In other words, its marginal\ndistributions are at most overlapping. In the same time,\nthese problems are usually high dimensional, such as, sev-\neral thousands of features. Thus, the combination of high\ndimensionality and missing values make the relationship in\nconditional probabilities between two domains hard to mea-\nsure and model. To address these challenges, we propose\na framework that first brings the marginal distributions of\ntwo domains closer by \u201cfilling up\u201d those missing values of\ndisjoint features. Afterwards, it looks for those compara-\nble sub-structures in the\u201clatent-space\u201das mapped from the\nexpanded feature vector, where both marginal and condi-\ntional distribution are similar. With these sub-structures in\nlatent space, the proposed approach then find common con-\ncepts that are transferable across domains with high prob-\nability. During prediction, unlabeled instances are treated\nas \u201cqueries\u201d, the mostly related labeled instances from out-\ndomain are retrieved, and the classification is made by weighted\nvoting using retrieved out-domain examples. We formally\nshow that importing feature values across domains and latent-\nsemantic index can jointly make the distributions of two\nrelated domains easier to measure than in original feature\nspace, the nearest neighbor method employed to retrieve re-\nlated out domain examples is bounded in error when predict-\ning in-domain examples. Software and datasets are available\nfor download."}
{"Title": "StatSnowball: a Statistical Approach to Extracting Entity\nRelationships", "Abstract": "Traditional relation extraction methods require pre-specified\nrelations and relation-specific human-tagged examples. Boot-\nstrapping systems significantly reduce the number of train-\ning examples, but they usually apply heuristic-based meth-\nods to combine a set of strict hard rules, which limit the\nability to generalize and thus generate a low recall. Further-\nmore, existing bootstrapping methods do not perform open\ninformation extraction (Open IE), which can identify var-\nious types of relations without requiring pre-specifications.\nIn this paper, we propose a statistical extraction framework\ncalled Statistical Snowball (StatSnowball), which is a boot-\nstrapping system and can perform both traditional relation\nextraction and Open IE.\nStatSnowball uses the discriminative Markov logic net-\nworks (MLNs) and softens hard rules by learning their weights\nin a maximum likelihood estimate sense. MLN is a general\nmodel, and can be configured to perform different levels of\nrelation extraction. In StatSnwoball, pattern selection is\nperformed by solving an \u2113 1 -norm penalized maximum like-\nlihood estimation, which enjoys well-founded theories and\nefficient solvers. We extensively evaluate the performance of\nStatSnowball in different configurations on both a small but\nfully labeled data set and large-scale Web data. Empirical\nresults show that StatSnowball can achieve a significantly\nhigher recall without sacrificing the high precision during it-\nerations with a small number of seeds, and the joint inference\nof MLN can improve the performance. Finally, StatSnowball\nis efficient and we have developed a working entity relation\nsearch engine called Renlifang based on it."}
{"Title": "Matchbox: Large Scale Online Bayesian\nRecommendations", "Abstract": "We present a probabilistic model for generating personalised\nrecommendations of items to users of a web service. The\nMatchbox system makes use of content information in the\nform of user and item meta data in combination with col-\nlaborative filtering information from previous user behavior\nin order to predict the value of an item for a user. Users and\nitems are represented by feature vectors which are mapped\ninto a low-dimensional \u2018trait space\u2019 in which similarity is\nmeasured in terms of inner products. The model can be\ntrained from different types of feedback in order to learn\nuser-item preferences. Here we present three alternatives:\ndirect observation of an absolute rating each user gives to\nsome items, observation of a binary preference (like/ don\u2019t\nlike) and observation of a set of ordinal ratings on a user-\nspecific scale. Efficient inference is achieved by approxi-\nmate message passing involving a combination of Expecta-\ntion Propagation (EP) and Variational Message Passing. We\nalso include a dynamics model which allows an item\u2019s popu-\nlarity, a user\u2019s taste or a user\u2019s personal rating scale to drift\nover time. By using Assumed-Density Filtering (ADF) for\ntraining, the model requires only a single pass through the\ntraining data. This is an on-line learning algorithm capable\nof incrementally taking account of new data so the system\ncan immediately reflect the latest user preferences. We eval-\nuate the performance of the algorithm on the MovieLens and\nNetflix data sets consisting of approximately 1,000,000 and\n100,000,000 ratings respectively. This demonstrates that\ntraining the model using the on-line ADF approach yields\nstate-of-the-art performance with the option of improving\nperformance further if computational resources are available\nby performing multiple EP passes over the training data."}
{"Title": "Learning Consensus Opinion:\nMining Data from a Labeling Game", "Abstract": "We consider the problem of identifying the consensus rank-\ning for the results of a query, given preferences among those\nresults from a set of individual users. Once consensus rank-\nings are identified for a set of queries, these rankings can\nserve for both evaluation and training of retrieval and learn-\ning systems. We present a novel approach to collecting the\nindividual user preferences over image-search results: we use\na collaborative game in which players are rewarded for agree-\ning on which image result is best for a query. Our approach\nis distinct from other labeling games because we are able\nto elicit directly the preferences of interest with respect to\nimage queries extracted from query logs. As a source of rel-\nevance judgments, this data provides a useful complement\nto click data. Furthermore, the data is free of positional\nbiases and is collected by the game without the risk of frus-\ntrating users with non-relevant results; this risk is prevalent\nin standard mechanisms for debiasing clicks. We describe\ndata collected over 34 days from a deployed version of this\ngame that amounts to about 18 million expressed prefer-\nences between pairs. Finally, we present several approaches\nto modeling this data in order to extract the consensus rank-\nings from the preferences and better sort the search results\nfor targeted queries."}
{"Title": "Rated Aspect Summarization of Short Comments", "Abstract": "Web 2.0 technologies have enabled more and more people\nto freely comment on different kinds of entities (e.g. sellers,\nproducts, services). The large scale of information poses the\nneed and challenge of automatic summarization. In many\ncases, each of the user-generated short comments comes with\nan overall rating. In this paper, we study the problem of gen-\nerating a\u201crated aspect summary\u201dof short comments, which\nis a decomposed view of the overall ratings for the major as-\npects so that a user could gain different perspectives towards\nthe target entity. We formally define the problem and de-\ncompose the solution into three steps. We demonstrate the\neffectiveness of our methods by using eBay sellers\u2019 feedback\ncomments. We also quantitatively evaluate each step of our\nmethods and study how well human agree on such a summa-\nrization task. The proposed methods are quite general and\ncan be used to generate rated aspect summary automati-\ncally given any collection of short comments each associated\nwith an overall rating."}
{"Title": "How Opinions are Received by Online Communities:\nA Case Study on Amazon.com Helpfulness Votes", "Abstract": "There are many on-line settings in which users publicly express\nopinions. A number of these offer mechanisms for other users\nto evaluate these opinions; a canonical example is Amazon.com,\nwhere reviews come with annotations like \u201c26 of 32 people found\nthe following review helpful.\u201d Opinion evaluation appears in many\noff-line settings as well, including market research and political\ncampaigns. Reasoning about the evaluation of an opinion is funda-\nmentally different from reasoning about the opinion itself: rather\nthan asking, \u201cWhat did Y think of X?\u201d, we are asking, \u201cWhat did Z\nthink of Y\u2019s opinion of X?\u201d Here we develop a framework for an-\nalyzing and modeling opinion evaluation, using a large-scale col-\nlection of Amazon book reviews as a dataset. We find that the per-\nceived helpfulness of a review depends not just on its content but\nalso but also in subtle ways on how the expressed evaluation relates\nto other evaluations of the same product. As part of our approach,\nwe develop novel methods that take advantage of the phenomenon\nof review \u201cplagiarism\u201d to control for the effects of text in opin-\nion evaluation, and we provide a simple and natural mathematical\nmodel consistent with our findings. Our analysis also allows us\nto distinguish among the predictions of competing theories from\nsociology and social psychology, and to discover unexpected dif-\nferences in the collective opinion-evaluation behavior of user pop-\nulations from different countries."}
{"Title": "Exploiting Web Search to Generate Synonyms for Entities", "Abstract": "Tasks recognizing named entities such as products, people\nnames, or locations from documents have recently received\nsignificant attention in the literature. Many solutions to\nthese tasks assume the existence of reference entity tables.\nAn important challenge that needs to be addressed in the\nentity extraction task is that of ascertaining whether or not\na candidate string approximately matches with a named en-\ntity in a given reference table. Prior approaches have relied\non string-based similarity which only compare a candidate\nstring and an entity it matches with. In this paper, we ex-\nploit web search engines in order to define new similarity\nfunctions. We then develop efficient techniques to facilitate\napproximate matching in the context of our proposed simi-\nlarity functions. In an extensive experimental evaluation, we\ndemonstrate the accuracy and efficiency of our techniques."}
{"Title": "Smart Miner: A New Framework for\nMining Large Scale Web Usage Data", "Abstract": "In this paper, we propose a novel framework called Smart-\nMiner for web usage mining problem which uses link infor-\nmation for producing accurate user sessions and frequent\nnavigation patterns. Unlike the simple session concepts in\nthe time and navigation based approaches, where sessions\nare sequences of web pages requested from the server or\nviewed in the browser, Smart Miner sessions are set of paths\ntraversed in the web graph that corresponds to users\u2019 naviga-\ntions among web pages. We have modeled session construc-\ntion as a new graph problem and utilized a new algorithm,\nSmart-SRA, to solve this problem efficiently. For the pat-\ntern discovery phase, we have developed an efficient version\nof the Apriori-All technique which uses the structure of web\ngraph to increase the performance. From the experiments\nthat we have performed on both real and simulated data,\nwe have observed that Smart-Miner produces at least 30%\nmore accurate web usage patterns than other approaches\nincluding previous session construction methods. We have\nalso studied the effect of having the referrer information in\nthe web server logs to show that different versions of Smart-\nSRA produce similar results. Our another contribution is\nthat we have implemented distributed version of the Smart\nMiner framework by employing Map/Reduce Paradigm. We\nconclude that we can efficiently process terabytes of web\nserver logs belonging to multiple web sites by our scalable\nframework."}
{"Title": "Releasing Search Queries and Clicks Privately", "Abstract": "The question of how to publish an anonymized search log was\nbrought to the forefront by a well-intentioned, but privacy-unaware\nAOL search log release. Since then a series of ad-hoc techniques\nhave been proposed in the literature, though none are known to be\nprovably private. In this paper, we take a major step towards a so-\nlution: we show how queries, clicks and their associated perturbed\ncounts can be published in a manner that rigorously preserves pri-\nvacy. Our algorithm is decidedly simple to state, but non-trivial to\nanalyze. On the opposite side of privacy is the question of whether\nthe data we can safely publish is of any use. Our findings offer a\nglimmer of hope: we demonstrate that a non-negligible fraction of\nqueries and clicks can indeed be safely published via a collection of\nexperiments on a real search log. In addition, we select an applica-\ntion, keyword generation, and show that the keyword suggestions\ngenerated from the perturbed data resemble those generated from\nthe original data."}
{"Title": "Incorporating Site-Level Knowledge to Extract Structured\nData from Web Forums", "Abstract": "Web forums have become an important data resource for\nmany web applications, but extracting structured data from\nunstructured web forum pages is still a challenging task due\nto both complex page layout designs and unrestricted user\ncreated posts. In this paper, we study the problem of struc-\ntured data extraction from various web forum sites. Our\ntarget is to find a solution as general as possible to extract\nstructured data, such as post title, post author, post time,\nand post content from any forum site. In contrast to most\nexisting information extraction methods, which only lever-\nage the knowledge inside an individual page, we incorporate\nboth page-level and site-level knowledge and employ Markov\nlogic networks (MLNs) to effectively integrate all useful evi-\ndence by learning their importance automatically. Site-level\nknowledge includes (1) the linkages among different object\npages, such as list pages and post pages, and (2) the inter-\nrelationships of pages belonging to the same object. The\nexperimental results on 20 forums show a very encouraging\ninformation extraction performance, and demonstrate the\nability of the proposed approach on various forums. We\nalso show that the performance is limited if only page-level\nknowledge is used, while when incorporating the site-level\nknowledge both precision and recall can be significantly im-\nproved."}
{"Title": "Towards Context-Aware Search by Learning A Very Large\nVariable Length Hidden Markov Model from Search Logs", "Abstract": "Capturing the context of a user\u2019s query from the previous\nqueries and clicks in the same session may help understand\nthe user\u2019s information need. A context-aware approach to\ndocument re-ranking, query suggestion, and URL recom-\nmendation may improve users\u2019 search experience substan-\ntially. In this paper, we propose a general approach to\ncontext-aware search. To capture contexts of queries, we\nlearn a variable length Hidden Markov Model (vlHMM) from\nsearch sessions extracted from log data. Although the math-\nematical model is intuitive, how to learn a large vlHMM\nwith millions of states from hundreds of millions of search\nsessions poses a grand challenge. We develop a strategy\nfor parameter initialization in vlHMM learning which can\ngreatly reduce the number of parameters to be estimated in\npractice. We also devise a method for distributed vlHMM\nlearning under the map-reduce model. We test our approach\non a real data set consisting of 1.8 billion queries, 2.6 billion\nclicks, and 840 million search sessions, and evaluate the ef-\nfectiveness of the vlHMM learned from the real data on three\nsearch applications: document re-ranking, query suggestion,\nand URL recommendation. The experimental results show\nthat our approach is both effective and efficient."}
{"Title": "A Class-Feature-Centroid Classifier for Text Categorization", "Abstract": "Automated text categorization is an important technique\nfor many web applications, such as document indexing, doc-\nument filtering, and cataloging web resources. Many dif-\nferent approaches have been proposed for the automated\ntext categorization problem. Among them, centroid-based\napproaches have the advantages of short training time and\ntesting time due to its computational efficiency. As a result,\ncentroid-based classifiers have been widely used in many web\napplications. However, the accuracy of centroid-based clas-\nsifiers is inferior to SVM, mainly because centroids found\nduring construction are far from perfect locations.\nWe design a fast Class-Feature-Centroid (CFC) classifier\nfor multi-class, single-label text categorization. In CFC,\na centroid is built from two important class distributions:\ninter-class term index and inner-class term index. CFC\nproposes a novel combination of these indices and employs\na denormalized cosine measure to calculate the similarity\nscore between a text vector and a centroid. Experiments on\nthe Reuters-21578 corpus and 20-newsgroup email collection\nshow that CFC consistently outperforms the state-of-the-art\nSVM classifiers on both micro-F1 and macro-F1 scores. Par-\nticularly, CFC is more effective and robust than SVM when\ndata is sparse."}
{"Title": "Large Scale Multi-Label Classification via MetaLabeler", "Abstract": "The explosion of online content has made the management of\nsuch content non-trivial. Web-related tasks such as web page\ncategorization, news filtering, query categorization, tag rec-\nommendation, etc. often involve the construction of multi-\nlabel categorization systems on a large scale. Existing multi-\nlabel classification methods either do not scale or have un-\nsatisfactory performance. In this work, we propose MetaLa-\nbeler to automatically determine the relevant set of labels for\neach instance without intensive human involvement or ex-\npensive cross-validation. Extensive experiments conducted\non benchmark data show that the MetaLabeler tends to out-\nperform existing methods. Moreover, MetaLabeler scales to\nmillions of multi-labeled instances and can be deployed eas-\nily. This enables us to apply the MetaLabeler to a large\nscale query categorization problem in Yahoo!, yielding a sig-\nnificant improvement in performance."}
{"Title": "Bid Optimization for Broad Match Ad Auctions", "Abstract": "Ad auctions in sponsored search support\u201cbroad match\u201dthat\nallows an advertiser to target a large number of queries while\nbidding only on a limited number. While giving more ex-\npressiveness to advertisers, this feature makes it challenging\nto optimize bids to maximize their returns: choosing to bid\non a query as a broad match because it provides high profit\nresults in one bidding for related queries which may yield\nlow or even negative profits.\nWe abstract and study the complexity of the bid optimiza-\ntion problem which is to determine an advertiser\u2019s bids on a\nsubset of keywords (possibly using broad match) so that her\nprofit is maximized. In the query language model when the\nadvertiser is allowed to bid on all queries as broad match,\nwe present a linear programming (LP)-based polynomial-\ntime algorithm that gets the optimal profit. In the model\nin which an advertiser can only bid on keywords, ie., a sub-\nset of keywords as an exact or broad match, we show that\nthis problem is not approximable within any reasonable ap-\nproximation factor unless P=NP. To deal with this hardness\nresult, we present a constant-factor approximation when the\noptimal profit significantly exceeds the cost. This algorithm\nis based on rounding a natural LP formulation of the prob-\nlem. Finally, we study a budgeted variant of the problem,\nand show that in the query language model, one can find\ntwo budget constrained ad campaigns in polynomial time\nthat implement the optimal bidding strategy. Our results\nare the first to address bid optimization under the broad\nmatch feature which is common in ad auctions."}
{"Title": "General Auction Mechanism for Search Advertising", "Abstract": "In sponsored search, a number of advertising slots is available on\na search results page, and have to be allocated among a set of ad-\nvertisers competing to display an ad on the page. This gives rise to\na bipartite matching market that is typically cleared by the way of\nan automated auction. Several auction mechanisms have been pro-\nposed, with variants of the Generalized Second Price (GSP) being\nwidely used in practice.\nThere is a rich body of work on bipartite matching markets that\nbuilds upon the stable marriage model of Gale and Shapley and the\nassignment model of Shapley and Shubik. This line of research\noffers deep insights into the structure of stable outcomes in such\nmarkets and their incentive properties.\nIn this paper, we model advertising auctions in terms of an as-\nsignment model with linear utilities, extended with bidder and item\nspecific maximum and minimum prices. Auction mechanisms like\nthecommonlyusedGSPorthewell-knownVickrey-Clarke-Groves\n(VCG) can be interpreted as simply computing a bidder-optimal\nstable matching in this model, for a suitably defined set of bid-\nder preferences, but our model includes much richer bidders and\npreferences. We prove that in our model the existence of a stable\nmatching is guaranteed, and under a non-degeneracy assumption\na bidder-optimal stable matching exists as well. We give an algo-\nrithm to find such matching in polynomial time, and use it to de-\nsign truthful mechanism that generalizes GSP, is truthful for profit-\nmaximizing bidders, correctly implements features like bidder-spe-\ncific minimum prices and position-specific bids, and works for rich\nmixtures of bidders and preferences. Our main technical contri-\nbutions are the existence of bidder-optimal matchings and strate-\ngyproofness of the resulting mechanism, and are proved by induc-\ntion on the progress of the matching algorithm."}
{"Title": "Adaptive Bidding for Display Advertising", "Abstract": "Motivated by the emergence of auction-based marketplaces\nfor display ads such as the Right Media Exchange, we study\nthe design of a bidding agent that implements a display ad-\nvertising campaign by bidding in such a marketplace. The\nbidding agent must acquire a given number of impressions\nwith a given target spend, when the highest external bid in\nthe marketplace is drawn from an unknown distribution P.\nThe quantity and spend constraints arise from the fact that\ndisplay ads are usually sold on a CPM basis. We consider\nboth the full information setting, where the winning price\nin each auction is announced publicly, and the partially ob-\nservable setting where only the winner obtains information\nabout the distribution; these differ in the penalty incurred\nby the agent while attempting to learn the distribution. We\nprovide algorithms for both settings, and prove performance\nguarantees using bounds on uniform closeness from statis-\ntics, and techniques from online learning. We experimen-\ntally evaluate these algorithms: both algorithms perform\nvery well with respect to both target quantity and spend;\nfurther, our algorithm for the partially observable case per-\nforms nearly as well as that for the fully observable setting\ndespite the higher penalty incurred during learning."}
{"Title": "How much can Behavioral Targeting Help Online\nAdvertising?", "Abstract": "Behavioral Targeting (BT) is a technique used by online\nadvertisers to increase the effectiveness of their campaigns, and is\nplaying an increasingly important role in the online advertising\nmarket. However, it is underexplored in academia how much BT\ncan truly help online advertising in search engines. In this paper\nwe provide an empirical study on the click-through log of\nadvertisements collected from a commercial search engine. From\nthe experiment results over a period of seven days, we draw three\nimportant conclusions: (1) Users who clicked the same ad will\ntruly have similar behaviors on the Web; (2) Click-Through Rate\n(CTR) of an ad can be averagely improved as high as 670% by\nproperly segmenting users for behavioral targeted advertising in a\nsponsored search; (3) Using short term user behaviors to represent\nusers is more effective than using long term user behaviors for BT.\nWe conducted statistical t-test which verified that all conclusions\ndrawn in the paper are statistically significant. To the best of our\nknowledge, this work is the first empirical study for BT on the\nclick-through log of real world ads"}
{"Title": "Web Service Derivatives", "Abstract": "Web service development and usage has shifted from simple\ninformation processing services to high-value business ser-\nvices that are crucial to productivity and success. In order\nto deal with an increasing risk of unavailability or failure of\nmission-critical Web services we argue the need for advanced\nreservation of services in the form of derivatives.\nThe contribution of this paper is twofold: First we provide\nan abstract model of a market design that enables the trade\nof derivatives for mission-critical Web services. Our model\nsatisfies requirements that result from service characteristics\nsuch as intangibility and the impossibility to inventor ser-\nvices in order to meet fluctuating demand. It comprehends\nprinciples from models of incomplete markets such as the\nabsence of a tradeable underlying and consistent arbitrage-\nfree derivative pricing.\nFurthermore we provide an architecture for a Web service\nmarket that implements our model and describes the strat-\negy space and interaction of market participants in the trad-\ning process of service derivatives. We compare the under-\nlying pricing processes to existing derivative models in en-\nergy exchanges, discuss eventual shortcomings, and propose\nWavelets as a preprocessing tool to analyze actual data and\nextract long- and short-term seasonalities."}
{"Title": "Rapid Prototyping of Semantic Mash-Ups through\nSemantic Web Pipes", "Abstract": "The use of RDF data published on the Web for applica-\ntions is still a cumbersome and resource-intensive task due\nto the limited software support and the lack of standard pro-\ngramming paradigms to deal with everyday problems such as\ncombination of RDF data from different sources, object iden-\ntifier consolidation, ontology alignment and mediation, or\nplain querying and filtering tasks. In this paper we present\nSemantic Web Pipes that support fast implementation of Se-\nmantic data mash-ups while preserving desirable properties\nsuch as abstraction, encapsulation, component-orientation,\ncode re-usability and maintainability which are common and\nwell supported in other application areas."}
{"Title": "idMesh:\nGraph-Based Disambiguation of Linked Data", "Abstract": "We tackle the problem of disambiguating entities on the Web. We\npropose a user-driven scheme where graphs of entities \u2013 repre-\nsented by globally identifiable declarative artifacts \u2013 self-organize\nin a dynamic and probabilistic manner. Our solution has the fol-\nlowing two desirable properties: i) it lets end-users freely define\nassociations between arbitrary entities and ii) it probabilistically in-\nfers entity relationships based on uncertain links using constraint-\nsatisfaction mechanisms. We outline the interface between our\nscheme and the current data Web, and show how higher-layer ap-\nplications can take advantage of our approach to enhance search\nand update of information relating to online entities. We describe a\ndecentralized infrastructure supporting efficient and scalable entity\ndisambiguation and demonstrate the practicability of our approach\nin a deployment over several hundreds of machines."}
{"Title": "OpenRuleBench:\nAn Analysis of the Performance of Rule Engines", "Abstract": "The Semantic Web initiative has led to an upsurge of the in-\nterest in rules as a general and powerful way of processing,\ncombining, and analyzing semantic information. Since sev-\neral of the technologies underlying rule-based systems are al-\nready quite mature, it is important to understand how such\nsystems might perform on the Web scale. OpenRuleBench\nis a suite of benchmarks for analyzing the performance and\nscalability of different rule engines. Currently the study\nspans five different technologies and eleven systems, but\nOpenRuleBench is an open community resource, and contri-\nbutions from the community are welcome. In this paper, we\ndescribe the tested systems and technologies, the methodol-\nogy used in testing, and analyze the results."}
{"Title": "Large Scale Integration of Senses for the Semantic Web", "Abstract": "Nowadays, the increasing amount of semantic data available\non the Web leads to a new stage in the potential of Seman-\ntic Web applications. However, it also introduces new issues\ndue to the heterogeneity of the available semantic resources.\nOne of the most remarkable is redundancy, that is, the ex-\ncess of different semantic descriptions, coming from different\nsources, to describe the same intended meaning.\nIn this paper, we propose a technique to perform a large\nscale integration of senses (expressed as ontology terms), in\norder to cluster the most similar ones, when indexing large\namounts of online semantic information. It can dramati-\ncally reduce the redundancy problem on the current Seman-\ntic Web. In order to make this objective feasible, we have\nstudied the adaptability and scalability of our previous work\non sense integration, to be translated to the much larger sce-\nnario of the Semantic Web. Our evaluation shows a good\nbehaviour of these techniques when used in large scale ex-\nperiments, then making feasible the proposed approach."}
{"Title": "Triplify \u2013 Light-Weight Linked Data Publication from\nRelational Databases", "Abstract": "In this paper we present Triplify \u2013 a simplistic but effective\napproach to publish Linked Data from relational databases.\nTriplify is based on mapping HTTP-URI requests onto re-\nlational database queries. Triplify transforms the result-\ning relations into RDF statements and publishes the data\non the Web in various RDF serializations, in particular as\nLinked Data. The rationale for developing Triplify is that\nthe largest part of information on the Web is already stored\nin structured form, often as data contained in relational\ndatabases, but usually published by Web applications only\nas HTML mixing structure, layout and content. In order\nto reveal the pure structured information behind the cur-\nrent Web, we have implemented Triplify as a light-weight\nsoftware component, which can be easily integrated into\nand deployed by the numerous, widely installed Web ap-\nplications. Our approach includes a method for publishing\nupdate logs to enable incremental crawling of linked data\nsources. Triplify is complemented by a library of configura-\ntions for common relational schemata and a REST-enabled\ndata source registry. Triplify configurations containing map-\npings are provided for many popular Web applications, in-\ncluding osCommerce, WordPress, Drupal, Gallery, and ph-\npBB. We will show that despite its light-weight architec-\nture Triplify is usable to publish very large datasets, such as\n160GB of geo data from the OpenStreetMap project."}
{"Title": "SOFIE: A Self-Organizing Framework\nfor Information Extraction", "Abstract": "This paper presents SOFIE, a system for automated on-\ntology extension. SOFIE can parse natural language docu-\nments, extract ontological facts from them and link the facts\ninto an ontology. SOFIE uses logical reasoning on the exist-\ning knowledge and on the new knowledge in order to disam-\nbiguate words to their most probable meaning, to reason on\nthe meaning of text patterns and to take into account world\nknowledge axioms. This allows SOFIE to check the plau-\nsibility of hypotheses and to avoid inconsistencies with the\nontology. The framework of SOFIE unites the paradigms of\npattern matching, word sense disambiguation and ontolog-\nical reasoning in one unified model. Our experiments show\nthat SOFIE delivers high-quality output, even from unstruc-\ntured Internet documents."}
{"Title": "Evaluating Similarity Measures\nfor Emergent Semantics of Social Tagging", "Abstract": "Social bookmarking systems and their emergent information struc-\ntures, knownasfolksonomies, areincreasinglyimportantdatasources\nfor Semantic Web applications. A key question for harvesting se-\nmantics from these systems is how to extend and adapt traditional\nnotions of similarity to folksonomies, and which measures are best\nsuited for applications such as navigation support, semantic search,\nand ontology learning. Here we build an evaluation framework to\ncompare various general folksonomy-based similarity measures de-\nrived from established information-theoretic, statistical, and prac-\ntical measures. Our framework deals generally and symmetrically\nwith users, tags, and resources. For evaluation purposes we fo-\ncus on similarity among tags and resources, considering different\nways to aggregate annotations across users. After comparing how\ntag similarity measures predict user-created tag relations, we pro-\nvide an external grounding by user-validated semantic proxies based\non WordNet and the Open Directory. We also investigate the issue\nof scalability. We find that mutual information with distributional\nmicro-aggregation across users yields the highest accuracy, but is\nnot scalable; per-user projection with collaborative aggregation pro-\nvides the best scalable approach via incremental computations. The\nresults are consistent across resource and tag similarity."}
{"Title": "Measuring the Similarity between Implicit Semantic\nRelations from the Web", "Abstract": "Measuringthesimilaritybetweensemanticrelationsthatholdamong\nentities is an important and necessary step in various Web related\ntasks such as relation extraction, information retrieval and analogy\ndetection. For example, consider the case in which a person knows\na pair of entities (e.g. Google, YouTube), between which a partic-\nular relation holds (e.g. acquisition). The person is interested in\nretrieving other such pairs with similar relations (e.g. Microsoft,\nPowerset). Existing keyword-based search engines cannot be ap-\nplied directly in this case because, in keyword-based search, the\ngoal is to retrieve documents that are relevant to the words used in\na query \u2013 not necessarily to the relations implied by a pair of words.\nWe propose a relational similarity measure, using a Web search en-\ngine, to compute the similarity between semantic relations implied\nby two pairs of words. Our method has three components: repre-\nsenting the various semantic relations that exist between a pair of\nwords using automatically extracted lexical patterns, clustering the\nextracted lexical patterns to identify the different patterns that ex-\npress a particular semantic relation, and measuring the similarity\nbetween semantic relations using a metric learning approach. We\nevaluate the proposed method in two tasks: classifying semantic\nrelations between named entities, and solving word-analogy ques-\ntions. The proposed method outperforms all baselines in a relation\nclassification task with a statistically significant average precision\nscore of 0.74. Moreover, it reduces the time taken by Latent Rela-\ntional Analysis to process 374 word-analogy questions from 9 days\nto less than 6 hours, with an SAT score of 51%."}
{"Title": "Extracting Key Terms From Noisy and Multi-theme\nDocuments", "Abstract": "We present a novel method for key term extraction from text\ndocuments. In our method, document is modeled as a graph\nof semantic relationships between terms of that document.\nWe exploit the following remarkable feature of the graph:\nthe terms related to the main topics of the document tend to\nbunch up into densely interconnected subgraphs or commu-\nnities, while non-important terms fall into weakly intercon-\nnected communities, or even become isolated vertices. We\napply graph community detection techniques to partition\nthe graph into thematically cohesive groups of terms. We\nintroduce a criterion function to select groups that contain\nkey terms discarding groups with unimportant terms. To\nweight terms and determine semantic relatedness between\nthem we exploit information extracted from Wikipedia.\nUsing such an approach gives us the following two ad-\nvantages. First, it allows effectively processing multi-theme\ndocuments. Second, it is good at filtering out noise infor-\nmation in the document, such as, for example, navigational\nbars or headers in web pages.\nEvaluations of the method show that it outperforms exist-\ning methods producing key terms with higher precision and\nrecall. Additional experiments on web pages prove that our\nmethod appears to be substantially more effective on noisy\nand multi-theme documents than existing methods."}
{"Title": "Tagommenders: Connecting Users to Items through Tags", "Abstract": "Tagging has emerged as a powerful mechanism that enables users\nto find, organize, and understand online entities. Recommender\nsystems similarly enable users to efficiently navigate vast collec-\ntions of items. Algorithms combining tags with recommenders\nmay deliver both the automation inherent in recommenders, and\nthe flexibility and conceptual comprehensibility inherent in tagging\nsystems. In this paper we explore tagommenders, recommender al-\ngorithms that predict users\u2019 preferences for items based on their\ninferred preferences for tags. We describe tag preference inference\nalgorithms based on users\u2019 interactions with tags and movies, and\nevaluate these algorithms based on tag preference ratings collected\nfrom 995 MovieLens users. We design and evaluate algorithms that\npredict users\u2019 ratings for movies based on their inferred tag prefer-\nences. Our tag-based algorithms generate better recommendation\nrankings than state-of-the-art algorithms, and they may lead to flex-\nible recommender systems that leverage the characteristics of items\nusers find most important."}
{"Title": "Collaborative Filtering for Orkut Communities:\nDiscovery of User Latent Behavior", "Abstract": "Users of social networking services can connect with each\nother by forming communities for online interaction. Yet as\nthe number of communities hosted by such websites grows\nover time, users have even greater need for effective commu-\nnity recommendations in order to meet more users. In this\npaper, we investigate two algorithms from very different do-\nmains and evaluate their effectiveness for personalized com-\nmunity recommendation. First is association rule mining\n(ARM), which discovers associations between sets of com-\nmunities that are shared across many users. Second is latent\nDirichlet allocation (LDA), which models user-community\nco-occurrences using latent aspects. In comparing LDA with\nARM, we are interested in discovering whether modeling\nlow-rank latent structure is more effective for recommen-\ndations than directly mining rules from the observed data.\nWe experiment on an Orkut data set consisting of 492,104\nusers and 118,002 communities. Our empirical comparisons\nusing the top-k recommendations metric show that LDA\nperforms consistently better than ARM for the community\nrecommendation task when recommending a list of 4 or more\ncommunities. However, for recommendation lists of up to 3\ncommunities, ARM is still a bit better. We analyze exam-\nples of the latent information learned by LDA to explain\nthis finding. To efficiently handle the large-scale data set,\nwe parallelize LDA on distributed computers [1] and demon-\nstrate our parallel implementation\u2019s scalability with varying\nnumbers of machines."}
{"Title": "Personalized Recommendation on Dynamic Content\nUsing Predictive Bilinear Models", "Abstract": "In Web-based services of dynamic content (such as news arti-\ncles), recommender systems face the difficulty of timely iden-\ntifying new items of high-quality and providing recommen-\ndations for new users. We propose a feature-based machine\nlearning approach to personalized recommendation that is\ncapable of handling the cold-start issue effectively. We main-\ntain profiles of content of interest, in which temporal charac-\nteristics of the content, e.g. popularity and freshness, are up-\ndated in real-time manner. We also maintain profiles of users\nincluding demographic information and a summary of user\nactivities within Yahoo! properties. Based on all features\nin user and content profiles, we develop predictive bilinear\nregression models to provide accurate personalized recom-\nmendations of new items for both existing and new users.\nThis approach results in an offline model with light computa-\ntional overhead compared with other recommender systems\nthat require online re-training. The proposed framework is\ngeneral and flexible for other personalized tasks. The supe-\nrior performance of our approach is verified on a large-scale\ndata set collected from the Today-Module on Yahoo! Front\nPage, with comparison against six competitive approaches."}
{"Title": "Social Search in \u201cSmall-World\u201d Experiments", "Abstract": "The \u201calgorithmic small-world hypothesis\u201d states that not\nonly are pairs of individuals in a large social network con-\nnected by short paths, but that ordinary individuals can\nfind these paths. Although theoretically plausible, empiri-\ncal evidence for the hypothesis is limited, as most chains in\n\u201csmall-world\u201d experiments fail to complete, thereby biasing\nestimates of \u201ctrue\u201d chain lengths. Using data from two re-\ncent small-world experiments, comprising a total of 162,328\nmessage chains, and directed at one of 30 \u201ctargets\u201d spread\nacross 19 countries, we model heterogeneity in chain attri-\ntion rates as a function of individual attributes. We then\nintroduce a rigorous way of estimating true chain lengths\nthat is provably unbiased, and can account for empirically-\nobserved variation in attrition rates. Our findings provide\nmixed support for the algorithmic hypothesis. On the one\nhand, it appears that roughly half of all chains can be com-\npleted in 6-7 steps\u2014thus supporting the\u201csix degrees of sep-\naration\u201d assertion\u2014but on the other hand, estimates of the\nmean are much longer, suggesting that for at least some of\nthe population, the world is not \u201csmall\u201d in the algorithmic\nsense. We conclude that search distances in social networks\nare fundamentally different from topological distances, for\nwhich the mean and median of the shortest path lengths\nbetween nodes tend to be similar."}
{"Title": "Behavioral Profiles for Advanced Email Features", "Abstract": "We examine the behavioral patterns of email usage in a\nlarge-scale enterprise over a three-month period. In partic-\nular, we focus on two main questions: (Q1) what do replies\ndepend on? and (Q2) what is the gain of augmenting con-\ntacts through the friends of friends from the email social\ngraph? For Q1, we identify and evaluate the significance\nof several factors that affect the reply probability and the\nemail response time. We find that all factors of our con-\nsidered set are significant, provide their relative ordering,\nand identify the recipient list size, and the intensity of email\ncommunication between the correspondents as the dominant\nfactors. We highlight various novel threshold behaviors and\nprovide support for existing hypotheses such as that of the\nleast-effort reply. For Q2, we find that the number of new\ncontacts extracted from the friends-of-friends relationships\namounts to a large number, but which is still a limited por-\ntion of the total enterprise size. We believe that our results\nprovide significant insights towards informed design of ad-\nvanced email features, including those of social-networking\ntype."}
{"Title": "A Measurement-driven Analysis of\nInformation Propagation in the Flickr Social Network", "Abstract": "Online social networking sites like MySpace, Facebook, and Flickr\nhave become apopular way toshare and disseminate content. Their\nmassive popularity has led to viral marketing techniques that at-\ntempt to spread content, products, and ideas on these sites. How-\never, there is little data publicly available on viral propagation in\nthe real world and few studies have characterized how information\nspreads over current online social networks.\nIn this paper, we collect and analyze large-scale traces of infor-\nmation dissemination in the Flickr social network. Our analysis,\nbased on crawls of the favorite markings of 2.5 million users on\n11 million photos, aims at answering three key questions: (a) how\nwidely does information propagate in the social network? (b) how\nquickly does information propagate? and (c) what is the role of\nword-of-mouth exchanges between friends in the overall propaga-\ntion of information in the network? Contrary to viral marketing\n\u201cintuition,\u201d we find that (a) even popular photos do not spread\nwidely throughout the network, (b) even popular photos spread\nslowly through the network, and (c) information exchanged be-\ntween friends is likely to account for over 50% of all favorite-\nmarkings, but with a significant delay at each hop."}
{"Title": "Network Analysis of Collaboration Structure in Wikipedia", "Abstract": "In this paper we give models and algorithms to describe and\nanalyze the collaboration among authors of Wikipedia from\na network analytical perspective. The edit network encodes\nwho interacts how with whom when editing an article; it sig-\nnificantly extends previous network models that code author\ncommunities in Wikipedia. Several characteristics summa-\nrizing some aspects of the organization process and allowing\nthe analyst to identify certain types of authors can be ob-\ntained from the edit network. Moreover, we propose several\nindicators characterizing the global network structure and\nmethods to visualize edit networks. It is shown that the\nstructural network indicators are correlated with quality la-\nbels of the associated Wikipedia articles."}
{"Title": "The Slashdot Zoo: Mining\na Social Network with Negative Edges", "Abstract": "We analyse the corpus of user relationships of the Slash-\ndot technology news site. The data was collected from the\nSlashdot Zoo feature where users of the website can tag other\nusers as friends and foes, providing positive and negative en-\ndorsements. We adapt social network analysis techniques to\nthe problem of negative edge weights. In particular, we con-\nsider signed variants of global network characteristics such as\nthe clustering coefficient, node-level characteristics such as\ncentrality and popularity measures, and link-level character-\nistics such as distances and similarity measures. We evaluate\nthese measures on the task of identifying unpopular users,\nas well as on the task of predicting the sign of links and show\nthat the network exhibits multiplicative transitivity which\nallows algebraic methods based on matrix multiplication to\nbe used. We compare our methods to traditional methods\nwhich are only suitable for positively weighted edges."}
{"Title": "Community Gravity: Measuring Bidirectional Effects by\nTrust and Rating on Online Social Networks", "Abstract": "Several attempts have been made to analyze customer be-\nhavior on online E-commerce sites. Some studies particu-\nlarly emphasize the social networks of customers. Users\u2019\nreviews and ratings of a product exert effects on other con-\nsumers\u2019 purchasing behavior. Whether a user refers to other\nusers\u2019 ratings depends on the trust accorded by a user to the\nreviewer. On the other hand, the trust that is felt by a user\nfor another user correlates with the similarity of two users\u2019\nratings. This bidirectional interaction that involves trust\nand rating is an important aspect of understanding con-\nsumer behavior in online communities because it suggests\nclustering of similar users and the evolution of strong com-\nmunities. This paper presents a theoretical model along with\nanalyses of an actual online E-commerce site. We analyzed\na large community site in Japan: @cosme. The noteworthy\ncharacteristics of @cosme are that users can bookmark their\ntrusted users; in addition, they can post their own ratings\nof products, which facilitates our analyses of the ratings\u2019\nbidirectional effects on trust and ratings. We describe an\noverview of the data in @cosme, analyses of effects from\ntrust to rating and vice versa, and our proposition of a mea-\nsure of of community gravity, which measures how strongly a\nuser might be attracted to a community. Our study is based\non the @cosme dataset in addition to the Epinions dataset.\nIt elucidates important insights and proposes a potentially\nimportant measure for mining online social networks."}
{"Title": "Mapping the World\u2019s Photos", "Abstract": "Weinvestigate how toorganize a largecollection of geotagged pho-\ntos, working with a dataset of about 35 million images collected\nfrom Flickr. Our approach combines content analysis based on text\ntags and image data with structural analysis based on geospatial\ndata. We use the spatial distribution of where people take photos\nto define a relational structure between the photos that are taken at\npopular places. We then study the interplay between this structure\nand the content, using classification methods for predicting such\nlocations from visual, textual and temporal features of the photos.\nWe find that visual and temporal features improve the ability to\nestimate the location of a photo, compared to using just textual fea-\ntures. We illustrate using these techniques to organize a large photo\ncollection, while also revealing various interesting properties about\npopular cities and landmarks at a global scale."}
{"Title": "Ranking and Classifying Attractiveness of Photos in\nFolksonomies", "Abstract": "Web 2.0 applications like Flickr, YouTube, or Del.icio.us are\nincreasingly popular online communities for creating, editing\nand sharing content. The growing size of these folksonomies\nposes new challenges in terms of search and data mining.\nIn this paper we introduce a novel methodology for auto-\nmatically ranking and classifying photos according to their\nattractiveness for folksonomy members. To this end, we\nexploit image features known for having significant effects\non the visual quality perceived by humans (e.g. sharpness\nand colorfulness) as well as textual meta data, in what is\na multi-modal approach. Using feedback and annotations\navailable in the Web 2.0 photo sharing system Flickr, we\nassign relevance values to the photos and train classification\nand regression models based on these relevance assignments.\nWith the resulting machine learning models we categorize\nand rank photos according to their attractiveness. Appli-\ncations include enhanced ranking functions for search and\nrecommender methods for attractive content. Large scale\nexperiments on a collection of Flickr photos demonstrate\nthe viability of our approach."}
{"Title": "Constructing Folksonomies from User-Specified Relations\non Flickr", "Abstract": "Automatic folksonomy construction from tags has attracted\nmuch attention recently. However, inferring hierarchical re-\nlations between concepts from tags has a drawback in that\nit is difficult to distinguish between more popular and more\ngeneral concepts. Instead of tags we propose to use user-\nspecified relations for learning folksonomy. We explore two\nstatistical frameworks for aggregating many shallow indi-\nvidual hierarchies, expressed through the collection/set re-\nlations on the social photosharing site Flickr, into a common\ndeeper folksonomy that reflects how a community organizes\nknowledge. Our approach addresses a number of challenges\nthat arise while aggregating information from diverse users,\nnamely noisy vocabulary, and variations in the granularity\nlevel of the concepts expressed. Our second contribution is a\nmethod for automatically evaluating learned folksonomy by\ncomparing it to a reference taxonomy, e.g., the Web direc-\ntory created by the Open Directory Project. Our empirical\nresults suggest that user-specified relations are a good source\nof evidence for learning folksonomies."}
{"Title": "Ranking User-Created Contents by Search User\u2019s\nInclination in Online Communities", "Abstract": "Searching posts effectively has become an important issue in\nlarge-scale online communities. Especially, if search users have\ndifferent inclinations when they search posts, they have different\nkinds of posts in their minds. To address this problem, in this\npaper, we propose a scheme of ranking posts based on search\nusers\u2019 inclination. User ranking score is employed to capture\nposts that are relevant to a specific user inclination. Specifically,\nwe present a scheme to rank posts in terms of user expertise and\npopularity. Experimental results show that different user\ninclinations can produce quite different search results and the\nproposed scheme achieves about 70 % accuracy."}
{"Title": "Semantic Wiki aided Business Process Specification", "Abstract": "This paper formulates a collaborative system for modeling\nbusiness application. The system uses a Semantic Wiki to enable\ncollaboration between the various stakeholders involved in the\ndesign of the system and translates the captured intelligence into\nbusiness models which are used for designing a business system."}
{"Title": "Using automatic keyword extraction to detect off-topic\nposts in online discussion boards", "Abstract": "Online discussions boards represent a rich repository of\nknowledge organized in a collection of user generated content.\nThese conversational cyberspaces allow users to express opinions,\nideas and pose questions and answers without imposing strict\nlimitations about the content. This freedom, in turn, creates an\nenvironment in which discussions are not bounded and often stray\nfrom the initial topic being discussed. In this paper we focus on\napproaches to assess the relevance of posts to a thread and\ndetecting when discussions have been steered off-topic.\nA set of metrics estimating the level of novelty in online\ndiscussion posts are presented. These metrics are based on topical\nestimation and contextual similarity between posts within a given\nthread. The metrics are aggregated to rank posts based on the\ndegree of relevance they maintain. The aggregation scheme is\ndata-dependent and is normalized relative to the post length."}
{"Title": "Web-Scale Classification with Naive Bayes", "Abstract": "Traditional Naive Bayes Classifier performs miserably on\nweb-scale taxonomies. In this paper, we investigate the rea-\nsons behind such bad performance. We discover that the\nlow performance are not completely caused by the intrin-\nsic limitations of Naive Bayes, but mainly comes from two\nlargely ignored problems: contradiction pair problem and\ndiscriminative evidence cancelation problem. We propose\nmodifications that can alleviate the two problems while pre-\nserving the advantages of Naive Bayes. The experimental\nresults show our modified Naive Bayes can significantly im-\nprove the performance on real web-scale taxonomies."}
{"Title": "An Integrated Approach for Relation Extraction from\nWikipedia Texts", "Abstract": "Linguistic-based methods and web mining-based methods are two\ntypes of leading methods for semantic relation extraction task. By\nintegrating linguistic analysis with frequent Web information, this\npaper presents an unsupervised relation extraction approach, for\ndiscovering and enhancing relations in which a specified concept\nparticipates. We focus on concepts described in Wikipedia articles.\nBy making use of the characteristics of Wikipedia and Web corpus,\nwe define a novel distance function and develop a linear clustering\nalgorithm on the combination of two kinds of patterns: dependency\npatterns from dependency analysis of texts in Wikipedia, surface\npatterns generated from high redundant information from the Web\ncorpus. The experiments on two different domains demonstrate\nthe superiority of our approach comparing with previous method.\nIn essence, our approach shows how deep linguistic features con-\ntribute complementally with Web surface features to generate a\nbroad variety of relations."}
{"Title": "Detection of Harassment on Web 2.0", "Abstract": "Web 2.0 has led to the development and evolution of web-based\ncommunities and applications. These communities provide places\nfor information sharing and collaboration. They also open the door\nfor inappropriate online activities, such as harassment, in which\nsome users post messages in a virtual community that are intention-\nally offensive to other members of the community. It is a new and\nchallenging task to detect online harassment; currently few systems\nattempt to solve this problem.\nIn this paper, we use a supervised learning approach for detect-\ning harassment. Our technique employs content features, sentiment\nfeatures, and contextual features of documents. The experimental\nresults described herein show that our method achieves significant\nimprovements over several baselines, including Term Frequency-\nInverse Document Frequency (TFIDF) approaches. Identification\nof online harassment is feasible when TFIDF is supplemented with\nsentiment and contextual feature attributes."}
{"Title": "An Effective Semantic Search Technique using Ontology", "Abstract": "In this paper, we present a semantic search technique considering\nthe type of desired Web resources and the semantic relationships\nbetween the resources and the query keywords in the ontology. In\norder to effectively retrieve the most relevant top-k resources, we\npropose a novel ranking model. To do this, we devise a measure to\ndetermine the weight of the semantic relationship. In addition, we\nconsider the number of meaningful semantic relationships between\na resource and keywords, the coverage of keywords, and the distin-\nguishability of keywords. Through experiments using real datasets,\nwe observe that our ranking model provides more accurate seman-\ntic search results compared to existing ranking models."}
{"Title": "Bucefalo: A Tool for Intelligent Search and Filtering for\nWeb-based Personal Health Records", "Abstract": "In this poster, a tool named BUCEFALO is presented. This\ntool is specially designed to improve the information re-\ntrieval tasks in web-based Personal Health Records (PHR).\nThis tool implements semantic and multilingual query ex-\npansion techniques and information filtering algorithms in\norder to help users find the most valuable information about\na specific clinical case. The filtering model is based on fuzzy\nprototypes based filtering, data quality measures, user pro-\nfiles and healthcare ontologies. The first experimental re-\nsults illustrate the feasibility of this tool."}
{"Title": "Building Term Suggestion Relational Graphs from\nCollective Intelligence", "Abstract": "This paper proposes an effective approach to provide relevant\nsearch terms for conceptual Web search. \u2018Semantic Term\nSuggestion\u2019 function has been included so that users can find the\nmost appropriate query term to what they really need.\nConventional approaches for term suggestion involve extracting\nfrequently occurring key terms from retrieved documents. They\nmust deal with term extraction difficulties and interference from\nirrelevant documents. In this paper, we propose a semantic term\nsuggestion function called Collective Intelligence based Term\nSuggestion (CITS). CITS provides a novel social-network based\nframework for relevant terms suggestion with a semantic graph of\nthe search term without limiting to the specific query term. A\nvisualization of semantic graph is presented to the users to help\nbrowsing search results from related terms in the semantic graph.\nThe search results are ranked each time according to their\nrelevance to the related terms in the entire query session.\nComparing to two popular commercial search engines, a user\nstudy of 18 users on 50 search terms showed better user\nsatisfactions and indicated the potential usefulness of proposed\nmethod in real-world search applications."}
{"Title": "Advertising Keyword Generation Using Active Learning", "Abstract": "This paper proposes an efficient relevance feedback based in-\nteractive model for keyword generation in sponsored search\nadvertising. We formulate the ranking of relevant terms as\na supervised learning problem and suggest new terms for\nthe seed by leveraging user relevance feedback information.\nActive learning is employed to select the most informative\nsamples from a set of candidate terms for user labeling. Ex-\nperiments show our approach improves the relevance of gen-\nerated terms significantly with little user effort required."}
{"Title": "Rare Item Detection in e-Commerce Site", "Abstract": "As the largest online marketplace in the world, eBay has a\nhuge inventory where there are plenty of great rare items\nwith potentially large, even rapturous buyers. These items\nare obscured in long tail of eBay item listing and hard to\nfind through existing searching or browsing methods. It is\nobserved that there are great rarity demands from users ac-\ncording to eBay query log. To keep up with the demands, the\npaper proposes a method to automatically detect rare items\nin eBay online listing. A large set of features relevant to\nthe task are investigated to filter items and further measure\nitem rareness. The experiments on the most rarity-demand-\nintensitive domains show that the method may effectively\ndetect rare items (> 90% precision)."}
{"Title": "A Declarative Framework\nfor Semantic Link Discovery over Relational Data", "Abstract": "In this paper, we present a framework for online discov-\nery of semantic links from relational data. Our framework\nis based on declarative specification of the linkage require-\nments by the user, that allows matching data items in many\nreal-world scenarios. These requirements are translated to\nqueries that can run over the relational data source, poten-\ntially using the semantic knowledge to enhance the accuracy\nof link discovery. Our framework lets data publishers to eas-\nily find and publish high-quality links to other data sources,\nand therefore could significantly enhance the value of the\ndata in the next generation of web."}
{"Title": "Modeling Semantics and Structure of Discussion Threads", "Abstract": "The abundant knowledge in web communities has motivated\nthe research interests in discussion threads. The dynamic\nnature of discussion threads poses interesting and challeng-\ning problems for computer scientists. Although techniques\nsuch as semantic models or structural models have been\nshown to be useful in a number of areas, they are ineffi-\ncient in understanding discussion threads due to the tempo-\nral dependence among posts in a discussion thread. Such de-\npendence causes that semantics and structure coupled with\neach other in discussion threads. In this paper, we propose a\nsparse coding-based model named SMSS to Simultaneously\nModel Semantic and Structure of discussion threads."}
{"Title": "Combining Anchor Text Categorization and Graph\nAnalysis for Paid Link Detection", "Abstract": "In order to artificially boost the rank of commercial pages in\nsearch engine results, search engine optimizers pay for links to\nthese pages on other websites. Identifying paid links is important\nfor a web search engine to produce highly relevant results. In this\npaper we introduce a novel method of identifying such links. We\nstart with training a classifier of anchor text topics and analyzing\nweb pages for diversity of their outgoing commercial links. Then\nwe use this information and analyze link graph of the Russian\nWeb to find pages that sell links and sites that buy links and to\nidentify the paid links. Testing on manually marked samples\nshowed high efficiency of the algorithm."}
{"Title": "Purely URL-based Topic Classification", "Abstract": "Given only the URL of a web page, can we identify its topic?\nThis is the question that we examine in this paper.\nUsually, web pages are classified using their content [7],\nbut a URL-only classifier is preferable, (i) when speed is cru-\ncial, (ii) to enable content filtering before an (objectionable)\nweb page is downloaded, (iii) when a page\u2019s content is hid-\nden in images, (iv) to annotate hyperlinks in a personalized\nweb browser, without fetching the target page, and (v) when\na focused crawler wants to infer the topic of a target page\nbefore devoting bandwidth to download it.\nWe apply a machine learning approach to the topic iden-\ntification task and evaluate its performance in extensive ex-\nperiments on categorized web pages from the Open Direc-\ntory Project (ODP). When training separate binary clas-\nsifiers for each topic, we achieve typical F-measure values\nbetween 80 and 85, and a typical precision of around 85.\nWe also ran experiments on a small data set of univer-\nsity web pages. For the task of classifying these pages into\nfaculty, student, course and project pages, our methods im-\nprove over previous approaches by 13.8 points of F-measur"}
{"Title": "Automatic Web Service Composition with\nAbstraction and Refinement", "Abstract": "The behavioral description based Web Service Composition\n(WSC) problem aims at the automatic construction of a co-\nordinator web service that controls a set of web services to\nreach a goal state. However, solving the WSC problem ex-\nactly with a realistic model is doubly-exponential in the num-\nber of variables in web service descriptions. In this paper,\nwe propose a novel efficient approximation-based algorithm\nusing automatic abstraction and refinement to dramatically\nreduce the number of variables needed to solve the problem."}
{"Title": "Automated Synthesis of Composite Services with\nCorrectness Guarantee", "Abstract": "In this paper, we propose a novel approach for composing\nexisting web services to satisfy the correctness constraints\nto the design, including freeness of deadlock and unspecified\nreception, and temporal constraints in Computation Tree\nLogic formula. An automated synthesis algorithm based on\nlearning algorithm is introduced, which guarantees that the\ncomposite service is the most general way of coordinating\nservices so that the correctness is ensured. We have imple-\nmented a prototype system evaluating the effectiveness and\nefficiency of our synthesis approach through an experimental\nstudy."}
{"Title": "Crosslanguage Blog Mining and Trend Visualisation", "Abstract": "People use weblogs to express thoughts, present ideas and\nshare knowledge, therefore weblogs are extraordinarily valu-\nable resources, amongs others, for trend analysis. Trends are\nderived from the chronological sequence of blog post count\nper topic. The comparison with a reference corpus allows\nqualitative statements over identified trends. We propose a\ncrosslanguage blog mining and trend visualisation system to\nanalyse blogs across languages and topics. The trend visu-\nalisation facilitates the identification of trends and the com-\nparison with the reference news article corpus. To prove the\ncorrectness of our system we computed the correlation be-\ntween trends in blogs and news articles for a subset of blogs\nand topics. The evaluation corroborated our hypothesis of\na high correlation coefficient for these subsets and therefore\nthe correctness of our system for different languages and\ntopics is proven."}
{"Title": "Crawling English-Japanese Person-Name Transliterations\nfrom the Web", "Abstract": "Automatic compilation of lexicon is a dream of lexicon com-\npilers as well as lexicon users. This paper proposes a system\nthat crawls English-Japanese person-name transliterations\nfrom the Web, which works a back-end collector for auto-\nmatic compilation of bilingual person-name lexicon. Our\ncrawler collected 561K transliterations in five months. From\nthem, an English-Japanese person-name lexicon with 406K\nentries has been compiled by an automatic post processing.\nThis lexicon is much larger than other similar resources in-\ncluding English-Japanese lexicon of HeiNER obtained from\nWikipedia."}
{"Title": "Mining Multilingual Topics from Wikipedia", "Abstract": "In this paper, we try to leverage a large-scale and multilingual\nknowledge base, Wikipedia, to help effectively analyze and\norganize Web information written in different languages. Based\non the observation that one Wikipedia concept may be described\nby articles in different languages, we adapt existing topic\nmodeling algorithm for mining multilingual topics from this\nknowledge base. The extracted \u201cuniversal\u201d topics have multiple\ntypes of representations, with each type corresponding to one\nlanguage. Accordingly, new documents of different languages can\nbe represented in a space using a group of universal topics, which\nmakes various multilingual Web applications feasible."}
{"Title": "Automatically Filling Form-Based\nWeb Interfaces with Free Text Inputs", "Abstract": "On the web of today the most prevalent solution for users to\ninteract with data-intensive applications is the use of form-\nbased interfaces composed by several data input fields, such\nas text boxes, radio buttons, pull-down lists, check boxes,\netc. Although these interfaces are popular and effective,\nin many cases, free text interfaces are preferred over form-\nbased ones. In this paper we discuss the proposal and the\nimplementation of a novel IR-based method for using data\nrich free text to interact with form-based interfaces. Our\nsolution takes a free text as input, extracts implicitly data\nvalues from it and fills appropriate fields using them. For\nthis task, we rely on values of previous submissions for each\nfield, which are freely obtained from the usage of form-based\ninterfaces."}
{"Title": "Estimating Web Site Readability Using Content Extraction", "Abstract": "Nowadays, information is primarily searched on the WWW.\nFrom a user perspective, the readability is an important cri-\nterion for measuring the accessibility and thereby the quality\nof an information. We show that modern content extraction\nalgorithms help to estimate the readability of a web docu-\nment quite accurate."}
{"Title": "An Experimental Study of Large-Scale Mobile Social\nNetwork", "Abstract": "Mobile social network is a typical social network where one or\nmore individuals of similar interests or commonalities, convers-\ning and connecting with one another using the mobile phone. Our\nworks in this paper focus on the experimental study for this kind of\nsocial network with the support of large-scale real mobile call data.\nThe main contributions can be summarized as three-fold: firstly,\na large-scale real mobile phone call log of one city has been ex-\ntracted from a mobile phone carrier in China to construct mobile\nsocial network; secondly, common features of traditional social\nnetworks, such as power law distribution and small diameter etc,\nhave been experimented, with which we confirm that the mobile\nsocial network is a typical scale-free network and has small-world\nphenomenon; lastly, different from traditional analytical methods,\nimportant properties of the actors, such as gender and age, have\nbeen introduced into our experiments with some interesting find-\nings about human behavior, for example, the middle-age people\nare more active than the young and old people, and the female is\nunusual more active than the male while in the old age."}
{"Title": "Link Based Small Sample Learning for Web Spam\nDetection", "Abstract": "Robust statistical learning based web spam detection sys-\ntem often requires large amounts of labeled training data.\nHowever, labeled samples are more difficult, expensive and\ntime consuming to obtain than unlabeled ones. This pa-\nper proposed link based semi-supervised learning algorithms\nto boost the performance of a classifier, which integrates\nthe traditional Self-training with the topological dependency\nbased link learning. The experiments with a few labeled\nsamples on standard WEBSPAM-UK2006 benchmark showed\nthat the algorithms are effective."}
{"Title": "Extracting Community Structure through Relational\nHypergraphs", "Abstract": "Social media websites promote diverse user interaction on media\nobjects as well as user actions with respect to other users. The\ngoal of this work is to discover community structure in rich media\nsocial networks, and observe how it evolves over time, through\nanalysis of multi-relational data. The problem is important in the\nenterprise domain where extracting emergent community structure\non enterprise social media, can help in forming new collaborative\nteams, aid in expertise discovery, and guide long term enterprise\nreorganization. Our approach consists of three main parts: (1) a\nrelational hypergraph model for modeling various social context\nand interactions; (2) a novel hypergraph factorization method for\ncommunity extraction on multi-relational social data; (3) an on-\nline method to handle temporal evolution through incremental\nhypergraph factorization. Extensive experiments on real-world\nenterprise data suggest that our technique is scalable and can\nextract meaningful communities. To evaluate the quality of our\nmining results, we use our method to predict users\u2019 future\ninterests. Our prediction outperforms baseline methods (frequency\ncounts, pLSA) by 36-250% on the average, indicating the utility\nof leveraging multi-relational social context by using our method."}
{"Title": "Discovering the Staring People From Social Networks", "Abstract": "In this paper, we study a novel problem of staring people dis-\ncovery from social networks, which is concerned with finding\npeople who are not only authoritative but also sociable in the\nsocial network. We formalize this problem as an optimiza-\ntion programming problem. Taking the co-author network\nas a case study, we define three objective functions and pro-\npose two methods to combine these objective functions. A\ngenetic algorithm based method is further presented to solve\nthis problem. Experimental results show that the proposed\nsolution can effectively find the staring people from social\nnetworks."}
{"Title": "Analysis of Community Structure in Wikipedia (Poster)", "Abstract": "We present the results of a community detection analysis\nof the Wikipedia graph. Distinct communities in Wikipe-\ndia contain semantically closely related articles. The central\ntopic of a community can be identified using PageRank. Ex-\ntracted communities can be organized hierarchically similar\nto manually created Wikipedia category structure."}
{"Title": "Content Hole Search in Community-type Content", "Abstract": "In community-type content such as blogs and SNSs, we call\nthe user\u2019s unawareness of information as a \u201dcontent hole\u201dand\nthe search for this information as a \u201dcontent hole search.\u201d\nA content hole search differs from similarity searching and\nhas a variety of types. In this paper, we propose different\ntypes of content holes and define each type. We also propose\nan analysis of dialogue related to community-type content\nand introduce content hole search by using Wikipedia as an\nexample."}
{"Title": "Buzz-Based Recommender System", "Abstract": "In this paper, we describe a buzz-based recommender system\nbased on a large source of queries in an eCommerce application.\nThe system detects bursts in query trends. These bursts are linked\nto external entities like news and inventory information to find the\nqueries currently in-demand which we refer to as buzz queries.\nThe system follows the paradigm of limited quantity\nmerchandising, in the sense that on a per-day basis the system\nshows recommendations around a single buzz query with the\nintent of increasing user curiosity, and improving activity and\nstickiness on the site. A semantic neighborhood of the chosen\nbuzz query is selected and appropriate recommendations are made\non products that relate to this neighborhood."}
{"Title": "Bootstrapped Extraction of Class Attributes", "Abstract": "As an alternative to previous studies on extracting class attributes\nfrom unstructured text, which consider either Web documents or\nquery logs as the source of textual data, A bootstrapped method\nextracts class attributes simultaneously from both sources, using a\nsmall set of seed attributes. The method improves extraction preci-\nsion and also improves attribute relevance across 40 test classes."}
{"Title": "Finding Influentials Based on the Temporal Order of\nInformation Adoption in Twitter ", "Abstract": "Twitter offers an explicit mechanism to facilitate information diffu-\nsion and has emerged as a new medium for communication. Many\napproaches to find influentials have been proposed, but they do not\nconsider the temporal order of information adoption. In this work,\nwe propose a novel method to find influentials by considering both\nthe link structure and the temporal order of information adoption in\nTwitter. Our method finds distinct influentials who are not discov-\nered by other methods."}
{"Title": "A Characterization of Online Search Behavior", "Abstract": "In this paper we undertake a large-scale study of online user search behavior based on search and\ntoolbar logs. We identify three types of search: web, multimedia, and item. Together, we show that these\ndifferent flavors represent almost 10% of all online pageviews, and indirectly result in over 21% of all\npageviews.\nWestudysearchqueriesthemselves, andshowthatmorethanhalfofthemcontaindirectreferencesto\nsome type of structured object; we characterize the types of objects that occur in these queries. We then\nrevisit the relationship search and navigation specifically in the context of e-commerce, and consider\nhow search aids users in online shopping tasks."}
{"Title": "Empirical Comparison of Algorithms for\nNetwork Community Detection", "Abstract": "Detecting clusters or communities in large real-world graphs such\naslargesocial or information networks isaproblem of considerable\ninterest. In practice, one typically chooses an objective function\nthat captures the intuition of a network cluster as set of nodes with\nbetter internal connectivity than external connectivity, and then one\napplies approximation algorithms or heuristics to extract sets of\nnodes that are related to the objective function and that \u201clook like\u201d\ngood communities for the application of interest.\nIn this paper, we explore a range of network community detec-\ntion methods in order to compare them and to understand their rela-\ntiveperformance and thesystematic biasesinthe clusterstheyiden-\ntify. We evaluate several common objective functions that are used\nto formalize the notion of a network community, and we examine\nseveral different classes of approximation algorithms that aim to\noptimize such objective functions. In addition, rather than simply\nfixing anobjective and asking for an approximation tothebest clus-\nter of any size, we consider a size-resolved version of the optimiza-\ntion problem. Considering community quality as a function of its\nsize provides a much finer lens with which to examine community\ndetection algorithms, since objective functions and approximation\nalgorithms often have non-obvious size-dependent behavior"}
{"Title": "Context-aware Citation Recommendation", "Abstract": "When you write papers, how many times do you want to\nmake some citations at a place but you are not sure which\npapers to cite? Do you wish to have a recommendation\nsystem which can recommend a small number of good can-\ndidates for every place that you want to make some cita-\ntions? In this paper, we present our initiative of building a\ncontext-aware citation recommendation system. High qual-\nity citation recommendation is challenging: not only should\nthe citations recommended be relevant to the paper under\ncomposition, but also should match the local contexts of the\nplaces citations are made. Moreover, it is far from trivial to\nmodel how the topic of the whole paper and the contexts of\nthe citation places should affect the selection and ranking\nof citations. To tackle the problem, we develop a context-\naware approach. The core idea is toeffectively. Moreover,\nit can recommend a set of citations for a paper with high\nquality. We implement a prototype system in CiteSeerX.\nAn extensive empirical evaluation in the CiteSeerX digital\nlibrary against many baselines demonstrates the effective-\nness and the scalability of our approach."}
{"Title": "Data Summaries for On-Demand Queries over Linked Data", "Abstract": "Typical approaches for querying structured Web Data col-\nlect (crawl) and pre-process (index) large amounts of data\nin a central data repository before allowing for query an-\nswering. However, this time-consuming pre-processing phase\nhowever leverages the benefits of Linked Data \u2013 where struc-\ntured data is accessible live and up-to-date at distributed\nWeb resources that may change constantly \u2013 only to a lim-\nited degree, as query results can never be current. An ideal\nquery answering system for Linked Data should return cur-\nrent answers in a reasonable amount of time, even on corpora\nas large as the Web. Query processors evaluating queries di-\nrectly on the live sources require knowledge of the contents\nof data sources. In this paper, we develop and evaluate an\napproximate index structure summarising graph-structured\ncontent of sources adhering to Linked Data principles, pro-\nvide an algorithm for answering conjunctive queries over\nLinked Data on the Web exploiting the source summary, and\nevaluate the system using synthetically generated queries.\nThe experimental results show that our lightweight index\nstructure enables complete and up-to-date query results over\nLinked Data, while keeping the overhead for querying low\nand providing a satisfying source ranking at no additional\ncost."}
{"Title": "Inferring Relevant Social Networks\nfrom Interpersonal Communication", "Abstract": "Researchers increasingly use electronic communication data\nto construct and study large social networks, effectively in-\nferring unobserved ties (e.g. i is connected to j) from ob-\nserved communication events (e.g. i emails j). Often over-\nlooked, however, is the impact of tie definition on the corre-\nsponding network, and in turn the relevance of the inferred\nnetwork to the research question of interest. Here we study\nthe problem of network inference and relevance for two email\ndata sets of different size and origin. In each case, we gener-\nate a family of networks parameterized by a threshold con-\ndition on the frequency of emails exchanged between pairs\nof individuals. After demonstrating that different choices of\nthe threshold correspond to dramatically different network\nstructures, we then formulate the relevance of these networks\nin terms of a series of prediction tasks that depend on vari-\nous network features. In general, we find: a) that prediction\naccuracy is maximized over a non-trivial range of thresholds\ncorresponding to 5\u201310 reciprocated emails per year; b) that\nfor any prediction task, choosing the optimal value of the\nthreshold yields a sizable (\u223c 30%) boost in accuracy over\nna\u00a8\u0131ve choices; and c) that the optimal threshold value ap-\npears to be (somewhat surprisingly) consistent across data\nsets and prediction tasks. We emphasize the practical utility\nin defining ties via their relevance to the prediction task(s)\nat hand and discuss implications of our empirical results."}
{"Title": "Measurement and Analysis of an Online Content Voting\nNetwork: A Case Study of Digg", "Abstract": "In online content voting networks, aggregate user activities\n(e.g., submitting and rating content) make high-quality con-\ntent thrive through the unprecedented scale, high dynamics\nand divergent quality of user generated content (UGC). To\nbetter understand the nature and impact of online content\nvoting networks, we have analyzed Digg, a popular online\nsocial news aggregator and rating website. Based on a large\namount of data collected, we provide an in-depth study of\nDigg. We study structural properties of Digg social network,\nrevealing some strikingly distinct properties such as low link\nsymmetry and the power-law distribution of node outdegree\nwith truncated tails. We explore impact of the social net-\nwork on user digging activities, and investigate the issues of\ncontent promotion, content filtering, vote spam and content\ncensorship, which are inherent to content rating networks.\nWe also provide insight into design of content promotion\nalgorithms and recommendation-assisted content discovery.\nOverall, we believe that the results presented in this paper\nare crucial in understanding online content rating networks."}
{"Title": "Measurement-calibrated Graph Models for\nSocial Network Experiments", "Abstract": "Access to realistic, complex graph datasets is critical to research\non social networking systems and applications. Simulations on\ngraph data provide critical evaluation of new systems and appli-\ncations ranging from community detection to spam filtering and\nsocial web search. Due to the high time and resource costs of gath-\nering real graph datasets through direct measurements, researchers\nare anonymizing and sharing a small number of valuable datasets\nwiththecommunity. However, performingexperimentsusingshared\nreal datasets faces three key disadvantages: concerns that graphs\ncanbede-anonymized toreveal privateinformation, increasing costs\nof distributing large datasets, and that a small number of available\nsocial graphs limits the statistical confidence in the results.\nThe use of measurement-calibrated graph models is an attractive\nalternative to sharing datasets. Researchers can \u201cfit\u201d a graph model\nto a real social graph, extract a set of model parameters, and use\nthem to generate multiple synthetic graphs statistically similar to\nthe original graph. While numerous graph models have been pro-\nposed, it is unclear if they can produce synthetic graphs that accu-\nrately match the properties of the original graphs. In this paper, we\nexplore the feasibility of measurement-calibrated synthetic graphs\nusing six popular graph models and a variety of real social graphs\ngathered from the Facebook social network ranging from 30,000\nto 3 million edges. We find that two models consistently produce\nsynthetic graphs with common graph metric values similar to those\nof the original graphs. However, only one produces high fidelity\nresults in our application-level benchmarks. While this shows that\ngraph models can produce realistic synthetic graphs, it also high-\nlights the fact that current graph metrics remain incomplete, and\nsome applications expose graph properties that do not map to ex-\nisting metrics."}
{"Title": "Mind the Data Skew: Distributed Inferencing by\nSpeeddating in Elastic Regions", "Abstract": "Semantic Web data exhibits very skewed frequency distri-\nbutions among terms. Efficient large-scale distributed rea-\nsoning methods should maintain load-balance in the face of\nsuch highly skewed distribution of input data. We show that\nterm-based partitioning, used by most distributed reason-\ning approaches, has limited scalability due to load-balancing\nproblems.\nWe address this problem with a method for data distri-\nbution based on clustering in elastic regions. Instead of as-\nsigning data to fixed peers, data flows semi-randomly in the\nnetwork. Data items \u201cspeed-date\u201d while being temporarily\ncollocated in the same peer. We introduce a bias in the rout-\ning to allow semantically clustered neighborhoods to emerge.\nOur approach is self-organising, efficient and does not re-\nquire any central coordination.\nWe have implemented this method on the MaRVIN plat-\nform and have performed experiments on large real-world\ndatasets, using a cluster of up to 64 nodes. We compute\nthe RDFS closure over different datasets and show that our\nclustering algorithm drastically reduces computation time,\ncalculating the RDFS closure of 200 million triples in 7.2\nminutes."}
{"Title": "Modeling Relationship Strength\nin Online Social Networks", "Abstract": "Previous work analyzing social networks has mainly focused\non binary friendship relations. However, in online social\nnetworks the low cost of link formation can lead to net-\nworks with heterogeneous relationship strengths (e.g., ac-\nquaintances and best friends mixed together). In this case,\nthe binary friendship indicator provides only a coarse repre-\nsentation of relationship information. In this work, we de-\nvelop an unsupervised model to estimate relationship strength\nfrom interaction activity (e.g., communication, tagging) and\nuser similarity. More specifically, we formulate a link-based\nlatent variable model, along with a coordinate ascent op-\ntimization procedure for the inference. We evaluate our\napproach on real-world data from Facebook and LinkedIn,\nshowing that the estimated link weights result in higher au-\ntocorrelation and lead to improved classification accuracy."}
{"Title": "On the High Density of Leadership Nuclei\nin Endorsement Social Networks", "Abstract": "In this paper we study the community structure of endorse-\nment networks, i.e., social networks in which a directed edge\nu \u2192 v is asserting an action of support from user u to user\nv. Examples include scenarios in which a user u is favoring\na photo, liking a post, or following the microblog of user v.\nStarting from the hypothesis that the footprint of a com-\nmunity in an endorsement network is a bipartite directed\nclique from a set of followers to a set of leaders, we apply fre-\nquent itemset mining techniques to discover such bicliques.\nOur analysis of real networks discovers that an interesting\nphenomenon is taking place: the leaders of a community are\nendorsing each other forming a very dense nucleus."}
{"Title": "How Accurately Can One\u2019s Interests Be Inferred From\nFriends?", "Abstract": "Search and recommendation systems must effectively model\nuser interests in order to provide personalized results. The\nproliferation of social software makes social network an in-\ncreasingly important source for user interest modeling, be-\ncause of the social influence and correlation among friends.\nHowever, there are large variations in people\u2019s contribution\nof social content. Therefore, it is impractical to accurately\nmodel interests for all users. As a result, applications need\nto decide whether to utilize a user interest model based on its\naccuracy. To address this challenge, we present a study on\nthe accuracy of user interests inferred from three types of so-\ncial content: social bookmarking, file sharing, and electronic\ncommunication, in an organizational social network within\na large-scale enterprise. First, we demonstrate that combin-\ning different types of social content to infer user interests\noutperforms methods that use only one type of social con-\ntent. Second, we present a technique to predict the inference\naccuracy based on easily observed network characteristics,\nincluding user activeness, network in-degree, out-degree, and\nbetweenness centrality."}
{"Title": "Predicting Positive and Negative Links\nin Online Social Networks", "Abstract": "We study online social networks in which relationships can be ei-\nther positive (indicating relations such as friendship) or negative\n(indicating relations such as opposition or antagonism). Such a mix\nof positive and negative links arise in a variety of online settings;\nwe study datasets from Epinions, Slashdot and Wikipedia. We find\nthat the signs of links in the underlying social networks can be pre-\ndicted with high accuracy, using models that generalize across this\ndiverse range of sites. These models provide insight into some of\nthe fundamental principles that drive the formation of signed links\nin networks, shedding light on theories of balance and status from\nsocial psychology; they also suggest social computing applications\nby which the attitude of one user toward another can be estimated\nfrom evidence provided by their relationships with other members\nof the surrounding social network."}
{"Title": "Randomization Tests for Distinguishing\nSocial Influence and Homophily Effects", "Abstract": "Relational autocorrelation is ubiquitous in relational domains.\nThis observed correlation between class labels of linked in-\nstances in a network (e.g., two friends are more likely to\nshare political beliefs than two randomly selected people)\ncan be due to the effects of two different social processes.\nIf social influence effects are present, instances are likely to\nchange their attributes to conform to their neighbor values.\nIf homophily effects are present, instances are likely to link to\nother individuals with similar attribute values. Both these\neffects will result in autocorrelated attribute values. When\nanalyzing static relational networks it is impossible to de-\ntermine how much of the observed correlation is due each of\nthese factors. However, the recent surge of interest in social\nnetworks has increased the availability of dynamic network\ndata. In this paper, we present a randomization technique\nfor temporal network data where the attributes and links\nchange over time. Given data from two time steps, we mea-\nsure the gain in correlation and assess whether a significant\nportion of this gain is due to influence and/or homophily. We\ndemonstrate the efficacy of our method on semi-synthetic\ndata and then apply the method to a real-world social net-\nworks dataset, showing the impact of both influence and\nhomophily effects."}
{"Title": "Factorizing Personalized Markov Chains\nfor Next-Basket Recommendation", "Abstract": "Recommender systems are an important component of many\nwebsites. Two of the most popular approaches are based on\nmatrix factorization (MF) and Markov chains (MC). MF\nmethods learn the general taste of a user by factorizing the\nmatrix over observed user-item preferences. On the other\nhand, MC methods model sequential behavior by learning a\ntransition graph over items that is used to predict the next\naction based on the recent actions of a user. In this paper, we\npresent a method bringing both approaches together. Our\nmethod is based on personalized transition graphs over un-\nderlying Markov chains. That means for each user an own\ntransition matrix is learned \u2013 thus in total the method uses\na transition cube. As the observations for estimating the\ntransitions are usually very limited, our method factorizes\nthe transition cube with a pairwise interaction model which\nis a special case of the Tucker Decomposition. We show\nthat our factorized personalized MC (FPMC) model sub-\nsumes both a common Markov chain and the normal matrix\nfactorization model. For learning the model parameters, we\nintroduce an adaption of the Bayesian Personalized Ranking\n(BPR) framework for sequential basket data. Empirically,\nwe show that our FPMC model outperforms both the com-\nmon matrix factorization and the unpersonalized MC model\nboth learned with and without factorization."}
{"Title": "Sampling Community Structure", "Abstract": "We propose a novel method, based on concepts from ex-\npander graphs, to sample communities in networks. We\nshow that our sampling method, unlike previous techniques,\nproduces subgraphs representative of community structure\nin the original network. These generated subgraphs may be\nviewed as stratified samples in that they consist of members\nfrom most or all communities in the network. Using sam-\nples produced by our method, we show that the problem of\ncommunity detection may be recast into a case of statistical\nrelational learning. We empirically evaluate our approach\nagainst several real-world datasets and demonstrate that\nour sampling method can effectively be used to infer and\napproximate community affiliation in the larger network."}
{"Title": "Earthquake Shakes Twitter Users:\nReal-time Event Detection by Social Sensors", "Abstract": "Twitter, a popular microblogging service, has received much\nattention recently. An important characteristic of Twitter\nis its real-time nature. For example, when an earthquake\noccurs, people make many Twitter posts (tweets) related\nto the earthquake, which enables detection of earthquake\noccurrence promptly, simply by observing the tweets. As\ndescribed in this paper, we investigate the real-time inter-\naction of events such as earthquakes, in Twitter, and pro-\npose an algorithm to monitor tweets and to detect a target\nevent. To detect a target event, we devise a classifier of\ntweets based on features such as the keywords in a tweet,\nthe number of words, and their context. Subsequently, we\nproduce a probabilistic spatiotemporal model for the tar-\nget event that can find the center and the trajectory of the\nevent location. We consider each Twitter user as a sensor\nand apply Kalman filtering and particle filtering, which are\nwidely used for location estimation in ubiquitous/pervasive\ncomputing. The particle filter works better than other com-\npared methods in estimating the centers of earthquakes and\nthe trajectories of typhoons. As an application, we con-\nstruct an earthquake reporting system in Japan. Because\nof the numerous earthquakes and the large number of Twit-\nter users throughout the country, we can detect an earth-\nquake by monitoring tweets with high probability (96% of\nearthquakes of Japan Meteorological Agency (JMA) seis-\nmic intensity scale 3 or more are detected). Our system\ndetects earthquakes promptly and sends e-mails to regis-\ntered users. Notification is delivered much faster than the\nannouncements that are broadcast by the JMA."}
{"Title": "The Anatomy of a Large-Scale Social Search Engine", "Abstract": "We present Aardvark, a social search engine. With Aard-\nvark, users ask a question, either by instant message, email,\nweb input, text message, or voice. Aardvark then routes the\nquestion to the person in the user\u2019s extended social network\nmost likely to be able to answer that question. As compared\nto a traditional web search engine, where the challenge lies\nin finding the right document to satisfy a user\u2019s information\nneed, the challenge in a social search engine like Aardvark\nlies in finding the right person to satisfy a user\u2019s information\nneed. Further, while trust in a traditional search engine is\nbased on authority, in a social search engine like Aardvark,\ntrust is based on intimacy. We describe how these considera-\ntions inform the architecture, algorithms, and user interface\nof Aardvark, and how they are reflected in the behavior of\nAardvark users."}
{"Title": "Using a Model of Social Dynamics to Predict\nPopularity of News", "Abstract": "Popularity of content in social media is unequally distributed, with\nsome items receiving a disproportionate share of attention from\nusers. Predicting which newly-submitted items will become popu-\nlar is critically important for both companies that host social media\nsites and their users. Accurate and timely prediction would enable\nthe companies to maximize revenue through differential pricing for\naccess to content or ad placement. Prediction would also give con-\nsumers an important tool for filtering the ever-growing amount of\ncontent. Predicting popularity of content in social media, however,\nis challenging due to the complex interactions among content qual-\nity, how the social media site chooses to highlight content, and in-\nfluence among users. While these factors make it difficult to predict\npopularity a priori, we show that stochastic models of user behav-\nior on these sites allows predicting popularity based on early user\nreactions to new content. By incorporating aspects of the web site\ndesign, such models improve on predictions based on simply ex-\ntrapolating from the early votes. We validate this claim on the so-\ncial news portal Digg using a previously-developed model of social\nvoting based on the Digg user interface."}
{"Title": "What is Twitter, a Social Network or a News Media?", "Abstract": "Twitter, a microblogging service less than three years old, com-\nmands more than 41 million users as of July 2009 and is growing\nfast. Twitter users tweet about any topic within the 140-character\nlimit and follow others to receive their tweets. The goal of this\npaper is to study the topological characteristics of Twitter and its\npower as a new medium of information sharing.\nWe have crawled theentire Twitter site and obtained41.7 million\nuser profiles, 1.47 billion social relations, 4,262 trending topics,\nand 106 million tweets. In its follower-following topology analysis\nwe have found a non-power-law follower distribution, a short effec-\ntive diameter, and low reciprocity, which all mark a deviation from\nknown characteristics of human social networks [28]. In order to\nidentifyinfluentialsonTwitter, wehaverankedusersbythenumber\nof followers and by PageRank and found two rankings to be sim-\nilar. Ranking by retweets differs from the previous two rankings,\nindicating a gap in influence inferred from the number of followers\nand that from the popularity of one\u2019s tweets. We have analyzed the\ntweets of top trending topics and reported on their temporal behav-\nior and user participation. We have classified the trending topics\nbased on the active period and the tweets and show that the ma-\njority (over 85%) of topics are headline news or persistent news in\nnature. A closer look at retweets reveals that any retweeted tweet\nis to reach an average of 1,000 users no matter what the number\nof followers is of the original tweet. Once retweeted, a tweet gets\nretweeted almost instantly on next hops, signifying fast diffusion\nof information after the 1st retweet.\nTo the best of our knowledge this work is the first quantitative\nstudy on the entire Twittersphere and information diffusion on it."}
{"Title": "Limiting the Spread of Misinformation in Social Networks", "Abstract": "In this work, we study the notion of competing campaigns\nin a social network. By modeling the spread of influence\nin the presence of competing campaigns, we provide neces-\nsary tools for applications such as emergency response where\nthe goal is to limit the spread of misinformation. We study\nthe problem of influence limitation where a\u201cbad\u201dcampaign\nstarts propagating from a certain node in the network and\nuse the notion of limiting campaigns to counteract the effect\nof misinformation. The problem can be summarized as iden-\ntifying a subset of individuals that need to be convinced to\nadopt the competing (or\u201cgood\u201d) campaign so as to minimize\nthe number of people that adopt the\u201cbad\u201dcampaign at the\nend of both propagation processes. We show that this op-\ntimization problem is NP-hard and provide approximation\nguarantees for a greedy solution for various definitions of this\nproblem by proving that they are submodular. Although the\ngreedy algorithm is a polynomial time algorithm, for today\u2019s\nlarge scale social networks even this solution is computation-\nally very expensive. Therefore, we study the performance of\nthe degree centrality heuristic as well as other heuristics that\nhave implications on our specific problem. The experiments\non a number of close-knit regional networks obtained from\nthe Facebook social network show that in most cases inex-\npensive heuristics do in fact compare well with the greedy\napproach."}
{"Title": "From Actors, Politicians, to CEOs: Domain Adaptation of\nRelational Extractors using a Latent Relational Mapping", "Abstract": "We propose a method to adapt an existing relation extraction sys-\ntem to extract new relation types with minimum supervision. Our\nproposedmethodcomprisestwostages: learningalower-dimensional\nprojection between different relations, and learning a relational\nclassifier for the target relation type with instance sampling. We\nevaluate the proposed method using a dataset that contains 2000\ninstances for 20 different relation types. Our experimental results\nshow that the proposed method achieves a statistically significant\nmacro-average F-score of 62.77. Moreover, the proposed method\noutperformsnumerousbaselinesandapreviouslyproposedweakly-\nsupervised relation extraction method."}
{"Title": "Identifying Overlapping Communities in Folksonomies or\nTripartite Hypergraphs", "Abstract": "Online folksonomies are modeled as tripartite hypergraphs,\nand detecting communities from such networks is a chal-\nlenging and well-studied problem. However, almost every\nexisting algorithm known to us for community detection in\nhypergraphs assign unique communities to nodes, whereas\nin reality, nodes in folksonomies belong to multiple overlap-\nping communities e.g. users have multiple topical interests,\nand the same resource is often tagged with semantically dif-\nferent tags. In this paper, we propose an algorithm to detect\noverlapping communities in folksonomies by customizing a\nrecently proposed edge-clustering algorithm (that is origi-\nnally for traditional graphs) for use on hypergraphs."}
{"Title": "Trust Analysis with Clustering", "Abstract": "Web provides rich information about a variety of objects.\nTrustability is a major concern on the web. Truth estab-\nlishment is an important task so as to provide the right\ninformation to the user from the most trustworthy source.\nTrustworthiness of information provider and the confidence\nof the facts it provides are inter-dependent on each other and\nhence can be expressed iteratively in terms of each other.\nHowever, a single information provider may not be the most\ntrustworthy for all kinds of information. Every information\nprovider has its own area of competence where it can per-\nform better than others. We derive a model that can evalu-\nate trustability on objects and information providers based\non clusters (groups). We propose a method which groups\nthe set of objects for which similar set of providers provide\n\u201cgood\u201d facts, and provides better accuracy in addition to\nhigh quality object clusters"}
{"Title": "Predicting Popular Messages in Twitter", "Abstract": "Social network services have become a viable source of informa-\ntion for users. In Twitter, information deemed important by the\ncommunity propagates through retweets. Studying the characteris-\ntics of such popular messages is important for a number of tasks,\nsuch as breaking news detection, personalized message recommen-\ndation, viral marketingand others. Thispaper investigates theprob-\nlem of predicting the popularity of messages as measured by the\nnumber of future retweets and sheds some light on what kinds of\nfactors influence information propagation in Twitter. We formulate\nthe task into a classification problem and study two of its variants\nby investigating a wide spectrum of features based on the content\nof the messages, temporal information, metadata of messages and\nusers, as well as structural properties of the users\u2019 social graph on a\nlarge scale dataset. We show that our method can successfully pre-\ndict messages which will attract thousands of retweets with good\nperformance."}
{"Title": "Finding Influential Mediators in Social Networks", "Abstract": "Given a social network, who are the key players controlling the\nbottlenecks of influence propagation if some persons would like to\nactivate specific individuals? In this paper, we tackle the problem\nof selecting a set of k mediator nodes as the influential gateways\nwhose existence determines the activation probabilities of targeted\nnodes from some given seed nodes. We formally define the k-\nMediators problem. To have an effective and efficient solution, we\npropose a three-step greedy method by considering the probabilistic\ninfluence and the structural connectivity on the pathways from\nsources to targets. To the best of our knowledge, this is the first\nwork to consider the k-Mediators problem in networks.\nExperiments on the DBLP co-authorship graph show the\neffectiveness and efficiency of the proposed method."}
{"Title": "Web Information Extraction Using Markov Logic Networks", "Abstract": "In this paper, we consider the problem of extracting structured data\nfrom web pages taking into account both the content of individual\nattributes as well as the structure of pages and sites. We use Markov\nLogic Networks (MLNs) to capture both content and structural fea-\ntures in a single unified framework, and this enables us to perform\nmoreaccurateinference. Weshowthatinferenceinourinformation\nextraction scenario reduces to solving an instance of the maximum\nweight subgraph problem. We develop specialized procedures for\nsolving the maximum subgraph variants that are far more efficient\nthan previously proposed inference methods for MLNs that solve\nvariants of MAX-SAT. Experiments with real-life datasets demon-\nstrate the effectiveness of our approach."}
{"Title": "How to Choose Combinations in a Join of Search Results", "Abstract": "We present novel measures for estimating the effectiveness of du-\nplication-removal operations over a join of ranked lists. We intro-\nduce a duplication-removal approach, namely optimality rank, that\noutperforms existing approaches, according to the new measures."}
{"Title": "Evaluation of Valuable User Generated Content\non Social News Web Sites", "Abstract": "Social news websites have gained significant popularity over\nthe last few years. The participants of such websites are not\nonly allowed to share news links but also to annotate, to\nevaluate and to comment them. To quantify interestingness\nand attractiveness of the user generated content in respect\nto the original link source we introduce the User Generated\nContent add-on (UGC + ) index. Based on the definition of\nUGC + we also propose a concept for comparing groups of\nlinks filtered by different properties, e.g. authorship or topic-\ncategories. We apply the proposed measure on the Spanish\nDigg-clone Men\u00b4 eame."}
{"Title": "Automatic Construction of a Context-Aware\nSentiment Lexicon: An Optimization Approach", "Abstract": "The explosion of Web opinion data has made essential the\nneed for automatic tools to analyze and understand people\u2019s\nsentiments toward different topics. In most sentiment anal-\nysis applications, the sentiment lexicon plays a central role.\nHowever, it is well known that there is no universally opti-\nmal sentiment lexicon since the polarity of words is sensitive\nto the topic domain. Even worse, in the same domain the\nsame word may indicate different polarities with respect to\ndifferent aspects. For example, in a laptop review, \u201clarge\u201d\nis negative for the battery aspect while being positive for\nthe screen aspect. In this paper, we focus on the problem of\nlearning a sentiment lexicon that is not only domain specific\nbut also dependent on the aspect in context given an unla-\nbeled opinionated text collection. We propose a novel opti-\nmization framework that provides a unified and principled\nway to combine different sources of information for learning\nsuch a context-dependent sentiment lexicon. Experiments\non two data sets (hotel reviews and customer feedback sur-\nveys on printers) show that our approach can not only iden-\ntify new sentiment words specific to the given domain but\nalso determine the different polarities of a word depending\non the aspect in context. In further quantitative evaluation,\nour method is proved to be effective in constructing a high\nquality lexicon by comparing with a human annotated gold\nstandard. In addition, using the learned context-dependent\nsentiment lexicon improved the accuracy in an aspect-level\nsentiment classification task."}
{"Title": "Parallel Boosted Regression Trees for Web Search Ranking", "Abstract": "Gradient Boosted Regression Trees (GBRT) are the current\nstate-of-the-art learning paradigm for machine learned web-\nsearch ranking \u2014 a domain notorious for very large data\nsets. In this paper, we propose a novel method for par-\nallelizing the training of GBRT. Our technique parallelizes\nthe construction of the individual regression trees and oper-\nates using the master-worker paradigm as follows. The data\nare partitioned among the workers. At each iteration, the\nworker summarizes its data-partition using histograms. The\nmaster processor uses these to build one layer of a regres-\nsion tree, and then sends this layer to the workers, allowing\nthe workers to build histograms for the next layer. Our algo-\nrithm carefully orchestrates overlap between communication\nand computation to achieve good performance.\nSince this approach is based on data partitioning, and re-\nquires a small amount of communication, it generalizes to\ndistributed and shared memory machines, as well as clouds.\nWe present experimental results on both shared memory\nmachines and clusters for two large scale web search rank-\ning data sets. We demonstrate that the loss in accuracy\ninduced due to the histogram approximation in the regres-\nsion tree creation can be compensated for through slightly\ndeeper trees. As a result, we see no significant loss in ac-\ncuracy on the Yahoo data sets and a very small reduction\nin accuracy for the Microsoft LETOR data. In addition,\non shared memory machines, we obtain almost perfect lin-\near speed-up with up to about 48 cores on the large data\nsets. On distributed memory machines, we get a speedup\nof 25 with 32 processors. Due to data partitioning our ap-\nproach can scale to even larger data sets, on which one can\nreasonably expect even higher speedups."}
{"Title": "We Know Who You Followed Last Summer: Inferring\nSocial Link Creation Times in Twitter", "Abstract": "Understanding a network\u2019s temporal evolution appears to\nrequire multiple observations of the graph over time. These\noften expensive repeated crawls are only able to answer ques-\ntions about what happened from observation to observation,\nand not what happened before or between network snap-\nshots. Contrary to this picture, we propose a method for\nTwitter\u2019s social network that takes a single static snapshot\nof network edges and user account creation times to accu-\nrately infer when these edges were formed. This method\ncan be exact in theory, and we demonstrate empirically for\na large subset of Twitter relationships that it is accurate to\nwithin a few hours in practice.\nWe study users who have a very large number of edges or\nwho are recommended by Twitter. We examine the graph\nformed by these nearly 1,800 Twitter celebrities and their\n862 million edges in detail, showing that a single static\nsnapshot can give novel insights about Twitter\u2019s evolution.\nWe conclude from this analysis that real-world events and\nchanges to Twitter\u2019s interface for recommending users strongly\ninfluence network growth."}
{"Title": "Like like alike \u2014 Joint Friendship and Interest\nPropagation in Social Networks", "Abstract": "Targeting interest to match a user with services (e.g. news,\nproducts, games, advertisements) and predicting friendship\nto build connections among users are two fundamental tasks\nfor social network systems. In this paper, we show that the\ninformation contained in interest networks (i.e. user-service\ninteractions) and friendship networks (i.e. user-user connec-\ntions) is highly correlated and mutually helpful. We propose\na framework that exploits homophily to establish an inte-\ngrated network linking a user to interested services and con-\nnecting different users with common interests, upon which\nboth friendship and interests could be efficiently propagated.\nThe proposed friendship-interest propagation (FIP) frame-\nwork devises a factor-based random walk model to explain\nfriendship connections, and simultaneously it uses a coupled\nlatent factor model to uncover interest interactions. We dis-\ncuss the flexibility of the framework in the choices of loss ob-\njectives and regularization penalties and benchmark differ-\nent variants on the Yahoo! Pulse social networking system.\nExperiments demonstrate that by coupling friendship with\ninterest, FIP achieves much higher performance on both in-\nterest targeting and friendship prediction than systems using\nonly one source of information."}
{"Title": "Dynamics of Bidding in a P2P Lending Service:\nEffects of Herding and Predicting Loan Success", "Abstract": "Online peer-to-peer (P2P) lending services are a new type of social\nplatform that enables individuals borrow and lend money directly\nfrom one to another. In this paper, we study the dynamics of bid-\nding behavior in a P2P loan auction website, Prosper.com. We in-\nvestigate the change of various attributes of loan requesting listings\nover time, such as the interest rate and the number of bids. We ob-\nserve that there is herding behavior during bidding, and for most of\nthe listings, the numbers of bids they receive reach spikes at very\nsimilar time points. We explain these phenomena by showing that\nthere are economic and social factors that lenders take into account\nwhen deciding to bid on a listing.We also observe that the profits\nthe lenders make are tied with their bidding preferences. Finally,\nwe build a model based on the temporal progression of the bidding,\nthat reliably predicts the success of a loan request listing, as well as\nwhether a loan will be paid back or not."}
{"Title": "Finding Hierarchy in Directed Online Social Networks", "Abstract": "Social hierarchy and stratification among humans is a well\nstudied concept in sociology. The popularity of online social\nnetworks presents an opportunity to study social hierarchy\nfor different types of networks and at different scales. We\nadopt the premise that people form connections in a so-\ncial network based on their perceived social hierarchy; as a\nresult, the edge directions in directed social networks can\nbe leveraged to infer hierarchy. In this paper, we define a\nmeasure of hierarchy in a directed online social network,\nand present an efficient algorithm to compute this mea-\nsure. We validate our measure using ground truth including\nWikipedia notability score. We use this measure to study\nhierarchy in several directed online social networks includ-\ning Twitter, Delicious, YouTube, Flickr, LiveJournal, and\ncurated lists of several categories of people based on dif-\nferent occupations, and different organizations. Our experi-\nments on different online social networks show how hierarchy\nemerges as we increase the size of the network. This is in\ncontrast to random graphs, where the hierarchy decreases\nas the network size increases. Further, we show that the\ndegree of stratification in a network increases very slowly as\nwe increase the size of the graph."}
{"Title": "Estimating Sizes of Social Networks via Biased Sampling", "Abstract": "Online social networks have become very popular in recent\nyears and their number of users is already measured in many\nhundreds of millions. For various commercial and sociolog-\nical purposes, an independent estimate of their sizes is im-\nportant. In this work, algorithms for estimating the num-\nber of users in such networks are considered. The proposed\nschemes are also applicable for estimating the sizes of net-\nworks\u2019 sub-populations.\nThe suggested algorithms interact with the social networks\nvia their public APIs only, and rely on no other external in-\nformation. Due to obvious traffic and privacy concerns, the\nnumber of such interactions is severely limited. We there-\nfore focus on minimizing the number of API interactions\nneeded for producing good size estimates. We adopt the\nabstraction of social networks as undirected graphs and use\nrandom node sampling. By counting the number of col-\nlisions or non-unique nodes in the sample, we produce a\nsize estimate. Then, we show analytically that the estimate\nerror vanishes with high probability for smaller number of\nsamples than those required by prior-art algorithms. More-\nover, although our algorithms are provably correct for any\ngraph, they excel when applied to social network-like graphs.\nThe proposed algorithms were evaluated on synthetic as well\nreal social networks such as Facebook, IMDB, and DBLP.\nOur experiments corroborated the theoretical results, and\ndemonstrated the effectiveness of the algorithms."}
{"Title": "A CRM system for Social Media", "Abstract": "The social Customer Relationship Management (CRM) land-\nscape is attracting significant attention from customers and\nenterprises alike as a sustainable channel for tracking, man-\naging and improving customer relations. Enterprises are\ntaking a hard look at this open, unmediated platform be-\ncause the community effect generated on this channel can\nhave a telling effect on their brand image, potential market\nopportunity and customer loyalty. In this work we present\nour experiences in building a system that mines conversa-\ntions on social platforms to identify and prioritize those\nposts and messages that are relevant to enterprises. The\nsystem presented in this work aims to empower an agent\nor a representative in an enterprise to monitor, track and\nrespond to customer communication while also encouraging\ncommunity participation."}
{"Title": "Towards a Robust Modeling of Temporal Interest Change\nPatterns for Behavioral Targeting", "Abstract": "Modern web-scale behavioral targeting platforms leverage\nhistorical activity of billions of users to predict user interests\nand inclinations, and consequently future activities. Future\nactivities of particular interest involve purchases or transac-\ntions, and are referred to as conversions. Unlike ad-clicks,\nconversions directly translate to advertiser\u2019s revenue, and\nthus provide a very concrete metric for return on advertis-\ning investment. A typical behavioral targeting system faces\ntwo main challenges: the web-scale amounts of user histories\nto process on a daily basis, and the relative sparsity of con-\nversions (compared to clicks in a traditional setting). These\nchallenges call for generation of effective and efficient user\nprofiles. Most existing works use the historical intensity of a\nuser\u2019s interest in various topics to model future interest. In\nthis paper we explore how the change in user behavior can\nbe used to predict future actions and show how it comple-\nments the traditional models of decaying interest and action\nrecency to build a complete picture about the user inter-\nests and better predict conversions. Our evaluation over a\nreal-world set of campaigns indicates that the combination\nof change of interest, decaying intensity, and action recency\nhelps in: 1) scoring significant improvements in optimizing\nfor conversions over traditional baselines, 2) substantially\nimproving the targeting efficiency for campaigns with highly\nsparse conversions, and 3) highly reducing the overall his-\ntory sizes used in targeting. Furthermore, our techniques\nhave been deployed to production and scored a substantial\nimprovement in targeting performance while imposing a neg-\nligible overhead in terms of overall platform running time."}
{"Title": "The Anatomy of LDNS Clusters: Findings and Implications\nfor Web Content Delivery", "Abstract": "We present a large-scale measurement of clusters of hosts\nsharing the same local DNS servers. We analyze properties\nof these\u201cLDNS clusters\u201dfrom the perspective of content de-\nlivery networks, which commonly use DNS for load distribu-\ntion. We found that not only LDNS clusters differ widely in\nterms of their size and geographical compactness but that\nthe largest clusters are actually extremely compact. This\nsuggests potential benefits of a load distribution strategy\nwith nuanced treatment of different LDNS clusters based on\nthe combination of their size and compactness. We further\nobserved interesting variations in LDNS setups including a\nwide use of \u201cLDNS pools\u201d (which as we explain in the pa-\nper are different from setups where end-hosts simply utilize\nmultiple resolvers)."}
{"Title": "Steering User Behavior with Badges", "Abstract": "An increasingly common feature of online communities and social\nmedia sites is a mechanism for rewarding user achievements based\non a system of badges. Badges are given to users for particular\ncontributions to a site, such as performing a certain number of ac-\ntions of a given type. They have been employed in many domains,\nincluding news sites like the Huffington Post, educational sites like\nKhan Academy, and knowledge-creation sites like Wikipedia and\nStack Overflow. At the most basic level, badges serve as a sum-\nmary of a user\u2019s key accomplishments; however, experience with\nthese sites also shows that users will put in non-trivial amounts of\nwork to achieve particular badges, and as such, badges can act as\npowerful incentives. Thus far, however, the incentive structures\ncreated by badges have not been well understood, making it diffi-\ncult to deploy badges with an eye toward the incentives they are\nlikely to create.\nIn this paper, we study how badges can influence and steer user\nbehavior on a site\u2014leading both to increased participation and to\nchanges in the mix of activities a user pursues on the site. We intro-\nduce a formal model for reasoning about user behavior in the pres-\nence of badges, and in particular for analyzing the ways in which\nbadges can steer users to change their behavior. To evaluate the\nmain predictions of our model, we study the use of badges and their\neffects on the widely used Stack Overflow question-answering site,\nand find evidence that their badges steer behavior in ways closely\nconsistent with the predictions of our model. Finally, we inves-\ntigate the problem of how to optimally place badges in order to\ninduce particular user behaviors. Several robust design principles\nemerge from our framework that could potentially aid in the design\nof incentives for a broad range of sites."}
{"Title": "Reactive Crowdsourcing", "Abstract": "An essential aspect for building effective crowdsourcing com-\nputations is the ability of \u201ccontrolling the crowd\u201d, i.e. of\ndynamically adapting the behaviour of the crowdsourcing\nsystems as response to the quantity and quality of com-\npleted tasks or to the availability and reliability of perform-\ners. Most crowdsourcing systems only provide limited and\npredefined controls; in contrast, we present an approach to\ncrowdsourcing which provides fine-level, powerful and flex-\nible controls. We model each crowdsourcing application as\ncomposition of elementary task types and we progressively\ntransform these high level specifications into the features of a\nreactive execution environment that supports task planning,\nassignment and completion as well as performer monitoring\nand exclusion. Controls are specified as active rules on top\nof data structures which are derived from the model of the\napplication; rules can be added, dropped or modified, thus\nguaranteeing maximal flexibility with limited effort.\nWe also report on our prototype platform that implements\nthe proposed framework and we show the results of our ex-\nperimentations with different rule sets, demonstrating how\nsimple changes to the rules can substantially affect time,\neffort and quality involved in crowdsourcing activities"}
{"Title": "Inferring the Demographics of Search Users", "Abstract": "Knowing users\u2019 views and demographic traits offers a great\npotential for personalizing web search results or related ser-\nvices such as query suggestion and query completion. Such\nsignals however are often only available for a small fraction\nof search users, namely those who log in with their social net-\nwork account and allow its use for personalization of search\nresults. In this paper, we offer a solution to this problem\nby showing how user demographic traits such as age and\ngender, and even political and religious views can be effi-\nciently and accurately inferred based on their search query\nhistories. This is accomplished in two steps; we first train\npredictive models based on the publically available myPer-\nsonality dataset containing users\u2019 Facebook Likes and their\ndemographic information. We then match Facebook Likes\nwith search queries using Open Directory Project categories.\nFinally, we apply the model trained on Facebook Likes to\nlarge-scale query logs of a commercial search engine while ex-\nplicitly taking into account the difference between the traits\ndistribution in both datasets. We find that the accuracy of\nclassifying age and gender, expressed by the area under the\nROC curve (AUC), are 77% and 84% respectively for predic-\ntions based on Facebook Likes, and only degrade to 74% and\n80% when based on search queries. On a US state-by-state\nbasis we find a Pearson correlation of 0.72 for political views\nbetween the predicted scores and Gallup data, and 0.54 for\naffiliation with Judaism between predicted scores and data\nfrom the US Religious Landscape Survey. We conclude that\nit is indeed feasible to infer important demographic data of\nusers from their query history based on labelled Likes data\nand believe that this approach could provide valuable in-\nformation for personalization and monetization even in the\nabsence of demographic data."}
{"Title": "Crowdsourced Judgement Elicitation with Endogenous\nProficiency", "Abstract": "Crowdsourcing is now widely used to replace judgement or eval-\nuation by an expert authority with an aggregate evaluation from a\nnumber of non-experts, in applications ranging from rating and cat-\negorizing online content all the way to evaluation of student assign-\nmentsin massivelyopenonline courses (MOOCs)viapeer grading.\nA key issue in these settings, where direct monitoring of both effort\nand accuracy is infeasible, is incentivizing agents in the \u2018crowd\u2019 to\nput in effort to make good evaluations, as well as to truthfully report\ntheir evaluations. We study the design of mechanisms for crowd-\nsourced judgement elicitation when workers strategically choose\nboth their reports and the effort they put into their evaluations. This\nleads to a new family of information elicitation problems with un-\nobservable ground truth, where an agent\u2019s proficiency\u2014 the prob-\nability with which she correctly evaluates the underlying ground\ntruth\u2014 is endogenously determined by her strategic choice of how\nmuch effort to put into the task.\nOur main contribution is a simple, new, mechanism for binary\ninformation elicitation for multiple tasks when agents have endoge-\nnous proficiencies, with the following properties: (i) Exerting max-\nimumeffortfollowedbytruthfulreportingofobservationsisaNash\nequilibrium. (ii) This is the equilibrium with maximum payoff to all\nagents, even when agents have different maximum proficiencies,\ncan use mixed strategies, and can choose a different strategy for\neach of their tasks. Our information elicitation mechanism requires\nonly minimal bounds on the priors, asks agents to only report their\nown evaluations, and does not require any conditions on a diverging\nnumber of agent reports per task to achieve its incentive properties.\nThe main idea behind our mechanism is to use the presence of mul-\ntipletasksandratingstoestimateareportingstatistictoidentifyand\npenalize low-effort agreement\u2014 the mechanism rewards agents for\nagreeing with another \u2018reference\u2019 report on the same task, but also\npenalizes for blind agreement by subtracting out this statistic term,\ndesigned so that agents obtain rewards only when they put in effort\ninto their observations."}
{"Title": "Mining Expertise and Interests from Social Media", "Abstract": "The rising popularity of social media in the enterprise presents\nnew opportunities for one of the organization\u2019s most important\nneeds\u2014expertise location. Social media data can be very useful\nfor expertise mining due to the variety of existing applications, the\nrich metadata, and the diversity of user associations with content.\nIn this work, we provide an extensive study that explores the use\nof social media to infer expertise within a large global\norganization.  We  examine  eight  different  social  media\napplications by evaluating the data they produce through a large\nuser survey, with 670 enterprise social media users. We\ndistinguish between two semantics that relate a user to a topic:\nexpertise in the topic and interest in it and compare these two\nsemantics across the different social media applications."}
{"Title": "Organizational Overlap on Social Networks\nand its Applications", "Abstract": "Online social networks have become important for networking,\ncommunication, sharing, and discovery. A considerable challenge\nthese networks face is the fact that an online social network is par-\ntially observed because two individuals might know each other, but\nmay not have established a connection on the site. Therefore, link\nprediction and recommendations are important tasks for any online\nsocial network. In this paper, we address the problem of comput-\ning edge affinity between two users on a social network, based on\nthe users belonging to organizations such as companies, schools,\nand online groups. We present experimental insights from so-\ncial network data on organizational overlap, a novel mathematical\nmodel to compute the probability of connection between two peo-\nple based on organizational overlap, and experimental validation\nof this model based on real social network data. We also present\nnovel ways in which the organization overlap model can be applied\nto link prediction and community detection, which in itself could\nbe useful for recommending entities to follow and generating per-\nsonalized news feed."}
{"Title": "Unsupervised Sentiment Analysis with Emotional Signals", "Abstract": "The explosion of social media services presents a great op-\nportunity to understand the sentiment of the public via ana-\nlyzing its large-scale and opinion-rich data. In social media,\nit is easy to amass vast quantities of unlabeled data, but\nvery costly to obtain sentiment labels, which makes unsuper-\nvised sentiment analysis essential for various applications.\nIt is challenging for traditional lexicon-based unsupervised\nmethods due to the fact that expressions in social media\nare unstructured, informal, and fast-evolving. Emoticons\nand product ratings are examples of emotional signals that\nare associated with sentiments expressed in posts or words.\nInspired by the wide availability of emotional signals in so-\ncial media, we propose to study the problem of unsupervised\nsentiment analysis with emotional signals. In particular, we\ninvestigate whether the signals can potentially help senti-\nment analysis by providing a unified way to model two main\ncategories of emotional signals, i.e., emotion indication and\nemotion correlation. We further incorporate the signals into\nan unsupervised learning framework for sentiment analysis.\nIn the experiment, we compare the proposed framework with\nthe state-of-the-art methods on two Twitter datasets and\nempirically evaluate our proposed framework to gain a deep\nunderstanding of the effects of emotional signals."}
{"Title": "SoCo: A Social Network Aided Context-Aware\nRecommender System", "Abstract": "Contexts and social network information have been proven\nto be valuable information for building accurate recommender\nsystem. However, to the best of our knowledge, no exist-\ning works systematically combine diverse types of such in-\nformation to further improve recommendation quality. In\nthis paper, we propose SoCo, a novel context-aware recom-\nmender system incorporating elaborately processed social\nnetwork information. We handle contextual information by\napplying random decision trees to partition the original user-\nitem-rating matrix such that the ratings with similar con-\ntexts are grouped. Matrix factorization is then employed\nto predict missing preference of a user for an item using\nthe partitioned matrix. In order to incorporate social net-\nwork information, we introduce an additional social regular-\nization term to the matrix factorization objective function\nto infer a user\u2019s preference for an item by learning opin-\nions from his/her friends who are expected to share similar\ntastes. A context-aware version of Pearson Correlation Coef-\nficient is proposed to measure user similarity. Real datasets\nbased experiments show that SoCo improves the perfor-\nmance (in terms of root mean square error) of the state-of-\nthe-art context-aware recommender system and social rec-\nommendation model by 15.7% and 12.2% respectively."}
{"Title": "The FLDA Model for Aspect-based Opinion Mining:\nAddressing the Cold Start Problem", "Abstract": "Aspect-based opinion mining from online reviews has attracted a\nlot of attention recently. The main goal of all of the proposed meth-\nods is extracting aspects and/or estimating aspect ratings. Recent\nworks, whichareoftenbasedonLatentDirichletAllocation(LDA),\nconsider both tasks simultaneously. These models are normally\ntrained at the item level, i.e., a model is learned for each item sepa-\nrately. Learning a model per item is fine when the item has been re-\nviewed extensively and has enough training data. However, in real-\nlife datasets such as those from Epinions.com and Amazon.com\nmore than 90% of items have less than 10 reviews, so-called cold\nstart items. State-of-the-art LDA models for aspect-based opinion\nmining are trained at the item level and therefore perform poorly\nfor cold start items due to the lack of sufficient training data. In this\npaper, we propose a probabilistic graphical model based on LDA,\ncalled Factorized LDA (FLDA), to address the cold start problem.\nThe underlying assumption of FLDA is that aspects and ratings of a\nreview are influenced not only by the item but also by the reviewer.\nIt further assumes that both items and reviewers can be modeled by\na set of latent factors which represent their aspect and rating dis-\ntributions. Different from state-of-the-art LDA models, FLDA is\ntrained at the category level and learns the latent factors using the\nreviews of all the items of a category, in particular the non cold start\nitems, and uses them as prior for cold start items. Our experiments\non three real-life datasets demonstrate the improved effectiveness\nof the FLDA model in terms of likelihood of the held-out test set.\nWe also evaluate the accuracy of FLDA based on two application-\noriented measures."}
{"Title": "One-class Collaborative Filtering with Random Graphs", "Abstract": "The bane of one-class collaborative filtering is interpreting\nand modelling the latent signal from the missing class. In\nthis paper we present a novel Bayesian generative model for\nimplicit collaborative filtering. It forms a core component of\nthe Xbox Live architecture, and unlike previous approaches,\ndelineates the odds of a user disliking an item from sim-\nply being unaware of it. The latent signal is treated as an\nunobserved random graph connecting users with items they\nmight have encountered. We demonstrate how large-scale\ndistributed learning can be achieved through a combination\nof stochastic gradient descent and mean field variational in-\nference over random graph samples. A fine-grained compar-\nison is done against a state of the art baseline on real world\ndata."}
{"Title": "Wisdom in the Social Crowd: an Analysis of Quora", "Abstract": "Efforts such as Wikipedia have shown the ability of user commu-\nnities to collect, organize and curate information on the Internet.\nRecently, a number of question and answer (Q&A) sites have suc-\ncessfully built large growing knowledge repositories, each driven\nby a wide range of questions and answers from its users commu-\nnity. While sites like Yahoo Answers have stalled and begun to\nshrink, one site still going strong is Quora, a rapidly growing ser-\nvice that augments aregular Q&A system withsocial links between\nusers. Despite its success, however, little is known about what\ndrives Quora\u2019s growth, and how it continues to connect visitors and\nexperts to the right questions as it grows.\nIn this paper, we present results of a detailed analysis of Quora\nusing measurements. We shed light on the impact of three different\nconnection networks (or graphs) inside Quora, a graph connect-\ning topics to users, a social graph connecting users, and a graph\nconnecting related questions. Our results show that heterogeneity\nin the user and question graphs are significant contributors to the\nquality of Quora\u2019s knowledge base. One drives the attention and\nactivity of users, and the other directs them to a small set of popu-\nlar and interesting questions."}
{"Title": "A Biterm Topic Model for Short Texts", "Abstract": "Uncovering the topics within short texts, such as tweets and\ninstant messages, has become an important task for many\ncontent analysis applications. However, directly applying\nconventional topic models (e.g. LDA and PLSA) on such\nshort texts may not work well. The fundamental reason\nlies in that conventional topic models implicitly capture the\ndocument-level word co-occurrence patterns to reveal topics,\nand thus suffer from the severe data sparsity in short docu-\nments. In this paper, we propose a novel way for modeling\ntopics in short texts, referred as biterm topic model (BTM).\nSpecifically, in BTM we learn the topics by directly modeling\nthe generation of word co-occurrence patterns (i.e. biterms)\nin the whole corpus. The major advantages of BTM are\nthat 1) BTM explicitly models the word co-occurrence pat-\nterns to enhance the topic learning; and 2) BTM uses the\naggregated patterns in the whole corpus for learning topics\nto solve the problem of sparse word co-occurrence patterns\nat document-level. We carry out extensive experiments on\nreal-world short text collections. The results demonstrate\nthat our approach can discover more prominent and coher-\nent topics, and significantly outperform baseline methods on\nseveral evaluation metrics. Furthermore, we find that BTM\ncan outperform LDA even on normal texts, showing the po-\ntential generality and wider usage of the new topic model."}
{"Title": "Unified Entity Search in Social Media Community ", "Abstract": "The search for entities is the most common search behavior\non the Web, especially in social media communities where\nentities (such as images, videos, people, locations, and tags)\nare highly heterogeneous and correlated. While previous\nresearch usually deals with these social media entities sep-\narately, we are investigating in this paper a unified, multi-\nlevel, and correlative entity graph to represent the unstruc-\ntured social media data, through which various applications\n(e.g., friend suggestion, personalized image search, image\ntagging, etc.) can be realized more effectively in one sin-\ngle framework. We regard the social media objects equally\nas \u201centities\u201d and all of these applications as \u201centity search\u201d\nproblem which searches for entities with different types. We\nfirst construct a multi-level graph which organizes the het-\nerogeneous entities into multiple levels, with one type of\nentities as vertices in each level. The edges between graphs\npairwisely connect the entities weighted by intra-relations in\nthe same level and inter-links across two different levels dis-\ntilled from the social behaviors (e.g., tagging, commenting,\nand joining communities). To infer the strength of intra-\nrelations, we propose a circular propagation scheme, which\nreinforces the mutual exchange of information across differ-\nent entity types in a cyclic manner. Based on the construct-\ned unified graph, we explicitly formulate entity search as\na global optimization problem in a unified Bayesian frame-\nwork, in which various applications are efficiently realized.\nEmpirically, we validate the effectiveness of our unified en-\ntity graph for various social media applications on million-\nscale real-world dataset."}
{"Title": "Predicting Positive and Negative Links in Signed Social\nNetworks by Transfer Learning", "Abstract": "Different from a large body of research on social networks\nthat has focused almost exclusively on positive relationships,\nwe study signed social networks with both positive and neg-\native links. Specifically, we focus on how to reliably and ef-\nfectively predict the signs of links in a newly formed signed\nsocial network (called a target network). Since usually only\na very small amount of edge sign information is available\nin such newly formed networks, this small quantity is not\nadequate to train a good classifier. To address this chal-\nlenge, we need assistance from an existing, mature signed\nnetwork (called a source network) which has abundant edge\nsign information. We adopt the transfer learning approach\nto leverage the edge sign information from the source net-\nwork, which may have a different yet related joint distribu-\ntion of the edge instances and their class labels.\nAs there is no predefined feature vector for the edge in-\nstances in a signed network, we construct generalizable fea-\ntures that can transfer the topological knowledge from the\nsource network to the target. With the extracted features,\nwe adopt an AdaBoost-like transfer learning algorithm with\ninstance weighting to utilize more useful training instances\nin the source network for model learning. Experimental re-\nsults on three real large signed social networks demonstrate\nthat our transfer learning algorithm can improve the predic-\ntion accuracy by 40% over baseline methods."}
{"Title": "TopRec: Domain-Specific Recommendation through\nCommunity Topic Mining in Social Network", "Abstract": "Traditionally, Collaborative Filtering assumes that similar\nusers have similar responses to similar items. However, hu-\nman activities exhibit heterogenous features across multiple\ndomains such that users own similar tastes in one domain\nmay behave quite differently in other domains. Moreover,\nhighly sparse data presents crucial challenge in preference\nprediction. Intuitively, if users\u2019 interested domains are cap-\ntured first, the recommender system is more likely to provide\nthe enjoyed items while filter out those uninterested ones.\nTherefore, it is necessary to learn preference profiles from\nthe correlated domains instead of the entire user-item ma-\ntrix. In this paper, we propose a unified framework, TopRec,\nwhich detects topical communities to construct interpretable\ndomains for domain-specific collaborative filtering. In order\nto mine communities as well as the corresponding topics, a\nsemi-supervised probabilistic topic model is utilized by in-\ntegrating user guidance with social network. Experimental\nresults on real-world data from Epinions and Ciao demon-\nstrate the effectiveness of the proposed framework."}
{"Title": "Predicting Purchase Behaviors from Social Media", "Abstract": "In the era of social commerce, users often connect from e-\ncommerce websites to social networking venues such as Face-\nbook and Twitter. However, there have been few efforts\non understanding the correlations between users\u2019 social me-\ndia profiles and their e-commerce behaviors. This paper\npresents a system for predicting a user\u2019s purchase behaviors\non e-commerce websites from the user\u2019s social media profile.\nWe specifically aim at understanding if the user\u2019s profile in-\nformation in a social network (for example Facebook) can be\nleveraged to predict what categories of products the user will\nbuy from (for example eBay Electronics). The paper pro-\nvides an extensive analysis on how users\u2019 Facebook profile\ninformation correlates to purchases on eBay, and analyzes\nthe performance of different feature sets and learning algo-\nrithms on the task of purchase behavior prediction."}
{"Title": "Security Implications of Password Discretization\nfor Click-based Graphical Passwords", "Abstract": "Discretization is a standard technique used in click-based\ngraphical passwords for tolerating input variance so that\napproximately correct passwords are accepted by the system. In\nthis paper, we show for the first time that two representative\ndiscretization schemes leak a significant amount of password\ninformation, undermining the security of such graphical\npasswords. We exploit such information leakage for successful\ndictionary attacks on Persuasive Cued Click Points (PCCP),\nwhich is to date the most secure click-based graphical password\nscheme and was considered to be resistant to such attacks. In our\nexperiments, our purely automated attack successfully guessed\n69.2% of the passwords when Centered Discretization was used to\nimplement PCCP, and 39.4% of the passwords when Robust\nDiscretization was used. Each attack dictionary we used was of\napproximately 2 ?? entries, whereas the full password space was of\n2 ?? entries. For Centered Discretization, our attack still\nsuccessfully guessed 50% of the passwords when the dictionary\nsize was reduced to approximately 2 ?? entries. Our attack is also\napplicable to common implementations of other click-based\ngraphical password systems such as PassPoints and Cued Click\nPoints \u2013 both have been extensively studied in the research\ncommunities."}
{"Title": "Active Learning for Multi-relational Data Construction", "Abstract": "Knowledge on the Web relies heavily on multi-relational\nrepresentations, such as RDF and Schema.org. Automat-\nically extracting knowledge from documents and linking ex-\nisting databases are common approaches to construct multi-\nrelational data. Complementary to such approaches, there\nis still a strong demand for manually encoding human expert\nknowledge. For example, human annotation is necessary for\nconstructing a common-sense knowledge base, which stores\nfacts implicitly shared in a community, because such knowl-\nedge rarely appears in documents. As human annotation\nis both tedious and costly, an important research challenge\nis how to best use limited human resources, whiles maxi-\nmizing the quality of the resulting dataset. In this paper,\nwe formalize the problem of dataset construction as active\nlearning problems and present the Active Multi-relational\nData Construction (AMDC) method. AMDC repeatedly\ninterleaves multi-relational learning and expert input acqui-\nsition, allowing us to acquire helpful labels for data con-\nstruction. Experiments on real datasets demonstrate that\nour solution increases the number of positive triples by a\nfactor of 2.28 to 17.0, and that the predictive performance\nof the multi-relational model in AMDC achieves the highest\nor comparable to the best performance throughout the data\nconstruction process."}
{"Title": "Collaborative Ranking with a Push at the Top", "Abstract": "The goal of collaborative filtering is to get accurate recom-\nmendations at the top of the list for a set of users. From such\na perspective, collaborative ranking based formulations with\nsuitable ranking loss functions are natural. While recent lit-\nerature has explored the idea based on objective functions\nsuch as NDCG or Average Precision, such objectives are dif-\nficult to optimize directly. In this paper, building on recent\nadvances from the learning to rank literature, we introduce a\nnovel family of collaborative ranking algorithms which focus\non accuracy at the top of the list for each user while learn-\ning the ranking functions collaboratively. We consider three\nspecific formulations, based on collaborative p-norm push,\ninfinite push, and reverse-height push, and propose e?cient\noptimization methods for learning these models. Experi-\nmental results illustrate the value of collaborative ranking,\nand show that the proposed methods are competitive, usu-\nally better than existing popular approaches to personalized\nrecommendation."}
{"Title": "Crowd Fraud Detection in Internet Advertising", "Abstract": "The rise of crowdsourcing brings new types of malpractices\nin Internet advertising. One can easily hire web workers\nthrough malicious crowdsourcing platforms to attack oth-\ner advertisers. Such human generated crowd frauds are hard\nto detect by conventional fraud detection methods. In this\npaper, we carefully examine the characteristics of the group\nbehaviors of crowd fraud and identify three persistent pat-\nterns, which are moderateness, synchronicity and dispersiv-\nity. Then we propose an effective crowd fraud detection\nmethod for search engine advertising based on these pattern-\ns, which consists of a constructing stage, a clustering stage\nand a filtering stage. At the constructing stage, we remove\nirrelevant data and reorganize the click logs into a surfer-\nadvertiser inverted list; At the clustering stage, we define the\nsync-similarity between surfers\u2019 click histories and transfor-\nm the coalition detection to a clustering problem, solved by\na nonparametric algorithm; and finally we build a dispersity\nfilter to remove false alarm clusters. The nonparametric na-\nture of our method ensures that we can find an unbounded\nnumber of coalitions with nearly no human interaction. We\nalso provide a parallel solution to make the method scalable\nto Web data and conduct extensive experiments. The em-\npirical results demonstrate that our method is accurate and\nscalable."}
{"Title": "Describing and Understanding Neighborhood\nCharacteristics through Online Social Media", "Abstract": "Geotagged data can be used to describe regions in the world\nand discover local themes. However, not all data produced\nwithin a region is necessarily specifically descriptive of that\narea. To surface the content that is characteristic for a re-\ngion, we present the geographical hierarchy model (GHM), a\nprobabilistic model based on the assumption that data ob-\nserved in a region is a random mixture of content that per-\ntains to different levels of a hierarchy. We apply the GHM to\na dataset of 8 million Flickr photos in order to discriminate\nbetween content (i.e., tags) that specifically characterizes\na region (e.g., neighborhood) and content that characterizes\nsurrounding areas or more general themes. Knowledge of the\ndiscriminative and non-discriminative terms used through-\nout the hierarchy enables us to quantify the uniqueness of\na given region and to compare similar but distant regions.\nOur evaluation demonstrates that our model improves upon\ntraditional Naive Bayes classification by 47% and hierarchi-\ncal TF-IDF by 27%. We further highlight the differences\nand commonalities with human reasoning about what is lo-\ncally characteristic for a neighborhood, distilled from ten\ninterviews and a survey that covered themes such as time,\nevents, and prior regional knowledge."}
{"Title": "Donor Retention in Online Crowdfunding Communities:\nA Case Study of DonorsChoose.org", "Abstract": "Online crowdfunding platforms like DonorsChoose.org and Kick-\nstarter allow specific projects to get funded by targeted contribu-\ntions from a large number of people. Critical for the success of\ncrowdfunding communities is recruitment and continued engage-\nment of donors. With donor attrition rates above 70%, a significant\nchallenge for online crowdfunding platforms as well as traditional\noffline non-profit organizations is the problem of donor retention.\nWe present a large-scale study of millions of donors and dona-\ntions on DonorsChoose.org, a crowdfunding platform for education\nprojects. Studying an online crowdfunding platform allows for an\nunprecedented detailed view of how people direct their donations.\nWe explore various factors impacting donor retention which allows\nus to identify different groups of donors and quantify their propen-\nsity to return for subsequent donations. We find that donors are\nmore likely to return if they had a positive interaction with the re-\nceiver of the donation. We also show that this includes appropriate\nand timely recognition of their support as well as detailed commu-\nnication of their impact. Finally, we discuss how our findings could\ninform steps to improve donor retention in crowdfunding commu-\nnities and non-profit organizations."}
{"Title": "Enriching Structured Knowledge with Open Informatio", "Abstract": "We propose an approach for semantifying web extracted\nfacts. In particular, we map subject and object terms of\nthese facts to instances; and relational phrases to object\nproperties defined in a target knowledge base. By doing\nthis we resolve the ambiguity inherent in the web extracted\nfacts, while simultaneously enriching the target knowledge\nbase with a significant number of new assertions. In this\npaper, we focus on the mapping of the relational phrases\nin the context of the overall workflow. Furthermore, in an\nopen extraction setting identical semantic relationships can\nbe represented by different surface forms, making it neces-\nsary to group these surface forms together. To solve this\nproblem we propose the use of markov clustering. In this\nwork we present a complete, ontology independent, general-\nized workflow which we evaluate on facts extracted by Nell\nand Reverb. Our target knowledge base is DBpedia. Our\nevaluation shows promising results in terms of producing\nhighly precise facts. Moreover, the results indicate that the\nclustering of relational phrases pays off in terms of an im-\nproved instance and property mapping."}
{"Title": "Events and Controversies: Influences of a\nShocking News Event on Information Seeking", "Abstract": "It has been suggested that online search and retrieval contributes\nto the intellectual isolation of users within their preexisting ide-\nologies, where people\u2019s prior views are strengthened and alterna-\ntive viewpoints are infrequently encountered. This so-called \u201cfilter\nbubble\u201d phenomenon has been called out as especially detrimen-\ntal when it comes to dialog among people on controversial, emo-\ntionally charged topics, such as the labeling of genetically modi-\nfied food, the right to bear arms, the death penalty, and online pri-\nvacy. We seek to identify and study information-seeking behavior\nand access to alternative versus reinforcing viewpoints following\nshocking, emotional, and large-scale news events. We choose for a\ncase study to analyze search and browsing on gun control/rights, a\nstrongly polarizing topic for both citizens and leaders of the United\nStates. We study the period of time preceding and following a\nmass shooting to understand how its occurrence, follow-on dis-\ncussions, and debate may have been linked to changes in the pat-\nterns of searching and browsing. We employ information-theoretic\nmeasures to quantify the diversity of Web domains of interest to\nusers and understand the browsing patterns of users. We use these\nmeasures to characterize the influence of news events on these web\nsearch and browsing patterns."}
{"Title": "Incorporating Social Context and Domain Knowledge for\nEntity Recognition", "Abstract": "Recognizing entity instances in documents according to a knowl-\nedge base is a fundamental problem in many data mining applica-\ntions. The problem is extremely challenging for short documents\nin complex domains such as social media and biomedical domains.\nLarge concept spaces and instance ambiguity are key issues that\nneed to be addressed.\nMostofthedocumentsarecreatedinasocialcontextbycommon\nauthors via social interactions, such as reply and citations. Such\nsocial contexts are largely ignored in the instance-recognition liter-\nature. How can users\u2019 interactions help entity instance recognition?\nHow can the social context be modeled so as to resolve the ambi-\nguity of different instances?\nIn this paper, we propose the SOCINST model to formalize the\nproblem into a probabilistic model. Given a set of short documents\n(e.g., tweets or paper abstracts) posted by users who may connect\nwith each other, SOCINST can automatically construct a context\nof subtopics for each instance, with each subtopic representing one\npossiblemeaningoftheinstance. Themodelisalsoabletoincorpo-\nrate social relationships between users to help build social context.\nWe further incorporate domain knowledge into the model using a\nDirichlet tree distribution.\nWe evaluate the proposed model on three different genres of\ndatasets: ICDM\u201912 Contest, Weibo, and I2B2. In ICDM\u201912 Con-\ntest, the proposed model clearly outperforms (+21.4%; p ? 1e\u22125\nwith t-test) all the top contestants. In Weibo and I2B2, our results\nalso show that the recognition accuracy of SOCINST is up to 5.3-\n26.6% better than those of several alternative methods."}
{"Title": "Language Understanding in the Wild: Combining\nCrowdsourcing and Machine Learning", "Abstract": "Social media has led to the democratisation of opinion shar-\ning. A wealth of information about public opinions, cur-\nrent events, and authors\u2019 insights into specific topics can\nbe gained by understanding the text written by users. How-\never, there is a wide variation in the language used by differ-\nent authors in different contexts on the web. This diversity\nin language makes interpretation an extremely challenging\ntask. Crowdsourcing presents an opportunity to interpret\nthe sentiment, or topic, of free-text. However, the subjec-\ntivity and bias of human interpreters raise challenges in in-\nferring the semantics expressed by the text. To overcome\nthis problem, we present a novel Bayesian approach to lan-\nguage understanding that relies on aggregated crowdsourced\njudgements. Our model encodes the relationships between\nlabels and text features in documents, such as tweets, web\narticles, and blog posts, accounting for the varying reliability\nof human labellers. It allows inference of annotations that\nscales to arbitrarily large pools of documents. Our evalu-\nation using two challenging crowdsourcing datasets shows\nthat by efficiently exploiting language models learnt from\naggregated crowdsourced labels, we can provide up to 25%\nimproved classifications when only a small portion, less than\n4% of documents has been labelled. Compared to the six\nstate-of-the-art methods, we reduce by up to 67% the num-\nber of crowd responses required to achieve comparable accu-\nracy. Our method was a joint winner of the CrowdFlower -\nCrowdScale 2013 Shared Task challenge at the conference on\nHuman Computation and Crowdsourcing (HCOMP 2013)."}
{"Title": "Open Domain Question Answering via Semantic Enrichment", "Abstract": "Most recent question answering (QA) systems query large-\nscale knowledge bases (KBs) to answer a question, after\nparsing and transforming natural language questions to KBs-\nexecutable forms (e.g., logical forms). As a well-known fact,\nKBs are far from complete, so that information required\nto answer questions may not always exist in KBs. In this\npaper, we develop a new QA system that mines answers\ndirectly from the Web, and meanwhile employs KBs as a\nsignificant auxiliary to further boost the QA performance.\nSpecifically, to the best of our knowledge, we make the\nfirst attempt to link answer candidates to entities in Free-\nbase, during answer candidate generation. Several remark-\nable advantages follow: (1) Redundancy among answer can-\ndidates is automatically reduced. (2) The types of an answer\ncandidate can be effortlessly determined by those of its cor-\nresponding entity in Freebase. (3) Capitalizing on the rich\ninformation about entities in Freebase, we can develop se-\nmantic features for each answer candidate after linking them\nto Freebase. Particularly, we construct answer-type related\nfeatures with two novel probabilistic models, which directly\nevaluate the appropriateness of an answer candidate\u2019s types\nunder a given question. Overall, such semantic features turn\nout to play significant roles in determining the true answer-\ns from the large answer candidate pool. The experimental\nresults show that across two testing datasets, our QA sys-\ntem achieves an 18% \u223c 54% improvement under F 1 metric,\ncompared with various existing QA systems."}
{"Title": "Opinion Spam Detection in Web Forum:\nA Real Case Study", "Abstract": "Opinion spamming refers to the illegal marketing practice which\ninvolves delivering commercially advantageous opinions as\nregular users. In this paper, we conduct a real case study based on\na set of internal records of opinion spams leaked from a shady\nmarketing campaign. We explore the characteristics of opinion\nspams and spammers in a web forum to obtain some insights,\nincluding subtlety property of opinion spams, spam post ratio,\nspammer accounts, first post and replies, submission time of\nposts, activeness of threads, and collusion among spammers. Then\nwe present features that could be potentially helpful in detecting\nspam opinions in threads. The results of spam detection on first\nposts show: (1) spam first posts put more focus on certain topics\nsuch as the user experiences\u2019 on the promoted items, (2) spam\nfirst posts generally use more words and pictures to showcase the\npromoted items in an attempt to impress people, (3) spam first\nposts tend to be submitted during work time, and (4) the threads\nthat spam first posts initiate are more active to be placed at\nstriking positions. The spam detection on replies is more\nchallenging. Besides lower spam ratio and less content, replies\neven do not mention the promoted items. Their major intention is\nto keep the discussion in a thread alive to attract more attention on\nit. Submission time of replies, thread activeness, position of\nreplies, and spamicity of first post are more useful than content-\nbased features in spam detection on replies."}
{"Title": "Optimizing Display Advertising in Online Social Networks", "Abstract": "Advertising is a significant source of revenue for most online social\nnetworks. Conventional online advertising methods need to be cus-\ntomized for online social networks in order to address their distinct\ncharacteristics. Recent experimental studies have shown that pro-\nviding social cues along with ads, e.g. information about friends\nliking the ad or clicking on an ad, leads to higher click rates. In\nother words, the probability of a user clicking an ad is a function of\nthe set of friends that have clicked the ad. In this work, we propose\nformal probabilistic models to capture this phenomenon, and study\nthe algorithmic problem that then arises. Our work is in the context\nof display advertising where a contract is signed to show an ad to a\npre-determined number of users. The problem we study is the fol-\nlowing: given a certain number of impressions, what is the optimal\ndisplay strategy, i.e. the optimal order and the subset of users to\nshow the ad to, so as to maximize the expected number of clicks?\nUnlike previous models of influence maximization, we show that\nthis optimization problem is hard to approximate in general, and\nthat it is related to finding dense subgraphs of a given size. In\nlight of the hardness result, we propose several heuristic algorithms\nincluding a two-stage algorithm inspired by influence-and-exploit\nstrategies in viral marketing. We evaluate the performance of these\nheuristics on real data sets, and observe that our two-stage heuristic\nsignificantly outperforms the natural baselines."}
{"Title": "Predicting Pinterest:\nAutomating a Distributed Human Computation", "Abstract": "Everyday, millions of users save content items for future use on\nsites like Pinterest, by \u201cpinning\u201d them onto carefully categorised\npersonal pinboards, thereby creating personal taxonomies of the\nWeb. This paper seeks to understand Pinterest as a distributed hu-\nman computation that categorises images from around the Web. We\nshow that despite being categorised onto personal pinboards by in-\ndividual actions, there is a generally a global agreement in implic-\nitly assigning images into a coarse-grained global taxonomy of 32\ncategories, and furthermore, users tend to specialise in a handful\nof categories. By exploiting these characteristics, and augmenting\nwith image-related features drawn from a state-of-the-art deep con-\nvolutional neural network, we develop a cascade of predictors that\ntogether automate a large fraction of Pinterest actions. Our end-to-\nend model is able to both predict whether a user will repin an image\nonto her own pinboard, and also which pinboard she might choose,\nwith an accuracy of 0.69 (Accuracy@5 of 0.75)."}
{"Title": "Summarizing Entity Descriptions for Effective and Efficient\nHuman-centered Entity Linking", "Abstract": "Entity linking connects the Web of documents with knowl-\nedge bases. It is the task of linking an entity mention in\ntext to its corresponding entity in a knowledge base. Where-\nas a large body of work has been devoted to automatically\ngenerating candidate entities, or ranking and choosing from\nthem, manual efforts are still needed, e.g., for defining gold-\nstandard links for evaluating automatic approaches, and for\nimproving the quality of links in crowdsourcing approaches.\nHowever, structured descriptions of entities in knowledge\nbases are sometimes very long. To avoid overloading hu-\nman users with too much information and help them more\nefficiently choose an entity from candidates, we aim to sub-\nstitute entire entity descriptions with compact, equally effec-\ntive structured summaries that are automatically generated.\nTo achieve it, our approach analyzes entity descriptions in\nthe knowledge base and the context of entity mention from\nmultiple perspectives, including characterizing and differen-\ntiating power, information overlap, and relevance to contex-\nt. Extrinsic evaluation (where human users carry out entity\nlinking tasks) and intrinsic evaluation (where human user-\ns rate summaries) demonstrate that summaries generated\nby our approach help human users carry out entity linking\ntasks more efficiently (22\u201323% faster), without significant-\nly affecting the quality of links obtained; and our approach\noutperforms existing approaches to summarizing entity de-\nscriptions."}
{"Title": "The Web as a Jungle: Non-Linear Dynamical Systems\nfor Co-evolving Online Activities", "Abstract": "Given a large collection of co-evolving online activities, such as\nsearches for the keywords \u201cXbox\u201d, \u201cPlayStation\u201d and \u201cWii\u201d, how\ncan we find patterns and rules? Are these keywords related? If so,\nare they competing against each other? Can we forecast the volume\nof user activity for the coming month?\nWe conjecture that online activities compete for user attention in\nthe same way that species in an ecosystem compete for food. We\npresent E CO W EB , (i.e., Ecosystem on the Web), which is an in-\ntuitive model designed as a non-linear dynamical system for min-\ning large-scale co-evolving online activities. Our second contri-\nbution is a novel, parameter-free, and scalable fitting algorithm,\nE CO W EB -F IT , that estimates the parameters of E CO W EB .\nExtensive experiments on real data show that E CO W EB is ef-\nfective, in that it can capture long-range dynamics and meaningful\npatterns such as seasonalities, and practical, in that it can provide\naccurate long-range forecasts. E CO W EB consistently outperforms\nexisting methods in terms of both accuracy and execution speed."}
{"Title": "TrueView: Harnessing the Power of Multiple Review Sites", "Abstract": "Online reviews on products and services can be very useful\nfor customers, but they need to be protected from manipu-\nlation. So far, most studies have focused on analyzing online\nreviews from a single hosting site. How could one leverage\ninformation from multiple review hosting sites? This is the\nkey question in our work. In response, we develop a system-\natic methodology to merge, compare, and evaluate reviews\nfrom multiple hosting sites. We focus on hotel reviews and\nuse more than 15 million reviews from more than 3.5 mil-\nlion users spanning three prominent travel sites. Our work\nconsists of three thrusts: (a) we develop novel features capa-\nble of identifying cross-site discrepancies effectively, (b) we\nconduct arguably the first extensive study of cross-site varia-\ntions using real data, and develop a hotel identity-matching\nmethod with 93% accuracy, (c) we introduce the TrueView\nscore, as a proof of concept that cross-site analysis can bet-\nter inform the end user. Our results show that: (1) we detect\n7 times more suspicious hotels by using multiple sites com-\npared to using the three sites in isolation, and (2) we find\nthat 20% of all hotels appearing in all three sites seem to\nhave low trustworthiness score.\nOur work is an early effort that explores the advantages\nand the challenges in using multiple reviewing sites towards\nmore informed decision making."}
{"Title": "User Review Sites as a Resource\nfor Large-Scale Sociolinguistic Studies", "Abstract": "Sociolinguistic studies investigate the relation between\nlanguage and extra-linguistic variables. This requires both\nrepresentative text data and the associated socio-economic\nmeta-data of the subjects. Traditionally, sociolinguistic\nstudies use small samples of hand-curated data and meta-\ndata. This can lead to exaggerated or false conclusions. Us-\ning social media data offers a large-scale source of language\ndata, but usually lacks reliable socio-economic meta-data.\nOur research aims to remedy both problems by exploring\na large new data source, international review websites with\nuser profiles. They provide more text data than manually\ncollected studies, and more meta-data than most available\nsocial media text. We describe the data and present vari-\nous pilot studies, illustrating the usefulness of this resource\nfor sociolinguistic studies. Our approach can help gener-\nate new research hypotheses based on data-driven findings\nacross several countries and languages."}
{"Title": "When Does Improved Targeting Increase Revenue?", "Abstract": "In second price auctions with symmetric bidders, we find\nthat improved targeting via enhanced information disclosure\ndecreases revenue when there are two bidders and increases\nrevenue if there are at least four bidders. With asymme-\ntries, improved targeting increases revenue if the most fre-\nquent winner wins less than 30.4% of the time, but can de-\ncrease revenue otherwise. We derive analogous results for\nposition auctions. Finally, we show that revenue can vary\nnon-monotonically with the number of bidders who are able\nto take advantage of improved targeting."}
{"Title": "Social Networks Under Stress", "Abstract": "Social network research has begun to take advantage of fine-\ngrained communications regarding coordination, decision-\nmaking, and knowledge sharing. These studies, however,\nhave not generally analyzed how external events are asso-\nciated with a social network\u2019s structure and communicative\nproperties. Here, we study how external events are associ-\nated with a network\u2019s change in structure and communica-\ntions. Analyzing a complete dataset of millions of instant\nmessages among the decision-makers in a large hedge fund\nand their network of outside contacts, we investigate the\nlink between price shocks, network structure, and change\nin the affect and cognition of decision-makers embedded in\nthe network. When price shocks occur the communication\nnetwork tends not to display structural changes associated\nwith adaptiveness. Rather, the network \u201cturtles up\u201d. It\ndisplays a propensity for higher clustering, strong tie inter-\naction, and an intensification of insider vs. outsider commu-\nnication. Further, we find changes in network structure pre-\ndict shifts in cognitive and affective processes, execution of\nnew transactions, and local optimality of transactions better\nthan prices, revealing the important predictive relationship\nbetween network structure and collective behavior within a\nsocial network."}
{"Title": "Measuring Urban Social Diversity\nUsing Interconnected Geo-Social Networks", "Abstract": "Large metropolitan cities bring together diverse individuals, creat-\ning opportunities for cultural and intellectual exchanges, which can\nultimately lead to social and economic enrichment. In this work,\nwe present a novel network perspective on the interconnected na-\nture of people and places, allowing us to capture the social diversity\nof urban locations through the social network and mobility patterns\nof their visitors. We use a dataset of approximately 37K users and\n42K venues in London to build a network of Foursquare places and\nthe parallel Twitter social network of visitors through check-ins.\nWe define four metrics of the social diversity of places which re-\nlate to their social brokerage role, their entropy, the homogeneity\nof their visitors and the amount of serendipitous encounters they\nare able to induce. This allows us to distinguish between places\nthat bring together strangers versus those which tend to bring to-\ngether friends, as well as places that attract diverse individuals as\nopposed to those which attract regulars. We correlate these prop-\nerties with wellbeing indicators for London neighbourhoods and\ndiscover signals of gentrification in deprived areas with high en-\ntropy and brokerage, where an influx of more affluent and diverse\nvisitors points to an overall improvement of their rank according to\nthe UK Indexof Multiple Deprivation for thearea over the five-year\ncensus period. Our analysis sheds light on the relationship between\nthe prosperity of people and places, distinguishing between differ-\nent categories and urban geographies of consequence to the devel-\nopment of urban policy and the next generation of socially-aware\nlocation-based applications."}
{"Title": "Recommendations in Signed Social Networks", "Abstract": "Recommender systems play a crucial role in mitigating the\ninformation overload problem in social media by suggesting\nrelevant information to users. The popularity of pervasively\navailable social activities for social media users has encour-\naged a large body of literature on exploiting social networks\nfor recommendation. The vast majority of these systems fo-\ncus on unsigned social networks (or social networks with only\npositive links), while little work exists for signed social net-\nworks (or social networks with positive and negative links).\nThe availability of negative links in signed social networks\npresents both challenges and opportunities in the recommen-\ndation process. We provide a principled and mathematical\napproach to exploit signed social networks for recommen-\ndation, and propose a model, RecSSN, to leverage positive\nand negative links in signed social networks. Empirical re-\nsults on real-world datasets demonstrate the effectiveness of\nthe proposed framework. We also perform further experi-\nments to explicitly understand the effect of signed networks\nin RecSSN."}
{"Title": "HeteroSales: Utilizing Heterogeneous Social Networks to\nIdentify the Next Enterprise Customer", "Abstract": "Nowadays, a modern e-commerce company may have both online\nsales and offline sales departments. Normally, online sales attempt\nto sell in small quantities to individual customers through broad-\ncasting a large amount of emails or promotion codes, which heav-\nily rely on the designed backend algorithms. Offline sales, on the\nother hand, try to sell in much larger quantities to enterprise cus-\ntomers through contacts initiated by sales representatives, which\nare more costly compared to online sales. Unlike many previous\nresearch works focusing on machine learning algorithms to sup-\nport online sales, this paper introduces an approach that utilizes\nheterogenous social networks to improve the effectiveness of of-\nfline sales. More specifically, we propose a two-phase framework,\nHeteroSales, which first constructs a company-to-company graph,\na.k.a. Company Homophily Graph (CHG), from semantics based\nmeta-path learning, and then adopts label propagation on the graph\nto predict promising companies that we may successfully close an\noffline deal with. Based on the statistical analysis on the world\u2019s\nlargest professional social network, LinkedIn, we demonstrate in-\nteresting discoveries showing that not all the social connections in\na heterogeneous social network are useful in this task. In other\nwords, some proper data preprocessing is essential to ensure the\neffectiveness of offline sales. Finally, through the experiments on\nLinkedIn social network data and third-party offline sales records,\nwe demonstrate the power of HereroSales to identify potential en-\nterprise customers in offline sales."}
{"Title": "Immersive Recommendation: News and Event\nRecommendations Using Personal Digital Traces", "Abstract": "We propose a new user-centric recommendation model, called Im-\nmersive Recommendation, that incorporates cross-platform and di-\nverse personal digital traces into recommendations. Our context-\naware topic modeling algorithm systematically profiles users\u2019 in-\nterests based on their traces from different contexts, and our hybrid\nrecommendation algorithm makes high-quality recommendations\nby fusing users\u2019 personal profiles, item profiles, and existing rat-\nings. Specifically, in this work we target personalized news and lo-\ncal event recommendations for their utility and societal importance.\nWe evaluated the model with a large-scale offline evaluation lever-\naging users\u2019 public Twitter traces. In addition, we conducted a di-\nrect evaluation of the model\u2019s recommendations in a 33-participant\nstudy using Twitter, Facebook and email traces. In the both cases,\nthe proposed model showed significant improvement over the state-\nof-the-art algorithms, suggesting the value of using this new user-\ncentric recommendation model to improve recommendation qual-\nity, including in cold-start situations."}
{"Title": "Beyond Collaborative Filtering: The List Recommendation\nProblem", "Abstract": "Most Collaborative Filtering (CF) algorithms are optimized\nusing a dataset of isolated user-item tuples. However, in\ncommercial applications recommended items are usually served\nas an ordered list of several items and not as isolated items.\nIn this setting, inter-item interactions have an effect on the\nlist\u2019s Click-Through Rate (CTR) that is unaccounted for\nusing traditional CF approaches. Most CF approaches also\nignore additional important factors like click propensity vari-\nation, item fatigue, etc. In this work, we introduce the\nlist recommendation problem. We present useful insights\ngleaned from user behavior and consumption patterns from\na large scale real world recommender system. We then pro-\npose a novel two-layered framework that builds upon ex-\nisting CF algorithms to optimize a list\u2019s click probability.\nOur approach accounts for inter-item interactions as well\nas additional information such as item fatigue, trendiness\npatterns, contextual information etc. Finally, we evaluate\nour approach using a novel adaptation of Inverse Propensity\nScoring (IPS) which facilitates off-policy estimation of our\nmethod\u2019s CTR and showcases its effectiveness in real-world\nsettings."}
{"Title": "Economic Recommendation with Surplus Maximization", "Abstract": "A prime function of many major World Wide Web appli-\ncations is Online Service Allocation (OSA), the function of\nmatching individual consumers with particular services/goods\n(which may include loans or jobs as well as products) each\nwith its own producer. In the applications of interest, con-\nsumers are free to choose, so OSA usually takes the form of\npersonalized recommendation or search in practice. The per-\nformance metrics of recommender and search systems cur-\nrently tend to focus on just one side of the match, in some\ncases the consumers (e.g. satisfaction) and in other cases the\nproducers (e.g., profit). However, a sustainable OSA plat-\nform needs benefit both consumers and producers; otherwise\nthe neglected party eventually may stop using it.\nIn this paper, we show how to adapt economists\u2019 tradi-\ntional idea of maximizing total surplus (the sum of consumer\nnet benefit and producer profit) to the heterogeneous world\nof online service allocation, in an effort to promote the web\nintelligence for social good in online eco-systems. Modifica-\ntions of traditional personalized recommendation algorithms\nenable us to apply Total Surplus Maximization (TSM) to\nthree very different types of real-world tasks \u2013 e-commerce,\nP2P lending and freelancing. The results for all three tasks\nsuggest that TSM compares very favorably to currently pop-\nular approaches, to the benefit of both producers and con-\nsumers."}
{"Title": "When do Recommender Systems Work the Best? The\nModerating Effects of Product Attributes and Consumer\nReviews on Recommender Performance", "Abstract": "We investigate the moderating effect of product attributes\nand consumer reviews on the efficacy of a collaborative fil-\ntering recommender system on an e-commerce site. We run\na randomized field experiment on a top North American re-\ntailer\u2019s website with 184,375 users split into a recommender-\ntreated group and a control group with 37,215 unique prod-\nucts in the dataset. By augmenting the dataset with Ama-\nzon Mechanical Turk tagged product attributes and con-\nsumer review data from the website, we study their moder-\nating influence on recommenders in generating conversion.\nWe first confirm that the use of recommenders increases\nthe baseline conversion rate by 5.9%. We find that the rec-\nommenders act as substitutes for high average review rat-\nings with the effect of using recommenders increasing the\nconversion rate as much as about 1.4 additional average\nstar ratings. Additionally, we find that the positive im-\npacts on conversion from recommenders are greater for hedo-\nnic products compared to utilitarian products while search-\nexperience quality did not have any impact. We also find\nthat the higher the price, the lower the positive impact of\nrecommenders, while having lengthier product descriptions\nand higher review volumes increased the recommender\u2019s ef-\nfectiveness. More findings are discussed in the Results.\nFor managers, we 1) identify the products and product\nattributes for which the recommenders work well, 2) show\nhow other product information sources on e-commerce sites\ninteract with recommenders. Additionally, the insights from\nthe results could inform novel recommender algorithm de-\nsigns that are aware of strength and shortcomings. From an\nacademic standpoint, we provide insight into the underlying\nmechanism"}
{"Title": "TrackMeOrNot: Enabling Flexible Control on Web Tracking", "Abstract": "Recent advance in web tracking technologies has raised many pri-\nvacy concerns. To combat users\u2019 fear of privacy invasion, online\nvendors have taken measures such as being more transparent with\nusers about their data use and providing options for users to man-\nage their online activities. Such efforts gain users\u2019 trust in online\nvendors and improve their willingness to share their digital foot-\nprints. However, there are still a significant amount of users who\nactively limit involuntarily sharing of data because vendor provided\nmanagement tools only restrict the use of collected data and users\nworry vendors do not have enough measures in place to protect their\nprivacy sensitive information.\nIn this paper, we propose TrackMeOrNot , a new anti-tracking\nmechanism. It allows users to selectively share their online foot-\nprints with vendors. With TrackMeOrNot , users are no longer\nconcerned with privacy. Using it, users can specify their privacy\nsensitive activities and selectively disclose their activities to ven-\ndors based on their specified privacy demands. We implemented\nTrackMeOrNot on Chromium browser and systematically evalu-\nated its performance using a large set of test cases. We show that\nTrackMeOrNot can efficiently and effectively shield privacy sensi-\ntive browsing activities."}
{"Title": "In a World That Counts: Clustering and Detecting\nFake Social Engagement at Scale", "Abstract": "How can web services that depend on user generated content\ndiscern fake social engagement activities by spammers from\nlegitimate ones? In this paper, we focus on the social site of\nYouTube and the problem of identifying bad actors posting\ninorganic contents and inflating the count of social engage-\nment metrics. We propose an effective method, Leas (Local\nExpansion at Scale), and show how the fake engagement ac-\ntivities on YouTube can be tracked over time by analyzing\nthe temporal graph based on the engagement behavior pat-\ntern between users and YouTube videos. With the domain\nknowledge of spammer seeds, we formulate and tackle the\nproblem in a semi-supervised manner \u2014 with the objective\nof searching for individuals that have similar pattern of be-\nhavior as the known seeds \u2014 based on a graph diffusion\nprocess via local spectral subspace. We offer a fast, scalable\nMapReduce deployment adapted from the localized spec-\ntral clustering algorithm. We demonstrate the effectiveness\nof our deployment at Google by achieving a manual review\naccuracy of 98% on YouTube Comments graph in practice.\nComparing with the state-of-the-art algorithm CopyCatch,\nLeas achieves 10 times faster running time on average. Leas\nis now actively in use at Google, searching for daily deceptive\npractices on YouTube\u2019s engagement graph spanning over a\nbillion users."}
{"Title": "Tracking the Trackers", "Abstract": "Online tracking poses a serious privacy challenge that has\ndrawn significant attention in both academia and industry.\nExisting approaches for preventing user tracking, based on\ncurated blocklists, suffer from limited coverage and coarse-\ngrained resolution for classification, rely on exceptions that\nimpact sites\u2019 functionality and appearance, and require sig-\nnificant manual maintenance. In this paper we propose a\nnovel approach, based on the concepts leveraged from k-\nAnonymity, in which users collectively identify unsafe data\nelements, which have the potential to identify uniquely an\nindividual user, and remove them from requests. We de-\nployed our system to 200,000 German users running the\nCliqz Browser or the Cliqz Firefox extension to evaluate\nits efficiency and feasibility. Results indicate that our ap-\nproach achieves better privacy protection than blocklists, as\nprovided by Disconnect, while keeping the site breakage to\na minimum, even lower than the community-optimized Ad-\nBlock Plus. We also provide evidence of the prevalence and\nreach of trackers to over 21 million pages of 350,000 unique\nsites, the largest scale empirical evaluation to date. 95%\nof the pages visited contain 3rd party requests to potential\ntrackers and 78% attempt to transfer unsafe data. Tracker\norganizations are also ranked, showing that a single organi-\nzation can reach up to 42% of all page visits in Germany."}
{"Title": "Crowdsourcing Annotations for Websites\u2019 Privacy Policies:\nCan It Really Work?", "Abstract": "Website privacy policies are often long and difficult to understand.\nWhile research shows that Internet users care about their privacy,\nthey do not have time to understand the policies of every website\nthey visit, and most users hardly ever read privacy policies. Sev-\neral recent efforts aim to crowdsource the interpretation of privacy\npolicies and use the resulting annotations to build more effective\nuser interfaces that provide users with salient policy summaries.\nHowever, very little attention has been devoted to studying the ac-\ncuracy and scalability of crowdsourced privacy policy annotations,\nthetypesofquestionscrowdworkerscaneffectivelyanswer, andthe\nways in which their productivity can be enhanced. Prior research\nindicates that most Internet users often have great difficulty under-\nstanding privacy policies, suggesting limits to the effectiveness of\ncrowdsourcing approaches. In this paper, we assess the viability\nof crowdsourcing privacy policy annotations. Our results suggest\nthat, if carefully deployed, crowdsourcing can indeed result in the\ngeneration of non-trivial annotations and can also help identify ele-\nments of ambiguity in policies. We further introduce and evaluate a\nmethod to improve the annotation process by predicting and high-\nlighting paragraphs relevant to specific data practices"}
{"Title": "Abusive Language Detection in Online User Content", "Abstract": "Detection of abusive language in user generated online con-\ntent has become an issue of increasing importance in recent\nyears. Most current commercial methods make use of black-\nlists and regular expressions, however these measures fall\nshort when contending with more subtle, less ham-fisted ex-\namples of hate speech. In this work, we develop a machine\nlearning based method to detect hate speech on online user\ncomments from two domains which outperforms a state-of-\nthe-art deep learning approach. We also develop a corpus of\nuser comments annotated for abusive language, the first of\nits kind. Finally, we use our detection tool to analyze abu-\nsive language over time and in different settings to further\nenhance our knowledge of this behavior."}
{"Title": "Hidden Topic Sentiment Model", "Abstract": "Various topic models have been developed for sentiment analysis\ntasks. But the simple topic-sentiment mixture assumption prohibit-\ns them from finding fine-grained dependency between topical as-\npects and sentiments. In this paper, we build a Hidden Topic Sen-\ntiment Model (HTSM) to explicitly capture topic coherence and\nsentiment consistency in an opinionated text document to accurate-\nly extract latent aspects and corresponding sentiment polarities. In\nHTSM, 1) topic coherence is achieved by enforcing words in the\nsame sentence to share the same topic assignment and modeling\ntopic transition between successive sentences; 2) sentiment con-\nsistency is imposed by constraining topic transitions via tracking\nsentiment changes; and 3) both topic transition and sentiment tran-\nsition are guided by a parameterized logistic function based on the\nlinguistic signals directly observable in a document. Extensive ex-\nperiments on four categories of product reviews from both Amazon\nand NewEgg validate the effectiveness of the proposed model."}
{"Title": "Mining Aspect-Specific Opinion using a Holistic\nLifelong Topic Model", "Abstract": "Aspect-level sentiment analysis or opinion mining consists of sev-\neral core sub-tasks: aspect extraction, opinion identification, po-\nlarity classification, and separation of general and aspect-specific\nopinions. Various topic models have been proposed by researchers\nto address some of these sub-tasks. However, there is little work\non modeling all of them together. In this paper, we first propose\na holistic fine-grained topic model, called the JAST (Joint Aspect-\nbased Sentiment Topic) model, that can simultaneously model all\nof above problems under a unified framework. To further improve\nit, we incorporate the idea of lifelong machine learning and pro-\npose a more advanced model, called the LAST (Lifelong Aspect-\nbased Sentiment Topic) model. LAST automatically mines the\nprior knowledge of aspect, opinion, and their correspondence from\nother products or domains. Such knowledge is automatically ex-\ntracted and incorporated into the proposed LAST model without\nany human involvement. Our experiments using reviews of a large\nnumber of product domains show major improvements of the pro-\nposed models over state-of-the-art baselines."}
{"Title": "Bayesian Budget Feasibility with Posted Pricing", "Abstract": "We consider the problem of budget feasible mechanism de-\nsign proposed by Singer [22], but in a Bayesian setting. A\nprincipal has a public value for hiring a subset of the agents\nand a budget, while the agents have private costs for be-\ning hired. We consider both additive and submodular value\nfunctions of the principal. We show that there are simple,\npractical, ex post budget balanced posted pricing mecha-\nnisms that approximate the value obtained by the Bayesian\noptimal mechanism that is budget balanced only in expecta-\ntion. A main motivating application for this work is crowd-\nsourcing, e.g., on Mechanical Turk, where workers are drawn\nfrom a large population and posted pricing is standard. Our\nanalysis methods relate to contention resolution schemes in\nsubmodular optimization of Vondr\u00e0k et al. [26] and the cor-\nrelation gap analysis of Yan [27]."}
{"Title": "Mean Field Equilibria for Competitive Exploration in\nResource Sharing Settings", "Abstract": "We consider a model of nomadic agents exploring and com-\npeting for time-varying location-specific resources, arising in\ncrowdsourced transportation services, online communities,\nand in traditional location-based economic activity. This\nmodel comprises a group of agents, and a set of locations\neach endowed with a dynamic stochastic resource process.\nEach agent derives a periodic reward determined by the\noverall resource level at her location, and the number of\nother agents there. Each agent is strategic and free to move\nbetween locations, and at each time decides whether to stay\nat the same node or switch to another one. We study the\nequilibrium behavior of the agents as a function of dynam-\nics of the stochastic resource process and the nature of the\nexternality each agent imposes on others at the same loca-\ntion. In the asymptotic limit with the number of agents and\nlocations increasing proportionally, we show that an equilib-\nrium exists and has a threshold structure, where each agent\ndecides to switch to a different location based only on their\ncurrent location\u2019s resource level and the number of other\nagents at that location. This result provides insight into\nhow system structure affects the agents\u2019 collective ability to\nexplore their domain to find and effectively utilize resource-\nrich areas. It also allows assessing the impact of changing\nthe reward structure through penalties or subsidies."}
{"Title": "Understanding User Economic Behavior in the City Using\nLarge-scale Geotagged and Crowdsourced Data", "Abstract": "The pervasiveness of mobile technologies today have facili-\ntated the creation of massive crowdsourced and geotagged\ndata from individual users in real time and at different loca-\ntions in the city. Such ubiquitous user-generated data allow\nus to infer various patterns of human behavior, which help\nus understand the interactions between humans and cities.\nIn this study, we focus on understanding users economic be-\nhavior in the city by examining the economic value from\ncrowdsourced and geotaggged data. Specifically, we extract\nmultiple traffic and human mobility features from publicly\navailable data sources using NLP and geo-mapping tech-\nniques, and examine the effects of both static and dynamic\nfeatures on economic outcome of local businesses. Our study\nis instantiated on a unique dataset of restaurant bookings\nfrom OpenTable for 3,187 restaurants in New York City from\nNovember 2013 to March 2014. Our results suggest that\nfoot traffic can increase local popularity and business per-\nformance, while mobility and traffic from automobiles may\nhurt local businesses, especially the well-established chain-\ns and high-end restaurants. We also find that on average\none more street closure nearby leads to a 4.7% decrease in\nthe probability of a restaurant being fully booked during the\ndinner peak. Our study demonstrates the potential of how\nto best make use of the large volumes and diverse sources of\ncrowdsourced and geotagged user-generated data to create\nmatrices to predict local economic demand in a manner that\nis fast, cheap, accurate, and meaningful."}
{"Title": "Mechanism Design for Mixed Bidders", "Abstract": "The Generalized Second Price (GSP) auction has appealing prop-\nerties when ads are simple (text based and identical in size), but\ndoes not generalize to richer ad settings, whereas truthful mecha-\nnisms such as VCG do. However, a straight switch from GSP to\nVCG incurs significant revenue loss for the search engine. We in-\ntroduce a transitional mechanism which encourages advertisers to\nupdate their bids to their valuations, while mitigating revenue loss.\nIn this setting, it is easier to propose first a payment function rather\nthan an allocation function, so we give a general framework which\nguarantees incentive compatibility by requiring that the payment\nfunctions satisfy two specific properties. Finally, we analyze the\nrevenue impacts of our mechanism on a sample of Bing data."}
{"Title": "Semantics and Expressive Power of\nSubqueries and Aggregates in SPARQL 1.1", "Abstract": "Answering aggregate queries is a key requirement of emerg-\ning applications of Semantic Technologies, such as data ware-\nhousing, business intelligence and sensor networks. In order\nto fulfill the requirements of such applications, the standard-\nisation of SPARQL 1.1 led to the introduction of a wide\nrange of constructs that enable value computation, aggre-\ngation, and query nesting. In this paper we provide an in-\ndepth formal analysis of the semantics and expressive power\nof these new constructs as defined in the SPARQL 1.1 speci-\nfication, and hence lay the necessary foundations for the de-\nvelopment of robust, scalable and extensible query engines\nsupporting complex numerical and analytics tasks."}
{"Title": "Reverse Engineering SPARQL Queries", "Abstract": "Semantic Web systems provide open interfaces for end-users to ac-\ncess data via a powerful high-level query language, SPARQL. But\nusers unfamiliar with either the details of SPARQL or properties\nof the target dataset may find it easier to query by example \u2014 give\nexamples of the information they want (or examples of both what\nthey want and what they do not want) and let the system reverse\nengineer the desired query from the examples. This approach has\nbeen heavily used in the setting of relational databases. We pro-\nvide here an investigation of the reverse engineering problem in the\ncontext of SPARQL. We first provide a theoretical study, formal-\nising variants of the reverse engineering problem and giving tight\nbounds on its complexity. We next explain an implementation of\na reverse engineering tool for positive examples. An experimental\nanalysis of the tool shows that it scales well in the data size, number\nof examples, and in the size of the smallest query that fits the data.\nWe also give evidence that reverse engineering tools can provide\nbenefits on real-life datasets."}
{"Title": "Profiling the Potential of Web Tables for Augmenting\nCross-domain Knowledge Bases", "Abstract": "Cross-domain knowledge bases such as DBpedia, YAGO, or\nthe Google Knowledge Graph have gained increasing atten-\ntion over the last years and are starting to be deployed within\nvarious use cases. However, the content of such knowledge\nbases is far from being complete, far from always being cor-\nrect, and suffers from deprecation (i.e. population numbers\nbecome outdated after some time). Hence, there are efforts\nto leverage various types of Web data to complement, update\nand extend such knowledge bases. A source of Web data\nthat potentially provides a very wide coverage are millions\nof relational HTML tables that are found on the Web. The\nexisting work on using data from Web tables to augment\ncross-domain knowledge bases reports only aggregated per-\nformance numbers. The actual content of the Web tables\nand the topical areas of the knowledge bases that can be\ncomplemented using the tables remain unclear. In this paper,\nwe match a large, publicly available Web table corpus to the\nDBpedia knowledge base. Based on the matching results, we\nprofile the potential of Web tables for augmenting different\nparts of cross-domain knowledge bases and report detailed\nstatistics about classes, properties, and instances for which\nmissing values can be filled using Web table data as evidence.\nIn order to estimate the potential quality of the new values,\nwe empirically examine the Local Closed World Assumption\nand use it to determine the maximal number of correct facts\nthat an ideal data fusion strategy could generate. Using this\nas ground truth, we compare three data fusion strategies and\nconclude that knowledge-based trust outperforms PageRank-\nand voting-based fusion."}
{"Title": "Foundations of JSON Schema", "Abstract": "JSON \u2013 the most popular data format for sending API re-\nquests and responses \u2013 is still lacking a standardized schema\nor meta-data definition that allows the developers to spec-\nify the structure of JSON documents. JSON Schema is an\nattempt to provide a general purpose schema language for\nJSON, but it is still work in progress, and the formal spec-\nification has not yet been agreed upon. Why this could be\na problem becomes evident when examining the behaviour\nof numerous tools for validating JSON documents against\nthis initial schema proposal: although they agree on most\ngeneral cases, when presented with the greyer areas of the\nspecification they tend to differ significantly. In this paper\nwe provide the first formal definition of syntax and seman-\ntics for JSON Schema and use it to show that implementing\nthis layer on top of JSON is feasible in practice. This is done\nboth by analysing the theoretical aspects of the validation\nproblem and by showing how to set up and validate a JSON\nSchema for Wikidata, the central storage for Wikimedia"}
{"Title": "Mining Online Social Data for Detecting Social Network\nMental Disorders", "Abstract": "An increasing number of social network mental disorders (SN-\nMDs), such as Cyber-Relationship Addiction, Information Over-\nload, and Net Compulsion, have been recently noted. Symp-\ntoms of these mental disorders are usually observed passively\ntoday, resulting in delayed clinical intervention. In this paper,\nwe argue that mining online social behavior provides an op-\nportunity to actively identify SNMDs at an early stage. It is\nchallenging to detect SNMDs because the mental factors con-\nsidered in standard diagnostic criteria (questionnaire) cannot\nbe observed from online social activity logs. Our approach,\nnew and innovative to the practice of SNMD detection, does\nnot rely on self-revealing of those mental factors via question-\nnaires. Instead, we propose a machine learning framework,\nnamely, Social Network Mental Disorder Detection (SNMDD),\nthat exploits features extracted from social network data to\naccurately identify potential cases of SNMDs. We also exploit\nmulti-source learning in SNMDD and propose a new SNMD-\nbased Tensor Model (STM) to improve the performance. Our\nframework is evaluated via a user study with 3126 online social\nnetwork users. We conduct a feature analysis, and also apply\nSNMDD on large-scale datasets and analyze the characteris-\ntics of the three SNMD types. The results show that SNMDD\nis promising for identifying online social network users with\npotential SNMDs."}
{"Title": "Predicting Pre-click Quality for Native Advertisements", "Abstract": "Native advertising is a specific form of online advertising\nwhere ads replicate the look-and-feel of their serving plat-\nform. In such context, providing a good user experience\nwith the served ads is crucial to ensure long-term user en-\ngagement. In this work, we explore the notion of ad quality,\nnamely the effectiveness of advertising from a user experi-\nence perspective. We design a learning framework to predict\nthe pre-click quality of native ads. More specifically, we look\nat detecting offensive native ads, showing that, to quantify\nad quality, ad offensive user feedback rates are more reliable\nthan the commonly used click-through rate metrics. We\nthen conduct a crowd-sourcing study to identify which cri-\nteria drive user preferences in native advertising. We trans-\nlate these criteria into a set of ad quality features that we\nextract from the ad text, image and advertiser, and then\nuse them to train a model able to identify offensive ads. We\nshow that our model is very effective in detecting offensive\nads, and provide in-depth insights on how different features\naffect ad quality. Finally, we deploy a preliminary version\nof such model and show its effectiveness in the reduction of\nthe offensive ad feedback rate."}
{"Title": "The Lifecycle and Cascade of WeChat Social Messaging\nGroups", "Abstract": "Social instant messaging services are emerging as a transformative\nform with which people connect, communicate with friends in their\ndaily life \u2014 they catalyze the formation of social groups, and they\nbring people stronger sense of community and connection. How-\never, research community still knows little about the formation and\nevolution of groups in the context of social messaging \u2014 their life-\ncycles, the change in their underlying structures over time, and the\ndiffusion processes by which they develop new members.\nIn this paper, we analyze the daily usage logs from WeChat\ngroup messaging platform \u2014 the largest standalone messaging\ncommunication service in China \u2014 with the goal of understanding\nthe processes by which social messaging groups come together,\ngrow new members, and evolve over time. Specifically, we dis-\ncover a strong dichotomy among groups in terms of their lifecycle,\nand develop a separability model by taking into account a broad\nrange of group-level features, showing that long-term and short-\nterm groups are inherently distinct. We also found that the lifecycle\nof messaging groups is largely dependent on their social roles and\nfunctions in users\u2019 daily social experiences and specific purposes.\nGiven the strong separability between the long-term and short-term\ngroups, we further address the problem concerning the early pre-\ndiction of successful communities.\nIn addition to modeling the growth and evolution from group-\nlevel perspective, we investigate the individual-level attributes of\ngroup members and study the diffusion process by which groups\ngain new members. By considering members\u2019 historical engage-\nment behavior as well as the local social network structure that\nthey embedded in, we develop a membership cascade model and\ndemonstrate the effectiveness by achieving AUC of 95.31% in pre-\ndicting inviter, and an AUC of 98.66% in predicting invitee."}
{"Title": "Characterizing Long-tail SEO Spam\non Cloud Web Hosting Services", "Abstract": "The popularity of long-tail search engine optimization (SEO)\nbrings with new security challenges: incidents of long-tail\nkeyword poisoning to lower competition and increase rev-\nenue have been reported. The emergence of cloud web host-\ning services provides a new and effective platform for long-\ntail SEO spam attacks. There is growing evidence that large-\nscale long-tail SEO campaigns are being carried out on cloud\nhosting platforms because they offer low-cost, high-speed\nhosting services. In this paper, we take the first step toward\nunderstanding how long-tail SEO spam is implemented on\ncloud hosting platforms. After identifying 3,186 cloud direc-\ntories and 318,470 doorway pages on the leading cloud plat-\nforms for long-tail SEO spam, we characterize their abusive\nbehavior. One highlight of our findings is the effectiveness of\nthe cloud-based long-tail SEO spam, with 6% of the doorway\npages successfully appearing in the top 10 search results of\nthe poisoned long-tail keywords. Examples of other impor-\ntant discoveries include how such doorway pages monetize\ntraffic and their ability to manage cloud platform\u2019s counter-\nmeasures. These findings bring such abuse to the spotlight\nand provide some insights to eliminating this practice."}
{"Title": "Automatic Extraction of Indicators of\nCompromise for Web Applications", "Abstract": "Indicators of Compromise (IOCs) are forensic artifacts that\nare used as signs that a system has been compromised by\nan attack or that it has been infected with a particular ma-\nlicious software. In this paper we propose for the first time\nan automated technique to extract and validate IOCs for\nweb applications, by analyzing the information collected by\na high-interaction honeypot.\nOur approach has several advantages compared with tra-\nditional techniques used to detect malicious websites. First\nof all, not all the compromised web pages are malicious or\nharmful for the user. Some may be defaced to advertise\nproduct or services, and some may be part of affiliate pro-\ngrams to redirect users toward (more or less legitimate) on-\nline shopping websites. In any case, it is important to detect\nthese pages to inform their owners and to alert the users on\nthe fact that the content of the page has been compromised\nand cannot be trusted.\nAlso in the case of more traditional drive-by-download\npages, the use of IOCs allows for a prompt detection and cor-\nrelation of infected pages, even before they may be blocked\nby more traditional URLs blacklists.\nOur experiments show that our system is able to automat-\nically generate web indicators of compromise that have been\nused by attackers for several months (and sometimes years)\nin the wild without being detected. So far, these apparently\nharmless scripts were able to stay under the radar of the\nexisting detection methodologies \u2013 despite being hosted for\na long time on public web sites."}
{"Title": "Cracking Classifiers for Evasion: A Case Study on the\nGoogle\u2019s Phishing Pages Filter", "Abstract": "Various classifiers based on the machine learning techniques have\nbeen widely used in security applications. Meanwhile, they also\nbecame an attack target of adversaries. Many existing studies have\npaid much attention to the evasion attacks on the online classifiers\nand discussed defensive methods. However, the security of the\nclassifiers deployed in the client environment has not got the\nattention it deserves. Besides, earlier studies only concentrated on\nthe experimental classifiers developed for research purposes only.\nThe security of widely-used commercial classifiers still remains\nunclear. In this paper, we use the Google\u2019s phishing pages filter\n(GPPF), a classifier deployed in the Chrome browser which owns\nover one billion users, as a case to investigate the security\nchallenges for the client-side classifiers. We present a new attack\nmethodology targeting on client-side classifiers, called classifiers\ncracking. With the methodology, we successfully cracked the\nclassification model of GPPF and extracted sufficient knowledge\ncan be exploited for evasion attacks, including the classification\nalgorithm, scoring rules and features, etc. Most importantly, we\ncompletely reverse engineered 84.8% scoring rules, covering most\nof high-weighted rules. Based on the cracked information, we\nperformed two kinds of evasion attacks to GPPF, using 100 real\nphishing pages for the evaluation purpose. The experiments show\nthat all the phishing pages (100%) can be easily manipulated to\nbypass the detection of GPPF. Our study demonstrates that the\nexisting client-side classifiers are very vulnerable to classifiers\ncracking attacks."}
{"Title": "Understanding the Detection of View Fraud\nin Video Content Portals", "Abstract": "While substantial effort has been devoted to understand fraudulent\nactivity in traditional online advertising (search and banner), more\nrecent forms such as video ads have received little attention. The\nunderstanding and identification of fraudulent activity (i.e., fake\nviews) in video ads for advertisers, is complicated as they rely\nexclusively on the detection mechanisms deployed by video hosting\nportals. In this context, the development of independent tools able\nto monitor and audit the fidelity of these systems are missing today\nand needed by both industry and regulators.\nIn this paper we present a first set of tools to serve this purpose.\nUsing our tools, we evaluate the performance of the audit systems\nof five major online video portals. Our results reveal that YouTube\u2019s\ndetection system significantly outperforms all the others. Despite\nthis, a systematic evaluation indicates that it may still be susceptible\nto simple attacks. Furthermore, we find that YouTube penalizes its\nvideos\u2019 public and monetized view counters differently, the former\nbeing more aggressive. This means that views identified as fake and\ndiscounted from the public view counter are still monetized. We\nspeculate that even though YouTube\u2019s policy puts in lots of effort to\ncompensate users after an attack is discovered, this practice places\nthe burden of the risk on the advertisers, who pay to get their ads\ndisplayed."}
{"Title": "On the Temporal Dynamics of Opinion Spamming: Case\nStudies on Yelp", "Abstract": "Recently, the problem of opinion spam has been widespread and\nhas attracted a lot of research attention. While the problem has been\napproached on a variety of dimensions, the temporal dynamics in\nwhich opinion spamming operates is unclear. Are there specific\nspamming policies that spammers employ? What kind of changes\nhappen with respect to the dynamics to the truthful ratings on\nentities. How do buffered spamming operate for entities that need\nspamming to retain threshold popularity and reduced spamming for\nentities making better success? We analyze these questions in the\nlight of time-series analysis on Yelp. Our analyses discover various\ntemporal patterns and their relationships with the rate at which fake\nreviews are posted. Building on our analyses, we employ vector\nautoregression to predict the rate of deception across different\nspamming policies. Next, we explore the effect of filtered reviews\non (long-term and imminent) future rating and popularity\nprediction of entities. Our results discover novel temporal\ndynamics of spamming which are intuitive, arguable and also\nrender confidence on Yelp\u2019s filtering. Lastly, we leverage our\ndiscovered temporal patterns in deception detection. Experimental\nresults on large-scale reviews show the effectiveness of our\napproach that significantly improves the existing approaches."}
{"Title": "Scaling up Dynamic Topic Models", "Abstract": "Dynamic topic models (DTMs) are very effective in discover-\ning topics and capturing their evolution trends in time series\ndata. To do posterior inference of DTMs, existing methods\nare all batch algorithms that scan the full dataset before\neach update of the model and make inexact variational ap-\nproximations with mean-field assumptions. Due to a lack of\na more scalable inference algorithm, despite the usefulness,\nDTMs have not captured large topic dynamics.\nThis paper fills this research void, and presents a fast\nand parallelizable inference algorithm using Gibbs Sampling\nwith Stochastic Gradient Langevin Dynamics that does not\nmake any unwarranted assumptions. We also present a\nMetropolis-Hastings based O(1) sampler for topic assign-\nments for each word token. In a distributed environment,\nour algorithm requires very little communication between\nworkers during sampling (almost embarrassingly parallel)\nand scales up to large-scale applications. We are able to\nlearn the largest Dynamic Topic Model to our knowledge,\nand learned the dynamics of 1,000 topics from 2.6 million\ndocuments in less than half an hour, and our empirical re-\nsults show that our algorithm is not only orders of magnitude\nfaster than the baselines but also achieves lower perplexity."}
{"Title": "Learning Global Term Weights for Content-based\nRecommender Systems", "Abstract": "Recommender systems typically leverage two types of sig-\nnals to effectively recommend items to users: user activ-\nities and content matching between user and item profiles,\nand recommendation models in literature are usually catego-\nrized into collaborative filtering models, content-based mod-\nels and hybrid models. In practice, when rich profiles about\nusers and items are available, and user activities are sparse\n(cold-start), effective content matching signals become much\nmore important in the relevance of the recommendation.\nThe de-facto method to measure similarity between two\npieces of text is computing the cosine similarity of the two\nbags of words, and each word is weighted by TF (term fre-\nquency within the document) \u00d7 IDF (inverted document\nfrequency of the word within the corpus). In general sense,\nTF can represent any local weighting scheme of the word\nwithin each document, and IDF can represent any global\nweighting scheme of the word across the corpus. In this pa-\nper, we focus on the latter, i.e., optimizing the global term\nweights, for a particular recommendation domain by lever-\naging supervised approaches. The intuition is that some\nfrequent words (lower IDF, e.g. \u201cdatabase\u201d) can be essential\nand predictive for relevant recommendation, while some rare\nwords (higher IDF, e.g. the name of a small company) could\nhave less predictive power. Given plenty of observed activ-\nities between users and items as training data, we should\nbe able to learn better domain-specific global term weights,\nwhich can further improve the relevance of recommendation."}
{"Title": "Exploring Patterns of Identity Usage in Tweets", "Abstract": "Sociologists have long been interested in the ways that iden-\ntities, or labels for people, are created, used and applied\nacross various social contexts. The present work makes two\ncontributions to the study of identity, in particular the study\nof identity in text. We first consider the following novel NLP\ntask: given a set of text data (here, from Twitter), label\neach word in the text as being representative of a (possi-\nbly multi-word) identity. To address this task, we develop\na comprehensive feature set that leverages several avenues\nof recent NLP work on Twitter and use these features to\ntrain a supervised classifier. Our model outperforms a sur-\nprisingly strong rule-based baseline by 33%. We then use\nour model for a case study, applying it to a large corpora\nof Twitter data from users who actively discussed the Eric\nGarner and Michael Brown cases. Among other findings,\nwe observe that the identities used by individuals differ in\ninteresting ways based on social context measures derived\nfrom census data."}
{"Title": "The Death and Life of Great Italian Cities:\nA Mobile Phone Data Perspective", "Abstract": "The Death and Life of Great American Cities was written\nin 1961 and is now one of the most influential book in city\nplanning. In it, Jane Jacobs proposed four conditions that\npromote life in a city. However, these conditions have not\nbeen empirically tested until recently. This is mainly because\nit is hard to collect data about \u201ccity life\u201d. The city of Seoul\nrecently collected pedestrian activity through surveys at\nan unprecedented scale, with an effort spanning more than\na decade, allowing researchers to conduct the first study\nsuccessfully testing Jacobs\u2019s conditions. In this paper, we\nidentify a valuable alternative to the lengthy and costly\ncollection of activity survey data: mobile phone data. We\nextract human activity from such data, collect land use and\nsocio-demographic information from the Italian Census and\nOpen Street Map, and test the four conditions in six Italian\ncities. Although these cities are very different from the places\nfor which Jacobs\u2019s conditions were spelled out (i.e., great\nAmerican cities) and from the places in which they were\nrecently tested (i.e., the Asian city of Seoul), we find those\nconditions to be indeed associated with urban life in Italy as\nwell. Our methodology promises to have a great impact on\nurban studies, not least because, if replicated, it will make it\npossible to test Jacobs\u2019s theories at scale."}
{"Title": "Beyond the Baseline: Establishing the Value in Mobile\nPhone Based Poverty Estimates", "Abstract": "Within the remit of \u2018Data for Development\u2019 there have been\na number of promising recent works that investigate the use\nof mobile phone Call Detail Records (CDRs) to estimate\nthe spatial distribution of poverty or socio-economic status.\nThe methods being developed have the potential to offer\nimmense value to organisations and agencies who currently\nstruggle to identify the poorest parts of a country, due to the\nlack of reliable and up to date survey data in certain parts of\nthe world. However, the results of this research have thus far\nonly been presented in isolation rather than in comparison\nto any alternative approach or benchmark. Consequently,\nthe true practical value of these methods remains unknown.\nHere, we seek to allay this shortcoming, by proposing two\nbaseline poverty estimators grounded on concrete usage sce-\nnarios: one that exploits correlation with population density\nonly, to be used when no poverty data exists at all; and one\nthat also exploits spatial autocorrelation, to be used when\npoverty data has been collected for a few regions within a\ncountry. We then compare the predictive performance of\nthese baseline models with models that also include features\nderived from CDRs, so to establish their real added value.\nWe present extensive analysis of the performance of all these\nmodels on data acquired for two developing countries \u2013 Sene-\ngal and Ivory Coast. Our results reveal that CDR-based\nmodels do provide more accurate estimates in most cases;\nhowever, the improvement is modest and more significant\nwhen estimating (extreme) poverty intensity rates rather\nthan mean wealth."}
{"Title": "Pushing the Frontier: Exploring the African Web\nEcosystem", "Abstract": "It is well known that Africa\u2019s mobile and fixed Internet in-\nfrastructure is progressing at a rapid pace. A flurry of re-\ncent research has quantified this, highlighting the expansion\nof its underlying connectivity network. However, improving\nthe infrastructure is not useful without appropriately provi-\nsioned services to utilise it. This paper measures the avail-\nability of web content infrastructure in Africa. Whereas oth-\ners have explored web infrastructure in developed regions,\nwe shed light on practices in developing regions. To achieve\nthis, we apply a comprehensive measurement methodology\nto collect data from a variety of sources. We focus on a large\ncontent delivery network to reveal that Africa\u2019s content in-\nfrastructure is, indeed, expanding. However, we find much\nweb content is still served from the US and Europe. We dis-\ncover that many of the problems faced are actually caused\nby significant inter-AS delays in Africa, which contribute\nto local ISPs not sharing their cache capacity. We discover\nthat a related problem is the poor DNS configuration used\nby some ISPs, which confounds the attempts of providers to\noptimise their delivery. We then explore a number of other\nwebsites to show that large web infrastructure deployments\nare a rarity in Africa and that even regional websites host\ntheir services abroad. We conclude by making suggestions\nfor improvements."}
{"Title": "Latent Space Model for Multi-Modal Social Data", "Abstract": "With the emergence of social networking services, researchers\nenjoy the increasing availability of large-scale heterogenous\ndatasets capturing online user interactions and behaviors.\nTraditional analysis of techno-social systems data has fo-\ncused mainly on describing either the dynamics of social\ninteractions, or the attributes and behaviors of the users.\nHowever, overwhelming empirical evidence suggests that the\ntwo dimensions affect one another, and therefore they should\nbe jointly modeled and analyzed in a multi-modal frame-\nwork. The benefits of such an approach include the ability\nto build better predictive models, leveraging social network\ninformation as well as user behavioral signals. To this pur-\npose, here we propose the Constrained Latent Space Model\n(CLSM), a generalized framework that combines Mixed Mem-\nbership Stochastic Blockmodels (MMSB) and Latent Dirich-\nlet Allocation (LDA) incorporating a constraint that forces\nthe latent space to concurrently describe the multiple data\nmodalities. We derive an efficient inference algorithm based\non Variational Expectation Maximization that has a compu-\ntational cost linear in the size of the network, thus making it\nfeasible to analyze massive social datasets. We validate the\nproposed framework on two problems: prediction of social\ninteractions from user attributes and behaviors, and behav-\nior prediction exploiting network information. We perform\nexperiments with a variety of multi-modal social systems,\nspanning location-based social networks (Gowalla), social\nmedia services (Instagram, Orkut), e-commerce and review\nsites (Amazon, Ciao), and finally citation networks (Cora).\nThe results indicate significant improvement in prediction\naccuracy over state of the art methods, and demonstrate\nthe flexibility of the proposed approach for addressing a va-\nriety of different learning problems commonly occurring with\nmulti-modal social data."}
{"Title": "Modeling a Retweet Network via an Adaptive Bayesian\nApproach", "Abstract": "Twitter (and similar microblogging services) has become a\ncentral nexus for discussion of the topics of the day. Twit-\nter data contains rich content and structured information\non users\u2019 topics of interest and behavior patterns. Correctly\nanalyzing and modeling Twitter data enables the prediction\nof the user behavior and preference in a variety of practi-\ncal applications, such as tweet recommendation and followee\nrecommendation. Although a number of models have been\ndeveloped on Twitter data in prior work, most of these only\nmodel the tweets from users, while neglecting their valuable\nretweet information in the data. Models would enhance their\npredictive power by incorporating users\u2019 retweet content as\nwell as their retweet behavior.\nIn this paper, we propose two novel Bayesian nonpara-\nmetric models, URM and UCM, on retweet data. Both of\nthem are able to integrate the analysis of tweet text and\nusers\u2019 retweet behavior in the same probabilistic framework.\nMoreover, they both jointly model users\u2019 interest in tweet\nand retweet. As nonparametric models, URM and UCM\ncan automatically determine the parameters of the models\nbased on input data, avoiding arbitrary parameter settings.\nExtensive experiments on real-world Twitter data show that\nboth URM and UCM are superior to all the baselines, while\nUCM further outperforms URM, confirming the appropri-\nateness of our models in retweet modeling."}
{"Title": "On Sampling Nodes in a Network", "Abstract": "Random walk is an important tool in many graph mining applica-\ntions including estimating graph parameters, sampling portions of\nthe graph, and extracting dense communities. In this paper we con-\nsider the problem of sampling nodes from a large graph according\nto a prescribed distribution by using random walk as the basic prim-\nitive. Our goal is to obtain algorithms that make a small number of\nqueries to the graph but output a node that is sampled according\nto the prescribed distribution. Focusing on the uniform distribution\ncase, we study the query complexity of three algorithms and show a\nnear-tight bound expressed in terms of the parameters of the graph\nsuch as average degree and the mixing time. Both theoretically and\nempirically, we show that some algorithms are preferable in prac-\ntice than the others. We also extend our study to the problem of\nsampling nodes according to some polynomial function of their de-\ngrees; this has implications for designing efficient algorithms for\napplications such as triangle counting."}
{"Title": "Distributed Estimation of Graph 4-Profiles", "Abstract": "We present a novel distributed algorithm for counting all\nfour-node induced subgraphs in a big graph. These counts,\ncalled the 4-profile, describe a graph\u2019s connectivity proper-\nties and have found several uses ranging from bioinformat-\nics to spam detection. We also study the more complicated\nproblem of estimating the local 4-profiles centered at each\nvertex of the graph. The local 4-profile embeds every vertex\nin an 11-dimensional space that characterizes the local ge-\nometry of its neighborhood: vertices that connect different\nclusters will have different local 4-profiles compared to those\nthat are only part of one dense cluster.\nOur algorithm is a local, distributed message-passing scheme\non the graph and computes all the local 4-profiles in paral-\nlel. We rely on two novel theoretical contributions: we show\nthat local 4-profiles can be calculated using compressed two-\nhop information and also establish novel concentration re-\nsults that show that graphs can be substantially sparsified\nand still retain good approximation quality for the global\n4-profile.\nWe empirically evaluate our algorithm using a distributed\nGraphLab implementation that we scaled up to 640 cores.\nWe show that our algorithm can compute global and local\n4-profiles of graphs with millions of edges in a few minutes,\nsignificantly improving upon the previous state of the art."}
{"Title": "Detecting Good Abandonment in Mobile Search", "Abstract": "Web search queries for which there are no clicks are re-\nferred to as abandoned queries and are usually considered\nas leading to user dissatisfaction. However, there are many\ncases where a user may not click on any search result page\n(SERP) but still be satisfied. This scenario is referred to\nas good abandonment and presents a challenge for most ap-\nproaches measuring search satisfaction, which are usually\nbased on clicks and dwell time. The problem is exacerbated\nfurther on mobile devices where search providers try to in-\ncrease the likelihood of users being satisfied directly by the\nSERP. This paper proposes a solution to this problem us-\ning gesture interactions, such as reading times and touch\nactions, as signals for differentiating between good and bad\nabandonment. These signals go beyond clicks and charac-\nterize user behavior in cases where clicks are not needed to\nachieve satisfaction. We study different good abandonment\nscenarios and investigate the different elements on a SERP\nthat may lead to good abandonment. We also present an\nanalysis of the correlation between user gesture features and\nsatisfaction. Finally, we use this analysis to build models to\nautomatically identify good abandonment in mobile search\nachieving an accuracy of 75%, which is significantly better\nthan considering query and session signals alone. Our find-\nings have implications for the study and application of user\nsatisfaction in search systems."}
{"Title": "Ups and Downs: Modeling the Visual Evolution of Fashion\nTrends with One-Class Collaborative Filtering", "Abstract": "Buildingasuccessfulrecommendersystemdependsonunderstand-\ning both the dimensions of people\u2019s preferences as well as their dy-\nnamics. In certain domains, such as fashion, modeling such prefer-\nences can be incredibly difficult, due to the need to simultaneously\nmodel the visual appearance of products as well as their evolution\novertime. Thesubtlesemanticsandnon-lineardynamicsoffashion\nevolution raise unique challenges especially considering the spar-\nsity and large scale of the underlying datasets. In this paper we\nbuild novel models for the One-Class Collaborative Filtering set-\nting, where our goal is to estimate users\u2019 fashion-aware personal-\nized ranking functions based on their past feedback. To uncover\nthe complex and evolving visual factors that people consider when\nevaluating products, our method combines high-level visual fea-\ntures extracted from a deep convolutional neural network, users\u2019\npast feedback, as well as evolving trends within the community.\nExperimentally we evaluate our method on two large real-world\ndatasets from Amazon.com, where we show it to outperform state-\nof-the-art personalized ranking measures, and also use it to visu-\nalize the high-level fashion trends across the 11-year span of our\ndataset."}
{"Title": "Modeling User Consumption Sequences", "Abstract": "We studysequences ofconsumption inwhich thesame itemmay be\nconsumed multiple times. We identify two macroscopic behavior\npatterns of repeated consumptions. First, in a given user\u2019s lifetime,\nvery few items live for a long time. Second, the last consumptions\nof an item exhibit growing inter-arrival gaps consistent with the\nnotion of increasing boredom leading up to eventual abandonment.\nWe then present what is to our knowledge the first holistic model\nof sequential repeated consumption, covering all observed aspects\nof this behavior. Our simple and purely combinatorial model in-\ncludes no planted notion of lifetime distributions or user boredom;\nnonetheless, the model correctly predicts both of these phenom-\nena. Further, we provide theoretical analysis of the behavior of\nthe model confirming these phenomena. Additionally, the model\nquantitatively matches a number of microscopic phenomena across\na broad range of datasets.\nIntriguingly, these findings suggest that the observation in a va-\nriety of domains of increasing user boredom leading to abandon-\nment may be explained simply by probabilistic conditioning on an\nextinction event in a simple model, without resort to explanations\nbased on complex human dynamics."}
{"Title": "A Neural Click Model for Web Search", "Abstract": "Understanding user browsing behavior in web search is key to im-\nproving web search effectiveness. Many click models have been\nproposed to explain or predict user clicks on search engine results.\nThey are based on the probabilistic graphical model (PGM) frame-\nwork, in which user behavior is represented as a sequence of ob-\nservable and hidden events. The PGM framework provides a math-\nematically solid way to reason about a set of events given some in-\nformation about other events. But the structure of the dependencies\nbetween the events has to be set manually. Different click models\nuse different hand-crafted sets of dependencies.\nWe propose an alternative based on the idea of distributed repre-\nsentations: to represent the user\u2019s information need and the infor-\nmation available to the user with a vector state. The components\nof the vector state are learned to represent concepts that are use-\nful for modeling user behavior. And user behavior is modeled as\na sequence of vector states associated with a query session: the\nvector state is initialized with a query, and then iteratively updated\nbased on information about interactions with the search engine re-\nsults. This approach allows us to directly understand user browsing\nbehavior from click-through data, i.e., without the need for a pre-\ndefined set of rules as is customary for PGM-based click models.\nWeillustrateourapproachusingasetofneuralclickmodels. Our\nexperimental results show that the neural click model that uses the\nsame training data as traditional PGM-based click models, has bet-\nter performance on the click prediction task (i.e., predicting user\nclick on search engine results) and the relevance prediction task\n(i.e., ranking documents by their relevance to a query). An analy-\nsis of the best performing neural click model shows that it learns\nsimilar concepts to those used in traditional click models, and that\nit also learns other concepts that cannot be designed manually."}
{"Title": "Query-Less: Predicting Task Repetition for NextGen\nProactive Search and Recommendation Engines", "Abstract": "Web search has been a reactive scenario for decades which\noften starts by users issuing queries. By studying the user\nbehavior in search engine logs, we have discovered that many\nof the search tasks such as stock-price checking, news read-\ning exhibit strong repeated patterns from day to day. In\naddition, users exhibit even stronger repetition on mobile\ndevices. This provides us chances to perform proactive rec-\nommendations without user issuing queries. In this work, we\naim at discovering and characterizing these types of tasks so\nthat we can automatically predict when and what types of\ntasks will be repeated by the users in the future, through an-\nalyzing search logs from a commercial Web search engine and\nuser interaction logs from a mobile App that offers proac-\ntive recommendations. We first introduce a set of novel fea-\ntures that can accurately capture task repetition. We then\npropose a novel deep learning framework that learns user\npreferences and makes automatic predictions. Our frame-\nwork is capable of learning both user-independent global\nmodels as well as catering personalized models via model\nadaptation. The model we developed significantly outper-\nforms other state-of-the-art predictive models by large mar-\ngins. We also demonstrate the power of our model and fea-\ntures through an application to improve the recommenda-\ntion quality of the mobile App. Results indicate a significant\nrelevance improvement over the current production system."}
{"Title": "Behavior Driven Topic Transition\nfor Search Task Identification", "Abstract": "Search tasks in users\u2019 query sequences are dynamic and intercon-\nnected. The formulation of search tasks can be influenced by mul-\ntiple latent factors such as user characteristics, product features and\nsearch interactions, which makes search task identification a chal-\nlenging problem. In this paper, we propose an unsupervised ap-\nproach to identify search tasks via topic membership along with\ntopic transition probabilities, thus it becomes possible to interpret\nhow user\u2019s search intent emerges and evolves over time. More-\nover, a novel hidden semi-Markov model is introduced to model\ntopic transitions by considering not only the semantic information\nof queries but also the latent search factors originated from user\nsearch behaviors. A variational inference algorithm is developed to\nidentify remarkable search behavior patterns, typical topic transi-\ntion tracks, and the topic membership of each query from query\nlogs. The learned topic transition tracks and the inferred topic\nmemberships enable us to identify both small search tasks, where\na user searches the same topic, and big search tasks, where a user\nsearches a series of related topics. We extensively evaluate the pro-\nposed approach and compare with several state-of-the-art search\ntask identification methods on both synthetic and real-world query\nlog data, and experimental results illustrate the effectiveness of our\nproposed model."}
{"Title": "A Piggyback System for Joint Entity Mention Detection\nand Linking in Web Queries", "Abstract": "In this paper we study the problem of linking open-domain\nweb-search queries towards entities drawn from the full en-\ntity inventory of Wikipedia articles. We introduce SMAPH-\n2, a second-order approach that, by piggybacking on a web\nsearch engine, alleviates the noise and irregularities that\ncharacterize the language of queries and puts queries in a\nlarger context in which it is easier to make sense of them.\nThe key algorithmic idea underlying SMAPH-2 is to first\ndiscover a candidate set of entities and then link-back those\nentities to their mentions occurring in the input query. This\nallows us to confine the possible concepts pertinent to the\nquery to only the ones really mentioned in it. The link-back\nis implemented via a collective disambiguation step based\nupon a supervised ranking model that makes one joint pre-\ndiction for the annotation of the complete query optimizing\ndirectly the F1 measure. We evaluate both known features,\nsuch as word embeddings and semantic relatedness among\nentities, and several novel features such as an approximate\ndistance between mentions and entities (which can handle\nspelling errors). We demonstrate that SMAPH-2 achieves\nstate-of-the-art performance on the ERD@SIGIR2014 bench-\nmark. We also publish GERDAQ (General Entity Recogni-\ntion, Disambiguation and Annotation in Queries), a novel,\npublic dataset built specifically for web-query entity link-\ning via a crowdsourcing effort. SMAPH-2 outperforms the\nbenchmarks by comparable margins also on GERDAQ."}
{"Title": "Towards Mobile Query Auto-Completion:\nAn Efficient Mobile Application-Aware Approach", "Abstract": "We study the new mobile query auto-completion (QAC) problem\nto exploit mobile devices\u2019 exclusive signals, such as those related\nto mobile applications (apps). We propose AppAware, a novel\nQAC model using installed app and recently opened app signals\nto suggest queries for matching input prefixes on mobile devices.\nTo overcome the challenge of noisy and voluminous signals, Ap-\npAware optimizes composite objectives with a lighter processing\ncost at a linear rate of convergence. We conduct experiments on a\nlarge commercial data set of mobile queries and apps. Installed app\nandrecentlyopenedappsignalsconsistentlyandsignificantlyboost\nthe accuracy of various baseline QAC models on mobile devices."}
{"Title": "Disinformation on the Web: Impact, Characteristics, and\nDetection of Wikipedia Hoaxes", "Abstract": "Wikipedia is a major source of information for many people. How-\never, false information on Wikipedia raises concerns about its cred-\nibility. One way in which false information may be presented on\nWikipedia is in the form of hoax articles, i.e., articles containing\nfabricated facts about nonexistent entities or events. In this paper\nwe study false information on Wikipedia by focusing on the hoax\narticles that have been created throughout its history. We make\nseveral contributions. First, we assess the real-world impact of\nhoax articles by measuring how long they survive before being de-\nbunked, how many pageviews they receive, and how heavily they\nare referred to by documents on the Web. We find that, while most\nhoaxes are detected quickly and have little impact on Wikipedia,\na small number of hoaxes survive long and are well cited across\nthe Web. Second, we characterize the nature of successful hoaxes\nby comparing them to legitimate articles and to failed hoaxes that\nwere discovered shortly after being created. We find characteristic\ndifferences in terms of article structure and content, embeddedness\ninto the rest of Wikipedia, and features of the editor who created\nthe hoax. Third, we successfully apply our findings to address a\nseries of classification tasks, most notably to determine whether a\ngiven article is a hoax. And finally, we describe and evaluate a task\ninvolving humans distinguishing hoaxes from non-hoaxes. We find\nthathumansare notgoodatsolving thistaskandthatour automated\nclassifier outperforms them by a big margin"}
{"Title": "Tweet Properly: Analyzing Deleted Tweets to Understand\nand Identify Regrettable Ones", "Abstract": "Inappropriate tweets can cause severe damages on authors\u2019\nreputation or privacy. However, many users do not realize\nthe negative consequences until they publish these tweets.\nPublished tweets have lasting effects that may not be elimi-\nnated by simple deletion because other users may have read\nthem or third-party tweet analysis platforms have cached\nthem. Regrettable tweets, i.e., tweets with identifiable re-\ngrettable contents, cause the most damage on their authors\nbecause other users can easily notice them. In this paper,\nwe study how to identify the regrettable tweets published by\nnormal individual users via the contents and users\u2019 histor-\nical deletion patterns. We identify normal individual users\nbased on their publishing, deleting, followers and friends\nstatistics. We manually examine a set of randomly sam-\npled deleted tweets from these users to identify regrettable\ntweets and understand the corresponding regrettable rea-\nsons. By applying content-based features and personalized\nhistory-based features, we develop classifiers that can effec-\ntively predict regrettable tweets."}
{"Title": "Winning Arguments: Interaction Dynamics and Persuasion\nStrategies in Good-faith Online Discussions", "Abstract": "Changing someone\u2019s opinion is arguably one of the most important\nchallenges of social interaction. The underlying process proves\ndifficult to study: it is hard to know how someone\u2019s opinions are\nformed and whether and how someone\u2019s views shift. Fortunately,\nChangeMyView, an active community on Reddit, provides a plat-\nform where users present their own opinions and reasoning, invite\nothers to contest them, and acknowledge when the ensuing discus-\nsions change their original views. In this work, we study these\ninteractions to understand the mechanisms behind persuasion.\nWe find that persuasive arguments are characterized by interest-\ningpatternsofinteractiondynamics, suchasparticipantentry-order\nand degree of back-and-forth exchange. Furthermore, by compar-\ning similar counterarguments to the same opinion, we show that\nlanguage factors play an essential role. In particular, the interplay\nbetween the language of the opinion holder and that of the coun-\nterargument provides highly predictive cues of persuasiveness. Fi-\nnally, since even in this favorable setting people may not be per-\nsuaded, we investigate the problem of determining whether some-\none\u2019s opinion is susceptible to being changed at all. For this more\ndifficult task, we show that stylistic choices in how the opinion is\nexpressed carry predictive power."}
{"Title": "Addressing Complex and Subjective Product-Related\nQueries with Customer Reviews", "Abstract": "Online reviews are often our first port of call when considering\nproducts and purchases online. When evaluating a potential pur-\nchase, we may have a specific query in mind, e.g. \u2018will this baby\nseat fit in the overhead compartment of a 747?\u2019 or \u2018will I like this\nalbum if I liked Taylor Swift\u2019s 1989?\u2019. To answer such questions\nwe must either wade through huge volumes of consumer reviews\nhoping to find one that is relevant, or otherwise pose our question\ndirectly to the community via a Q/A system.\nIn this paper we hope to fuse these two paradigms: given a large\nvolume of previously answered queries about products, we hope\nto automatically learn whether a review of a product is relevant to\na given query. We formulate this as a machine learning problem\nusing a mixture-of-experts-type framework\u2014here each review is\nan \u2018expert\u2019 that gets to vote on the response to a particular query;\nsimultaneously we learn a relevance function such that \u2018relevant\u2019\nreviews are those that vote correctly. At test time this learned rele-\nvance function allows us to surface reviews that are relevant to new\nqueries on-demand. We evaluate our system, Moqa, on a novel cor-\npus of 1.4 million questions (and answers) and 13 million reviews.\nWe show quantitatively that it is effective at addressing both binary\nand open-ended queries, and qualitatively that it surfaces reviews\nthat human evaluators consider to be relevant."}
{"Title": "A robust framework for estimating linguistic alignment in\nTwitter conversations", "Abstract": "When people talk, they tend to adopt the behaviors, ges-\ntures, and language of their conversational partners. This\n\u201caccommodation\u201dto one\u2019s partners is largely automatic, but\nthe degree to which it occurs is influenced by social factors,\nsuch as gender, relative power, and attraction. In settings\nwhere such social information is not known, this accommo-\ndation can be a useful cue for the missing information. This\nis especially important in web-based communication, where\nsocial dynamics are often fluid and rarely stated explicitly.\nBut connecting accommodation and social dynamics on the\nweb requires accurate quantification of the di\u21b5erent amounts\nof accommodation being made.\nWe focus specifically on accommodation in the form of\n\u201clinguistic alignment\u201d: the amount that one person\u2019s word\nuse is influenced by another\u2019s. Previous studies have used\nmany measures for linguistic alignment, with no clear stan-\ndard. In this paper, we lay out a set of desiderata for a\nlinguistic alignment measure, including robustness to sparse\nand short messages, explicit conditionality, and consistency\nacross linguistic features with di\u21b5erent baseline frequencies.\nWe propose a straightforward and flexible model-based frame-\nwork for calculating linguistic alignment, with a focus on\nthe sparse data and limited social information observed in\nsocial media. We show that this alignment measure fulfills\nour desiderata on simulated data. We then analyze a large\ncorpus of Twitter data, both replicating previous results and\nextending them: Our measure\u2019s improved resolution reveals\na previously undetectable e\u21b5ect of interpersonal power in\nTwitter interactions."}
{"Title": "Detecting Evolution of Concepts based on Cause-Effect\nRelationships in Online Reviews", "Abstract": "Analyzing how technology evolves is important for understanding\ntechnological progress and its impact on society. Although the\nconcept of evolution has been explored in many domains (e.g.,\nevolution of topics, events or terminology, evolution of species),\nlittle research has been done on automatically analyzing the evolu-\ntion of products and technology in general. In this paper, we pro-\npose a novel approach for investigating the technology evolution\nbased on collections of product reviews. We are particularly inter-\nested in understanding social impact of technology and in discover-\ning how changes of product features influence changes in our social\nlives. We address this challenge by first distinguishing two kinds of\nproduct-related terms: physical product features and terms describ-\ning situations when products are used. We then detect changes in\nboth types of terms over time by tracking fluctuations in their popu-\nlarity and usage. Finally, we discover cases when changes of physi-\ncal product features trigger the changes in product\u2019s use. We exper-\nimentally demonstrate the effectiveness of our approach on the\nAmazon Product Review Dataset that spans over 18 years."}
{"Title": "The QWERTY Effect on the Web", "Abstract": "The QWERTY effect postulates that the keyboard layout\ninfluences word meanings by linking positivity to the use of\nthe right hand and negativity to the use of the left hand. For\nexample, previous research has established that words with\nmore right hand letters are rated more positively than words\nwith more left hand letters by human subjects in small scale\nexperiments. In this paper, we perform large scale investiga-\ntions of the QWERTY effect on the web. Using data from\neleven web platforms related to products, movies, books,\nand videos, we conduct observational tests whether a hand-\nmeaning relationship can be found in text interpretations\nby web users. Furthermore, we investigate whether writing\ntext on the web exhibits the QWERTY effect as well, by an-\nalyzing the relationship between the text of online reviews\nand their star ratings in four additional datasets. Overall,\nwe find robust evidence for the QWERTY effect both at the\npoint of text interpretation (decoding) and at the point of\ntext creation (encoding). We also find under which condi-\ntions the effect might not hold. Our findings have impli-\ncations for any algorithmic method aiming to evaluate the\nmeaning of words on the web, including for example seman-\ntic or sentiment analysis, and show the existence of \u201ddactilar\nonomatopoeias\u201d that shape the dynamics of word-meaning\nassociations. To the best of our knowledge, this is the first\nwork to reveal the extent to which the QWERTY effect ex-\nists in large scale human-computer interaction on the web."}
{"Title": "Exploring Limits to Prediction in Complex Social Systems", "Abstract": "How predictable is success in complex social systems? In\nspite of a recent profusion of prediction studies that ex-\nploit online social and information network data, this ques-\ntion remains unanswered, in part because it has not been\nadequately specified. In this paper we attempt to clarify\nthe question by presenting a simple stylized model of suc-\ncess that attributes prediction error to one of two generic\nsources: insu?ciency of available data and/or models on the\none hand; and inherent unpredictability of complex social\nsystems on the other. We then use this model to motivate\nan illustrative empirical study of information cascade size\nprediction on Twitter. Despite an unprecedented volume\nof information about users, content, and past performance,\nour best performing models can explain less than half of the\nvariance in cascade sizes. In turn, this result suggests that\neven with unlimited data predictive performance would be\nbounded well below deterministic accuracy. Finally, we ex-\nplore this potential bound theoretically using simulations of\na di\u21b5usion process on a random scale free network similar\nto Twitter. We show that although higher predictive power\nis possible in theory, such performance requires a homoge-\nneous system and perfect ex-ante knowledge of it: even a\nsmall degree of uncertainty in estimating product quality or\nslight variation in quality across products leads to substan-\ntially more restrictive bounds on predictability. We conclude\nthat realistic bounds on predictive accuracy are not dissim-\nilar from those we have obtained empirically, and that such\nbounds for other complex social systems for which data is\nmore di?cult to obtain are likely even lower."}
{"Title": "Do Cascades Recur?\nJustin Cheng 1 , Lada A Adamic 2 , Jon Kleinberg 3 , Jure Les", "Abstract": "Cascadesofinformation-sharingareaprimarymechanismbywhich\ncontent reaches its audience on social media, and an active line of\nresearch has studied how such cascades, which form as content is\nreshared from person to person, develop and subside. In this paper,\nwe perform a large-scale analysis of cascades on Facebook over\nsignificantly longer time scales, and find that a more complex pic-\nture emerges, in which many large cascades recur, exhibiting mul-\ntiple bursts of popularity with periods of quiescence in between.\nWe characterize recurrence by measuring the time elapsed between\nbursts, their overlap and proximity in the social network, and the\ndiversity in the demographics of individuals participating in each\npeak. We discover that content virality, as revealed by its initial\npopularity, is a main driver of recurrence, with the availability of\nmultiple copies of that content helping to spark new bursts. Still,\nbeyond a certain popularity of content, the rate of recurrence drops\nas cascades start exhausting the population of interested individu-\nals. We reproduce these observed patterns in a simple model of\ncontent recurrence simulated on a real social network. Using only\ncharacteristics of a cascade\u2019s initial burst, we demonstrate strong\nperformance in predicting whether it will recur in the future."}
{"Title": "TribeFlow: Mining & Predicting User Trajectories", "Abstract": "Which song will Smith listen to next? Which restaurant will Alice\ngo to tomorrow? Which product will John click next? These appli-\ncations have in common the prediction of user trajectories that are\nin a constant state of flux over a hidden network (e.g. website links,\ngeographic location). Moreover, what users are doing now may be\nunrelated to what they will be doing in an hour from now. Mind-\nful of these challenges we propose TribeFlow, a method designed\nto cope with the complex challenges of learning personalized pre-\ndictive models of non-stationary, transient, and time-heterogeneous\nuser trajectories. TribeFlow is a general method that can perform\nnext product recommendation, next song recommendation, next lo-\ncation prediction, and general arbitrary-length user trajectory pre-\ndiction without domain-specific knowledge. TribeFlow is more ac-\ncurate and up to 413\u00d7 faster than top competitors."}
{"Title": "Linking Users Across Domains with Location Data:\nTheory and Validation", "Abstract": "Linking accounts of the same user across datasets \u2013 even\nwhen personally identifying information is removed or un-\navailable \u2013 is an important open problem studied in many\ncontexts. Beyond many practical applications, (such as cross\ndomain analysis, recommendation, and link prediction), un-\nderstanding this problem more generally informs us on the\nprivacy implications of data disclosure. Previous work has\ntypically addressed this question using either different por-\ntions of the same dataset or observing the same behavior\nacross thematically similar domains. In contrast, the general\ncross-domain case where users have different profiles inde-\npendently generated from a common but unknown pattern\nraises new challenges, including difficulties in validation, and\nremains under-explored.\nIn this paper, we address the reconciliation problem for\nlocation-based datasets and introduce a robust method for\nthis general setting. Location datasets are a particularly\nfruitful domain to study: such records are frequently pro-\nduced by users in an increasing number of applications and\nare highly sensitive, especially when linked to other data-\nsets. Our main contribution is a generic and self-tunable\nalgorithm that leverages any pair of sporadic location-based\ndatasets to determine the most likely matching between the\nusers it contains. While making very general assumptions\non the patterns of mobile users, we show that the maximum\nweight matching we compute is provably correct. Although\ntrue cross-domain datasets are a rarity, our experimental\nevaluation uses two entirely new data collections, including\none we crawled, on an unprecedented scale. The method we\ndesign outperforms naive rules and prior heuristics. As it\ncombines both sparse and dense properties of location-based\ndata and accounts for probabilistic dynamics of observation,\nit can be shown to be robust even when data gets sparse."}
{"Title": "Exploiting Dining Preference for Restaurant\nRecommendation", "Abstract": "The wide adoption of location-based services provide the po-\ntential to understand people\u2019s mobility pattern at an unprecedent-\ned level, which can also enable food-service industry to accurately\npredict consumers\u2019 dining behavior. In this paper, based on users\u2019\ndining implicit feedbacks (restaurant visit via check-ins), explicit\nfeedbacks (restaurant reviews) as well as some meta data (e.g., lo-\ncation, user demographics, restaurant attributes), we aim at recom-\nmending each user a list of restaurants for his next dining. Implicit\nand Explicit feedbacks of dining behavior exhibit different char-\nacteristics of user preference. Therefore, in our work, user\u2019s din-\ning preference mainly contains two parts: implicit preference com-\ning from check-in data (implicit feedbacks) and explicit preference\ncoming from rating and review data (explicit feedbacks). For im-\nplicit preference, we first apply a probabilistic tensor factorization\nmodel (PTF) to capture preference in a latent subspace. Then, in\norder to incorporate contextual signals from meta data, we extend\nPTF by proposing an Implicit Preference Model (IPM), which can\nsimultaneously capture users\u2019/restaurants\u2019/time\u2019 preference in the\ncollaborative filtering and dining preference in a specific context\n(e.g., spatial distance preference, environmental preference). For\nexplicit preference, we propose Explicit Preference Model (EPM)\nby combining matrix factorization with topic modeling to discover\nthe user preference embedded both in rating score and text content.\nFinally, we design a unified model termed as Collective Implicit\nExplicit Preference Model (CIEPM) to combine implicit and ex-\nplicit preference together for restaurant recommendation. To eval-\nuate the performance of our system, we conduct extensive experi-\nments with large-scale datasets covering hundreds of thousands of\nusers and restaurants. The results reveal that our system is effective\nfor restaurant recommendation."}
{"Title": "Non-Linear Mining of Competing Local Activities", "Abstract": "Given a large collection of time-evolving activities, such as Google\nsearch queries, which consist of d keywords/activities for m lo-\ncations of duration n, how can we analyze temporal patterns and\nrelationships among all these activities and find location-specific\ntrends? How do we go about capturing non-linear evolutions of lo-\ncal activities and forecasting future patterns? For example, assume\nthat we have the online search volume for multiple keywords, e.g.,\n\u201cNokia/Nexus/Kindle\u201dor\u201cCNN/BBC\u201dfor236countries/territories,\nfrom 2004 to 2015. Our goal is to analyze a large collection of\nmulti-evolving activities, and specifically, to answer the following\nquestions: (a) Is there any sign of interaction/competition between\ntwo different keywords? If so, who competes with whom? (b) In\nwhich country is the competition strong? (c) Are there any sea-\nsonal/annual activities? (d) How can we automatically detect im-\nportant world-wide (or local) events?\nWe present C OMP C UBE , a unifying non-linear model, which\nprovides a compact and powerful representation of co-evolving ac-\ntivities; and also a novel fitting algorithm, C OMP C UBE -F IT , which\nis parameter-free and scalable. Our method captures the following\nimportant patterns: (B)asic trends, i.e., non-linear dynamics of co-\nevolving activities, signs of (C)ompetition and latent interaction,\ne.g., Nokia vs. Nexus, (S)easonality, e.g., a Christmas spike for\niPod in the U.S. and Europe, and (D)eltas, e.g., unrepeated local\nevents such as the U.S. election in 2008. Thanks to its concise but\neffective summarization, C OMP C UBE can also forecast long-range\nfuture activities. Extensive experiments on real datasets demon-\nstrate that C OMP C UBE consistently outperforms the best state-of-\nthe-art methods in terms of both accuracy and execution speed."}
{"Title": "PCT: Partial Co-Alignment of Social Networks", "Abstract": "People nowadays usually participate in multiple online social net-\nworks simultaneously to enjoy more social network services. Be-\nsides the common users, social networks providing similar ser-\nvices can also share many other kinds of information entities, e.g.,\nlocations, videos and products. However, these shared informa-\ntion entities in different networks are mostly isolated without any\nknown corresponding connections. In this paper, we aim at in-\nferring such potential corresponding connections linking multiple\nkinds of shared entities across networks simultaneously. Formally,\nthe problem is referred to as the network \u201cPartial Co-alignmenT\u201d\n(PCT) problem. PCT is an important problem and can be the pre-\nrequisite for many concrete cross-network applications, like social\nnetwork fusion, mutual information exchange and transfer. Mean-\nwhile, the PCT problem is also very challenging to address due to\nvarious reasons, like (1) the heterogeneity of social networks, (2)\nlack of training instances to build models, and (3) one-to-one con-\nstraint on the correspondence connections. To resolve these chal-\nlenges, a novel unsupervised network alignment framework, UNI -\nCOAT (UNsupervIsed COncurrent AlignmenT)), is introduced in\nthis paper. Based on the heterogeneous information, UNI COAT\ntransforms the PCT problem into a joint optimization problem. To\nsolve the objective function, the one-to-one constraint on the cor-\nresponding relationships is relaxed, and the redundant non-existing\ncorresponding connections introduced by such a relaxation will be\npruned with a novel network co-matching algorithm proposed in\nthis paper. Extensive experiments conducted on real-world co-\naligned social network datasets demonstrate the effectiveness of\nUNI COAT in addressing the PCT problem."}
{"Title": "Improving Post-Click User Engagement on Native Ads via\nSurvival Analysis", "Abstract": "In this paper we focus on estimating the post-click engage-\nment on native ads by predicting the dwell time on the\ncorresponding ad landing pages. To infer relationships be-\ntween features of the ads and dwell time we resort to the\napplication of survival analysis techniques, which allow us to\nestimate the distribution of the length of time that the user\nwill spend on the ad. This information is then integrated\ninto the ad ranking function with the goal of promoting the\nrank of ads that are likely to be clicked and consumed by\nusers (dwell time greater than a given threshold). The online\nevaluation over live tra?c shows that considering post-click\nengagement has a consistent positive e\u21b5ect on both CTR,\ndecreases the number of bounces and increases the average\ndwell time, hence leading to a better user post-click experi-\nence."}
{"Title": "Table Cell Search for Question Answering", "Abstract": "Tables are pervasive on the Web. Informative web tables\nrange across a large variety of topics, which can naturally\nserve as a significant resource to satisfy user information\nneeds. Driven by such observations, in this paper, we inves-\ntigate an important yet largely under-addressed problem:\nGiven millions of tables, how to precisely retrieve table cells\nto answer a user question. This work proposes a novel table\ncell search framework to attack this problem. We first for-\nmulate the concept of a relational chain which connects two\ncells in a table and represents the semantic relation between\nthem. With the help of search engine snippets, our frame-\nwork generates a set of relational chains pointing to poten-\ntially correct answer cells. We further employ deep neural\nnetworks to conduct more fine-grained inference on which\nrelational chains best match the input question and finally\nextract the corresponding answer cells. Based on millions\nof tables crawled from the Web, we evaluate our framework\nin the open-domain question answering (QA) setting, us-\ning both the well-known WebQuestions dataset and user\nqueries mined from Bing search engine logs. On WebQues-\ntions, our framework is comparable to state-of-the-art QA\nsystems based on knowledge bases (KBs), while on Bing\nqueries, it outperforms other systems with a 56.7% rela-\ntive gain. Moreover, when combined with results from our\nframework, KB-based QA performance can obtain a relative\nimprovement of 28.1% to 66.7%, demonstrating that web ta-\nbles supply rich knowledge that might not exist or is difficult\nto be identified in existing KBs."}
{"Title": "Identifying Web Queries with Question Intent", "Abstract": "Vertical selection is the task of predicting relevant verticals\nfor a Web query so as to enrich the Web search results with\ncomplementary vertical results. We investigate a novel vari-\nant of this task, where the goal is to detect queries with a\nquestion intent. Specifically, we address queries for which\nthe user would like an answer with a human touch. We call\nthese CQA-intent queries, since answers to them are typi-\ncally found in community question answering (CQA) sites.\nA typical approach in vertical selection is using a vertical\u2019s\nspecific language model of relevant queries and computing\nthe query-likelihood for each vertical as a selective criterion.\nThis works quite well for many domains like Shopping, Lo-\ncal and Travel. Yet, we claim that queries with CQA intent\nare harder to distinguish by modeling content alone, since\nthey cover many different topics. We propose to also take\nthe structure of queries into consideration, reasoning that\nqueries with question intent have quite a different struc-\nture than other queries. We present a supervised classifi-\ncation scheme, random forest over word-clusters for variable\nlength texts, which can model the query structure. Our\nexperiments show that it substantially improves classifica-\ntion performance in the CQA-intent selection task compared\nto content-oriented based classification, especially as query\nlength grows."}
{"Title": "A Study of Retrieval Models for Long Documents and\nQueries in Information Retrieval", "Abstract": "Recent research has shown that long documents are unfairly\npenalised by a number of current retrieval methods. In\nthis paper, we formally analyse two important but distinct\nreasons for normalising documents with respect to length,\nnamely verbosity and scope, and discuss the practical impli-\ncations of not normalising accordingly. We review a number\nof language modelling approaches and a range of recently\ndeveloped retrieval methods, and show that most do not cor-\nrectly model both phenomena, thus limiting their retrieval\neffectiveness in certain situations. Furthermore, the retrieval\ncharacteristics of long natural language queries have not tra-\nditionally had the same attention as short keyword queries.\nWe develop a new discriminative query language modelling\napproach that demonstrates improved performance on long\nverbose queries by appropriately weighting salient aspects of\nthe query. When combined with query expansion, we show\nthat our new approach yields state-of-the-art performance\nfor long verbose queries."}
{"Title": "Effective Construction of Relative Lempel-Ziv Dictionaries", "Abstract": "Web crawls generate vast quantities of text, retained and archived\nby the search services that initiate them. To store such data and\nto allow storage costs to be minimized, while still providing some\nlevel of random access to the compressed data, efficient and effec-\ntive compression techniques are critical. The Relative Lempel Ziv\n(RLZ) scheme provides fast decompression and retrieval of docu-\nments from within large compressed collections, and even with a\nrelatively small RAM-resident dictionary, is competitive relative to\nadaptive compression schemes.\nTo date, the dictionaries required by RLZ compression have\nbeen formed from concatenations of substrings regularly sampled\nfrom the underlying document collection, then pruned in a man-\nner that seeks to retain only the high-use sections. In this work,\nwe develop new dictionary design heuristics, based on effective\nconstruction, rather than on pruning; we identify dictionary con-\nstruction as a (string) covering problem. To avoid the complica-\ntions of string covering algorithms on large collections, we focus\non k-mers and their frequencies. First, with a reservoir sampler, we\nefficiently identify the most common k-mers. Then, since a col-\nlection typically comprises regions of local similarity, we select in\neach \u201cepoch\u201d a segment whose k-mers together achieve, locally, the\nhighest coverage score. The dictionary is formed from the concate-\nnation of these epoch-derived segments. Our selection process is\ninspired by the greedy approach to the Set Cover problem.\nCompared with the best existing pruning method, CARE , our\nscheme has a similar construction time, but achieves better com-\npression effectiveness. Over several multi-gigabyte document col-\nlections, there are relative gains of up to 27%."}
{"Title": "Just in Time: Controlling Temporal Performance in\nCrowdsourcing Competitions", "Abstract": "Many modern data analytics applications in areas such as\ncrisis management, stock trading, and healthcare, rely on\ncomponents capable of nearly real-time processing of stream-\ning data produced at varying rates. In addition to automatic\nprocessing methods, many tasks involved in those applica-\ntions require further human assessment and analysis. How-\never, current crowdsourcing platforms and systems do not\nsupport stream processing with variable loads. In this pa-\nper, we investigate how incentive mechanisms in competi-\ntion based crowdsourcing can be employed in such scenar-\nios. More specifically, we explore techniques for stimulating\nworkers to dynamically adapt to both anticipated and sud-\nden changes in data volume and processing demand, and we\nanalyze effects such as data processing throughput, peak-\nto-average ratios, and saturation effects. To this end, we\nstudy a wide range of incentive schemes and utility func-\ntions inspired by real world applications. Our large-scale\nexperimental evaluation with more than 900 participants\nand more than 6200 hours of work spent by crowd work-\ners demonstrates that our competition based mechanisms\nare capable of adjusting the throughput of online workers\nand lead to substantial on-demand performance boosts."}
{"Title": "Averaging Gone Wrong: Using Time-Aware Analyses to\nBetter Understand Behavior", "Abstract": "Online communities provide a fertile ground for analyzing\npeople\u2019s behavior and improving our understanding of so-\ncial processes. Because both people and communities change\nover time, we argue that analyses of these communities that\ntake time into account will lead to deeper and more accurate\nresults. Using Reddit as an example, we study the evolution\nof users based on comment and submission data from 2007\nto 2014. Even using one of the simplest temporal differences\nbetween users\u2014yearly cohorts\u2014we find wide differences in\npeople\u2019s behavior, including comment activity, effort, and\nsurvival. Further, not accounting for time can lead us to\nmisinterpret important phenomena. For instance, we ob-\nserve that average comment length decreases over any fixed\nperiod of time, but comment length in each cohort of users\nsteadily increases during the same period after an abrupt\ninitial drop, an example of Simpson\u2019s Paradox. Dividing\ncohorts into sub-cohorts based on the survival time in the\ncommunity provides further insights; in particular, longer-\nlived users start at a higher activity level and make more\nand shorter comments than those who leave earlier. These\nfindings both give more insight into user evolution in Red-\ndit in particular, and raise a number of interesting questions\naround studying online behavior going forward."}
{"Title": "Scheduling Human Intelligence Tasks\nin Multi-Tenant Crowd-Powered Systems", "Abstract": "Micro-task crowdsourcing has become a popular approach to\ne\u21b5ectively tackle complex data management problems such\nas data linkage, missing values, or schema matching. How-\never, the backend crowdsourced operators of crowd-powered\nsystems typically yield higher latencies than the machine-\nprocessable operators, this is mainly due to inherent ef-\nficiency di\u21b5erences between humans and machines. This\nproblem can be further exacerbated by the lack of workers on\nthe target crowdsourcing platform, or when the workers are\nshared unequally among a number of competing requesters;\nincluding the concurrent users from the same organization\nwho execute crowdsourced queries with di\u21b5erent types, pri-\norities and prices. Under such conditions, a crowd-powered\nsystem acts mostly as a proxy to the crowdsourcing plat-\nform, and hence it is very di?cult to provide e?ency guar-\nantees to its end-users.\nScheduling is the traditional way of tackling such prob-\nlems in computer science, by prioritizing access to shared\nresources. In this paper, we propose a new crowdsourcing\nsystem architecture that leverages scheduling algorithms to\noptimize task execution in a shared resources environment,\nin this case a crowdsourcing platform. Our study aims at\nassessing the e?ciency of the crowd in settings where mul-\ntiple types of tasks are run concurrently. We present exten-\nsive experimental results comparing i) di\u21b5erent multi-tenant\ncrowdsourcing jobs, including a workload derived from real\ntraces, and ii) di\u21b5erent scheduling techniques tested with\nreal crowd workers. Our experimental results show that task\nscheduling can be leveraged to achieve fairness and reduce\nquery latency in multi-tenant crowd-powered systems, al-\nthough with very di\u21b5erent tradeo\u21b5s compared to traditional\nsettings not including human factors."}
{"Title": "Using Hierarchical Skills for Optimized\nTask Assignment in Knowledge-Intensive Crowdsourcing", "Abstract": "Besides the simple human intelligence tasks such as image la-\nbeling, crowdsourcing platforms propose more and more tasks\nthat require very specific skills, especially in participative sci-\nence projects. In this context, there is a need to reason about\nthe required skills for a task and the set of available skills in\nthe crowd, in order to increase the resulting quality. Most of\nthe existing solutions rely on unstructured tags to model skills\n(vector of skills). In this paper we propose to finely model tasks\nand participants using a skill tree, that is a taxonomy of skills\nequipped with a similarity distance within skills. This model\nof skills enables to map participants to tasks in a way that\nexploits the natural hierarchy among the skills. We illustrate\nthe effectiveness of our model and algorithms through extensive\nexperimentation with synthetic and real data sets."}
{"Title": "MapWatch: Detecting and Monitoring International Border\nPersonalization on Online Maps", "Abstract": "Maps have long played a crucial role in enabling people to\nconceptualize and navigate the world around them. How-\never, maps also encode the world-views of their creators.\nDisputed international borders are one example of this: gov-\nernments may mandate that cartographers produce maps\nthat conform to their view of a territorial dispute.\nToday, online maps maintained by private corporations\nhave become the norm. However, these new maps are still\nsubject to old debates. Companies like Google and Bing\nresolve these disputes by localizing their maps to meet gov-\nernment requirements and user preferences, i.e., users in dif-\nferent locations are shown maps with different international\nboundaries. We argue that this non-transparent personaliza-\ntion of maps may exacerbate nationalistic disputes by pro-\nmoting divergent views of geopolitical realities.\nTo address this problem, we present MapWatch, our sys-\ntem for detecting and cataloging personalization of inter-\nnational borders in online maps. Our system continuously\ncrawls all map tiles from Google and Bing maps, and lever-\nages crowdworkers to identify border personalization. In this\npaper, we present the architecture of MapWatch, and ana-\nlyze the instances of border personalization on Google and\nBing, including one border change that MapWatch identified\nlive, as Google was rolling out the update."}
{"Title": "What Links Alice and Bob?\nMatching and Ranking Semantic Patterns in\nHeterogeneous Networks", "Abstract": "An increasing number of applications are modeled and an-\nalyzed in network form, where nodes represent entities of\ninterest and edges represent interactions or relationships be-\ntween entities. Commonly, such relationship analysis tools\nassume homogeneity in both node type and edge type. Re-\ncent research has sought to redress the assumption of homo-\ngeneity and focused on mining heterogeneous information\nnetworks (HINs) where both nodes and edges can be of dif-\nferent types. Building on such efforts, in this work we articu-\nlate a novel approach for mining relationships across entities\nin such networks while accounting for user preference over\nrelationship type and interestingness metric. We formalize\nthe problem as a top-k lightest paths problem, contextu-\nalized in a real-world communication network, and seek to\nfind the k most interesting path instances matching the pre-\nferred relationship type. Our solution, PROphetic HEuris-\ntic Algorithm for Path Searching (PRO-HEAPS), leverages\na combination of novel graph preprocessing techniques, well\ndesigned heuristics and the venerable A* search algorithm.\nWe run our algorithm on real-world large-scale graphs and\nshow that our algorithm significantly outperforms a wide va-\nriety of baseline approaches with speedups as large as 100X.\nWe also conduct a case study and demonstrate valuable ap-\nplications of our algorithm."}
{"Title": "From Social Machines to Social Protocols: Software\nEngineering Foundations for Sociotechnical Systems", "Abstract": "The overarching vision of social machines is to facilitate so-\ncial processes by having computers provide administrative\nsupport. We conceive of a social machine as a sociotech-\nnical system (STS): a software-supported system in which\nautonomous principals such as humans and organizations\ninteract to exchange information and services. Existing ap-\nproaches for social machines emphasize the technical aspects\nand inadequately support the meanings of social processes,\nleaving them informally realized in human interactions. We\nposit that a fundamental rethinking is needed to incorporate\naccountability, essential for addressing the openness of the\nWeb and the autonomy of its principals.\nWe introduce Interaction-Oriented Software Engineering\n(IOSE) as a paradigm expressly suited to capturing the so-\ncial basis of STSs. Motivated by promoting openness and\nautonomy, IOSE focuses not on implementation but on so-\ncial protocols, specifying how social relationships, character-\nizing the accountability of the concerned parties, progress as\nthey interact. Motivated by providing computational sup-\nport, IOSE adopts the accountability representation to cap-\nture the meaning of a social machine\u2019s states and transitions.\nWe demonstrate IOSE via examples drawn from health-\ncare. We reinterpret the classical software engineering (SE)\nprinciples for the STS setting and show how IOSE is better\nsuited than traditional software engineering for supporting\nsocial processes. The contribution of this paper is a new\nparadigm for STSs, evaluated via conceptual analysis."}
{"Title": "An Empirical Study of Web Cookies", "Abstract": "Web cookies are used widely by publishers and 3rd parties to track\nusers and their behaviors. Despite the ubiquitous use of cookies,\nthere is little prior work on their characteristics such as standard at-\ntributes,placementpolicies,andtheknowledgethatcanbeamassed\nvia 3rd party cookies. In this paper, we present an empirical study\nof web cookie characteristics, placement practices and information\ntransmission. To conduct this study, we implemented a lightweight\nweb crawler that tracks and stores the cookies as it navigates to\nwebsites. We use this crawler to collect over 3.2M cookies from\nthe two crawls, separated by 18 months, of the top 100K Alexa web\nsites. We report on the general cookie characteristics and add con-\ntext via a cookie category index and website genre labels. We con-\nsider privacy implications by examining specific cookie attributes\nand placement behavior of 3rd party cookies. We find that 3rd party\ncookies outnumber 1st party cookies by a factor of two, and we\nilluminate the connection between domain genres and cookie at-\ntributes. We find that less than 1% of the entities that place cook-\nies can aggregate information across 75% of web sites. Finally, we\nconsider the issue of information transmission and aggregation by\ndomains via 3rd party cookies. We develop a mathematical frame-\nworktoquantifyuserinformationleakageforabroadclassofusers,\nand present findings using real world domains. In particular, we\ndemonstrate the interplay between a domain\u2019s footprint across the\nInternet and the browsing behavior of users, which has significant\nimpact on information transmission."}
{"Title": "As Time Goes By: Comprehensive Tagging of\nTextual Phrases with Temporal Scopes", "Abstract": "Temporal expressions (TempEx\u2019s for short) are increasingly\nimportant in search, question answering, information extrac-\ntion, and more. Techniques for identifying and normalizing\nexplicit temporal expressions work well, but are not designed\nfor and cannot cope with textual phrases that denote named\nevents, such as \u201cClinton\u2019s term as secretary of state\u201d. This\npaper addresses the problem of detecting such temponyms,\ninferring their temporal scopes, and mapping them to events\nin a knowledge base if present there.\nWe present methods for this kind of temponym resolution,\nusing an entity- and TempEx-oriented document model and\nthe Yago knowledge base for distant supervision. We de-\nvelop a family of Integer Linear Programs for jointly in-\nferring temponym mappings to the timeline and knowledge\nbase. This enriches the document representation and also\nextends the knowledge base by obtaining new alias names\nfor events. Experiments with three different corpora demon-\nstrate the viability of our methods."}
{"Title": "Probabilistic Bag-Of-Hyperlinks Model for Entity Linking", "Abstract": "Many fundamental problems in natural language process-\ning rely on determining what entities appear in a given text.\nCommonly referenced as entity linking, this step is a fun-\ndamental component of many NLP tasks such as text un-\nderstanding, automatic summarization, semantic search or\nmachine translation. Name ambiguity, word polysemy, con-\ntext dependencies and a heavy-tailed distribution of entities\ncontribute to the complexity of this problem.\nWe here propose a probabilistic approach that makes use\nof an effective graphical model to perform collective entity\ndisambiguation. Input mentions (i.e., linkable token spans)\nare disambiguated jointly across an entire document by com-\nbining a document-level prior of entity co-occurrences with\nlocal information captured from mentions and their sur-\nrounding context. The model is based on simple sufficient\nstatistics extracted from data, thus relying on few parame-\nters to be learned.\nOur method does not require extensive feature engineer-\ning, nor an expensive training procedure. We use loopy be-\nlief propagation to perform approximate inference. The low\ncomplexity of our model makes this step sufficiently fast for\nreal-time usage. We demonstrate the accuracy of our ap-\nproach on a wide range of benchmark datasets, showing that\nit matches, and in many cases outperforms, existing state-\nof-the-art methods."}
{"Title": "Discovering Structure in the Universe of Attribute Names", "Abstract": "Recently, search engines have invested significant effort to\nanswering entity\u2013attribute queries from structured data, but\nhave focused mostly on queries for frequent attributes. In\nparallel, several research efforts have demonstrated that there\nis a long tail of attributes, often thousands per class of enti-\nties, that are of interest to users. Researchers are beginning\nto leverage these new collections of attributes to expand the\nontologies that power search engines and to recognize entity\u2013\nattribute queries. Because of the sheer number of potential\nattributes, such tasks require us to impose some structure\non this long and heavy tail of attributes.\nThis paper introduces the problem of organizing the at-\ntributes by expressing the compositional structure of their\nnames as a rule-based grammar. These rules offer a compact\nand rich semantic interpretation of multi-word attributes,\nwhile generalizing from the observed attributes to new un-\nseen ones. The paper describes an unsupervised learning\nmethod to generate such a grammar automatically from a\nlarge set of attribute names. Experiments show that our\nmethod can discover a precise grammar over 100,000 at-\ntributes of Countries while providing a 40-fold compaction\nover the attribute names. Furthermore, our grammar en-\nables us to increase the precision of attributes from 47% to\nmore than 90% with only a minimal curation effort. Thus,\nour approach provides an efficient and scalable way to ex-\npand ontologies with attributes of user interest."}
{"Title": "Modeling User Exposure in Recommendation", "Abstract": "Collaborative filtering analyzes user preferences for items\n(e.g., books, movies, restaurants, academic papers) by ex-\nploiting the similarity patterns across users. In implicit feed-\nback settings, all the items, including the ones that a user\ndid not consume, are taken into consideration. But this\nassumption does not accord with the common sense under-\nstanding that users have a limited scope and awareness of\nitems. For example, a user might not have heard of a cer-\ntain paper, or might live too far away from a restaurant to\nexperience it. In the language of causal analysis [9], the as-\nsignment mechanism (i.e., the items that a user is exposed\nto) is a latent variable that may change for various user/item\ncombinations. In this paper, we propose a new probabilistic\napproach that directly incorporates user exposure to items\ninto collaborative filtering. The exposure is modeled as a\nlatent variable and the model infers its value from data. In\ndoing so, we recover one of the most successful state-of-the-\nart approaches as a special case of our model [8], and provide\na plug-in method for conditioning exposure on various forms\nof exposure covariates (e.g., topics in text, venue locations).\nWe show that our scalable inference algorithm outperforms\nexisting benchmarks in four different domains both with and\nwithout exposure covariates."}
{"Title": "On the Relevance of Irrelevant Alternatives", "Abstract": "Multinomial logistic regression is a powerful tool to model choice\nfrom a finite set of alternatives, but it comes with an underlying\nmodel assumption called the independence of irrelevant alterna-\ntives, stating that any item added to the set of choices will decrease\nall other items\u2019 likelihood by an equal fraction. We perform statis-\ntical tests of this assumption across a variety of datasets and give\nresults showing how often it is violated.\nWhen this axiom is violated, choice theorists will often invoke\na richer model known as nested logistic regression, in which infor-\nmation about competition among items is encoded in a tree struc-\nture known as a nest. However, to our knowledge there are no\nknown algorithms to induce the correct nest structure. We present\nthe first such algorithm, which runs in quadratic time under an ora-\ncle model, and we pair it with a matching lower bound.\nWe then perform experiments on synthetic and real datasets to\nvalidate the algorithm, and show that nested logit over learned nests\noutperforms traditional multinomial regression.\nFinally, in addition to automatically learning nests, we show how\nnests may be constructed by hand to test hypotheses about the data,\nand evaluated by their explanatory power."}
{"Title": "Growing Wikipedia Across Languages via\nRecommendation", "Abstract": "ThedifferentWikipedialanguageeditionsvarydramaticallyinhow\ncomprehensive they are. As a result, most language editions con-\ntain only a small fraction of the sum of information that exists\nacross all Wikipedias. In this paper, we present an approach to\nfilling gaps in article coverage across different Wikipedia editions.\nOur main contribution is an end-to-end system for recommending\narticles for creation that exist in one language but are missing in an-\nother. The system involves identifying missing articles, ranking the\nmissing articles according to their importance, and recommending\nimportant missing articles to editors based on their interests. We\nempirically validate our models in a controlled experiment involv-\ning 12,000 French Wikipedia editors. We find that personalizing\nrecommendations increases editor engagement by a factor of two.\nMoreover, recommending articles increases their chance of being\ncreated by a factor of 3.2. Finally, articles created as a result of our\nrecommendations are of comparable quality to organically created\narticles. Overall, our system leads to more engaged editors and\nfaster growth of Wikipedia with no effect on its quality."}
{"Title": "Using Shortlists to Support Decision Making\nand Improve Recommender System Performance", "Abstract": "In this paper, we study shortlists as an interface component\nfor recommender systems with the dual goal of supporting\nthe user\u2019s decision process, as well as improving implicit\nfeedback elicitation for increased recommendation quality.\nA shortlist is a temporary list of candidates that the user\nis currently considering, e.g., a list of a few movies the user\nis currently considering for viewing. From a cognitive per-\nspective, shortlists serve as digital short-term memory where\nusers can offload the items under consideration \u2013 thereby\ndecreasing their cognitive load. From a machine learning\nperspective, adding items to the shortlist generates a new\nimplicit feedback signal as a by-product of exploration and\ndecision making which can improve recommendation quality.\nShortlisting therefore provides additional data for training\nrecommendation systems without the increases in cognitive\nload that requesting explicit feedback would incur.\nWe perform an user study with a movie recommendation\nsetup to compare interfaces that offer shortlist support with\nthose that do not. From the user studies we conclude: (i)\nusers make better decisions with a shortlist; (ii) users prefer\nan interface with shortlist support; and (iii) the additional\nimplicit feedback from sessions with a shortlist improves the\nquality of recommendations by nearly a factor of two."}
{"Title": "Tell Me About Yourself: The Malicious CAPTCHA Attack", "Abstract": "We present the malicious CAPTCHA attack, allowing a\nrogue website to trick users into unknowingly disclosing their\nprivate information. The rogue site displays the private in-\nformation to the user in obfuscated manner, as if it is a\nCAPTCHA challenge; the user is unaware that solving the\nCAPTCHA, results in disclosing private information. This\ncircumvents the Same Origin Policy (SOP), whose goal is\nto prevent access by rogue sites to private information, by\nexploiting the fact that many websites allow display of pri-\nvate information (to the user), upon requests from any (even\nrogue) website. Information so disclosed includes name,\nphone number, email and physical addresses, search history,\npreferences, partial credit card numbers, and more.\nThe vulnerability is common and the attack works for\nmany popular sites, including nine out of the ten most pop-\nular websites. We evaluated the attack using IRB-approved,\nethical user experiments."}
{"Title": "Remedying Web Hijacking: Notification\nEffectiveness and Webmaster Comprehension", "Abstract": "As miscreants routinely hijack thousands of vulnerable web servers\nweekly for cheap hosting and traffic acquisition, security services\nhave turned to notifications both to alert webmasters of ongoing in-\ncidents as well as to expedite recovery. In this work we present the\nfirst large-scale measurement study on the effectiveness of combi-\nnations of browser, search, and direct webmaster notifications at\nreducing the duration a site remains compromised. Our study cap-\ntures the life cycle of 760,935 hijacking incidents from July, 2014\u2013\nJune, 2015, as identified by Google Safe Browsing and Search\nQuality. We observe that direct communication with webmasters\nincreases the likelihood of cleanup by over 50% and reduces infec-\ntion lengths by at least 62%. Absent this open channel for com-\nmunication, we find browser interstitials\u2014while intended to alert\nvisitors to potentially harmful content\u2014correlate with faster reme-\ndiation. As part of our study, we also explore whether webmas-\nters exhibit the necessary technical expertise to address hijacking\nincidents. Based on appeal logs where webmasters alert Google\nthat their site is no longer compromised, we find 80% of operators\nsuccessfully clean up symptoms on their first appeal. However, a\nsizeable fraction of site owners do not address the root cause of\ncompromise, with over 12% of sites falling victim to a new attack\nwithin 30 days. We distill these findings into a set of recommen-\ndations for improving web security and best practices for webmas-\nters."}
{"Title": "No Honor Among Thieves:\nA Large-Scale Analysis of Malicious Web Shells", "Abstract": "Web shells are malicious scripts that attackers upload to a\ncompromised web server in order to remotely execute arbi-\ntrary commands, maintain their access, and elevate their\nprivileges. Despite their high prevalence in practice and\nheavy involvement in security breaches, web shells have never\nbeen the direct subject of any study. In contrast, web shells\nhave been treated as malicious blackboxes that need to be\ndetected and removed, rather than malicious pieces of soft-\nware that need to be analyzed and, in detail, understood.\nIn this paper, we report on the first comprehensive study\nof web shells. By utilizing different static and dynamic anal-\nysis methods, we discover and quantify the visible and in-\nvisible features offered by popular malicious shells, and we\ndiscuss how attackers can take advantage of these features.\nFor visible features, we find the presence of password brute-\nforcers, SQL database clients, portscanners, and checks for\nthe presence of security software installed on the compro-\nmised server. In terms of invisible features, we find that\nabout half of the analyzed shells contain an authentication\nmechanism, but this mechanism can be bypassed in a third\nof the cases. Furthermore, we find that about a third of the\nanalyzed shells perform homephoning, i.e., the shells, upon\nexecution, surreptitiously communicate to various third par-\nties with the intent of revealing the location of new shell in-\nstallations. By setting up honeypots, we quantify the num-\nber of third-party attackers benefiting from shell installa-\ntions and show how an attacker, by merely registering the\nappropriate domains, can completely take over all installa-\ntions of specific vulnerable shells."}
{"Title": "Stress Testing the Booters: Understanding and\nUndermining the Business of DDoS Services", "Abstract": "DDoS-for-hire services, also known as booters, have com-\nmoditized DDoS attacks and enabled abusive subscribers\nof these services to cheaply extort, harass and intimidate\nbusinesses and people by taking them offline. However, due\nto the underground nature of these booters, little is known\nabout their underlying technical and business structure. In\nthis paper, we empirically measure many facets of their tech-\nnical and payment infrastructure. We also perform an anal-\nysis of leaked and scraped data from three major booters\u2014\nAsylum Stresser, Lizard Stresser and VDO\u2014which provides\nus with an in-depth view of their customers and victims. Fi-\nnally, we conduct a large-scale payment intervention in col-\nlaboration with PayPal and evaluate its effectiveness as a de-\nterrent to their operations. Based on our analysis, we show\nthat these booters are responsible for hundreds of thousands\nof DDoS attacks and identify potentially promising methods\nto undermine these services by increasing their costs of op-\neration."}
{"Title": "N -gram over Context", "Abstract": "Our proposal, N-gram over Context (NOC), is a nonpara-\nmetric topic model that aims to help our understanding of\na given corpus, and be applied to many text mining ap-\nplications. Like other topic models, NOC represents each\ndocument as a mixture of topics and generates each word\nfrom one topic. Unlike these models, NOC focuses on both\na topic structure as an internal linguistic structure, and N-\ngram as an external linguistic structure. To improve the\nquality of topic specific N-grams, NOC reveals a tree of\ntopics that captures the semantic relationship between top-\nics from a given corpus as context, and forms N-gram by\noffering power-law distributions for word frequencies on this\ntopic tree. To gain both these linguistic structures efficiently,\nNOC learns them from a given corpus in a unified manner.\nBy accessing this entire tree at the word level in the genera-\ntive process of each document, NOC enables each document\nto maintain a thematic coherence and form N-grams over\ncontext. We develop a parallelizable inference algorithm, D-\nNOC, to support large data sets. Experiments on review\narticles/papers/tweet show that NOC is useful as a gener-\native model to discover both the topic structure and the\ncorresponding N-grams, and well complements human ex-\nperts and domain specific knowledge. D-NOC can process\nlarge data sets while preserving full generative model perfor-\nmance, by the help of an open-source distributed machine\nlearning framework."}
{"Title": "Representing Documents via Latent Keyphrase Inference", "Abstract": "Many text mining approaches adopt bag-of-words or n-grams\nmodels to represent documents. Looking beyond just the\nwords, i.e., the explicit surface forms, in a document can\nimprove a computer\u2019s understanding of text. Being aware\nof this, researchers have proposed concept-based models that\nrely on a human-curated knowledge base to incorporate other\nrelated concepts in the document representation. But these\nmethods are not desirable when applied to vertical domains\n(e.g., literature, enterprise, etc.) due to low coverage of\nin-domain concepts in the general knowledge base and in-\nterference from out-of-domain concepts. In this paper, we\npropose a data-driven model named Latent Keyphrase In-\nference (LAKI) that represents documents with a vector of\nclosely related domain keyphrases instead of single words\nor existing concepts in the knowledge base. We show that\ngiven a corpus of in-domain documents, topical content units\ncan be learned for each domain keyphrase, which enables a\ncomputer to do smart inference to discover latent document\nkeyphrases, going beyond just explicit mentions. Compared\nwith the state-of-art document representation approaches,\nLAKI fills the gap between bag-of-words and concept-based\nmodels by using domain keyphrases as the basic representa-\ntion unit. It removes dependency on a knowledge base while\nproviding, with keyphrases, readily interpretable representa-\ntions. When evaluated against 8 other methods on two text\nmining tasks over two corpora, LAKI outperformed all."}
{"Title": "Unsupervised, Efficient and Semantic Expertise Retrieval", "Abstract": "We introduce an unsupervised discriminative model for the task\nof retrieving experts in online document collections. We exclu-\nsively employ textual evidence and avoid explicit feature engineer-\ning by learning distributed word representations in an unsupervised\nway. We compare our model to state-of-the-art unsupervised sta-\ntistical vector space and probabilistic generative approaches. Our\nproposed log-linear model achieves the retrieval performance lev-\nels of state-of-the-art document-centric methods with the low infer-\nence cost of so-called profile-centric approaches. It yields a statis-\ntically significant improved ranking over vector space and genera-\ntive models in most cases, matching the performance of supervised\nmethods on various benchmarks. That is, by using solely text we\ncan do as well as methods that work with external evidence and/or\nrelevance feedback. A contrastive analysis of rankings produced\nby discriminative and generative approaches shows that they have\ncomplementary strengths due to the ability of the unsupervised dis-\ncriminative model to perform semantic matching."}
{"Title": "Using Metafeatures to Increase the Effectiveness\nof Latent Semantic Models in Web Search", "Abstract": "Inwebsearch, latentsemanticmodelshavebeenproposedtobridge\nthe lexical gap between queries and documents that is due to the\nfact that searchers and content creators often use different vocab-\nularies and language styles to express the same concept. Modern\nsearch engines simply use the outputs of latent semantic models as\nfeatures for a so-called global ranker. We argue that this is not opti-\nmal, because a single value output by a latent semantic model may\nbe insufficient to describe all aspects of the model\u2019s prediction, and\nthus some information captured by the model is not used effectively\nby the search engine.\nTo increase the effectiveness of latent semantic models in web\nsearch, we propose to create metafeatures\u2014feature vectors that de-\nscribe the structure of the model\u2019s prediction for a given query-\ndocument pair\u2014and pass them to the global ranker along with the\nmodels\u2019 scores. We provide simple guidelines to represent the la-\ntent semantic model\u2019s prediction with more than a single number,\nand illustrate these guidelines using several latent semantic models.\nWe test the impact of the proposed metafeatures on a web doc-\nument ranking task using four latent semantic models. Our exper-\niments show that (1) through the use of metafeatures, the perfor-\nmance of each individual latent semantic model can be improved\nby 10.2% and 4.2% in NDCG scores at truncation levels 1 and 10;\nand (2) through the use of metafeatures, the performance of a com-\nbination of latent semantic models can be improved by 7.6% and\n3.8% in NDCG scores at truncation levels 1 and 10, respectively."}
{"Title": "A Field Guide to Personalized Reserve Prices", "Abstract": "We study the question of setting and testing reserve prices\nin single item auctions when the bidders are not identical.\nAt a high level, there are two generalizations of the standard\nsecond price auction: in the lazy version we first determine\nthe winner, and then apply reserve prices; in the eager ver-\nsion we first discard the bidders not meeting their reserves,\nand then determine the winner among the rest. We show\nthat the two versions have dramatically different properties:\nlazy reserves are easy to optimize, and A/B test in produc-\ntion, whereas eager reserves always lead to higher welfare,\nbut their optimization is NP-complete, and naive A/B test-\ning will lead to incorrect conclusions. Despite their differ-\nent characteristics, we show that the overall revenue for the\ntwo scenarios is always within a factor of 2 of each other,\neven in the presence of correlated bids. Moreover, we prove\nthat the eager auction dominates the lazy auction on rev-\nenue whenever the bidders are independent or symmetric.\nWe complement our theoretical results with simulations on\nreal world data that show that even suboptimally set eager\nreserve prices are preferred from a revenue standpoint."}
{"Title": "People and Cookies: Imperfect Treatment Assignment in\nOnline Experiments", "Abstract": "Identifying the same internet user across devices or over time\nis often infeasible. This presents a problem for online experi-\nments, as it precludes person-level randomization. Random-\nization must instead be done using imperfect proxies for peo-\nple, like cookies, email addresses, or device identifiers. Users\nmay be partially treated and partially untreated as some of\ntheir cookies are assigned to the test group and some to the\ncontrol group, complicating statistical inference. We show\nthat the estimated treatment e\u21b5ect in a cookie-level experi-\nment converges to a weighted average of the marginal e\u21b5ects\nof treating more of a user\u2019s cookies. If the marginal e\u21b5ects\nof cookie treatment exposure are positive and constant, it\nunderestimates the true person-level e\u21b5ect by a factor equal\nto the number of cookies per person. Using two separate\ndatasets\u2014cookie assignment data from Atlas and advertis-\ning exposure and purchase data from Facebook\u2014we empir-\nically quantify the di\u21b5erences between cookie and person-\nlevel advertising e\u21b5ectiveness experiments. The e\u21b5ects are\nsubstantial: cookie tests underestimate the true person-level\ne\u21b5ects by a factor of about three, and require two to three\ntimes the number of people to achieve the same power as a\ntest with perfect treatment assignment."}
{"Title": "Objective Variables for Probabilistic Revenue Maximization\nin Second-Price Auctions with Reserve", "Abstract": "Many online companies sell advertisement space in second-price\nauctions with reserve. In this paper, we develop a probabilistic\nmethod to learn a profitable strategy to set the reserve price. We\nuse historical auction data with features to fit a predictor of the\nbest reserve price. This problem is delicate\u2014the structure of the\nauction is such that a reserve price set too high is much worse\nthan a reserve price set too low. To address this we develop ob-\njective variables, an approach for combining probabilistic model-\ning with optimal decision-making. Objective variables are \"hallu-\ncinated observations\" that transform the revenue maximization task\ninto a regularized maximum likelihood estimation problem, which\nwe solve with the EM algorithm. This framework enables a variety\nof prediction mechanisms to set the reserve price. As examples,\nwe study objective variable methods with regression, kernelized\nregression, and neural networks on simulated and real data. Our\nmethods outperform previous approaches both in terms of scalabil-\nity and profit."}
{"Title": "Competition on Price and Quality in Cloud Computing", "Abstract": "The public cloud \u201cinfrastructure as a service\u201d market pos-\nsesses unique features that make it difficult to predict long-\nrun economic behavior. On the one hand, major providers\nbuy their hardware from the same manufacturers, operate\nin similar locations and offer a similar menu of products.\nOn the other hand, the competitors use different propri-\netary \u201cfabric\u201d to manage virtualization, resource allocation\nand data transfer. The menus offered by each provider in-\nvolve a discrete number of choices (virtual machine sizes)\nand allow providers to locate in different parts of the price-\nquality space. We document this differentiation empirically\nby running benchmarking tests. This allows us to calibrate\na model of firm technology. Firm technology is an input\ninto our theoretical model of price-quality competition. The\nmonopoly case highlights the importance of competition in\nblocking \u201cbad equilibrium\u201d where performance is intention-\nally slowed down or options are unduly limited. In duopoly,\nprice competition is fierce, but prices do not converge to\nthe same level because of price-quality differentiation. The\nmodel helps explain market trends, such the healthy oper-\nating profit margin recently reported by Amazon Web Ser-\nvices. Our empirically calibrated model helps not only ex-\nplain price cutting behavior but also how providers can man-\nage a profit despite predictions that the market \u201cshould be\u201d\ntotally commoditized."}
{"Title": "IncApprox: A Data Analytics System for\nIncremental Approximate Computing", "Abstract": "Incremental and approximate computations are increasingly\nbeing adopted for data analytics to achieve low-latency exe-\ncution and efficient utilization of computing resources. Incre-\nmental computation updates the output incrementally instead of\nre-computing everything from scratch for successive runs of a\njob with input changes. Approximate computation returns an\napproximate output for a job instead of the exact output.\nBoth paradigms rely on computing over a subset of data items\ninstead of computing over the entire dataset, but they differ in\ntheir means for skipping parts of the computation. Incremental\ncomputing relies on the memoization of intermediate results of\nsub-computations, and reusing these memoized results across\njobs. Approximate computing relies on representative sampling\nof the entire dataset to compute over a subset of data items.\nInthispaper,weobservethatthesetwoparadigmsarecomple-\nmentary, and can be married together! Our idea is quite simple:\ndesignasamplingalgorithmthatbiasesthesampleselectiontothemem-\noizeddataitemsfrompreviousruns. Torealizethisidea,wedesigned\nan online stratified sampling algorithm that uses self-adjusting\ncomputation to produce an incrementally updated approximate\noutput with bounded error. We implemented our algorithm in a\ndata analytics system called I NC A PPROX based on Apache Spark\nStreaming. Our evaluation using micro-benchmarks and real-\nworld case-studies shows that I NC A PPROX achieves the benefits\nof both incremental and approximate computing."}
{"Title": "From Diversity-based Prediction to Better Ontology &\nSchema Matching", "Abstract": "Ontology & schema matching predictors assess the quality\nof matchers in the absence of an exact match. We propose\nMCD (Match Competitor Deviation), a new diversity-based\npredictor that compares the strength of a matcher confi-\ndence in the correspondence of a concept pair with respect\nto other correspondences that involve either concept. We\nalso propose to use MCD as a regulator to optimally control\na balance between Precision and Recall and use it towards\n1 : 1 matching by combining it with a similarity measure\nthat is based on solving a maximum weight bipartite graph\nmatching (MWBM). Optimizing the combined measure is\nknown to be an NP-Hard problem. Therefore, we propose\nCEM, an approximation to an optimal match by efficiently\nscanning multiple possible matches, using rare event estima-\ntion. Using a thorough empirical study over several bench-\nmark real-world datasets, we show that MCD outperforms\nother state-of-the-art predictor and that CEM significantly\noutperform existing matchers."}
{"Title": "The Effect of Recommendations\non Network Structure", "Abstract": "Online social networks regularly offer users personalized, al-\ngorithmic suggestions of whom to connect to. Here we ex-\namine the aggregate effects of such recommendations on net-\nwork structure, focusing on whether these recommendations\nincrease the popularity of niche users or, conversely, those\nwho are already popular. We investigate this issue by empir-\nically and theoretically analyzing abrupt changes in Twit-\nter\u2019s network structure around the mid-2010 introduction of\nits \u201cWho to Follow\u201d feature. We find that users across the\npopularity spectrum benefitted from the recommendations;\nhowever, the most popular users profited substantially more\nthan average. We trace this\u201crich get richer\u201dphenomenon to\nthree intertwined factors. First, as is typical of network rec-\nommenders, the system relies on a \u201cfriend-of-friend\u201d-style\nalgorithm, which we show generally results in users being\nrecommended proportional to their degree. Second, we find\nthat the baseline growth rate of users is sublinear in degree.\nThis mismatch between the recommender and the natural\nnetwork dynamics thus alters the structural evolution of the\nnetwork. Finally, we find that people are much more likely to\nrespond positively to recommendations for popular users\u2014\nperhaps because of their greater name recognition\u2014further\namplifying the cumulative advantage of well-known individ-\nuals."}
{"Title": "Gender, Productivity, and Prestige in Computer Science\nFaculty Hiring Networks", "Abstract": "Women are dramatically underrepresented in computer sci-\nence at all levels in academia and account for just 15% of\ntenure-track faculty. Understanding the causes of this gen-\nder imbalance would inform both policies intended to rec-\ntify it and employment decisions by departments and indi-\nviduals. Progress in this direction, however, is complicated\nby the complexity and decentralized nature of faculty hir-\ning and the non-independence of hires. Using comprehen-\nsive data on both hiring outcomes and scholarly productiv-\nity for 2659 tenure-track faculty across 205 Ph.D.-granting\ndepartments in North America, we investigate the multi-\ndimensional nature of gender inequality in computer science\nfaculty hiring through a network model of the hiring pro-\ncess. Overall, we find that hiring outcomes are most di-\nrectly affected by (i) the relative prestige between hiring\nand placing institutions and (ii) the scholarly productivity\nof the candidates. After including these, and other features,\nthe addition of gender did not significantly reduce modeling\nerror. However, gender differences do exist, e.g., in schol-\narly productivity, postdoctoral training rates, and in career\nmovements up the rankings of universities, suggesting that\nthe effects of gender are indirectly incorporated into hiring\ndecisions through gender\u2019s covariates. Furthermore, we find\nevidence that more highly ranked departments recruit fe-\nmale faculty at higher than expected rates, which appears\nto inhibit similar efforts by lower ranked departments. These\nfindings illustrate the subtle nature of gender inequality in\nfaculty hiring networks and provide new insights to the un-\nderrepresentation of women in computer science."}
{"Title": "Which to View: Personalized Prioritization for Broadcast\nEmails", "Abstract": "Email is one of the most important communication tools\ntoday, but email overload resulting from the large number\nof unimportant or irrelevant emails is causing trillion-level\neconomy loss every year. Thus personalized email prioritiza-\ntion algorithms are of urgent need. Despite lots of previous\neffort on this topic, broadcast email, an important type of\nemail, is overlooked in previous literature. Broadcast emails\nare significantly different from normal emails, introducing\nboth new challenges and opportunities. On one hand, lack\nof real senders and limited user interactions invalidate the\nkey features exploited by traditional email prioritization al-\ngorithms; on the other hand, thousands of receivers for one\nbroadcast email bring us the opportunity to predict impor-\ntance through collaborative filtering. However, broadcast\nemails face a severe cold-start problem which hinders the\ndirect application of collaborative filtering. In this paper,\nwe propose the first framework for broadcast email prioriti-\nzation by designing a novel active learning model that con-\nsiders the collaborative filtering, implicit feedback and time\nsensitive responsiveness features of broadcast emails. Our\nmethod is thoroughly evaluated on a large scale real world\nindustrial dataset from Samsung Electronics. Our method\nis proved highly effective and outperforms state-of-the-art\npersonalized email prioritization methods."}
{"Title": "Learning-to-Rank for Real-Time High-Precision Hashtag\nRecommendation for Streaming News", "Abstract": "We address the problem of real-time recommendation of\nstreaming Twitter hashtags to an incoming stream of news\narticles. The technical challenge can be framed as large scale\ntopic classification where the set of topics (i.e., hashtags) is\nhuge and highly dynamic. Our main applications come from\ndigital journalism, e.g., promoting original content to Twit-\nter communities and social indexing of news to enable better\nretrieval and story tracking. In contrast to the state-of-the-\nart that focuses on topic modelling approaches, we propose\na learning-to-rank approach for modelling hashtag relevance.\nThis enables us to deal with the dynamic nature of the prob-\nlem, since a relevance model is stable over time, while a\ntopic model needs to be continuously retrained. We present\nthe data collection and processing pipeline, as well as our\nmethodology for achieving low latency, high precision rec-\nommendations. Our empirical results show that our method\noutperforms the state-of-the-art, delivering more than 80%\nprecision. Our techniques are implemented in a real-time\nsystem that is currently under user trial with a big news\norganisation."}
{"Title": "Discovery of Topical Authorities in Instagram", "Abstract": "Instagram has more than 400 million monthly active ac-\ncounts who share more than 80 million pictures and videos\ndaily. This large volume of user-generated content is the ap-\nplication\u2019s notable strength, but also makes the problem of\nfinding the authoritative users for a given topic challenging.\nDiscovering topical authorities can be useful for providing\nrelevant recommendations to the users. In addition, it can\naid in building a catalog of topics and top topical authorities\nin order to engage new users, and hence provide a solution\nto the cold-start problem.\nIn this paper, we present a novel approach that we call\nthe Authority Learning Framework (ALF) to find topical au-\nthorities in Instagram. ALF is based on the self-described in-\nterests of the follower base of popular accounts. We infer reg-\nular users\u2019 interests from their self-reported biographies that\nare publicly available and use Wikipedia pages to ground\nthese interests as fine-grained, disambiguated concepts. We\npropose a generalized label propagation algorithm to propa-\ngate the interests over the follower graph to the popular ac-\ncounts. We show that even if biography-based interests are\nsparse at an individual user level they provide strong signals\nto infer the topical authorities and let us obtain a high preci-\nsion authority list per topic. Our experiments demonstrate\nthat ALF performs significantly better at user recommenda-\ntion task compared to fine-tuned and competitive methods,\nvia controlled experiments, in-the-wild tests, and over an\nexpert-curated list of topical authorities."}
{"Title": "Did You Say U2 or YouTube?", "Abstract": "Web search via voice is becoming increasingly popular, taking ad-\nvantage of recent advances in automatic speech recognition. Speech\nrecognition systems are trained using audio transcripts, which can\nbe generated by a paid annotator listening to some audio and man-\nually transcribing it. This paper considers an alternative source of\ntraining data for speech recognition, called implicit transcription.\nThis is based on Web search clicks and reformulations, which can be\ninterpreted as validating or correcting the recognition done during\na real Web search. This can give a large amount of free training\ndata that matches the exact characteristics of real incoming voice\nsearches and the implicit transcriptions can better reflect the needs\nof real users because they come from the user who generated the\naudio. On an overall basis we demonstrate that the new training data\nhas value in improving speech recognition. We further show that the\nin-context feedback from real users can allow the speech recognizer\nto exploit contextual signals, and reduce the recognition error rate\nfurther by up to 23%."}
{"Title": "Where Can I Buy a Boulder?\nSearching for Offline Retail Locations", "Abstract": "People commonly need to purchase things in person, from\nlarge garden supplies to home decor. Although modern\nsearch systems are very effective at finding online products,\nlittle research attention has been paid to helping users find\nplaces that sell a specific product offline. For instance, users\nsearching for an apron are not typically directed to a nearby\nkitchen store by a standard search engine.\nIn this paper, we investigate\u201cwhere can I buy\u201d-style queries\nrelated to in-person purchases of products and services. An-\nswering these queries is challenging since little is known\nabout the range of products sold in many stores, especially\nthose which are smaller in size. To better understand this\nclass of queries, we first present an in-depth analysis of typi-\ncal offline purchase needs as observed by a major search en-\ngine, producing an ontology of such needs. We then propose\nranking features for this new problem, and learn a ranking\nfunction that returns stores most likely to sell a queried item\nor service, even if there is very little information available\nonline about some of the stores. Our final contribution is a\nnew evaluation framework that combines distance with store\nrelevance in measuring the effectiveness of such a search sys-\ntem. We evaluate our method using this approach and show\nthat it outperforms a modern web search engine."}
{"Title": "Exploiting Green Energy to Reduce the Operational Costs\nof Multi-Center Web Search Engines", "Abstract": "Carbon dioxide emissions resulting from fossil fuels (brown\nenergy) combustion are the main cause of global warming\ndue to the greenhouse effect. Large IT companies have re-\ncently increased their efforts in reducing the carbon dioxide\nfootprint originated from their data center electricity con-\nsumption. On one hand, better infrastructure and mod-\nern hardware allow for a more efficient usage of electric re-\nsources. On the other hand, data-centers can be powered\nby renewable sources (green energy) that are both environ-\nmental friendly and economically convenient.\nIn this paper, we tackle the problem of targeting the us-\nage of green energy to minimize the expenditure of running\nmulti-center Web search engines, i.e., systems composed by\nmultiple, geographically remote, computing facilities.\nWe propose a mathematical model to minimize the op-\nerational costs of multi-center Web search engines by ex-\nploiting renewable energies whenever available at different\nlocations. Using this model, we design an algorithm which\ndecides what fraction of the incoming query load arriving\ninto one processing facility must be forwarded to be pro-\ncessed at different sites to use green energy sources.\nWe experiment using real traffic from a large search engine\nand we compare our model against state of the art baselines\nfor query forwarding. Our experimental results show that\nthe proposed solution maintains an high query throughput,\nwhile reducing by up to \u223c25% the energy operational costs\nof multi-center search engines. Additionally, our algorithm\ncan reduce the brown energy consumption by almost 6%\nwhen energy-proportional servers are employed."}
{"Title": "Strengthening Weak Identities\nThrough Inter-Domain Trust Transfer", "Abstract": "On most current websites untrustworthy or spammy identities are\neasily created. Existing proposals to detect untrustworthy identi-\nties rely on reputation signals obtained by observing the activities\nof identities over time within a single site or domain; thus, there\nis a time lag before which websites cannot easily distinguish at-\ntackers and legitimate users. In this paper, we investigate the feasi-\nbility of leveraging information about identities that is aggregated\nacross multiple domains to reason about their trustworthiness. Our\nkey insight is that while honest users naturally maintain identities\nacross multiple domains (where they have proven their trustworthi-\nness and have acquired reputation over time), attackers are discour-\naged by the additional effort and costs to do the same. We propose\na flexible framework to transfer trust between domains that can be\nimplemented in today\u2019s systems without significant loss of privacy\nor significant implementation overheads. We demonstrate the po-\ntential for inter-domain trust assessment using extensive data col-\nlected from Pinterest, Facebook, and Twitter. Our results show that\nnewer domains such as Pinterest can benefit by transferring trust\nfrom more established domains such as Facebook and Twitter by\nbeing able to declare more users as likely to be trustworthy much\nearlier on (approx. one year earlier)."}
{"Title": "Entity Disambiguation with Linkless Knowledge Bases", "Abstract": "Named Entity Disambiguation is the task of disambiguat-\ning named entity mentions in natural language text and link\nthem to their corresponding entries in a reference knowledge\nbase (e.g. Wikipedia). Such disambiguation can help add\nsemantics to plain text and distinguish homonymous enti-\nties. Previous research has tackled this problem by making\nuse of two types of context-aware features derived from the\nreference knowledge base, namely, the context similarity and\nthe semantic relatedness. Both features heavily rely on the\ncross-document hyperlinks within the knowledge base: the\nsemantic relatedness feature is directly measured via those\nhyperlinks, while the context similarity feature implicitly\nmakes use of those hyperlinks to expand entity candidates\u2019\ndescriptions and then compares them against the query con-\ntext. Unfortunately, cross-document hyperlinks are rarely\navailable in many closed domain knowledge bases and it is\nvery expensive to manually add such links. Therefore few\nalgorithms can work well on linkless knowledge bases. In\nthis work, we propose the challenging Named Entity Disam-\nbiguation with Linkless Knowledge Bases (LNED) problem\nand tackle it by leveraging the useful disambiguation evi-\ndences scattered across the reference knowledge base. We\npropose a generative model to automatically mine such ev-\nidences out of noisy information. The mined evidences can\nmimic the role of the missing links and help boost the LNED\nperformance. Experimental results show that our proposed\nmethod substantially improves the disambiguation accuracy\nover the baseline approaches."}
{"Title": "Joint Recognition and Linking of Fine-Grained Locations\nfrom Tweets", "Abstract": "Many users casually reveal their locations such as restaurants, land-\nmarks, and shops in their tweets. Recognizing such fine-grained\nlocations from tweets and then linking the location mentions to\nwell-defined location profiles (e.g., with formal name, detailed ad-\ndress, and geo-coordinates etc.) offer a tremendous opportunity for\nmanyapplications. Differentfromexistingsolutionswhichperform\nlocation recognition and linking as two sub-tasks sequentially in a\npipelinesetting,inthispaper,weproposeanoveljointframeworkto\nperformlocationrecognitionandlocationlinkingsimultaneouslyin\na joint search space. We formulate this end-to-end location linking\nproblem as a structured prediction problem and propose a beam-\nsearch based algorithm. Based on the concept of multi-view learn-\ning, we further enable the algorithm to learn from unlabeled data\nto alleviate the dearth of labeled data. Extensive experiments are\nconductedtorecognizelocationsmentionedintweetsandlinkthem\nto location profiles in Foursquare. Experimental results show that\nthe proposed joint learning algorithm outperforms the state-of-the-\nart solutions, and learning from unlabeled data improves both the\nrecognition and linking accuracy."}
{"Title": "Internet Collaboration on Extremely Difficult Problems:\nResearch versus Olympiad Questions on the Polymath Site", "Abstract": "Despite the existence of highly successful Internet collaborations\noncomplexprojects, includingopen-sourcesoftware, littleisknown\nabout how Internet collaborations work for solving \u201cextremely\u201d\ndifficult problems, such as open-ended research questions. We\nquantitatively investigate a series of efforts known as the Polymath\nprojects, whichtacklemathematicalresearchproblemsthroughopen\nonline discussion. A key analytical insight is that we can contrast\nthe polymath projects with mini-polymaths \u2014 spinoffs that were\nconducted in the same manner as the polymaths but aimed at ad-\ndressing math Olympiad questions, which, while quite difficult, are\nknown to be feasible.\nOur comparative analysis shifts between three elements of the\nprojects: the roles and relationships of the authors, the temporal\ndynamics of how the projects evolved, and the linguistic properties\nof the discussions themselves. We find interesting differences be-\ntween the two domains through each of these analyses, and present\nthese analyses as a template to facilitate comparison between Poly-\nmath and other domains for collaboration and communication. We\nalso develop models that have strong performance in distinguish-\ning research-level comments based on any of our groups of fea-\ntures. Finally, we examine whether comments representing re-\nsearch breakthroughs can be recognized more effectively based on\ntheir intrinsic features, or by the (re-)actions of others, and find\ngood predictive power in linguistic features."}
{"Title": "The Communication Network Within the Crowd", "Abstract": "Since its inception, crowdsourcing has been considered a\nblack-box approach to solicit labor from a crowd of work-\ners. Furthermore, the \u201ccrowd\u201d has been viewed as a group\nof independent workers dispersed all over the world. Re-\ncent studies based on in-person interviews have opened up\nthe black box and shown that the crowd is not a collection\nof independent workers, but instead that workers commu-\nnicate and collaborate with each other. Put another way,\nprior work has shown the existence of edges between work-\ners. We build on and extend this discovery by mapping\nthe entire communication network of workers on Amazon\nMechanical Turk, a leading crowdsourcing platform. We ex-\necute a task in which over 10,000 workers from across the\nglobe self-report their communication links to other work-\ners, thereby mapping the communication network among\nworkers. Our results suggest that while a large percent-\nage of workers indeed appear to be independent, there is a\nrich network topology over the rest of the population. That\nis, there is a substantial communication network within the\ncrowd. We further examine how online forum usage relates\nto network topology, how workers communicate with each\nother via this network, how workers\u2019 experience levels relate\nto their network positions, and how U.S. workers di\u21b5er from\ninternational workers in their network characteristics. We\nconclude by discussing the implications of our findings for\nrequesters, workers, and platform providers like Amazon."}
{"Title": "An In-depth study of Mobile Browser Performance", "Abstract": "Mobile page load times are an order of magnitude slower com-\npared to non-mobile pages. It is not clear what causes the poor per-\nformance: the slower network, the slower computational speeds,\nor other reasons. Further, most Web optimizations are designed\nfor non-mobile browsers and do not translate well to the mobile\nbrowser. Towards understanding mobile Web page load times, in\nthis paper we: (1) perform an in-depth pairwise comparison of\nloading a page on a mobile versus a non-mobile browser, and (2)\ncharacterize the bottlenecks in the mobile browser vis-a-vis non-\nmobile browsers. To this end, we build a testbed that allows us to\ndirectly compare the low-level page load activities and bottlenecks\nwhen loading a page on a mobile versus a non-mobile browser. We\nfind that computation activities are the main bottleneck when load-\ning a page on mobile browsers. This is in contrast to non-mobile\nbrowsers where network activities are the main bottleneck. We also\nfind that the composition of the critical path during page load is\ndifferent when loading pages on the mobile versus the non-mobile\nbrowser. A key takeaway of our work is that we need to fundamen-\ntally rethink optimizations for mobile browsers."}
{"Title": "The Case for Robotic Wireless Networks", "Abstract": "This paper explores the possibility of injecting mobility into\nwireless network infrastructure. We envision WiFi access points\non wheels that move to optimize user performance. Movements\nneed not be all around the floor, neither do they have to operate\non batteries. As a first step, WiFi APs at home could remain teth-\nered to power and Ethernet outlets while moving in small areas\n(perhaps under the couch). If such systems prove successful,\nperhaps future buildings and cities could offer explicit support\nfor network infrastructure mobility.\nThis paper begins with a higher level discussion of robotic wire-\nless networks \u2013 the opportunities and the hurdles \u2013 and then piv-\nots by developing a smaller slice of the vision through a system\ncallediMob. WithiMob,aWiFiAPismountedonaRoombarobot\nand made to periodically move withina 2x2 sqft region. The core\nresearch questions pertain to finding the best location to move\nto,suchthattheSNRsfromitsclientsarestrong,andtheinterfer-\nencesfromotherAPsareweak. Ourmeasurementsshowthatthe\nrichness of wireless multipath offers significant opportunities \u2013\neven within a 2x2 sqft region, locations exist that are 1.7x better\nthantheaveragelocationintermsofthroughput. Whenmultiple\nAPs in a neighborhood coordinate, the gains can be even higher.\nIn sum, although infrastructure mobility has been discussed in\nthe context of Google Balloons, ad hoc networks, and delay tol-\nerant networks, we believe that the possibility of moving our per-\nsonal devices in homes and offices is relatively unexplored, and\ncould open doors to new kinds of innovation."}
{"Title": "GoCAD: GPU-Assisted Online Content-Adaptive Display\nPower Saving for Mobile Devices in Internet Streaming", "Abstract": "DuringInternetstreaming, asignificantportionofthebatterypower\nis always consumed by the display panel on mobile devices. To re-\nduce the display power consumption, backlight scaling, a scheme\nthat intelligently dims the backlight has been proposed. To main-\ntain perceived video appearance in backlight scaling, a compu-\ntationally intensive luminance compensation process is required.\nHowever, this step, if performed by the CPU as existing schemes\nsuggest, could easily offset the power savings gained from back-\nlight scaling. Furthermore, computing the optimal backlight scal-\ningvaluesrequiresper-frameluminanceinformation, whichistypi-\ncally too energy intensive for mobile devices to compute. Thus, ex-\nisting schemes require such information to be available in advance.\nAnd such an offline approach makes these schemes impractical.\nTo address these challenges, in this paper, we design and im-\nplement GoCAD, a GPU-assisted Online Content-Adaptive Dis-\nplay power saving scheme for mobile devices in Internet streaming\nsessions. In GoCAD, we employ the mobile device\u2019s GPU rather\nthan the CPU to reduce power consumption during the luminance\ncompensation phase. Furthermore, we compute the optimal back-\nlight scaling values for small batches of video frames in an online\nfashion using a dynamic programming algorithm. Lastly, we make\nnovel use of the widely available video storyboard, a pre-computed\nset of thumbnails associated with a video, to intelligently decide\nwhether or not to apply our backlight scaling scheme for a given\nvideo. For example, when the GPU power consumption would off-\nset the savings from dimming the backlight, no backlight scaling\nis conducted. To evaluate the performance of GoCAD, we imple-\nment a prototype within an Android application and use a Monsoon\npower monitor to measure the real power consumption. Experi-\nments are conducted on more than 460 randomly selected YouTube\nvideos. Results show that GoCAD can effectively produce power\nsavings without affecting rendered video quality."}
{"Title": "An Empirical Analysis of Algorithmic Pricing\non Amazon Marketplace", "Abstract": "The rise of e-commerce has unlocked practical applications for al-\ngorithmic pricing (also called dynamic pricing algorithms), where\nsellers set prices using computer algorithms. Travel websites and\nlarge, wellknowne-retailershavealreadyadoptedalgorithmicpric-\ning strategies, but the tools and techniques are now available to\nsmall-scale sellers as well.\nWhile algorithmic pricing can make merchants more competi-\ntive, it also creates new challenges. Examples have emerged of\ncaseswherecompetingpiecesofalgorithmicpricingsoftwareinter-\nacted in unexpected ways and produced unpredictable prices [37],\nas well as cases where algorithms were intentionally designed to\nimplement price fixing [5]. Unfortunately, the public currently lack\ncomprehensive knowledge about the prevalence and behavior of al-\ngorithmic pricing algorithms in-the-wild.\nIn this study, we develop a methodology for detecting algorith-\nmic pricing, and use it empirically to analyze their prevalence and\nbehavior on Amazon Marketplace. We gather four months of data\ncovering all merchants selling any of 1,641 best-seller products.\nUsing this dataset, we are able to uncover the algorithmic pricing\nstrategies adopted by over 500 sellers. We explore the characteris-\ntics of these sellers and characterize the impact of these strategies\non the dynamics of the marketplace."}
{"Title": "Voting with Their Feet: Inferring User Preferences from\nApp Management Activities", "Abstract": "Smartphone users have adopted an explosive number of mo-\nbile applications (a.k.a., apps) in the recent years. App mar-\nketplaces for iOS, Android and Windows Phone platforms\nhost millions of apps which have been downloaded for more\nthan 100 billion times. Investigating how people manage mo-\nbile apps in their everyday lives creates a unique opportunity\nto understand the behavior and preferences of mobile users,\nto infer the quality of apps, and to improve the user expe-\nrience. Existing literature provides very limited knowledge\nabout app management activities, due to the lack of user\nbehavioral data at scale. This paper takes the initiative to\nanalyze a very large app management log collected through\na leading Android app marketplace. The data set covers\nfive months of detailed downloading, updating, and unin-\nstallation activities, involving 17 million anonymized users\nand one million apps. We present a surprising finding that\nthe metrics commonly used by app stores to rank apps do\nnot truly reflect the users\u2019 real attitudes towards the apps.\nWe then identify useful patterns from the app management\nactivities that much more accurately predict the user pref-\nerences of an app even when no user rating is available."}
{"Title": "User Fatigue in Online News Recommendation", "Abstract": "Many aspects and properties of Recommender Systems have been\nwell studied in the past decade, however, the impact of User Fa-\ntigue has been mostly ignored in the literature. User fatigue repre-\nsents the phenomenon that a user quickly loses the interest on the\nrecommended item if the same item has been presented to this user\nmultiple times before. The direct impact caused by the user fatigue\nis the dramatic decrease of the Click Through Rate (CTR, i.e., the\nratio of clicks to impressions).\nIn this paper, we present a comprehensive study on the research\nof the user fatigue in online recommender systems. By analyz-\ning user behavioral logs from Bing Now news recommendation, we\nfind that user fatigue is a severe problem that greatly affects the\nuser experience. We also notice that different users engage dif-\nferently with repeated recommendations. Depending on the pre-\nvious users\u2019 interaction with repeated recommendations, we illus-\ntrate that under certain condition the previously seen items should\nbe demoted, while some other times they should be promoted. We\ndemonstrate how statistics about the analysis of the user fatigue\ncan be incorporated into ranking algorithms for personalized rec-\nommendations. Our experimental results indicate that significant\ngains can be achieved by introducing features that reflect users\u2019\ninteraction with previously seen recommendations (up to 15% en-\nhancement on all users and 34% improvement on heavy users)."}
{"Title": "Mining User Intentions from Medical Queries: A Neural\nNetwork Based Heterogeneous Jointly Modeling Approach", "Abstract": "Text queries are naturally encoded with user intentions. An\nintention detection task tries to model and discover inten-\ntions that user encoded in text queries. Unlike conventional\ntext classification tasks where the label of text is highly cor-\nrelated with some topic-specific words, words from different\ntopic categories tend to co-occur in medical related queries.\nBesides the existence of topic-specific words and word order,\nword correlations and the way words organized into sentence\nare crucial to intention detection tasks.\nIn this paper, we present a neural network based jointly\nmodeling approach to model and capture user intentions in\nmedical related text queries. Regardless of the exact words\nin text queries, the proposed method incorporates two types\nof heterogeneous information: 1) pairwise word feature\ncorrelations and 2) part-of-speech tags of a sentence\nto jointly model user intentions. Variable-length text queries\nare first inherently taken care of by a fixed-size pairwise fea-\nture correlation matrix. Moreover, convolution and pooling\noperations are applied on feature correlations to fully ex-\nploit latent semantic structure within the query. Sentence\nrephrasing is finally introduced as a data augmentation tech-\nnique to improve model generalization ability during model\ntraining. Experiment results on real world medical queries\nhave shown that the proposed method is able to extract\ncomplete and precise user intentions from text queries."}
{"Title": "Who Benefits from the \u201cSharing\u201d Economy of Airbnb?", "Abstract": "Sharing economy platforms have become extremely popular in the\nlast few years, and they have changed the way in which we com-\nmute, travel, and borrowamong many otheractivities. Despite their\npopularity among consumers, such companies are poorly regulated.\nFor example, Airbnb, one of the most successful examples of shar-\ning economy platform, is often criticized by regulators and policy\nmakers. While, in theory, municipalities should regulate the emer-\ngence of Airbnb through evidence-based policy making, in prac-\ntice, they engage in a false dichotomy: some municipalities allow\nthe business without imposing any regulation, while others ban it\naltogether. That is because there is no evidence upon which to draft\npolicies. Here we propose to gather evidence from the Web. Af-\nter crawling Airbnb data for the entire city of London, we find out\nwhere and when Airbnb listings are offered and, by matching such\nlisting information with census and hotel data, we determine the\nsocio-economic conditions of the areas that actually benefit from\nthe hospitality platform. The reality is more nuanced than one\nwould expect, and it has changed over the years. Airbnb demand\nand offering have changed over time, and traditional regulations\nhave not been able to respond to those changes. That is why, fi-\nnally, we rely on our data analysis to envision regulations that are\nresponsive to real-time demands, contributing to the emerging idea\nof \u201calgorithmic regulation\u201d."}
{"Title": "Socialized Language Model Smoothing via\nBi-directional Influence Propagation on Social Networks", "Abstract": "In recent years, online social networks are among the most popu-\nlar websites with high PV (Page View) all over the world, as they\nhave renewed the way for information discovery and distribution.\nMillions of users have registered on these websites and hence gen-\nerate formidable amount of user-generated contents every day. The\nsocial networks become \u201cgiants\u201d, likely eligible to carry on any\nresearch tasks. However, we have pointed out that these giants\nstill suffer from their \u201cAchilles Heel\u201d, i.e., extreme sparsity [34,\n32]. Compared with the extremely large data over the whole col-\nlection, individual posting documents such as microblogs seem to\nbe too sparse to make a difference under various research scenar-\nios, while actually these postings are different. In this paper we\npropose to tackle the Achilles Heel of social networks by smooth-\ning the language model via influence propagation. To further our\npreviously proposed work to tackle the sparsity issue, we extend\nthe socialized language model smoothing with bi-directional influ-\nence learned from propagation. Intuitively, it is insufficient not to\ndistinguish the influence propagated between information source\nand target without directions. Hence, we formulate a bi-directional\nsocialized factor graph model, which utilizes both the textual cor-\nrelations between document pairs and the socialized augmentation\nnetworks behind the documents, such as user relationships and so-\ncial interactions. These factors are modeled as attributes and de-\npendencies among documents and their corresponding users, and\nthen are distinguished on the direction level. We propose an effec-\ntive learning algorithm to learn the proposed factor graph model\nwith directions. Finally we propagate term counts to smooth doc-\numents based on the estimated influence. We run experiments on\ntwo instinctive datasets of Twitter and Weibo. The results validate\nthe effectiveness of the proposed model. By incorporating direction\ninformation into the socialized language model smoothing, our ap-\nproach obtains improvement over several alternative methods on\nboth intrinsic and extrinsic evaluations measured in terms of per-\nplexity, nDCG and MAP measurements."}
{"Title": "Collaborative Nowcasting for Contextual Recommendation", "Abstract": "Mobile digital assistants such as Microsoft Cortana and Google\nNow currently offer appealing proactive experiences to users,\nwhich aim to deliver the right information at the right time.\nTo achieve this goal, it is crucial to precisely predict users\u2019\nreal-time intent. Intent is closely related to context, which\nincludes not only the spatial-temporal information but also\nusers\u2019 current activities that can be sensed by mobile de-\nvices. The relationship between intent and context is highly\ndynamic and exhibits chaotic sequential correlation. The\ncontext itself is often sparse and heterogeneous. The dy-\nnamics and co-movement among contextual signals are also\nelusive and complicated. Traditional recommendation mod-\nels cannot directly apply to proactive experiences because\nthey fail to tackle the above challenges. Inspired by the\nnowcasting practice in meteorology and macroeconomics, we\npropose an innovative collaborative nowcasting model to ef-\nfectively resolve these challenges. The proposed model suc-\ncessfully addresses sparsity and heterogeneity of contextual\nsignals. It also effectively models the convoluted correlation\nwithin contextual signals and between context and intent.\nSpecifically, the model first extracts collaborative latent fac-\ntors, which summarize shared temporal structural patterns\nin contextual signals, and then exploits the collaborative\nKalman Filter to generate serially correlated personalized\nlatent factors, which are utilized to monitor each user\u2019s real-\ntime intent. Extensive experiments with real-world data sets\nfrom a commercial digital assistant demonstrate the effec-\ntiveness of the collaborative nowcasting model. The studied\nproblem and model provide inspiring implications for new\nparadigms of recommendations on mobile intelligent devices."}
{"Title": "From Freebase to Wikidata: The Great Migration", "Abstract": "Collaborative knowledge bases that make their data freely\navailable in a machine-readable form are central for the data\nstrategy of many projects and organizations. The two ma-\njor collaborative knowledge bases are Wikimedia\u2019s Wikidata\nand Google\u2019s Freebase. Due to the success of Wikidata,\nGoogle decided in 2014 to offer the content of Freebase to\nthe Wikidata community. In this paper, we report on the\nongoing transfer efforts and data mapping challenges, and\nprovide an analysis of the effort so far. We describe the Pri-\nmary Sources Tool, which aims to facilitate this and future\ndata migrations. Throughout the migration, we have gained\ndeep insights into both Wikidata and Freebase, and share\nand discuss detailed statistics on both knowledge bases."}
{"Title": "Automatic Discovery of Attribute Synonyms\nUsing Query Logs and Table Corpora", "Abstract": "Attribute synonyms are important ingredients for keyword-\nbased search systems. For instance, web search engines, rec-\nognize queries that seek the value of an entity on a specific\nattribute (referred to as e+a queries) and provide direct an-\nswers for them using a combination of knowledge bases, web\ntables and documents. However, users often refer to an at-\ntribute in their e+a query differently from how it is referred\nin the web table or text passage. In such cases, search en-\ngines may fail to return relevant answers. To address that\nproblem, we propose to automatically discover all the al-\nternate ways of referring to the attributes of a given class\nof entities (referred to as attribute synonyms) in order to\nimprove search quality. The state-of-the-art approach that\nrelies on attribute name co-occurrence in web tables suffers\nfrom low precision.\nOur main insight is to combine positive evidence of at-\ntribute synonymity from query click logs, with negative ev-\nidence from web table attribute name co-occurrences. We\nformalize the problem as an optimization problem on a graph,\nwith the attribute names being the vertices and the pos-\nitive and negative evidences from query logs and web ta-\nble schemas as weighted edges. We develop a linear pro-\ngramming based algorithm to solve the problem that has\nbi-criteria approximation guarantees. Our experiments on\nreal-life datasets show that our approach has significantly\nhigher precision and recall compared with the state-of-the-\nart."}
{"Title": "Quality Assurance in Crowdsourcing via Matrix\nFactorization based Task Routing", "Abstract": "We investigate a method of crowdsourced task routing based\non matrix factorization. From a preliminary analysis of a\nreal crowdsourced data, we begin an exploration of how\nto route crowdsourcing task via Matrix factorization (MF)\nwhich efficiently estimate missing values in a worker-task\nmatrix. Our preliminary results show the benefits of task\nrouting over random assignment, the strength of probabilis-\ntic MF over baseline methods."}
{"Title": "Spatial Semantic Search in Location-Based Web Services", "Abstract": "As GPS-enabled mobile devices have advanced, the location-based\nservice(LBS) became one of the most active subjects in the Web-\nbased services. Major Web-based services such as Google Picasa,\nTwitter, Facebook, and Flicker employ LBS as one of their main\nfeatures. Consequently, a large number of geotagged documents\nare generated by users in the Web-based services. Recently, there\nhave been studies on the spatial keyword search which aims to find\na set of documents in the Web-based services by evaluating the\nspatial relevance and keyword relevance. It is a combination of the\nspatial search and keyword search, each of which has been studied\nfor a long time.\nIn this paper, we address the spatial semantic search problem\nwhich is to find top k relevant sets of documents with spatial con-\nstraints and semantic constraints. For devising an effective solution\nof the spatial semantic search, we propose a hybrid index strategy,\na ranking model and an efficient search algorithm. In addition, we\npresent the current status of our research progress, and discuss re-\nmaining challenges and future works."}
{"Title": "Time-aware Topic-based Contextualization", "Abstract": "In the past, various studies have been proposed to acquire\nthe capacity to perceive and comprehend language in ar-\nticles or human communications. Recently, researchers fo-\ncus on higher semantic levels to what human would need\nto understand the contents of articles. While human can\nsmoothly interpret documents when they have knowledge of\nthe context of documents, they have difficulty with those as\ntheir context is lost or changes. In this PhD proposal, we\naddress three novel research questions: detecting uninter-\npretable pieces in documents, retrieving contextual informa-\ntion and constructing compact context for the documents,\nthen propose approaches to these tasks, and discuss related\nissues."}
{"Title": "Entity Linking on Graph Data", "Abstract": "With the emergence of massive information networks, graph\ndata have become ubiquitous for various applications. Al-\nthough many graph processing problems have been studied\nrecently, entity linking on graph data has not received e-\nnough attention by the academia and industry, which finds\nvertex pairs that refer to the same entity from two graphs.\nThere are two main research challenges arising in this prob-\nlem. The first one is how to determine whether two vertices\nrefer to the same entity which is rather hard for graph data,\nespecially uncertain data, e.g., social networks. The sec-\nond challenge is to efficiently link the vertices. As existing\ngraph data are rather large, it is very important to devise\nefficient algorithms to achieve high performance. To address\nthese challenges, in this paper we propose a similarity-based\nmethod which takes the vertex pairs with similarity larger\nthan a given threshold as linked entities. We extend existing\ntextual similarity and structural similarity to evaluate sim-\nilarity between vertices from different graphs. To achieve\nhigh quality, we also combine them and propose a hybrid\nsimilarity. We also discuss new algorithms to efficiently link\nentities. We conduct experimental studies on real datasets\nand the results proves show that our hybrid method achieves\nhigh performance and outperform the baseline approaches."}
{"Title": "Understanding, Leveraging and Improving Human\nNavigation on the Web", "Abstract": "Navigating websites represents a fundamental activity of users on\ntheWeb. Modelingthisactivity, i.e., understandinghowpredictable\nhuman navigation is and whether regularities can be detected has\nbeen of interest to researchers for nearly two decades. This is cru-\ncial for improving the Web experience of users by e.g., enhancing\ninterfaces or information network structures.\nThis thesis envisions to shedding light on human navigational\npatterns by trying to understand, leverage and improve human nav-\nigation on the Web. One main goal of this thesis is the construc-\ntion of a versatile framework for modeling human navigational data\nwith the use of Markov chains and for detecting the appropriate\nMarkov chain order by using several advanced inference methods.\nIt allows us to investigate memory and structure in human naviga-\ntion patterns. Furthermore, we are interested in detecting whether\npragmatic human navigational data can be leveraged by e.g., being\nuseful for the task of calculating semantic relatedness between con-\ncepts. Finally, we want to find ways of enhancing human naviga-\ntion models. Concretely, we plan on incorporating prior knowledge\nabout the semantic relatedness between concepts to our Markov\nchain models as it is known that humans navigate the Web intu-\nitively instead of randomly. Our experiments should be conducted\non a variety of distinct navigational data including both goal ori-\nented and free form navigation scenarios. We not only look at nav-\nigational paths over websites, but also abstract away to navigational\npaths over topics in order to get insights into cognitive patterns."}
{"Title": "Entity-centric Summarization:\nGenerating Text Summaries for Graph Snippets", "Abstract": "In recent times, focus of information retrieval community\nhas shifted from traditional keyword-based retrieval to tech-\nniques utilizing the semantics in the text. Since such tech-\nniques require the understanding of relationships between\nentities, efforts are ongoing to organize the Web into large\nentity-relationship graphs. These graphs can be leveraged to\nanswer complex relationship queries. However, most of the\nresearch has focused upon extracting structural information\nbetween entities such as a path, Steiner tree, or subgraphs.\nLittle attention has been paid to the comprehension of these\nstructural results, which is necessary for the user to under-\nstand relationships encapsulated in these structures.\nIn this doctoral proposal, we pursue the idea of entity-\ncentric summarization and propose a novel framework to\nproduce entity-centric summaries which describe the rela-\ntionships among input entities. We discuss the inherent\nchallenges associated with each module in the framework\nand present an evaluation plan. Results from our prelimi-\nnary experiments are encouraging and substantiate the fea-\nsibility of summarization problem."}
{"Title": "Enhancing Web Activities with Information Visualization", "Abstract": "Many activities people perform on the Web are biased, in-\ncluding activities like reading news, searching for information\nand connecting with people. Sometimes these biases are in-\nherent in social behavior (like homophily), and sometimes\nthey are external as they affect the system (like media bias).\nIn this thesis proposal, we describe our approach to use infor-\nmation visualization to enhance Web activities performed by\nregular people (i.e., non-experts) We understand enhancing\nas reducing bias effects and generating an engaging response\nfrom users. Our methodology is based on case studies. We\nselect a Web activity, identify the biases that affect it, and\nevaluate how the biases affect a population from online social\nnetworks using web mining techniques, and then, we design\na visualization following an interactive and playful design\napproach to diminish the previously identified biases. We pro-\npose to evaluate the effect of our visualization designs in user\nstudies by comparing them with state-of-the-art techniques\nconsidering a playful experiences framework."}
{"Title": "Dynamic Communities Formation through Semantic Tags", "Abstract": "Taggers in social tagging systems have the main role in giv-\ning identities to the objects. Tagged objects also represent\nperception of their taggers about them and can define identi-\nties of their taggers in return. Consequently, identities that\nare assigned to the objects and taggers have effect on the\nquality of their categorization and communities formation\naround them. Tags in social semantic tagging systems have\nformal definitions because they are mapped to the concepts\nthat are defined in ontologies. Semantic tags are not only\nable to improve quality of tag assignments by solving some\ncommon tags ambiguity problems related to classic folkson-\nomy systems (i.e., in particular polysemy and synonymy),\nbut also to provide some meta data on top of the social rela-\ntions based on contribution of taggers around semantic tags.\nThose meta data may be exploited to form dynamic com-\nmunities which addresses the problems of lack of commonly\nagreed and evolving meaning of tags in social semantic tag-\nging systems.\nThis paper proposes an approach to form dynamic com-\nmunities of related taggers around the tagged objects. Be-\ncause our perceptions in each specific area of knowledge is\nevolving over time, the goal of our approach is also to evolve\nthe represented knowledge in semantic tagging systems dy-\nnamically according to the latest perception of the related\nusers."}
{"Title": "Strategic Foundation of Computational Social Science", "Abstract": "For decades, scholars of various disciplines have been fretted\nover strategic interactions, presenting theoretical insights\nand empirical observations [3, 18, 25]. Despite the central\nrole played by strategic interactions in creating values in\nthe Internet environment, our ability to understand them\nscientifically and to manage them in practice has remained\nlimited. While engineering communities suffer from not hav-\ning enough theoretical resource to formalize such phenom-\nena, economics and social sciences lack adequate technology\nto properly operationalize their theoretical insights, thereby\ndemanding an integrative solution. This project aims to\ndevelop a rational-choice-theory-driven framework for com-\nputational social science, focusing on social interactions on\nthe Internet. In order to suggest theoretical foundations,\nvalidation of the predictions in a controlled environment,\nand verification of the results in actual platforms, general\napproaches and a few examples of ongoing research are pre-\nsented."}
{"Title": "Fine-grained Data Partitioning Framework for Distributed\nDatabase Systems", "Abstract": "With the increasing size of web data and widely adopted par-\nallel cloud computing paradigm, distributed database and\nother distributed data processing systems, for example Pregel\nand GraphLab, use data partitioning technology to divide\nthe large data set. By default, these systems use hash par-\ntitioning to randomly assign data to partitions, which leads\nto huge network traffic between partitions.\nFine-grained partitioning can better allocate data and min-\nimize the number of nodes involved within a transaction or\njob while balancing the workload across data nodes as well.\nIn this paper, we propose a novel prototype system, LuTe\n, to provide highly efficient fine-grained partitioning scheme\nfor these distributed systems. LuTe maintains a lookup ta-\nble for each partitioned data set that maps a key to a set of\npartition ID(s). We use a novel lookup table technology that\nprovides low cost of reading and writing lookup table. LuTe\nprovides transaction support and high concurrency writing\nwith Multi Version Concurrency Control (MVCC) as well.\nWe implemented a prototype distributed DBMS on Post-\ngresql and used LuTe as a middle-ware to provide fine-grained\npartitioning support. Extensive experiments conducted on\na cluster demonstrate the advantage of the proposed ap-\nproach. The evaluation results show that in comparison with\nother state-of-the-art lookup table salutations, our approach\ncan significantly improve throughput by about 20% to 70%\non TPC-C benchmark."}
{"Title": "Systematic SLA Data Management", "Abstract": "The cloud computing paradigm emerged with service ori-\nented principles. In the cloud setting, organizations out-\nsource their IT equipment and manage their business pro-\ncesses through virtual services that are typically exchanged\nover HTTP. Service Level Agreements (SLAs) depict the\nstatus of running services. SLAs represent operational con-\ntracts that allow providers to estimate their service avail-\nability according to their resource capacity.\nThe SLA data schema and content are operationally de-\nfined by the type, volume and relations of service elements\nthat organizations operate on their physical resources. Cur-\nrent lack of a uniform SLA standardization leads to se-\nmantic and operational differences between SLAs, that are\nproduced and consumed by different organizations. Such\ndifferences prohibit common business SLA practices in the\ncloud computing domain. Our research introduces system-\natic SLA data management to describe the formalization,\nstorage and processing of SLAs over distributed computing\nenvironments. Services in scope are framed within the cloud\ncomputing context."}
{"Title": "LASER: A Living AnalyticS ExpeRimentation System for\nLarge-scale Online Controlled Experiments", "Abstract": "Tracking user browsing data and measuring the effective-\nness of website design and web services are important to\nbusinesses that want to attract the consumers today who\nspend much more time online than before. Instead of using\nrandomized controlled experiments, the existing approach\nsimply tracks user browsing behaviors before and after a\nchange is made to website design or web services, and evalu-\nate the differences. To address the effects caused by hidden\nfactors (e.g. promotion activities on the website) and to give\nfair comparison of different website designs, we propose the\nLASER system, a unified experimentation platform that en-\nables randomized online controlled experiments to be easily\nconducted with minimal human effort and modifications to\nthe experimented websites. More importantly, the LASER\nsystem manages the various aspects of online controlled ex-\nperiments, namely the selection of participants into groups,\nexposure of different user interface features or recommenda-\ntion algorithms to these groups, measuring their responses,\nand summarizing the results in the visual manner."}
{"Title": "iFeel: A Web System that Compares and\nCombines Sentiment Analysis Methods", "Abstract": "Sentiment analysis methods are used to detect polarity in thoughts\nand opinions of users in online social media. As businesses and\ncompanies are interested in knowing how social media users per-\nceive their brands, sentiment analysis can help better evaluate their\nproduct and advertisement campaigns. In this paper, we present\niFeel, a Web application that allows one to detect sentiments in any\nform of text including unstructured social media data. iFeel is free\nandgivesaccesstosevenexistingsentimentanalysismethods: Sen-\ntiWordNet, Emoticons, PANAS-t, SASA, Happiness Index, Sentic-\nNet, and SentiStrength. With iFeel, users can also combine these\nmethods and create a new Combined-Method that achieves high\ncoverage and F-measure. iFeel provides a single platform to com-\npare the strengths and weaknesses of various sentiment analysis\nmethodswithauserfriendlyinterfacesuchasfileuploading, graph-\nical visualizing, and weight tuning."}
{"Title": "Infrastructure Support for Evaluation as a Service", "Abstract": "How do we conduct large-scale community-wide evaluations\nfor information retrieval if we are unable to distribute the\ndocument collection? This was the challenge we faced in\norganizing a task on searching tweets at the Text Retrieval\nConference (TREC), since Twitter\u2019s terms of service forbid\nredistribution of tweets. Our solution, which we call \u201ceval-\nuation as a service\u201d, was to provide an API through which\nthe collection can be accessed for completing the evaluation\ntask. This paper describes the infrastructure underlying the\nservice and its deployment at TREC 2013. We discuss the\nmerits of the approach and potential applicability to other\nevaluation scenarios."}
{"Title": "Why Not, WINE?", "Abstract": "Despite considerable progress in recent years on Tag-based Social\nImage Retrieval (TagIR), state-of-the-art TagIR systems fail to pro-\nvide a systematic framework for end users to ask why certain im-\nages are not in the result set of a given query and provide an expla-\nnation for such missing results. However, such why-not questions\nare natural when expected images are missing in the query results\nreturned by a TagIR system. In this demonstration, we present a\nsystem called wine (Why-not questIon aNswering Engine) which\ntakes the first step to systematically answer the why-not questions\nposed by end-users on TagIR systems. It is based on three explana-\ntion models, namely result reordering, query relaxation, and query\nsubstitution, that enable us to explain a variety of why-not ques-\ntions. Our answer not only involves the reason why desired images\nare missing in the results but also suggestion on how the search\nquery can be altered so that the user can view these missing images\nin sufficient number."}
{"Title": "CrowdFill: A System for Collecting Structured Data\nfrom the Crowd", "Abstract": "CrowdFill is a system for collecting structured data from the crowd.\nUnlikeatypicalmicrotask-basedapproach, CrowdFillshowsanen-\ntire partially-filled table to all participating workers; workers col-\nlaboratively complete the table by filling in empty cells, as well\nas upvoting and downvoting data entered by other workers, using\nCrowdFill\u2019s intuitive data entry interface. CrowdFill ensures data\nentryisleadingtoafinaltablethatsatisfiesprespecifiedconstraints,\nand its compensation scheme encourages workers to submit useful,\nhigh-quality work. We demonstrate how CrowdFill collects struc-\ntured data from the crowd, from the perspective of a user as well as\nfrom the perspective of workers."}
{"Title": "Online Behavioral Genome Sequencing from Usage Logs:\nDecoding the Search Behaviors", "Abstract": "We present a system to analyze user interests by analyz-\ning their online behaviors from large-scale usage logs. We\nsurmise that user interests can be characterized by a large\ncollection of features we call the behavioral genes that can\nbe deduced from both their explicit and implicit online be-\nhaviors. It is the goal of this research to sequence the entire\nbehavioral genome for online population, namely, to identify\nthe pertinent behavioral genes and uncover their relation-\nships in explaining and predicting user behaviors, so that\nhigh quality user profiles can be created and the online ser-\nvices can be better customized using these profiles. Within\nthe scope of this paper, we demonstrate the work using the\npartial genome derived from web search logs. Our demo\nsystem is supported by an open access web service we are\nreleasing and sharing with the research community. The\nmain functions of the web service are: (1) calculating query\nsimilarities based on their lexical, temporal and semantic s-\ncores, (2) clustering a group of user queries into tasks with\nthe same search and browse intent, and (3) inferring user\ntopical interests by providing a probability distribution over\na search taxonomy."}
{"Title": "Easy Access to the Freebase Dataset", "Abstract": "We demonstrate a system for fast and intuitive exploration\nof the Freebase dataset. This required solving several non-\ntrivial problems, including: entity scores for proper ranking\nand name disambiguation, a unique meaningful name for ev-\nery entity and every type, extraction of canonical binary re-\nlations from multi-way relations (which in Freebase are mod-\neled via so-called mediator objects), computing the transi-\ntive hull of selected relations, and identifying and merging\nduplicates. Our contribution is two-fold. First, we provide\nfor download an up-to-date version of the Freebase data,\nenriched and simplified as just sketched. Second, we offer a\nuser interface for exploring and searching this data set. The\ndata set, the user interface and a demo video are available\nfrom http://freebase-easy.cs.uni-freiburg.de."}
{"Title": "Collaborative Adaptive Case Management with Linked Data", "Abstract": "An increasing share of today\u2019s work is knowledge work. Adap-\ntive Case Management (ACM) assists knowledge workers in\nhandling this collaborative, emergent and unpredictable type\nof work. Finding suitable workers for specific functions still\nrelies on manual assessment and assignment by persons in\ncharge, which does not scale well. In this paper we discuss a\ntool for ACM to facilitate this expert finding leveraging exist-\ning Web technology. We propose a method to automatically\nrecommend a set of eligible workers utilizing linked data, en-\nriched user profile data from distributed social networks and\ninformation gathered from case descriptions. This semantic\nrecommendation method detects similarities between case\nrequirements and worker profiles. The algorithm traverses\ndistributed social graphs to retrieve a ranked list of suit-\nable contributors to a case according to adaptable metrics.\nFor this purpose, we introduce a vocabulary to specify case\nrequirements and a vocabulary to describe skill sets and per-\nsonal attributes of workers. The semantic recommendation\nmethod is demonstrated by a prototypical implementation\nusing a WebID-based distributed social network."}
{"Title": "EVIN: Building a Knowledge Base of Events", "Abstract": "We present EVIN: a system that extracts named events from\nnews articles, reconciles them into canonicalized events, and\norganizes them into semantic classes to populate a knowl-\nedge base. EVIN exploits different kinds of similarity mea-\nsures among news, referring to textual contents, entity oc-\ncurrences, and temporal ordering. These similarities are\ncaptured in a multi-view attributed graph. To distill canon-\nicalized events, EVIN coarsens the graph by iterative merg-\ning based on a judiciously designed loss function. To infer\nsemantic classes of events, EVIN uses statistical language\nmodels. EVIN provides a GUI that allows users to query\nthe constructed knowledge base of events, and to explore it\nin a visual manner."}
{"Title": "Event Registry \u2013 Learning About World Events From News", "Abstract": "Event Registry is a system that can analyze news articles\nand identify in them mentioned world events. The system\nis able to identify groups of articles that describe the same\nevent. It can identify groups of articles in different languages\nthat describe the same event and represent them as a sin-\ngle event. From articles in each event it can then extract\nevent\u2019s core information, such as event location, date, who\nis involved and what is it about. Extracted information is\nstored in a database. A user interface is available that allows\nusers to search for events using extensive search options, to\nvisualize and aggregate the search results, to inspect indi-\nvidual events and to identify related events."}
{"Title": "Enhancing Media Enrichment by Semantic Extraction", "Abstract": "The opportunities of the Internet combined with new devices and\ntechnologies change the end users\u2019 habits in media consumption.\nWhile end users often search for related information to the\ncurrently watched TV show by themselves, we propose to\nimprove this user experience by automatically enriching media\nusing semantic extraction. In our recent work we focused on how\nto apply media enrichment to distributed screens. Based on the\nfindings we made from our recent prototype we identify several\nproblems and describe how we deal with them. We illustrate a\nway to achieve cross-platform real-time synchronization using\nseveral transport protocols. We propose the usage of sessions to\nhandle  multi-user,  multi-screen  scenarios  and  introduce\ntechniques for new interaction and customization patterns. We\nextend our recent approach with the extraction of keywords from\ngiven subtitles by utilizing statistical algorithms and natural\nlanguage processing technologies, which are then used to discover\nand display related content from the Web. The prototype\npresented in this paper reflects the improvements of our work. We\ndiscuss next research steps and define challenges for further\nresearch."}
{"Title": "Databugger: A Test-Driven Framework for Debugging the\nWeb of Data", "Abstract": "Linked Open Data (LOD) comprises of an unprecedented\nvolume of structured data on the Web. However, these\ndatasets are of varying quality ranging from extensively cu-\nrated datasets to crowd-sourced or extracted data of often\nrelatively low quality. We present Databugger, a framework\nfor test-driven quality assessment of Linked Data, which is\ninspired by test-driven software development. Databugger\nensures a basic level of quality by accompanying vocabular-\nies, ontologies and knowledge bases with a number of test\ncases. The formalization behind the tool employs SPARQL\nquery templates, which are instantiated into concrete quality\ntest queries. The test queries can be instantiated automati-\ncally based on a vocabulary or manually based on the data\nsemantics. One of the main advantages of our approach is\nthat domain specific semantics can be encoded in the data\nquality test cases, thus being able to discover data quality\nproblems beyond conventional quality heuristics."}
{"Title": "Semantic Mashup with the Online IDE WikiNEXT", "Abstract": "The proposed demonstration requests DBPedia.org, gets the\nresults and uses them to populate wiki pages with semantic\nannotations using RDFaLite. These annotations are persisted in\na RDF store and we will show how this data can be reused by\nother applications, e.g. for a semantic mashup that displays all\ncollected metadata about cities on a single map page. It has been\ndeveloped using WikiNEXT, a mix between a semantic wiki and\na web-based IDE. The tool is online 1 , open source 2 ; screencasts\nare available on YouTube (look for \u201cWikiNext\u201d)."}
{"Title": "A Demonstration of Query-Oriented Distribution and\nReplication Techniques for Dynamic Graph Data", "Abstract": "Evolving networks can be modeled as series of graphs that\nrepresent those networks at di\u21b5erent points in time. Our G*\nsystem enables e?cient storage and querying of these graph\nsnapshots by taking advantage of their commonalities. In ex-\ntending G* for scalable and robust operation, we found the\nclassic challenges of data distribution and replication to be\nimbued with renewed significance. If multiple graph snap-\nshots are commonly queried together, traditional techniques\nthat distribute data over all servers or create identical data\nreplicas result in ine?cient query execution.\nWe propose to verify, using live demonstrations, the bene-\nfits of our graph snapshot distribution and replication tech-\nniques. Our distribution technique adjusts the set of servers\nstoring each graph snapshot in a manner optimized for pop-\nular queries. Our replication technique maintains each snap-\nshot replica on a di\u21b5erent number of servers, making avail-\nable the most e?cient replica configurations for di\u21b5erent\ntypes of queries."}
{"Title": "YouTube4Two: Socializing Video Platform for two\nCo-present People", "Abstract": "YouTube4Two is an application that exploits the YouTube\nmedia library (through its API) to demonstrate a new style\nof social interaction. Two co-present people can share a\nvideo and act autonomously to navigate the related-video\nand comment lists, and search for videos. The novelty is\nthat they can use their own smartphones connected via In-\nternet to control the shared application. The application\nhas been designed according to the responsive-web-design\n(RWD) principle to smoothly pass from desktop interface\n(controlled by mouse and keyboard) to smartphone inter-\nface (with touch control). YouTube4Two introduces the\nmulti-device responsive Web design (MD-RWD) style that\nextends the RWD style by introducing the separation be-\ntween displayed content (on a shared screen) and displayed\ncontrol commands (on personal smartphones) to support\nshared control over an application."}
{"Title": "Cross Domain Communication in the Web of Things\nA New Context for the Old Problem", "Abstract": "Cross domain communication has been a long-discussed sub-\nject in the field of web-based application, especially for any\nsort of mashups where a single web app combines resources\nfrom different locations. This issue becomes more impor-\ntant in the Web of Things context, where every physical\nresources are exposed to the Web and mashed up by other\nweb applications. In this paper we demonstrate a use case in\nwhich cross domain communication is applied in the Web of\nThings using the HTML5 Cross Document Messaging API\n(HTML5CDM). In addition, we contribute an advanced im-\nplementation of HTML5CDM that brings RESTful commu-\nnication model to HTML5CDM and supports better con-\ncurrent message exchange, which we believe will be of much\nbenefit to web developers. In addition, a time/space evalu-\nation that measures CPU and Memory usage for the devel-\noped HTML5CDM library is carried out and the results has\nproved our implementation\u2019s practicability."}
{"Title": "Measuring the Effectiveness of Multi-Channel Marketing\nCampaigns Using Online Chatter", "Abstract": "Measuring the effectiveness of marketing campaigns across differ-\nent channels is one of the most challenging tasks for today\u2019s brand\nmarketers. Such measurement usually relies on a combination of\nkey performance indicators (KPIs), used for assessing various as-\npects of marketing outcomes. Recently, with the availability of\nsocial-media sources, new options for collecting KPIs have become\navailable and numerous social-media monitoring tools and services\nhave emerged. Yet, given the vast media spectrum, which goes\nbeyond social-media channels, existing solutions fail to generalize\nwell and the curation of marketing performance KPIs for most mar-\nketing channels still relies on labor intensive means such as surveys\nand questionnaires. Trying to address the challenges, we propose\nto demonstrate a novel solution we have developed in IBM: Multi-\nchannel Marketing Monitoring Platform (M3P for short). M3P is\nbetter tailored for the marketing performance domain, where online\nchatter is being harnessed for effective collection of meaningful\nmarketing KPIs across all possible channels. We describe M3P\u2019s\nmain challenges and review some of its novel KPIs. We then de-\nscribe the M3P solution, focusing on its KPI extraction process.\nFinally, we describe the planned demonstration using a real-world\nmarketing use-case."}
{"Title": "Me-link: Link Me to the Media \u2013 Fusing Audio and Visual\nCues for Robust and Efficient Mobile Media Interaction", "Abstract": "In this demo, we present a scalable mobile video recogni-\ntion system, named \u201cMe-link,\u201d based on progressive fusion\nof light-weight audio visual features. With our system, users\nonly have to point the mobile camera to the video they\nare interested in. The system will capture the frames and\nsounds, then retrieve relevant information immediately. As\nthe users hold the mobile longer, the system progressively\naggregates the cues temporally and then returns more accu-\nrate results. We also consider the real world noisy environ-\nment, where users may not get clear visual or audio signals.\nIn the aggregation step of audio and visual cues, our sys-\ntem automatically detects the available channel for the final\nrank. On the server side, users can upload the videos with\ninformation via website. Besides, we also link the streaming\nsignals so that users can get the real time broadcasting with\n\u201cMe-link\u201d.\n1"}
{"Title": "PRISM: A System for Weighted Multi-Color Browsing of\nFashion Products", "Abstract": "Multiple color search technology helps users find fashion\nproducts in a more intuitive manner. Although fashion\nproduct images can be represented not only by a set of dom-\ninant colors but also by the relative ratio of colors, current\nonline fashion shopping malls often provide rather simple\ncolor filters. In this demo, we present PRISM (Perceptual\nRepresentation of Image SiMilarity), a weighted multi-color\nbrowsing system for fashion products retrieval. Our system\ncombines widely accepted backend web service stacks and\nvarious computer vision techniques including a product area\nparsing and a compact yet effective multi-color description.\nFinally, we demonstrate the benefits of PRISM system via\nweb service in which users freely browse fashion products."}
{"Title": "Online Abusive Users Analytics through Visualization", "Abstract": "In this demo, we present Abuse User Analytics (AuA), an an-\nalytical framework aiming to provide key information about\nthe behavior of online social network users. AuA efficiently\nprocesses data from users\u2019 discussions, and renders informa-\ntion about users\u2019 activities in a easy to-understand graphical\nfashion with the goal of identifying deviant or abusive activ-\nities. Using animated graphics, AuA visualizes users\u2019 degree\nof abusiveness, measured by several key metrics, over user\nselected time intervals. It is therefore possible to visualize\nhow users\u2019 activities lead to complex interaction networks,\nand highlight the degenerative connections among users and\nwithin certain threads."}
{"Title": "AIDR: Artificial Intelligence for Disaster Response", "Abstract": "We present AIDR (Artificial Intelligence for Disaster Re-\nsponse), a platform designed to perform automatic classi-\nfication of crisis-related microblog communications. AIDR\nenables humans and machines to work together to apply hu-\nman intelligence to large-scale data at high speed.\nThe objective of AIDR is to classify messages that peo-\nple post during disasters into a set of user-defined categories\nof information (e.g., \u201cneeds\u201d, \u201cdamage\u201d, etc.) For this pur-\npose, the system continuously ingests data from Twitter,\nprocesses it (i.e., using machine learning classification tech-\nniques) and leverages human-participation (through crowd-\nsourcing) in real-time. AIDR has been successfully tested to\nclassify informative vs. non-informative tweets posted dur-\ning the 2013 Pakistan Earthquake. Overall, we achieved a\nclassification quality (measured using AUC) of 80%. AIDR\nis available at http://aidr.qcri.org/."}
{"Title": "LiveCities: Revealing the Pulse of Cities by Location-\nBased Social Networks Venues and Users Analysis", "Abstract": "It would be very difficult even for a resident to characterise\nthe social dynamics of a city and to reveal to foreigners the\nevolving activity patterns which occur in its various areas.\nTo address this problem, however, large amount of data pro-\nduced by location-based social networks (LBSNs) can be\nexploited and combined effectively with techniques of user\nprofiling. The key idea we introduce in this demo is to im-\nprove city areas and venues classification using semantics\nextracted both from places and from the online profiles of\npeople who frequent those places. We present the results\nof our methodology in LiveCities 1 , a web application which\nshows the hidden character of several italian cities through\nclustering and information visualisations paradigms. In par-\nticular we give in-depth insights of the city of Florence, IT,\nfor which the majority of the data in our dataset have been\ncollected. The system provides personal recommendation\nof areas and venues matching user interests and allows the\nfree exploration of urban social dynamics in terms of people\nlifestyle, business, demographics, transport etc. with the ob-\njective to uncover the real \u2018pulse\u2019 of the city. We conducted a\nqualitative validation through an online questionnaire with\n28 residents of Florence to understand the shared percep-\ntion of city areas by its inhabitants and to check if their\nmental maps align to our results. Our evaluation shows how\nconsidering also contextual semantics like people profiles of\ninterests in venues categorisation can improve clustering al-\ngorithms and give good insights of the endemic characteris-\ntics and behaviours of the detected areas."}
{"Title": "CityBeat: Real-time Social Media Visualization of\nHyper-local City Data", "Abstract": "With the increasing volume of location-annotated content\nfrom various social media platforms like Twitter, Instagram\nand Foursquare, we now have real-time access to people\u2019s\ndaily documentation of local activities, interests and atten-\ntion. In this demo paper, we present CityBeat 1 , a real-time\nvisualization of hyper-local social media content for cities.\nThe main objective of CityBeat is to provide users \u2013 with\na specific focus on journalists \u2013 with information about the\ncity\u2019s ongoings, and alert them to unusual activities. The\nsystem collects a stream of geo-tagged photos as input, uses\ntime series analysis and classification techniques to detect\nhyper-local events, and compute trends and statistics. The\ndemo includes a visualization of this information that is de-\nsigned to be installed on a large-screen in a newsroom, as\nan ambient display."}
{"Title": "Help Yourself : A Virtual Self-Assist Agent", "Abstract": "In this work, we describe an unsupervised framework for\ncreating self-assist systems which can serve as virtual call\ncenter agents to guide the customer in performing differ-\nent domain-dependent tasks (like troubleshooting a problem,\nchanging settings etc.). We describe a framework for cre-\nating an intent graph from a corpus of knowledge articles\nfrom a given domain which is used in creating the dialogue\nsystem. To the best of our knowledge, this is the first work\nin creating virtual self-assist agents."}
{"Title": "Exploring the Web of Spatial Data with Facete", "Abstract": "The majority of data (including data published on the Web\nas Linked Open Data) has a spatial dimension. However,\nthe efficient, user friendly exploration of spatial data re-\nmains a major challenge. We present Facete, a web-based ex-\nploration and visualization application enabling the spatial-\nfaceted browsing of data with a spatial dimension. Facete\nimplements a novel spatial data exploration paradigm based\non the following three key components: First, a domain inde-\npendent faceted filtering module, which operates directly on\nSPARQL and supports nested facets. Second, an algorithm\nthat efficiently detects spatial information related to those\nresources that satisfy the facet selection. The detected rela-\ntions are used for automatically presenting data on a map.\nAnd third, a workflow for making the map display interact\nwith data sources that contain large amounts of geomet-\nric information. We demonstrate Facete in large-scale, real\nworld application scenarios."}
{"Title": "SocRoutes: Safe Routes Based on Tweet Sentiments", "Abstract": "Location-based services, and in particular personal navigation sys-\ntems, have become increasingly popular with the widespread use\nof GPS technology in smart devices. Existing navigation systems\nare designed to suggest routes based on the shortest distance or the\nfastesttimetoatarget. Inthispaper, weproposeanewtypeofroute\nnavigation based on regional context\u2014primarily sentiments. Our\nsystem, called SocRoutes, aims to find a safer, friendlier, and more\nenjoyable route based on sentiments inferred from real-time, geo-\ntagged messages from Twitter. SocRoutes tailors routes by avoid-\ning places with extremely negative sentiments, thereby potentially\nfinding a safer and more enjoyable route with marginal increase in\ntotal distance compared to the shortest path. The system supports\nthree types of traveling modes: walking, bicycling, and driving.\nWe validated the idea based on crime history data from the City of\nChicago Portal in December 2012, and sentiments extracted from\ngeotagged tweets during the same time. We discovered that there\nwas a significant correlation between regional Twitter posting sen-\ntiments and crime rate, in particular for high-crime and highly neg-\native sentiment areas. We also demonstrated that SocRoutes, by\nsolely utilizing social media sentiments, can find routes that bypass\ncrime hotspots."}
{"Title": "Scalability and Efficiency Challenges in\nLarge-Scale Web Search Engines", "Abstract": "The main goals of a web search engine are quality, e?ciency,\nand scalability. In this tutorial, we focus on the last two\ngoals, providing a fairly comprehensive overview of the scal-\nability and e?ciency challenges in large-scale web search en-\ngines. In particular, the tutorial provides an in-depth archi-\ntectural overview of a web search engine, mainly focusing\non the web crawling, indexing, and query processing com-\nponents. The scalability and e?ciency issues encountered in\nthese components are presented at four di\u21b5erent granulari-\nties: at the level of a single computer, a cluster of computers,\na single data center, and a multi-center search engine. The\ntutorial also points at open research problems and provides\nrecommendations to researchers who are new to the field."}
{"Title": "E-commerce Product Search: Personalization,\nDiversification, and beyond", "Abstract": "In this tutorial we discuss challenges, techniques and analytics in\nsearchrankingparticularlyappliedtoproductsearchine-commerce.\nSeveral challenges appear in this context, both from a research as\nwell as an application standpoint. We present various approaches\nadopted in the industry, review well-known research techniques\ndeveloped over the last decade, draw parallels to traditional web\nsearch highlighting the new challenges in this setting, and dig deep\ninto some of the algorithmic and technical approaches developed.\nA specific approach that advances theoretical techniques and il-\nlustrates practical impact considered here is of identifying most\nsuited results quickly from a large database. Settings span cold\nstart users and advanced users for whom personalization is possi-\nble. In this context, top-k and skylines are discussed as they form\na key approach that spans the web, data mining, and database com-\nmunities. These present powerful tools for search across multi-\ndimensional items with clear preferences within each attribute, like\nproduct search as opposed to regular web search."}
{"Title": "Towards a Social Media Analytics Platform: Event\nDetection and User Profiling for Twitter", "Abstract": "Microblog data differs significantly from the traditional text data\nwith respect to a variety of dimensions. Microblog data contains\nshort documents, SMS kind of language, and is full of code mix-\ning. Though a lot of it is mere social babble, it also contains fresh\nnews coming from human sensors at a humungous rate. Given such\ninteresting characteristics, the world wide web community has wit-\nnessed a large number of research tasks for microblogging plat-\nforms recently. Event detection on Twitter is one of the most pop-\nular such tasks with a large number of applications. The proposed\ntutorial on social analytics for Twitter will contain three parts. In\nthe first part, we will discuss research efforts towards detection of\neventsfromTwitterusingboththetweetcontentaswellasotherex-\nternal sources. We will also discuss various applications for which\nevent detection mechanisms have been put to use. Merely detecting\nevents is not enough. Applications require that the detector must be\nable to provide a good description of the event as well. In the sec-\nond part, we will focus on describing events using the best phrase,\nevent type, event timespan, and credibility. In the third part, we will\ndiscuss user profiling for Twitter with a special focus on user loca-\ntion prediction. We will conclude with a summary and thoughts on\nfuture directions."}
{"Title": "Tutorial on Social Recommender Systems", "Abstract": "In recent years, with the proliferation of the social web, users are\nexposed to an intensively growing social overload. Social\nrecommender systems aim to address this overload and are\nbecoming integral part of virtually any leading website, playing a\nkey factor in its success. In this tutorial, we will review the broad\ndomain of social recommender systems, the underlying techniques\nand methodologies; the data in use, recommended entities, and\ntarget population; evaluation techniques; applications; and open\nissues and challenges."}
{"Title": "The Mobile Semantic Web", "Abstract": "The combination of the versatility of smart devices and the\ncapabilities of semantic technologies forms a great founda-\ntion for a mobile Semantic Web that will contribute to fur-\nther realising the true potential of both disciplines. Mo-\ntivated by a service discovery and matchmaking example,\nthis tutorial provides an overview of background knowledge\nin ontology languages, basic reasoning problems, and how\nthey are applicable in the mobile environment. It aims at\npresenting a timely survey of state-of-the-art development\nand challenges on mobile ontology reasoning, focusing on\nthe reasoning and optimization techniques developed in the\nmTableaux framework. Finally, the tutorial closes with a\nsummary of important research problems and an outlook of\nfuture research directions in this area."}
{"Title": "Social Spam, Campaigns, Misinformation and\nCrowdturfing", "Abstract": "This tutorial will introduce peer-reviewed research work on\ninformation quality on social systems. Specifically, we will\naddress new threats such as social spam, campaigns, mis-\ninformation and crowdturfing, and overview modern tech-\nniques to improve information quality by revealing and de-\ntecting malicious participants (e.g., social spammers, con-\ntent polluters and crowdturfers) and low quality contents."}
{"Title": "Entity Resolution in the Web of Data", "Abstract": "This tutorial provides an overview of the key research results\nin the area of entity resolution that are relevant to addressing\nthe new challenges in entity resolution posed by the Web of\ndata, in which real world entities are described by interlinked\ndata rather than documents. Since such descriptions are\nusually partial, overlapping and sometimes evolving, entity\nresolution emerges as a central problem both to increase\ndataset linking but also to search the Web of data for entities\nand their relations."}
{"Title": "Trust in Social Computing", "Abstract": "The rapid development of social media exacerbates the infor-\nmation overload and credibility problems. Trust, providing\ninformation about with whom we can trust to share infor-\nmation and from whom we can accept information, plays an\nimportant role in helping users collect relevant and reliable\ninformation in social media. Trust has become a research\ntopic of increasing importance and of practical significance.\nIn this tutorial, we illustrate properties and representation\nmodels of trust, elucidate trust measurements with represen-\ntative algorithms, and demonstrate real-world applications\nwhere trust is explicitly used. As a new dimension of the\ntrust study, we discuss the concept of distrust and its roles\nin trust measurements and applications."}
{"Title": "How Effectively Can We Form Opinions?", "Abstract": "People make decisions and express their opinions according\nto their communities. An appropriate idea for controlling\nthe diffusion of an opinion is to find influential people, and\nemploy them to spread the desired opinion. We investigate\nan influencing problem when individuals\u2019 opinions are af-\nfected by their friends due to the model of Friedkin and\nJohnsen [4]. Our goal is to design efficient algorithms for\nfinding opinion leaders such that changing their opinions\nhas great impact on the overall opinion of the society.\nWe define a set of problems like maximizing the sum of in-\ndividual opinions or maximizing the number of individuals\nwhose opinions are above a threshold. We discuss the com-\nplexity of the defined problems and design optimum algo-\nrithms for the non NP-hard variants of the problems. Fur-\nthermore, we run simulations on real-world social network\ndata and show our proposed algorithm outperforms the clas-\nsical algorithms such as degree-based, closeness-based, and\npagerank-based algorithms."}
{"Title": "ComPAS: Maximizing Data Availability with Replication in\nAd-hoc Social Networks", "Abstract": "Although existing replica allocation protocols perform well\nin most cases, some challenges still need to be addressed to\nfurther improve their performance. The success of such pro-\ntocols for Ad-hoc Social Networks (ASNETs) depends on\nthe performance of data accessibility and on the easy con-\nsistency management of available replica. We contribute to\nthis line of research with replication protocol for maximiz-\ning availability of a data. Essentially, we propose ComPAS,\na community-partitioning aware replica allocation method.\nIts goals include integration of social relationship for plac-\ning copy of the data in the community to achieve better\nefficiency and consistency by keeping the replica read cost,\nrelocation cost and traffic as low as possible."}
{"Title": "Discovering and Learning\nSensational Episodes of News Events", "Abstract": "This paper studies the problem of discovering and learning sen-\nsational 2-episodes, i.e., pairs of co-occurring news events. To\nfind all frequent episodes, we propose an efficient algorithm, MEE-\nLO, which significantly outperforms conventional methods. Given\nmany frequent episodes, we rank them by their sensational effect.\nInstead of limiting ourselves to any individual subjective measure\nof sensational effect, we propose a learning-to-rank approach that\nexploits multiple features to capture the sensational effect of an\nepisode from various aspects. An experimental study on real data\nverified our approach\u2019s efficiency and effectiveness."}
{"Title": "Towards Semantic Faceted Search", "Abstract": "In this paper we present limitations of conventional faceted search\nin the way data, facets, and queries are modelled. We discuss how\nthese limitations can be addressed with Semantic Web technologies\nsuch as RDF, OWL 2, and SPARQL 1.1. We also present a system,\nSemFacet, that is a proof-of-concept prototype of our approach im-\nplemented on top of Yago knowledge base, powered by the OWL 2\nRL triple store RDFox, and the full text search engine Lucene."}
{"Title": "Metadata-Driven Hypertext Content Publishing and Styling", "Abstract": "A growing number of approaches and tools have been utilised\nattempting at generating hypertext content with embedded\nmetadata. However, little work has been carried out on find-\ning a generic solution for publishing and styling Web pages\nwith annotations derived from existing RDF data sets avail-\nable in various formats. This paper proposes a metadata-\ndriven publishing framework assisting publishers or web-\nmasters in generating semantically-enriched content (HTML\npages or snippets) by harnessing distributed RDF(a) docu-\nments or repositories with little human intervention. This\nframework also helps users to create and share so-called\nmicro-themes, which is applicable to the above generated\ncontent for the purpose of page styling and also highly reusa-\nble thanks to the adopted semantic attribute selectors."}
{"Title": "Collective Attention to Social Media\nEvolves According to Diffusion Models", "Abstract": "We investigate patterns of adoption of 175 social media ser-\nvices and Web businesses using data from Google Trends.\nFor each service, we collect aggregated search frequencies\nfrom 45 countries as well as global averages. This results in\nmore than 8.000 time series which we analyze using economic\ndiffusion models. The models are found to provide accurate\nand statistically significant fits to the data and show that\ncollective attention to social media grows and subsides in a\nhighly regular manner. Regularities persist across regions,\ncultures, and topics and thus hint at general mechanisms\nthat govern the adoption of Web-based services."}
{"Title": "Acquiring Commonsense Knowledge for Sentiment\nAnalysis using Human Computation", "Abstract": "Many Artificial Intelligence tasks need commonsense knowledge.\nExtracting this knowledge with statistical methods would require\nhuge amounts of data, so human computation offers a better alter-\nnative. We acquire contextual knowledge for sentiment analysis\nby asking workers to indicate the contexts that influence the po-\nlarities of sentiment words. The increased complexity of the task\ncauses some workers to give superficial answers. To increase moti-\nvation, we make the task more engaging by packaging it as a game.\nWith the knowledge compiled from only a small set of answers, we\nalready halve the gap between machine and human performance.\nThis proves the strong potential of human computation for acquir-\ning commonsense knowledge."}
{"Title": "BUbiNG: Massive Crawling for the Masses", "Abstract": "Although web crawlers have been around for twenty years\nby now, there is virtually no freely available, open-source\ncrawling software that guarantees high throughput, over-\ncomes the limits of single-machine tools and at the same\ntime scales linearly with the amount of resources available.\nThis paper aims at filling this gap."}
{"Title": "Status and Friendship:\nMechanisms of Social Network Evolution", "Abstract": "We examine the evolution of five social networking sites where\ncomplexnetworksofsocialrelationshipsdeveloped: Twitter, Flickr,\nDeviantArt, Delicious, and Yahoo! Answers. We study the differ-\nences and similarities in edge creation mechanisms in these social\nnetworks. We find large differences in edge reciprocation rates and\noverall structure of the underlying networks. We demonstrate that\ntwo mechanisms can explain these disparities: directed triadic clo-\nsure, which leads to networks that show characteristics of status-\noriented behavior, and reciprocation, which leads to friendship-\noriented behavior. We develop a model that demonstrates how vari-\nances in these mechanisms lead to characteristic differences in the\nexpression of network subgraph motifs. Lastly, we show how a\nuser\u2019s future popularity, her indegree, can be predicted based on\nher initial edge creation behavior."}
{"Title": "A Wiki Way of Programming for the Web of Data", "Abstract": "WikiNEXT is a wiki engine that enables users to write rapidly\napplications directly from the browser, in particular applications\nthat can exploit the web of data. WikiNEXT relies on semantic\nweb formalisms and technologies (RDF/RDFa lite) to describe\nwiki page content and embedded metadata, and to manipulate\nthem (for example, using the SPARQL language). WikiNEXT is a\nmix between a web-based IDE (Integrated Development\nEnvironment) and a semantic wiki. It embeds several editors (a\nWYSIWYG editor, and an HTML/JavaScript editor + a\nJavaScript library manager) for coding in the browser, provides\nan API for exploiting semantic metadata, and uses a graph based\ndata store and an object oriented database for persistence on the\nserver side. It has been specially designed for writing online\nprogramming tutorials (i.e. an HTML5 tutorial, a semantic web\ntutorial on how to consume linked data, etc.), or more generally\nfor developing web applications that can be mixed with more\nclassical wiki documents (in fact all WikiNEXT pages are web\napplications). The tool is online 1 , open source 2 ; screencasts are\navailable on YouTube (look for \u201cWikiNEXT\u201d)."}
{"Title": "The (un)supervised Detection of Overlapping Communities\nas Well as Hubs and Outliers via (Bayesian) NMF", "Abstract": "The detection of communities in various networks has been\nconsidered by many researchers. Moreover, it is preferable\nfor a community detection method to detect hubs and out-\nliers as well. This becomes even more interesting and chal-\nlenging when taking the unsupervised assumption, that is,\nwe do not assume the prior knowledge of the number K of\ncommunities. In this poster, we define a novel model to\nidentify overlapping communities as well as hubs and out-\nliers. When K is given, we propose a normalized symmetric\nnonnegative matrix factorization algorithm to learn the pa-\nrameters of the model. Otherwise, we introduce a Bayesian\nsymmetric nonnegative matrix factorization to learn the pa-\nrameters of the model, while determining K.Our experiment\nindicates its superior performance on various networks."}
{"Title": "Recommendation for Advertising Messages\non Mobile Devices", "Abstract": "Mobile devices, especially smart phones, have been popular\nin recent years. With users spending much time on mo-\nbile devices, service providers deliver advertising messages\nto mobile device users and look forward to increasing their\nrevenue. However, delivery of proper advertising messages\nis challenging since strategies of advertising in TV, SMS,\nor website may not be applied to the banner-based adver-\ntising on mobile devices. In this work, we study how to\nproperly recommend advertising messages for mobile device\nusers. We propose a novel approach which simultaneously\nconsiders several important factors: user profile, apps used,\nand clicking history. We apply experiments on real-world\nmobile log data, and the results demonstrate the effective-\nness of the proposed approach."}
{"Title": "A Pruning Algorithm for Optimal Diversified Search", "Abstract": "Given a number of possible sub-intents (also called subtopics) for\na certain query and their corresponding search results, diversified\nsearch aims to return a single result list that could satisfy as many\nusers' intents as possible. Previous studies have demonstrated that\nfinding the optimal solution for diversified search is NP-hard.\nTherefore, several algorithms have been proposed to obtain a\nlocal optimal ranking with greedy approximations. In this paper, a\npruned exhaustive search algorithm is proposed to decrease the\ncomplexity of the optimal search for the diversified search prob-\nlem. Experimental results indicate that the proposed algorithm can\ndecrease the computation complexity of exhaustive search without\nany performance loss."}
{"Title": "Sentiment-Enhanced Explanation of Product\nRecommendations", "Abstract": "Because of the important role of product reviews during users\u2019\ndecision process, we propose a novel explanation interface that\nparticularly fuses the feature sentiments as extracted from reviews\ninto explaining recommendations. Besides, it can explain multiple\nitems altogether by revealing their similarity in respect of feature\nsentiments as well as static specifications, so as to support users\u2019\ntradeoff making. Relative to existing works, we believe that this\ninterface can be more effective, trustworthy, and persuasive."}
{"Title": "Finding Local Experts on Twitter", "Abstract": "We address the problem of identifying local experts on Twitter.\nSpecifically, we propose a local expertise framework that integrates\nboth users\u2019 topical expertise and their local authority by leverag-\ning over 15 million geo-tagged Twitter lists. We evaluate the pro-\nposed approach across 16 queries coupled with over 2,000 indi-\nvidual judgments from Amazon Mechanical Turk. Our initial ex-\nperiments find significant improvement over a naive local expert\nfinding approach, suggesting the promise of exploiting geo-tagged\nTwitter lists for local expert finding."}
{"Title": "Adaptive Presentation of Linked Data on Mobile", "Abstract": "We present PRISSMA, a context-aware presentation layer\nfor Linked Data. PRISSMA extends the Fresnel vocabu-\nlary with the notion of mobile context. Besides, it includes\nan algorithm that determines whether the sensed context is\ncompatible with some context declarations."}
{"Title": "Recommending without Short Head", "Abstract": "We discuss a comprehensive study exploring the impact of\nrecommender systems when recommendations are forced to omit\npopular items (short head) and to use niche products only (long\ntail). This is an interesting issue in domains, such as e-tourism,\nwhere product availability is constrained, \u201cbest sellers\u201d most\npopular items are the first ones to be consumed, and the short\nhead may eventually become unavailable for recommendation\npurposes. Our work provides evidence that the effects resulting\nfrom item consumption may increase the utility of personalized\nrecommendations."}
{"Title": "The \u201cExpression Gap\u201d: Do you Like what you Share?", "Abstract": "Whilerecommendationprofilesincreasinglyleveragesocialactions\nsuch as \u201cshares\u201d, the predictive significance of such actions is un-\nclear. To what extent do public shares correlate with other on-\nline behaviors such as searches, views and purchases? Based on\nan analysis of 950,000 users\u2019 behavioral, transactional, and social\nsharing data on a global online commerce platform, we show that\nsocial \u201cshares\u201d, or publicly posted expressions of interest do not\ncorrelatewithnon-publicbehaviorssuchasviewsandpurchases. A\nkey takeaway is that there is a \u201cgap\u201d between public and non-public\nactions online, suggesting that marketers and advertisers need to be\ncautious in their estimation of the significance of social sharing."}
{"Title": "RDF Mapping Rules Refinements\naccording to Data Consumers\u2019 Feedback", "Abstract": "The missing feedback loop is considered the reason for bro-\nken Data Cycles on current Linked Open Data ecosystems.\nRead-Write platforms are proposed, but they are restricted\nto capture modifications after the data is released as Linked\nData. Triggering though a new iteration results in loosing\nthe data consumers\u2019 modifications, as a new version of the\nsource data is mapped, overwriting the currently published.\nWe propose a prime solution that interprets the data con-\nsumers\u2019 feedback to update the mapping rules. This way,\ndata publishers initiate a new iteration of the Data Cycle\nconsidering the data consumers\u2019 feedback when they map a\nnew version of the published data."}
{"Title": "How Social is Social Tagging?", "Abstract": "Social tagging systems have established themselves as an\nimportant part in today\u2019s web and have attracted the inter-\nest of our research community in a variety of investigations.\nThis has led to several assumptions about tagging, such as\nthat tagging systems exhibit a social component. In this\nwork we overcome the previous absence of data for testing\nsuch an assumption. We thoroughly study social interac-\ntion, leveraging for the first time live log data gathered from\nthe real-world public social tagging system BibSonomy. Our\nresults indicate that sharing of resources constitutes an im-\nportant and indeed social aspect of tagging."}
{"Title": "Who am I on Twitter? A Cross-Country Comparison", "Abstract": "Users often manage which aspects of their personal identities to\nbe manifested on social network sites (SNS). Thus, the content of\npersonal information disclosed on users\u2019 profiles can be\ninfluenced by a number of factors, such as motivation of using\nSNS and privacy concerns, both of which may vary depending on\nwhere users reside in. In this study, we compared the content of\n2800 United States (US) and Singapore (SG) Twitter users\u2019 bios\non their profile pages. We found US Twitter users were far more\nlikely to disclose personal information that may reveal their true\nidentity than SG users. The between country difference remained\nafter we took bio length and user activity level into account. The\nresults provide important insights on future studies to understand\nusers\u2019 privacy concern in different regions of the world."}
{"Title": "DBLP-Filter: Effectively Search on the DBLP Bibliography", "Abstract": "DBLP is a well-known online computer science bibliography.\nAs nearly all important journals and conferences on computer\nscience are tracked in DBLP, how to effectively search DBLP\nrecords has become a valuable topic for the computer science\ncommunity. In this paper we present DBLP-Filter, a new DBLP\nsearch tool. The major features of DBLP-Filter are: (1) it\nprovides new search options on concepts and literature\nimportance; (2) it can maintain user profiles and can support\nuser-area-aware search; (3) it provides the service of new\nliteratures alert. Compared with the existing DBLP search tools,\nDBLP-Filter is more functional and also shows better\neffectiveness in terms of MAP and F-measure when tested under\na set of randomly-selected queries."}
{"Title": "Perceptron-based Tagging of Query Boundaries for\nChinese Query Segmentation", "Abstract": "Query boundaries carry useful information for query seg-\nmentation, especially when analyzing queries in a language\nwith no space, e.g., Chinese. This paper presents our re-\nsearch on Chinese query segmentation via averaged percep-\ntron to model query boundaries through an L-R tagging\nscheme on a large amount of unlabeled queries. Experimen-\ntal results indicate that query boundaries are very informa-\ntive and they significantly improve supervised Chinese query\nsegmentation when labeled training data is very limited."}
{"Title": "RESTful Open Workflows for Data Provenance and Reuse", "Abstract": "In this paper, we present a workflow model together with\nan implementation following the Linked Data principles and\nthe principles for RESTful web services. By means of RDF-\nbased specifications of web services, workflows, and runtime\ninformation, we establish a full provenance chain for all re-\nsources created within these workflows."}
{"Title": "De-anonymizing Social Graphs via Node Similarity", "Abstract": "Recently, a number of anonymization algorithms have been\ndeveloped to protect the privacy of social graph data. How-\never, in order to satisfy higher level of privacy requirements,\nit is sometimes impossible to maintain sufficient utility. Is\nit really easy to de-anonymize \u201clightly\u201d anonymized social\ngraphs? Here \u201clight\u201d anonymization algorithms stand for\nthose algorithms that maintain higher data utility. To an-\nswer this question, we proposed a de-anonymization algo-\nrithm based on a node similarity measurement. Using the\nproposed algorithm, we evaluated the privacy risk of several\n\u201clight\u201d anonymization algorithms on real datasets."}
{"Title": "Contextual Insights", "Abstract": "In today\u2019s productivity environment, users are constantly\nresearching topics while consuming or authoring content in\napplications such as e-readers, word processors, presentation\nprograms, or social networks. However, none of these appli-\ncations sufficiently enable users to do their research directly\nwithin the application. In fact, users typically have to switch\nto a browser and write a query on a search engine. Switch-\ning to a search engine is distracting and hurts productivity.\nFurthermore, the main problem is that the search engine is\nnot aware of important user context such as the book that\nthey are reading or the document they are authoring. To\ntackle this problem, we introduce the notion of contextual\ninsights: providing users with information that is contex-\ntually relevant to the content that they are consuming or\nauthoring. We then present Leibniz, a system that provides\na solution for the contextual insights problem."}
{"Title": "Partout: A Distributed Engine for Efficient RDF Processing", "Abstract": "The increasing interest in Semantic Web technologies has led\nnot only to a rapid growth of semantic data on the Web but\nalso to an increasing number of backend applications relying\non efficient query processing. Confronted with such a trend,\nexisting centralized state-of-the-art systems for storing RDF\nand processing SPARQL queries are no longer sufficient. In\nthis paper, we introduce Partout, a distributed engine for\nfast RDF processing in a cluster of machines. We propose an\neffective approach for fragmenting RDF data sets based on\na query log and allocating the fragments to hosts in a clus-\nter of machines. Furthermore, Partout\u2019s query optimizer\nproduces efficient query execution plans for ad-hoc SPARQL\nqueries."}
{"Title": "Effective and Effortless Features for Popularity Prediction\nin Microblogging Network", "Abstract": "Predicting popularity of online contents is of remarkable practical\nvalue in various business and administrative applications. Existing\nstudies mainly focus on finding the most effective features for\nprediction. However, some effective features, such as structural\nfeatures which are extracted from the underlying user network, are\nhard to access. In this paper, we aim to identify features that are\nboth effective and effortless (easy to obtain or compute). Experi-\nments on Sina Weibo show the effectiveness and effortlessness of\nthe temporal features and satisfying prediction performance can be\nobtained based on only the temporal features of first 10 retweets."}
{"Title": "A Topic based Document Relevance Ranking Model", "Abstract": "Topic modelling has been widely used in the fields of in-\nformation retrieval, text mining, machine learning, etc. In\nthis paper, we propose a novel model, Pattern Enhanced\nTopic Model (PETM), which makes improvements to topic\nmodelling by semantically representing topics with discrim-\ninative patterns, and also makes innovative contributions to\ninformation filtering by utilising the proposed PETM to de-\ntermine document relevance based on topics distribution and\nmaximum matched patterns proposed in this paper. Exten-\nsive experiments are conducted to evaluate the effectiveness\nof PETM by using the TREC data collection Reuters Corpus\nVolume 1. The results show that the proposed model signifi-\ncantly outperforms both state-of-the-art term-based models\nand pattern-based models."}
{"Title": "A Semi-supervised Method for Opinion Target Extraction", "Abstract": "This paper proposes a semi-supervised self-learning method,\nwhich is based on a Naive Bayes classifier exploiting context\nfeatures and PMI scores, to extract opinion targets. The\nexperimental results indicate our bootstrapping framework\nis effective for this task and outperforms the state-of-the-art\nmodels on COAE2008 dataset2, especially in precision."}
{"Title": "Localized CAPTCHA Testing on Users and Farms", "Abstract": "The paper describes the experience of resisting the large-scale\nsolving of CAPTCHA through the CAPTCHA-farms and presents\nthe results of experimenting with different types of textual\nCAPTCHA on the farm worker and casual user crowds.\nLocalization of CAPTCHA led to cutting twice the absolute\nvolume of CAPTCHA parsing, but introducing the semantics into\nthe test complicated it to casual users and was not found\npromising."}
{"Title": "Allocating Tasks to Workers with Matching Constraints:\nTruthful Mechanisms for Crowdsourcing Markets", "Abstract": "Designing optimal pricing policies and mechanisms for allo-\ncating tasks to workers is central to the online crowdsourc-\ning markets. In this paper, we consider the following re-\nalistic setting of online crowdsourcing markets \u2013 there is a\nrequester with a limited budget and a heterogeneous set of\ntasks each requiring certain skills; there is a pool of workers\nand each worker has certain expertise and interests which\ndefine the set of tasks she can and is willing to do. Under\nthe matching constraints given by this bipartite graph be-\ntween workers and tasks, we design our incentive-compatible\nmechanism TM-Uniform which allocates the tasks to the\nworkers, while ensuring budget feasibility and achieves near-\noptimal utility for the requester. Apart from strong theoret-\nical guarantees, we carry out experiments on a realistic case\nstudy of Wikipedia translation project on Mechanical Turk.\nWe note that this is the first paper to address this setting\nfrom a mechanism design perspective."}
{"Title": "People of Opposing Views can Share Common Interests", "Abstract": "In online social networks, people tend to connect with like-\nminded people and read agreeable information. Direct rec-\nommendation of challenging content has not worked well\nbecause users do not value diversity and avoid challenging\ncontent. In this poster, we investigate the possibility of an\nindirect approach by introducing intermediary topics, which\nare topics that are common to people having opposing views\non sensitive issues, i.e., those issues that tend to divide people.\nThrough a case study about a sensitive issue discussed in\nTwitter, we show that such intermediary topics exist, open-\ning a path for future work in recommendation promoting\ndiversity of content to be shared."}
{"Title": "Generating Ad Targeting Rules using Sparse Principal\nComponent Analysis with Constraints", "Abstract": "Determining the right audience for an advertising campaign\nis a well-established problem, of central importance to many\nInternet companies. Two distinct targeting approaches ex-\nist, the model-based approach, which leverages machine learn-\ning, and the rule-based approach, which relies on manual\ngeneration of targeting rules. Common rules include identi-\nfying users that had interactions (website visits, emails re-\nceived, etc.) with the companies related to the advertiser, or\nsearch queries related to their product. We consider a prob-\nlem of discovering such rules from data using Constrained\nSparse PCA. The constraints are put in place to account for\ncases when evidence in data suggests a relation that is not\nappropriate for advertising. Experiments on real-world data\nindicate the potential of the proposed approach."}
{"Title": "Cross Market Modeling for Query-Entity Matching", "Abstract": "Given a query, the query-entity (QE) matching task involves iden-\ntifying the best matching entity for the query. When modeling this\ntaskasabinaryclassificationproblem, twoissuesarise: (1)features\nin specific global markets (like de-at: German users in Austria)\nare quite sparse compared to other markets like en-us, and (2) train-\ning data is expensive to obtain in multiple markets and hence lim-\nited. Can we leverage some form of cross market data/features for\neffective query-entity matching in sparse markets? Our solution\nconsists of three main modules: (1) Cross Market Training Data\nLeverage (CMTDL) (2) Cross Market Feature Leverage (CMFL),\nand (3) Cross Market Output Data Leverage (CMODL). Each of\nthese parts perform \u201csignal\u201d sharing at different points during the\nclassification process. Using a combination of these strategies, we\nshow significant improvements in query-impression weighted cov-\nerage for the query-entity matching task."}
{"Title": "Dynamic Provenance for SPARQL Updates using Named\nGraphs", "Abstract": "While the (Semantic) Web currently does have a way to\nexhibit static provenance information in the W3C PROV\nstandards, the Web does not have a way to describe dy-\nnamic changes to data. While some provenance models and\nannotation techniques originally developed with databases\nor workflows in mind transfer readily to RDF, RDFS and\nSPARQL, these techniques do not readily adapt to describ-\ning changes in dynamic RDF datasets over time. In this pa-\nper we explore how to adapt the dynamic copy-paste prove-\nnance model of Buneman et al. [1] to RDF datasets that\nchange over time in response to SPARQL updates, how\nto represent the resulting provenance records themselves as\nRDF using named graphs in a manner compatible with W3C\nPROV, and how the provenance information can be provided\nas a SPARQL query. The primary contribution is a semantic\nframework that enables the semantics of SPARQL Update to\nbe used as the basis for a \u2018cut-and-paste\u2019 provenance model\nin a principled manner."}
{"Title": "Characterizing User Interest Using Heterogeneous Media", "Abstract": "It is often hard to accurately estimate interests of social\nmedia users because their messages do not have additional\ninformation, such as a category. In this paper, we propose\nan approach that estimates user interest from social media\nto provide personalized services. Our approach employs het-\nerogeneous media to map social media onto categories. To\ndescribe the categories, we propose a hybrid method that\nintegrates a topic model with TF-ICF for extracting both\nexplicitly presented and implicitly latent features. Our eval-\nuation result shows that it gives the highest performance,\ncompared to other approaches. Thus, we expect that the\nproposed approach is helpful in advancing personalization\nof social media services."}
{"Title": "Bing-SF-IDF+: Semantics-Driven News Recommendation", "Abstract": "Content-based news recommendation is traditionally per-\nformed using the cosine similarity and TF-IDF weighting\nscheme for terms occurring in news messages and user pro-\nfiles. Semantics-driven variants such as SF-IDF additionally\ntake into account term meaning by exploiting synsets from\nsemantic lexicons. However, they ignore the various seman-\ntic relationships between synsets, providing only for a lim-\nited understanding of news semantics. Moreover, semantics-\nbased weighting techniques are not able to handle \u2013 often\ncrucial \u2013 named entities, which are usually not present in\nsemantic lexicons. Hence, we extend SF-IDF by also con-\nsidering the synset semantic relationships, and by employ-\ning named entity similarities using Bing page counts. Our\nproposed method, Bing-SF-IDF+, outperforms TF-IDF and\nSF-IDF in terms of F 1 scores and kappa statistics."}
{"Title": "Inferring Social Relationships from Mobile Sensor Data", "Abstract": "While mobile sensors are ubiquitous nowadays, the geographical\nactivities of human beings are feasible to be collected and the geo-\nspatial interactions between people can be derived. As we know\nthere is an underlying social network between mobile users, such\nsocial relationships are hidden and hold by service providers.\nAcquiring the social network over mobile users would enable lots\nof applications, such as friend recommendation and energy-saving\nmobile DB management. In this paper, we propose to infer the\nsocial relationships using the sensor data, which contains the\nencounter records between individuals, without any knowledge\nabout the real friendships in prior. We propose a two-phase\nprediction method for the social inference. Experiments conducted\non the CRAWDAD data demonstrate the encouraging results with\nsatisfying prediction scores of precision and recall."}
{"Title": "Inferring Visiting Time Distributions of Locations\nfrom Incomplete Check-in Data", "Abstract": "Online location-based services, such as Foursquare and Facebook,\nprovide a great resource for location recommendation. As we\nknow the time is one of the important factors on recommending\nplaces with proper time for users, since the pleasure of visiting a\nplace could be diminished if arriving at wrong time, we propose to\ninfer the visiting time distributions of locations. We assume the\ncheck-in data used is incomplete because in real-world scenarios it\nis hard or unavailable to collect all the temporal information of\nlocations and the check-in behaviors might be abnormal. To tackle\nsuch problem, we devise a visiting time inference framework,\nVisTime-Miner, which considers the route-based visiting correla-\ntion of time labels to model the visiting behavior of a location.\nExperiments on a large-scaled Gowalla check-in data show a\npromising result."}
{"Title": "Deriving Latent Social Impulses to Determine Longevous\nVideos", "Abstract": "Online video websites receive huge amount of videos daily from\nusers all around the world. How to provide valuable recommenda-\ntion of videos to viewers is important for video websites. Previous\nstudies focus on analyzing the view count of a video, which mea-\nsures the video\u2019s value in terms of popularity. However, the long-\nlasting value of an online video, namely longevity, is hidden behind\nthe history that a video accumulates its \u201cpopularity\u201d through time.\nGenerally speaking, a longevous video tends to constantly draw\nsociety\u2019s attention. With a focus on Youtube, this paper proposes a\nscoring mechanism to quantify the longevity of videos. We intro-\nduce the concept of latent social impulses and use them to assess a\nvideo\u2019s longevity. In order to derive latent social impulses, we view\nthe video website as a digital signal filter and formulate the task as\na convex minimization problem. The proposed longevity compu-\ntation is based on the derived social impulses. Unfortunately, the\nrequired information to derive social impulses is not always pub-\nlic, which disallows a third party to directly evaluate the longevity\nof all videos. Thus, we formulate a semi-supervised learning task\nby using videos of which the longevity scores are known to predict\nthe unknown ones. We develop a Gaussian Random Markov Field\nmodel with Loopy Belief Propagation to solve it. The experiments\non Youtube demonstrate that the proposed method significantly im-\nproves the prediction results comparing to two baseline models."}
{"Title": "Data Imputation Using a Trust Network for\nRecommendation", "Abstract": "Recommendation methods suffer from the data sparsity and\ncold-start user problems, often resulting in low accuracy.\nTo address these problems, we propose a novel imputation\nmethod, which effectively densifies a rating matrix by filling\nunevaluated ratings with probable values. In our method,\nwe use a trust network to estimate the unevaluated ratings\naccurately. We conduct experiments on the Epinions dataset\nand demonstrate that our method helps provide better rec-\nommendation accuracy than previous methods, especially\nfor cold-start users."}
{"Title": "Large Graph Mining:\nPatterns, Cascades, Fraud Detection, and Algorithms", "Abstract": "Given a large graph, like who-calls-whom, or who-likes-whom,\nwhat behavior is normal and what should be surprising, possibly\ndue to fraudulent activity? How do graphs evolve over time?\nHow does influence/news/viruses propagate, over time? We\nfocus on three topics: (a) anomaly detection in large static graphs\n(b) patterns and anomalies in large time-evolving graphs and (c)\ncascades and immunization.\nFor the first, we present a list of static and temporal laws,\nincluding advances patterns like 'eigenspokes'; we show how to\nuse them to spot suspicious activities, in on-line buyer-and-seller\nsettings, in FaceBook, in twitter-like networks. For the second,\nwe show how to handle time-evolving graphs as tensors, how to\nhandle large tensors in map-reduce environments, as well as some\ndiscoveries such settings.\nFor the third, we show that for virus propagation, a single number\nis enough to characterize the connectivity of graph, and thus we\nshow how to do efficient immunization for almost any type of\nvirus (SIS - no immunity; SIR - lifetime immunity; etc).\nWe conclude with some open research questions for graph\nmining."}
{"Title": "Taming the Web", "Abstract": "The World Wide Web (WWW) has become an indispensable part\nof the modern life, providing many benefits in diverse ways. For\ninstance, the huge amount of information from the web offers\npeople unprecedented levels of opportunities for education,\nentertainments, social activities, productivity improvements, and\nbusiness. The web, however, has also become perilous with many\ndangers, such as privacy violation and security breaches, and\ntherein exist many villains who would like to turn into victims\nscrupulous as well as casual users. In this regard, WWW has\nbecome almost like Wild Wild West, where wonderful\nopportunities and great perils co-existed.\nTizen (www.tizen.org) is a web-centric open-source, standards-\nbased software platform for smart devices, such as smartphones,\nsmart TVs, IVI (In-Vehicle Infotainment) and other consumer\ndevices like cameras, printers, and more. Tizen is web-centric in\nthat it directly supports web apps - applications (apps) written in\nHTML5 and Javascript - even outside the web-browsers and\nprovides seamless supports for the web. As such, Tizen not only\nshares the benefits and perils of the web with other platforms, but\nalso has the additional burden to meet the performance of non-\nweb platforms: platforms that directly support only conventional\nprogramming languages. In this talk, we present Tizen\u2019s\napproaches to taming the web to maximize its benefits while\nminimizing the risks of its perils. We also describe various\noptimizations  of  Tizen  that  enable  delivering  web-app\nperformance on par with that of non-web platforms."}
{"Title": "Organizing the Digital World to Empower\nPeople to Do More, Know More, and Be More", "Abstract": "The web is rapidly evolving into the web of the world where\npeople, places, things and their relationships are all digitally\nrepresented. This evolution opens up unparalleled opportunities to\norganize this vast digital universe for even greater human purpose.\nIn this talk, Dr. Lu will share an outline of Microsoft\u2019s quest and\naspiration to organize the digital universe with a pervasive\ncomputational fabric of digital information, digital services, and\ndigital experiences that empower every human being on the planet\nto accomplish more and enrich their life. Dr. Lu will discuss high\nlevel computational structures and present specific examples\nacross Bing, Windows and other products and services to illustrate\nMicrosoft\u2019s approach to delivering end user value and\naccelerating the pace of innovation for the industry as a whole."}
{"Title": "Machine Learning in an Auction Environment", "Abstract": "We consider a model of repeated online auctions in which\nan ad with an uncertain click-through rate faces a random\ndistribution of competing bids in each auction and there is\ndiscounting of payoffs. We formulate the optimal solution\nto this explore/exploit problem as a dynamic programming\nproblem and show that efficiency is maximized by making\na bid for each advertiser equal to the advertiser\u2019s expected\nvalue for the advertising opportunity plus a term propor-\ntional to the variance in this value divided by the number\nof impressions the advertiser has received thus far. We then\nuse this result to illustrate that the value of incorporating\nactive exploration into a machine learning system in an auc-\ntion environment is exceedingly small."}
{"Title": "Advertising in a Stream", "Abstract": "One of the most important innovations of social network-\ning websites is the notion of a \u201cfeed\u201d, a sequence of news\nitems presented to the user as a stream that expands as the\nuser scrolls down. The common method for monetizing such\nstreams is to insert ads in between news items. In this paper,\nwe model this setting, and observe that allocation and pric-\ning of ad insertions in a stream poses interesting algorithmic\nand mechanism design challenges. In particular, we formu-\nlate an optimization problem that captures a typical stream\nad placement setting. We give an approximation algorithm\nfor this problem that provably achieves a value close to the\noptimal, and show how this algorithm can be turned into\nan incentive compatible mechanism. Finally, we conclude\nwith a simple practical algorithm that makes the allocation\ndecisions in an online fashion. We prove this algorithm to\nbe approximately welfare-maximizing and show that it also\nhas good incentive properties."}
{"Title": "The Company You Keep: Mobile Malware Infection Rates\nand Inexpensive Risk Indicators", "Abstract": "There is little information from independent sources in the\npublic domain about mobile malware infection rates. The\nonly previous independent estimate (0.0009%) [11], was based\non indirect measurements obtained from domain-name reso-\nlution traces. In this paper, we present the first independent\nstudy of malware infection rates and associated risk factors\nusing data collected directly from over 55,000 Android de-\nvices. We find that the malware infection rates in Android\ndevices estimated using two malware datasets (0.28% and\n0.26%), though small, are significantly higher than the pre-\nvious independent estimate.\nBased on the hypothesis that some application stores have\na greater density of malicious applications and that adver-\ntising within applications and cross-promotional deals may\nact as infection vectors, we investigate whether the set of\napplications used on a device can serve as an indicator for\ninfection of that device. Our analysis indicates that, while\nnot an accurate indicator of infection by itself, the applica-\ntion set does serve as an inexpensive method for identify-\ning the pool of devices on which more expensive monitoring\nand analysis mechanisms should be deployed. Using our two\nmalware datasets we show that this indicator performs up to\nabout five times better at identifying infected devices than\nthe baseline of random checks. Such indicators can be used,\nfor example, in the search for new or previously undetected\nmalware. It is therefore a technique that can complement\nstandard malware scanning. Our analysis also demonstrates\na marginally significant difference in battery use between\ninfected and clean devices."}
{"Title": "Stranger Danger: Exploring the Ecosystem of Ad-based\nURL Shortening Services", "Abstract": "URL shortening services facilitate the need of exchanging\nlong URLs using limited space, by creating compact URL\naliases that redirect users to the original URLs when fol-\nlowed. Some of these services show advertisements (ads) to\nlink-clicking users and pay a commission of their advertising\nearnings to link-shortening users.\nIn this paper, we investigate the ecosystem of these in-\ncreasingly popular ad-based URL shortening services. Even\nthough traditional URL shortening services have been thor-\noughly investigated in previous research, we argue that, due\nto the monetary incentives and the presence of third-party\nadvertising networks, ad-based URL shortening services and\ntheir users are exposed to more hazards than traditional\nshortening services. By analyzing the services themselves,\nthe advertisers involved, and their users, we uncover a series\nof issues that are actively exploited by malicious advertisers\nand endanger the users. Moreover, next to documenting the\nongoing abuse, we suggest a series of defense mechanisms\nthat services and users can adopt to protect themselves."}
{"Title": "Automatic Detection and Correction of Web Application\nVulnerabilities using Data Mining to Predict False Positives", "Abstract": "Web application security is an important problem in today\u2019s\ninternet. A major cause of this status is that many program-\nmers do not have adequate knowledge about secure coding, so\nthey leave applications with vulnerabilities. An approach to\nsolve this problem is to use source code static analysis to find\nthese bugs, but these tools are known to report many false\npositives that make hard the task of correcting the applica-\ntion. This paper explores the use of a hybrid of methods to\ndetect vulnerabilities with less false positives. After an initial\nstep that uses taint analysis to flag candidate vulnerabilities,\nour approach uses data mining to predict the existence of false\npositives. This approach reaches a trade-off between two ap-\nparently opposite approaches: humans coding the knowledge\nabout vulnerabilities (for taint analysis) versus automatically\nobtaining that knowledge (with machine learning, for data\nmining). Given this more precise form of detection, we do au-\ntomatic code correction by inserting fixes in the source code.\nThe approach was implemented in the WAP tool\n1\nand an ex-\nperimental evaluation was performed with a large set of open\nsource PHP applications."}
{"Title": "Personalized Collaborative Clustering", "Abstract": "Westudytheproblemoflearningpersonalizedusermodelsfromrich\nuser interactions. In particular, we focus on learning from clustering\nfeedback (i.e., grouping recommended items into clusters), which\nenables users to express similarity or redundancy between different\nitems. We propose and study a new machine learning problem for\npersonalization, which we call collaborative clustering. Analogous\nto collaborative filtering, in collaborative clustering the goal is to\nleverage how existing users cluster or group items in order to predict\nsimilarity models for other users\u2019 clustering tasks. We propose a\nsimple yet effective latent factor model to learn the variability of\nsimilarity functions across a user population. We empirically eval-\nuate our approach using data collected from a clustering interface\nwe developed for a goal-oriented data exploration (or sensemaking)\ntask: asking users to explore and organize attractions in Paris. We\nevaluate using several realistic use cases, and show that our approach\nlearns more effective user models than conventional clustering and\nmetric learning approaches."}
{"Title": "Local Collaborative Ranking", "Abstract": "Personalized recommendation systems are used in a wide\nvariety of applications such as electronic commerce, social\nnetworks, web search, and more. Collaborative filtering ap-\nproaches to recommendation systems typically assume that\nthe rating matrix (e.g., movie ratings by viewers) is low-\nrank. In this paper, we examine an alternative approach\nin which the rating matrix is locally low-rank. Concretely,\nwe assume that the rating matrix is low-rank within certain\nneighborhoods of the metric space defined by (user, item)\npairs. We combine a recent approach for local low-rank ap-\nproximation based on the Frobenius norm with a general\nempirical risk minimization for ranking losses. Our exper-\niments indicate that the combination of a mixture of local\nlow-rank matrices each of which was trained to minimize a\nranking loss outperforms many of the currently used state-\nof-the-art recommendation systems. Moreover, our method\nis easy to parallelize, making it a viable approach for large\nscale real-world rank-based recommendation systems."}
{"Title": "CoBaFi \u2014 Collaborative Bayesian Filtering", "Abstract": "Given a large dataset of users\u2019 ratings of movies, what is the\nbest model to accurately predict which movies a person will\nlike? And how can we prevent spammers from tricking our\nalgorithms into suggesting a bad movie? Is it possible to\ninfer structure between movies simultaneously?\nIn this paper we describe a unified Bayesian approach to\nCollaborative Filtering that accomplishes all of these goals.\nIt models the discrete structure of ratings and is flexible\nto the often non-Gaussian shape of the distribution. Addi-\ntionally, our method finds a co-clustering of the users and\nitems, which improves the model\u2019s accuracy and makes the\nmodel robust to fraud. We offer three main contributions:\n(1) We provide a novel model and Gibbs sampling algorithm\nthat accurately models the quirks of real world ratings, such\nas convex ratings distributions. (2) We provide proof of\nour model\u2019s robustness to spam and anomalous behavior.\n(3) We use several real world datasets to demonstrate the\nmodel\u2019s effectiveness in accurately predicting user\u2019s ratings,\navoiding prediction skew in the face of injected spam, and\nfinding interesting patterns in real world ratings data."}
{"Title": "Efficient Estimation for High Similarities\nusing Odd Sketches", "Abstract": "Estimating set similarity is a central problem in many com-\nputer applications. In this paper we introduce the Odd\nSketch, a compact binary sketch for estimating the Jac-\ncard similarity of two sets. The exclusive-or of two sketches\nequals the sketch of the symmetric difference of the two\nsets. This means that Odd Sketches provide a highly space-\nefficient estimator for sets of high similarity, which is rele-\nvant in applications such as web duplicate detection, collab-\norative filtering, and association rule learning. The method\nextends to weighted Jaccard similarity, relevant e.g. for TF-\nIDF vector comparison.\nWe present a theoretical analysis of the quality of estima-\ntion to guarantee the reliability of Odd Sketch-based estima-\ntors. Our experiments confirm this efficiency, and demon-\nstrate the efficiency of Odd Sketches in comparison with\nb-bit minwise hashing schemes on association rule learning\nand web duplicate detection tasks."}
{"Title": "Composite Retrieval of Heterogeneous Web Search", "Abstract": "Traditional search systems generally present a ranked list of docu-\nments as answers to user queries. In aggregated search systems, re-\nsultsfromdifferentandincreasinglydiverseverticals(image, video,\nnews, etc.) are returned to users. For instance, many such search\nengines return to users both images and web documents as answers\nto the query \u201cflower\u201d. Aggregated search has become a very pop-\nular paradigm. In this paper, we go one step further and study a\ndifferent search paradigm: composite retrieval. Rather than return-\ning and merging results from different verticals, as is the case with\naggregated search, we propose to return to users a set of \u201cbundles\u201d,\nwhere a bundle is composed of \u201ccohesive\u201d results from several ver-\nticals. For example, for the query \u201cLondon Olympic\u201d, one bundle\nper sport could be returned, each containing results extracted from\nnews, videos, images, or Wikipedia. Composite retrieval can pro-\nmote exploratory search in a way that helps users understand the\ndiversity of results available for a specific query and decide what\nto explore in more detail. In this paper, we propose and evaluate a\nvariety of approaches to construct bundles that are relevant, cohe-\nsive and diverse. Compared with three baselines (traditional \u201cgen-\neral web only\u201d ranking, federated search ranking and aggregated\nsearch), our evaluation results demonstrate significant performance\nimprovement for a highly heterogeneous web collection."}
{"Title": "Contextual and Dimensional Relevance Judgments for\nReusable SERP-Level Evaluation", "Abstract": "Document-level relevance judgments are a major component in\nthe calculation of effectiveness metrics. Collecting high-quality\njudgments is therefore a critical step in information retrieval\nevaluation. However, the nature of, and the assumptions\nunderlying, relevance judgment collection have not received much\nattention. In particular, relevance judgments are typically\ncollected for each document in isolation, although users read each\ndocument in the context of other documents. In this work, we aim\nto investigate the nature of relevance judgment collection. We\ncollect relevance labels in both isolated and conditional settings,\nand ask for judgments in various dimensions of relevance, as well\nas overall relevance. Then we compare the relevance metrics\nbased on various types of judgments with other metrics of quality\nsuch as User Preference. Our analyses illuminate how these\nsettings for judgment collection affect the quality and the\ncharacteristics of the judgments. We also find that the metrics\nbased on conditional judgments show higher correlation with user\npreference than isolated judgments."}
{"Title": "Community-Based Bayesian Aggregation Models for\nCrowdsourcing", "Abstract": "This paper addresses the problem of extracting accurate la-\nbels from crowdsourced datasets, a key challenge in crowd-\nsourcing. Prior work has focused on modeling the reliabil-\nity of individual workers, for instance, by way of confusion\nmatrices, and using these latent traits to estimate the true\nlabels more accurately. However, this strategy becomes in-\neffective when there are too few labels per worker to reliably\nestimate their quality. To mitigate this issue, we propose a\nnovel community-based Bayesian label aggregation model,\nCommunityBCC, which assumes that crowd workers con-\nform to a few different types, where each type represents\na group of workers with similar confusion matrices. We\nassume that each worker belongs to a certain community,\nwhere the worker\u2019s confusion matrix is similar to (a pertur-\nbation of) the community\u2019s confusion matrix. Our model\ncan then learn a set of key latent features: (i) the confusion\nmatrix of each community, (ii) the community membership\nof each user, and (iii) the aggregated label of each item. We\ncompare the performance of our model against established\naggregation methods on a number of large-scale, real-world\ncrowdsourcing datasets. Our experimental results show that\nour CommunityBCC model consistently outperforms state-\nof-the-art label aggregation methods, gaining, on average,\n8% more accuracy with the same amount of labels."}
{"Title": "Quizz: Targeted Crowdsourcing\nwith a Billion (Potential) Users", "Abstract": "We describe Quizz, a gamified crowdsourcing system that\nsimultaneously assesses the knowledge of users and acquires\nnew knowledge from them. Quizz operates by asking users to\ncomplete short quizzes on specific topics; as a user answers\nthe quiz questions, Quizz estimates the user\u2019s competence.\nTo acquire new knowledge, Quizz also incorporates questions\nfor which we do not have a known answer; the answers given\nby competent users provide useful signals for selecting the\ncorrect answers for these questions. Quizz actively tries\nto identify knowledgeable users on the Internet by running\nadvertising campaigns, effectively leveraging the targeting\ncapabilities of existing, publicly available, ad placement ser-\nvices. Quizz quantifies the contributions of the users using\ninformation theory and sends feedback to the advertising\nsystem about each user. The feedback allows the ad targeting\nmechanism to further optimize ad placement.\nOur experiments, which involve over ten thousand users,\nconfirm that we can crowdsource knowledge curation for\nniche and specialized topics, as the advertising network can\nautomatically identify users with the desired expertise and\ninterest in the given topic. We present controlled experiments\nthat examine the effect of various incentive mechanisms,\nhighlighting the need for having short-term rewards as goals,\nwhich incentivize the users to contribute. Finally, our cost-\nquality analysis indicates that the cost of our approach is\nbelow that of hiring workers through paid-crowdsourcing\nplatforms, while offering the additional advantage of giving\naccess to billions of potential users all over the planet, and\nbeing able to reach users with specialized expertise that is\nnot typically available through existing labor marketplaces."}
{"Title": "The Wisdom of Minority: Discovering and Targeting the\nRight Group of Workers for Crowdsourcing", "Abstract": "Worker reliability is a longstanding issue in crowdsourcing,\nand the automatic discovery of high quality workers is an\nimportant practical problem. Most previous work on this\nproblem mainly focuses on estimating the quality of each\nindividual worker jointly with the true answer of each task.\nHowever, in practice, for some tasks, worker quality could be\nassociated with some explicit characteristics of the worker,\nsuch as education level, major and age. So the following\nquestion arises: how do we automatically discover related\nworker attributes for a given task, and further utilize the\nfindings to improve data quality? In this paper, we propose\na general crowd targeting framework that can automatically\ndiscover, for a given task, if any group of workers based\non their attributes have higher quality on average; and tar-\nget such groups, if they exist, for future work on the same\ntask. Our crowd targeting framework is complementary to\ntraditional worker quality estimation approaches. Further-\nmore, an advantage of our framework is that it is more bud-\nget efficient because we are able to target potentially good\nworkers before they actually do the task. Experiments on\nreal datasets show that the accuracy of final prediction can\nbe improved significantly for the same budget (or even less\nbudget in some cases). Our framework can be applied to\nmany real word tasks and can be easily integrated in cur-\nrent crowdsourcing platforms."}
{"Title": "Monitoring Web Browsing Behavior with Differential\nPrivacy", "Abstract": "Monitoring web browsing behavior has benefited many data min-\ning applications, such as top-K discovery and anomaly detection.\nHowever, releasing private user data to the greater public would\nconcern web users about their privacy, especially after the incident\nof AOL search log release where anonymization was not correctly\ndone. In this paper, we adopt differential privacy, a strong, provable\nprivacy definition, and show that differentially private aggregates\nof web browsing activities can be released in real-time while pre-\nserving the utility of shared data. Our proposed algorithms utilize\nthe rich correlation of the time series of aggregated data and adopt\na state-space approach to estimate the underlying, true aggregates\nfrom the perturbed values by the differential privacy mechanism.\nWe evaluate our algorithms with real-world web browsing data.\nUtility evaluations with three metrics demonstrate that the quality\nof the private, released data by our solutions closely resembles that\nof the original, unperturbed aggregates."}
{"Title": "Quite a Mess in My Cookie Jar!", "Abstract": "Browser-based defenses have recently been advocated as an\neffective mechanism to protect web applications against the\nthreats of session hijacking, fixation, and related attacks.\nIn existing approaches, all such defenses ultimately rely on\nclient-side heuristics to automatically detect cookies con-\ntaining session information, to then protect them against\ntheft or otherwise unintended use. While clearly crucial to\nthe effectiveness of the resulting defense mechanisms, these\nheuristics have not, as yet, undergone any rigorous assess-\nment of their adequacy. In this paper, we conduct the first\nsuch formal assessment, based on a gold set of cookies we col-\nlect from 70 popular websites of the Alexa ranking. To ob-\ntain the gold set, we devise a semi-automatic procedure that\ndraws on a novel notion of authentication token, which we in-\ntroduce to capture multiple web authentication schemes. We\ntest existing browser-based defenses in the literature against\nour gold set, unveiling several pitfalls both in the heuris-\ntics adopted and in the methods used to assess them. We\nthen propose a new detection method based on supervised\nlearning, where our gold set is used to train a binary classi-\nfier, and report on experimental evidence that our method\noutperforms existing proposals. Interestingly, the resulting\nclassification, together with our hands-on experience in the\nconstruction of the gold set, provides new insight on how\nweb authentication is implemented in practice."}
{"Title": "Reconciling Mobile App Privacy and Usability on\nSmartphones: Could User Privacy Profiles Help?", "Abstract": "As they compete for developers, mobile app ecosystems have\nbeen exposing a growing number of APIs through their software\ndevelopment kits. Many of these APIs involve accessing sensitive\nfunctionality and/or user data and require approval by users.\nAndroid for instance allows developers to select from over 130\npossible permissions. Expecting users to review and possibly\nadjust settings related to these permissions has proven unrealistic.\nIn this paper, we report on the results of a study analyzing\npeople\u2019s privacy preferences when it comes to granting\npermissions to different mobile apps. Our results suggest that,\nwhile people\u2019s mobile app privacy preferences are diverse, a\nrelatively small number of profiles can be identified that offer the\npromise of significantly simplifying the decisions mobile users\nhave to make.\nSpecifically, our results are based on the analysis of settings of 4.8\nmillion smartphone users of a mobile security and privacy\nplatform. The platform relies on a rooted version of Android\nwhere users are allowed to choose between \u201cgranting\u201d, \u201cdenying\u201d\nor \u201crequesting to be dynamically prompted\u201d when it comes to\ngranting 12 different Android permissions to mobile apps they\nhave downloaded."}
{"Title": "Random Walks Based Modularity: Application to\nSemi-Supervised Learning", "Abstract": "Although criticized for some of its limitations, modularity\nremains a standard measure for analyzing social networks.\nQuantifying the statistical surprise in the arrangement of\nthe edges of the network has led to simple and powerful\nalgorithms. However, relying solely on the distribution of\nedges instead of more complex structures such as paths lim-\nits the extent of modularity. Indeed, recent studies have\nshown restrictions of optimizing modularity, for instance its\nresolution limit. We introduce here a novel, formal and well-\ndefined modularity measure based on random walks. We\nshow how this modularity can be computed from paths in-\nduced by the graph instead of the traditionally used edges.\nWe argue that by computing modularity on paths instead\nof edges, more informative features can be extracted from\nthe network. We verify this hypothesis on a semi-supervised\nclassification procedure of the nodes in the network, where\nwe show that, under the same settings, the features of the\nrandom walk modularity help to classify better than the\nfeatures of the usual modularity. Additionally, the proposed\napproach outperforms the classical label propagation proce-\ndure on two data sets of labeled social networks."}
{"Title": "High Quality, Scalable and Parallel Community Detection\nfor Large Real Graphs", "Abstract": "Community detection has arisen as one of the most relevant\ntopics in the field of graph mining, principally for its applica-\ntions in domains such as social or biological networks anal-\nysis. Different community detection algorithms have been\nproposed during the last decade, approaching the problem\nfrom different perspectives. However, existing algorithms\nare, in general, based on complex and expensive computa-\ntions, making them unsuitable for large graphs with millions\nof vertices and edges such as those usually found in the real\nworld.\nIn this paper, we propose a novel disjoint community\ndetection algorithm called Scalable Community Detection\n(SCD). By combining different strategies, SCD partitions\nthe graph by maximizing the Weighted Community Clus-\ntering (WCC), a recently proposed community detection\nmetric based on triangle analysis. Using real graphs with\nground truth overlapped communities, we show that SCD\noutperforms the current state of the art proposals (even\nthose aimed at finding overlapping communities) in terms\nof quality and performance. SCD provides the speed of\nthe fastest algorithms and the quality in terms of NMI and\nF1Score of the most accurate state of the art proposals. We\nshow that SCD is able to run up to two orders of magni-\ntude faster than practical existing solutions by exploiting\nthe parallelism of current multi-core processors, enabling us\nto process graphs of unprecedented size in short execution\ntimes."}
{"Title": "Dynamic and Historical Shortest-Path Distance Queries on\nLarge Evolving Networks by Pruned Landmark Labeling", "Abstract": "We propose two dynamic indexing schemes for shortest-path\nand distance queries on large time-evolving graphs, which\nare useful in a wide range of important applications such as\nreal-time network-aware search and network evolution anal-\nysis. To the best of our knowledge, these methods are the\nfirst practical exact indexing methods to efficiently process\ndistance queries and dynamic graph updates.\nWe first propose a dynamic indexing scheme for queries\non the last snapshot. The scalability and efficiency of its\noffline indexing algorithm and query algorithm are compet-\nitive even with previous static methods. Meanwhile, the\nmethod is dynamic, that is, it can incrementally update in-\ndices as the graph changes over time. Then, we further de-\nsign another dynamic indexing scheme that can also answer\ntwo kinds of historical queries with regard to not only the\nlatest snapshot but also previous snapshots.\nThrough extensive experiments on real and synthetic evolv-\ning networks, we show the scalability and efficiency of our\nmethods. Specifically, they can construct indices from large\ngraphs with millions of vertices, answer queries in microsec-\nonds, and update indices in milliseconds."}
{"Title": "To Gather Together for a Better World:\nUnderstanding and Leveraging Communities\nin Micro-lending Recommendation", "Abstract": "Micro-finance organizations provide non-profit lending op-\nportunities to mitigate poverty by financially supporting\nimpoverished, yet skilled entrepreneurs who are in desper-\nate need of an institution that lends to them. In Kiva.org,\na widely-used crowd-funded micro-financial service, a vast\namount of micro-financial activities are done by lending teams,\nand thus, understanding their diverse characteristics is cru-\ncial in maintaining a healthy micro-finance ecosystem. As\nthe first step for this goal, we model different lending teams\nby using a maximum-entropy distribution approach based on\na wealthy set of heterogeneous information regarding micro-\nfinancial transactions available at Kiva. Based on this ap-\nproach, we achieved a competitive performance in predicting\nthe lending activities for the top 200 teams. Furthermore,\nwe provide deep insight about the characteristics of lend-\ning teams by analyzing the resulting team-specific lending\nmodels. We found that lending teams are generally more\ncareful in selecting loans by a loan\u2019s geo-location, a bor-\nrower\u2019s gender, a field partner\u2019s reliability, etc., when com-\npared to lenders without team affiliations. In addition, we\nidentified interesting lending behaviors of different lending\nteams based on lenders\u2019 background and interest such as\ntheir ethnic, religious, linguistic, educational, regional, and\noccupational aspects. Finally, using our proposed model,\nwe tackled a novel problem of lending team recommenda-\ntion and showed its promising performance results."}
{"Title": "Recommending Investors for Crowdfunding Projects", "Abstract": "To bring their innovative ideas to market, those embarking in new\nventures have to raise money, and, to do so, they have often re-\nsorted to banks and venture capitalists. Nowadays, they have an\nadditional option: that of crowdfunding. The name refers to the\nidea that funds come from a network of people on the Internet who\nare passionate about supporting others\u2019 projects. One of the most\npopular crowdfunding sites is Kickstarter. In it, creators post de-\nscriptions of their projects and advertise them on social media sites\n(mainly Twitter), while investors look for projects to support. The\nmost common reason for project failure is the inability of founders\nto connect with a sufficient number of investors, and that is mainly\nbecause hitherto there has not been any automatic way of matching\ncreators and investors. We thus set out to propose different ways of\nrecommending investors found on Twitter for specific Kickstarter\nprojects. We do so by conducting hypothesis-driven analyses of\npledging behavior and translate the corresponding findings into dif-\nferent recommendation strategies. The best strategy achieves, on\naverage, 84% of accuracy in predicting a list of potential investors\u2019\nTwitter accounts for any given project. Our findings also produced\nkey insights about the whys and wherefores of investors deciding\nto support innovative efforts."}
{"Title": "Understanding Spatial Homophily: The Case of Peer\nInfluence and Social Selection", "Abstract": "Homophily is a phenomenon observed very frequently in social\nnetworks and is related with the inclination of people to be in-\nvolved with others that exhibit similar characteristics. The roots of\nhomophily can be subtle and are mainly traced back to two mech-\nanisms: (i) social selection and (ii) peer influence. Decomposing\nthe effects of each of these mechanisms requires analysis of longi-\ntudinal data. This has been a burden to similar studies in traditional\nsocial sciences due to the hardness of collecting such information.\nHowever, the proliferation of online social media has enabled the\ncollection of massive amounts of information related with human\nactivities. In this work, we are interested in examining the forces\nof the above mechanisms in the context of the locations visited by\npeople. For our study, we use a longitudinal dataset collected from\nGowalla, a location-based social network (LBSN). LBSNs, unlike\nother online social media, bond users\u2019 online interactions with their\nactivities in real-world, physical locations. Prior work on LBSNs\nhas focused on the influence of geographical constraints on the for-\nmation of social ties. On the contrary, in this paper, we perform a\nmicroscopic study of the peer influence and social selection mech-\nanisms in LBSNs. Our analysis indicates that while the similarity\nof friends\u2019 spatial trails at a geographically global scale cannot be\nattributed to peer influence, the latter can explain up to 40% of the\ngeographically localized similarity between friends. Moreover, this\npercentage depends on the type of locations we examine, and it can\nbe even higher for specific categories (e.g., nightlife spots). Finally,\nwe find that the social selection mechanism, is only triggered by\nplaces that exhibit specific network characteristics. We believe that\nour work can have significant implications on obtaining a deeper\nunderstanding of the way that people create friendships, act and\nmove in real space, which can further facilitate and enhance appli-\ncations such as recommender systems, trip planning and marketing."}
{"Title": "Designing and Deploying Online Field Experiments", "Abstract": "Online experiments are widely used to compare specific design\nalternatives, but they can also be used to produce generalizable\nknowledge and inform strategic decision making. Doing so often\nrequires sophisticated experimental designs, iterative refinement,\nand careful logging and analysis. Few tools exist that support these\nneeds. We thus introduce a language for online field experiments\ncalled PlanOut. PlanOut separates experimental design from ap-\nplication code, allowing the experimenter to concisely describe\nexperimental designs, whether common \u201cA/B tests\u201d and factorial\ndesigns, or more complex designs involving conditional logic or\nmultiple experimental units. These latter designs are often useful\nfor understanding causal mechanisms involved in user behaviors.\nWe demonstrate how experiments from the literature can be im-\nplemented in PlanOut, and describe two large field experiments\nconducted on Facebook with PlanOut. For common scenarios in\nwhich experiments are run iteratively and in parallel, we introduce\na namespaced management system that encourages sound experi-\nmental practice."}
{"Title": "Local Business Ambience Characterization Through\nMobile Audio Sensing", "Abstract": "Local search users today decide what business to visit solely based\non distance information, and business ratings that can be sparse or\nstale. We believe that when users search for local businesses, such\nas bars or restaurants, they need to know more about the ambience\nof each business, such as how crowded it is, how loud and of what\ntype the music it plays is, as well as how loud the human chatter\nin the business is. Unfortunately, this information doesn\u2019t exist to-\nday. In this paper, we propose to automatically crowdsource such\nrich, local business ambience metadata through real user check-in\nevents. Every time a user checks into a business, the phone is in\nuser\u2019s hands, and the phone\u2019s sensors can sense the business en-\nvironment. We leverage the phone\u2019s microphone during this time\nto infer the occupancy and human chatter levels, the music type, as\nwell as the music and noise levels in the business. As people check-\nin to businesses throughout the day, business metadata can be au-\ntomatically updated over time, enabling a new generation of local\nsearch experience. Using approximately 150 audio traces collected\nfrom real businesses of various types over a period of 3 months, we\nshow that by properly extracting the temporal and frequency sig-\nnatures of the audio signal, it is feasible to train models that can\nsimultaneously infer occupancy, human chatter, music, and noise\nlevels in a business, with higher than 79% accuracy."}
{"Title": "Social Bootstrapping: How Pinterest and Last.fm Social\nCommunities Benefit by Borrowing Links from Facebook", "Abstract": "How does one develop a new online community that is highly en-\ngaging to each user and promotes social interaction? A number of\nwebsites offer friend-finding features that help users bootstrap so-\ncial networks on the website by copying links from an established\nnetwork like Facebook or Twitter. This paper quantifies the ex-\ntent to which such social bootstrapping is effective in enhancing\na social experience of the website. First, we develop a stylised\nanalytical model that suggests that copying tends to produce a gi-\nant connected component (i.e., a connected community) quickly\nand preserves properties such as reciprocity and clustering, up to\na linear multiplicative factor. Second, we use data from two web-\nsites, Pinterest and Last.fm, to empirically compare the subgraph\nof links copied from Facebook to links created natively. We find\nthat the copied subgraph has a giant component, higher reciprocity\nand clustering, and confirm that the copied connections see higher\nsocial interactions. However, the need for copying diminishes as\nusers become more active and influential. Such users tend to create\nlinks natively on the website, to users who are more similar to them\nthan their Facebook friends. Our findings give new insights into\nunderstanding how bootstrapping from established social networks\ncan help engage new users by enhancing social interactivity."}
{"Title": "Modeling Contextual Agreement in Preferences", "Abstract": "Personalization, or customizing the experience of each in-\ndividual user, is seen as a useful way to navigate the huge\nvariety of choices on the Web today. A key tenet of per-\nsonalization is the capacity to model user preferences. The\nparadigm has shifted from that of individual preferences,\nwhereby we look at a user\u2019s past activities alone, to that\nof shared preferences, whereby we model the similarities in\npreferences between pairs of users (e.g., friends, people with\nsimilar interests). However, shared preferences are still too\ngranular, because it assumes that a pair of users would share\npreferences across all items. We therefore postulate the need\nto pay attention to \u201ccontext\u201d, which refers to the specific\nitem on which the preferences between two users are to be\nestimated. In this paper, we propose a generative model for\ncontextual agreement in preferences. For every triplet con-\nsisting of two users and an item, the model estimates both\nthe prior probability of agreement between the two users, as\nwell as the posterior probability of agreement with respect to\nthe item at hand. The model parameters are estimated from\nratings data. To extend the model to unseen ratings, we fur-\nther propose several matrix factorization techniques focused\non predicting agreement, rather than ratings. Experiments\non real-life data show that our model yields context-specific\nsimilarity values that perform better on a prediction task\nthan models relying on shared preferences."}
{"Title": "A Monte Carlo Algorithm for Cold Start Recommendation", "Abstract": "Recommendation systems have been widely used in E-commerce\nsites, social networks, etc. One of the core tasks in rec-\nommendation systems is to predict the users\u2019 ratings on\nitems. Although many models and algorithms have been\nproposed, how to make accurate prediction for new users\nwith extremely few rating records still remains a big chal-\nlenge, which is called the cold start problem. Many exist-\ning methods utilize additional information, such as social\ngraphs, to cope with the cold start problem. However, the\nside information may not always be available. In contrast\nto such methods, we propose a more general solution to ad-\ndress the cold start problem based on the observed user rat-\ning records only. Specifically we define a random walk on a\nbipartite graph of users and items to simulate the preference\npropagation among users, in order to alleviate the data spar-\nsity problem for cold start users. Then we propose a Monte\nCarlo algorithm to estimate the similarity between different\nusers. This algorithm takes a precomputation approach, and\nthus can efficiently compute the user similarity given any\nnew user for rating prediction. In addition, our algorithm\ncan easily handle dynamic updates and can be parallelized\nnaturally, which are crucial for large recommendation sys-\ntems. Theoretical analysis is presented to demonstrate the\nefficiency and effectiveness of our algorithm, and extensive\nexperiments also confirm our theoretical findings."}
{"Title": "Was This Review Helpful to You? It Depends!\nContext and Voting Patterns in Online Content", "Abstract": "When a website hosting user-generated content asks users\na straightforward question \u2014 \u201cWas this content helpful?\u201d\nwith one \u201cYes\u201d and one \u201cNo\u201d button as the two possible\nanswers \u2014 one might expect to get a straightforward an-\nswer. In this paper, we explore how users respond to this\nquestion and find that their responses are not quite straight-\nforward after all. Using data from Amazon product reviews,\nwe present evidence that users do not make absolute, inde-\npendent voting decisions based on individual review quality\nalone. Rather, whether users vote at all, as well as the po-\nlarity of their vote for any given review, depends on the\ncontext in which they view it \u2014 reviews receive a larger\noverall number of votes when they are \u2018misranked\u2019, and the\npolarity of votes becomes more positive/negative when the\nreview is ranked lower/higher than it deserves. We distill\nthese empirical findings into a new probabilistic model of\nrating behavior that includes the dependence of rating de-\ncisions on context. Understanding and formally modeling\nvoting behavior is crucial for designing learning mechanisms\nand algorithms for review ranking, and we conjecture that\nmany of our findings also apply to user behavior in other\nonline content-rating settings."}
{"Title": "Reduce and Aggregate: Similarity Ranking in\nMulti-Categorical Bipartite Graphs", "Abstract": "We study the problem of computing similarity rankings in\nlarge-scale multi-categorical bipartite graphs, where the two\nsides of the graph represent actors and items, and the items\nare partitioned into an arbitrary set of categories. The prob-\nlem has several real-world applications, including identifying\ncompeting advertisers and suggesting related queries in an\nonline advertising system or finding users with similar inter-\nests and suggesting content to them. In these settings, we\nare interested in computing on-the-fly rankings of similar\nactors, given an actor and an arbitrary subset of categories\nof interest. Two main challenges arise: First, the bipartite\ngraphs are huge and often lopsided (e.g. the system might\nreceive billions of queries while presenting only millions of\nadvertisers). Second, the sheer number of possible combi-\nnations of categories prevents the pre-computation of the\nresults for all of them.\nWe present a novel algorithmic framework that addresses\nboth issues for the computation of several graph-theoretical\nsimilarity measures, including # common neighbors, and\nPersonalized PageRank. We show how to tackle the imbal-\nance in the graphs to speed up the computation and provide\nefficient real-time algorithms for computing rankings for an\narbitrary subset of categories.\nFinally, we show experimentally the accuracy of our ap-\nproach with real-world data, using both public graphs and\na very large dataset from Google AdWords."}
{"Title": "Robust Multivariate Autoregression\nfor Anomaly Detection in Dynamic Product Ratings", "Abstract": "User provided rating data about products and services is\none key feature of websites such as Amazon, TripAdvisor, or\nYelp. Since these ratings are rather static but might change\nover time, a temporal analysis of rating distributions pro-\nvides deeper insights into the evolution of a products\u2019 quality.\nGiven a time-series of rating distributions, in this work,\nwe answer the following questions: (1) How to detect the\nbase behavior of users regarding a product\u2019s evaluation over\ntime? (2) How to detect points in time where the rating dis-\ntribution differs from this base behavior, e.g., due to attacks\nor spontaneous changes in the product\u2019s quality? To achieve\nthese goals, we model the base behavior of users regarding\na product as a latent multivariate autoregressive process.\nThis latent behavior is mixed with a sparse anomaly signal\nfinally leading to the observed data. We propose an efficient\nalgorithm solving our objective and we present interesting\nfindings on various real world datasets."}
{"Title": "Mining Novelty-Seeking Trait Across Heterogeneous\nDomains", "Abstract": "An incisive understanding of personal psychological traits\nis not only essential to many scientific disciplines, but also\nhas a profound business impact on online recommendation.\nRecent studies in psychology suggest that novelty-seeking\ntrait is highly related to consumer behavior. In this pa-\nper, we focus on understanding individual novelty-seeking\ntrait embodied at different levels and across heterogeneous\ndomains. Unlike the questionnaire-based methods widely\nadopted in the past, we first present a computational frame-\nwork, Novel Seeking Model (NSM), for exploring the novelty-\nseeking trait implied by observable activities. Then, we ex-\nplore the novelty-seeking trait in two heterogeneous domains:\ncheck-in behavior in location based social networks, which\nreflects mobility patterns in the physical world, and online\nshopping behavior on e-commerce sites, which reflects con-\nsumption concepts in economic activities. To demonstrate\nthe effectiveness of NSM, we conducted extensive experi-\nments, with a large dataset covering the two-domain activ-\nities for hundreds of thousands of individuals. Our results\nsuggest that NSM offers a powerful paradigm for 1) present-\ning an effective measurement of a personality trait that can\nexplicitly explain the deviation of individuals from the habits\nof individuals and crowds; 2) uncovering the correlation of\nnovelty-seeking trait at different levels and across heteroge-\nneous domains. The proposed method provides emerging\nimplications for personalized cross-domain recommendation\nand targeted advertising."}
{"Title": "Discovering Emerging Entities with Ambiguous Names", "Abstract": "Knowledge bases (KB\u2019s) contain data about a large num-\nber of people, organizations, and other entities. However,\nthis knowledge can never be complete due to the dynamics\nof the ever-changing world: new companies are formed ev-\nery day, new songs are composed every minute and become\nof interest for addition to a KB. To keep up with the real\nworld\u2019s entities, the KB maintenance process needs to con-\ntinuously discover newly emerging entities in news and other\nWeb streams. In this paper we focus on the most difficult\ncase where the names of new entities are ambiguous. This\nraises the technical problem to decide whether an observed\nname refers to a known entity or represents a new entity.\nThis paper presents a method to solve this problem with\nhigh accuracy. It is based on a new model of measuring the\nconfidence of mapping an ambiguous mention to an existing\nentity, and a new model of representing a new entity with\nthe same ambiguous name as a set of weighted keyphrases.\nThe method can handle both Wikipedia-derived entities that\ntypically constitute the bulk of large KB\u2019s as well as entities\nthat exist only in other Web sources such as online commu-\nnities about music or movies. Experiments show that our\nentity discovery method outperforms previous methods for\ncoping with out-of-KB entities (called unlinkable in entity\nlinking)."}
{"Title": "Effective Named Entity Recognition\nfor Idiosyncratic Web Collections", "Abstract": "Named Entity Recognition (NER) plays an important role\nin a variety of online information management tasks includ-\ning text categorization, document clustering, and faceted\nsearch. While recent NER systems can achieve near-human\nperformance on certain documents like news articles, they\nstill remain highly domain-specific and thus cannot effec-\ntively identify entities such as original technical concepts in\nscientific documents. In this work, we propose novel ap-\nproaches for NER on distinctive document collections (such\nas scientific articles) based on n-grams inspection and clas-\nsification. We design and evaluate several entity recogni-\ntion features\u2014ranging from well-known part-of-speech tags\nto n-gram co-location statistics and decision trees\u2014to clas-\nsify candidates. In addition, we show how the use of exter-\nnal knowledge bases (either specific like DBLP or generic\nlike DBPedia) can be leveraged to improve the effectiveness\nof NER for idiosyncratic collections. We evaluate our sys-\ntem on two test collections created from a set of Computer\nScience and Physics papers and compare it against state-of-\nthe-art supervised methods. Experimental results show that\na careful combination of the features we propose yield up to\n85% NER accuracy over scientific collections and substan-\ntially outperforms state-of-the-art approaches such as those\nbased on maximum entropy."}
{"Title": "Deduplicating a Places Database", "Abstract": "We consider the problem of resolving duplicates in a database\nof places, where a place is defined as any entity that has a\nname and a physical location. When other auxiliary at-\ntributes like phone and full address are not available, dedu-\nplication based solely on names and approximate location\nbecomes an exceptionally challenging problem that requires\nboth domain knowledge as well an local geographical knowl-\nedge. For example, the pairs \u201cNewpark Mall Gap Outlet\u201d\nand \u201cNewpark Mall Sears Outlet\u201d have a high string simi-\nlarity, but determining that they are different requires the\ndomain knowledge that they represent two different store\nnames in the same mall. Similarly, in most parts of the\nworld, a local business called\u201cCentral Park Cafe\u201dmight sim-\nply be referred to by \u201cCentral Park\u201d, except in New York,\nwhere the keyword \u201cCafe\u201d in the name becomes important\nto differentiate it from the famous park in the city.\nIn this paper, we present a language model that can encap-\nsulate both domain knowledge as well as local geographical\nknowledge. We also present unsupervised techniques that\ncan learn such a model from a database of places. Finally,\nwe present deduplication techniques based on such a model,\nand we demonstrate, using real datasets, that our techniques\nare much more effective than simple TF-IDF based models\nin resolving duplicates. Our techniques are used in produc-\ntion at Facebook for deduplicating the Places database."}
{"Title": "The Dynamics of Repeat Consumption", "Abstract": "We study the patterns by which a user consumes the same item re-\npeatedly over time, in a wide variety domains ranging from check-\nins at the same business location to re-watches of the same video.\nWe find that recency of consumption is the strongest predictor of\nrepeat consumption. Based on this, we develop a model by which\nthe item from t timesteps ago is reconsumed with a probability pro-\nportional to a function of t. We study theoretical properties of this\nmodel, develop algorithms to learn reconsumption likelihood as a\nfunction of t, and show a strong fit of the resulting inferred function\nviaapowerlawwithexponentialcutoff. Wethenintroduceanotion\nof item quality, show that it alone underperforms our recency-based\nmodel, and develop a hybrid model that predicts user choice based\non a combination of recency and quality. We show how the pa-\nrameters of this model may be jointly estimated, and show that the\nresulting scheme outperforms other alternatives."}
{"Title": "From Devices to People:\nAttribution of Search Activity in Multi-User Settings", "Abstract": "Online services rely on unique identifiers of machines to tailor of-\nferings to their users. An implicit assumption is made that each ma-\nchine identifier maps to an individual. However, shared machines\nare common, leading to interwoven search histories and noisy sig-\nnals for applications such as personalized search and advertising.\nWe present methods for attributing search activity to individual\nsearchers. Using ground truth data for a sample of almost four mil-\nlion U.S. Web searchers\u2014containing both machine identifiers and\nperson identifiers\u2014we show that over half of the machine identifi-\ners comprise the queries of multiple people. We characterize varia-\ntions in features of topic, time, and other aspects such as the com-\nplexity of the information sought per the number of searchers on a\nmachine, and show significant differences in all measures. Based\non these insights, we develop models to accurately estimate when\nmultiple people contribute to the logs ascribed to a single machine\nidentifier. We also develop models to cluster search behavior on a\nmachine, allowing us to attribute historical data accurately and au-\ntomatically assign new search activity to the correct searcher. The\nfindings have implications for the design of applications such as\npersonalized search and advertising that rely heavily on machine\nidentifiers to custom-tailor their services."}
{"Title": "Demographics, Weather and Online Reviews:\nA Study of Restaurant Recommendations", "Abstract": "Online recommendation sites are valuable information sources that\npeople contribute to, and often use to choose restaurants. However,\nlittle is known about the dynamics behind participation in these on-\nline communities and how the recommendations in these commu-\nnities are formed. In this work, we take a first look at online restau-\nrant recommendation communities to study what endogenous (i.e.,\nrelated to entities being reviewed) and exogenous factors influence\npeople\u2019s participation in the communities, and to what extent. We\nanalyze an online community corpus of 840K restaurants and their\n1.1M associated reviews from 2002 to 2011, spread across every\nU.S. state. We construct models for number of reviews and rat-\nings by community members, based on several dimensions of en-\ndogenous and exogenous factors. We find that while endogenous\nfactors such as restaurant attributes (e.g., meal, price, service) af-\nfect recommendations, surprisingly, exogenous factors such as de-\nmographics (e.g., neighborhood diversity, education) and weather\n(e.g., temperature, rain, snow, season) also exert a significant effect\non reviews. We find that many of the effects in online communities\ncan be explained using offline theories from experimental psychol-\nogy. Our study is the first to look at exogenous factors and how it\nrelated to online online restaurant reviews. It has implications for\ndesigning online recommendation sites, and in general, social me-\ndia and online communities."}
{"Title": "TripleProv: Efficient Processing of\nLineage Queries in a Native RDF Store", "Abstract": "Given the heterogeneity of the data one can find on the\nLinked Data cloud, being able to trace back the provenance\nof query results is rapidly becoming a must-have feature\nof RDF systems. While provenance models have been ex-\ntensively discussed in recent years, little attention has been\ngiven to the efficient implementation of provenance-enabled\nqueries inside data stores. This paper introduces TripleProv:\na new system extending a native RDF store to efficiently\nhandle such queries. TripleProv implements two different\nstorage models to physically co-locate lineage and instance\ndata, and for each of them implements algorithms for trac-\ning provenance at two granularity levels. In the following,\nwe present the overall architecture of our system, its dif-\nferent lineage storage models, and the various query exe-\ncution strategies we have implemented to efficiently answer\nprovenance-enabled queries. In addition, we present the re-\nsults of a comprehensive empirical evaluation of our system\nover two different datasets and workloads."}
{"Title": "RDF Analytics: Lenses over Semantic Graphs", "Abstract": "The development of Semantic Web (RDF) brings new\nrequirements for data analytics tools and methods, going\nbeyond querying to semantics-rich analytics through\nwarehouse-style tools. In this work, we fully redesign, from\nthe bottom up, core data analytics concepts and tools in the\ncontext of RDF data, leading to the first complete formal\nframework for warehouse-style RDF analytics. Notably, we\ndefine i) analytical schemas tailored to heterogeneous,\nsemantics-rich RDF graph, ii) analytical queries which\n(beyond relational cubes) allow flexible querying of the data\nand the schema as well as powerful aggregation and iii)\nOLAP-style operations. Experiments on a fully-implemented\nplatform demonstrate the practical interest of our approach."}
{"Title": "Formalisation and Experiences of R2RML-based SPARQL\nto SQL query translation using Morph", "Abstract": "R2RML is used to specify transformations of data avail-\nable in relational databases into materialised or virtual RDF\ndatasets. SPARQL queries evaluated against virtual datasets\nare translated into SQL queries according to the R2RML\nmappings, so that they can be evaluated over the underly-\ning relational database engines. In this paper we describe\nan extension of a well-known algorithm for SPARQL to\nSQL translation, originally formalised for RDBMS-backed\ntriple stores, that takes into account R2RML mappings. We\npresent the result of our implementation using queries from\na synthetic benchmark and from three real use cases, and\nshow that SPARQL queries can be in general evaluated as\nfast as the SQL queries that would have been generated by\nSQL experts if no R2RML mappings had been used."}
{"Title": "Codewebs: Scalable Homework Search for Massive Open\nOnline Programming Courses", "Abstract": "Massive open online courses (MOOCs), one of the latest\ninternet revolutions have engendered hope that constant it-\nerative improvement and economies of scale may cure the\n\u201ccost disease\u201d of higher education. While scalable in many\nways, providing feedback for homework submissions (par-\nticularly open-ended ones) remains a challenge in the online\nclassroom. In courses where the student-teacher ratio can be\nten thousand to one or worse, it is impossible for instructors\nto personally give feedback to students or to understand the\nmultitude of student approaches and pitfalls. Organizing\nand making sense of massive collections of homework solu-\ntions is thus a critical web problem. Despite the challenges,\nthe dense solution space sampling in highly structured home-\nworks for some MOOCs suggests an elegant solution to pro-\nviding quality feedback to students on a massive scale.\nWe outline a method for decomposing online homework\nsubmissions into a vocabulary of \u201ccode phrases\u201d, and based\non this vocabulary, we architect a queryable index that al-\nlows for fast searches into the massive dataset of student\nhomework submissions. To demonstrate the utility of our\nhomework search engine we index over a million code sub-\nmissions from users worldwide in Stanford\u2019s Machine Learn-\ning MOOC and (a) semi-automatically learn shared struc-\nture amongst homework submissions and (b) generate spe-\ncific feedback for student mistakes.\nCodewebs is a tool that leverages the redundancy of densely\nsampled, highly structured homeworks in order to force-\nmultiply teacher effort. Giving articulate, instant feedback\nis a crucial component of the online learning process and\nthus by building a homework search engine we hope to take\na step towards higher quality free education."}
{"Title": "Joint Question Clustering and Relevance Prediction for\nOpen Domain Non-Factoid Question Answering", "Abstract": "Websearchesareincreasinglyformulatedasnaturallanguageques-\ntions, rather than keyword queries. Retrieving answers to such\nquestions requires a degree of understanding of user expectations.\nAn important step in this direction is to automatically infer the type\nof answer implied by the question, e.g., factoids, statements on a\ntopic, instructions, reviews, etc.\nAnswer Type taxonomies currently exist for factoid-style ques-\ntions, but not for open-domain questions. Building taxonomies for\nnon-factoid questions is a harder problem since these questions can\ncome from a very broad semantic space. A few attempts have been\nmade to develop taxonomies for non-factoid questions, but these\ntend to be too narrow or domain specific. In this paper, we address\nthis problem by modeling the Answer Type as a latent variable that\nis learned in a data-driven fashion, allowing the model to be more\nadaptive to new domains and data sets. We propose approaches\nthat detect the relevance of candidate answers to a user question by\njointly \u2018clustering\u2019 questions according to the hidden variable, and\nmodeling relevance conditioned on this hidden variable.\nIn this paper we propose 3 new models: (a) Logistic Regression\nMixture (LRM), (b) Glocal Logistic Regression Mixture (G-LRM)\nand (c) Mixture Glocal Logistic Regression Mixture (MG-LRM)\nthat automatically learn question-clusters and cluster-specific rele-\nvance models. All three models perform better than a baseline rel-\nevance model that uses explicit Answer Type categories predicted\nby a supervised Answer-Type classifier, on a newsgroups dataset.\nOur models also perform better than a baseline relevance model\nthat does not use any answer-type information on a blogs dataset."}
{"Title": "Knowledge Base Completion via Search-Based\nQuestion Answering", "Abstract": "Over the past few years, massive amounts of world knowledge have\nbeen accumulated in publicly available knowledge bases, such as\nFreebase, NELL,andYAGO.Yetdespitetheirseeminglyhugesize,\nthese knowledge bases are greatly incomplete. For example, over\n70% of people included in Freebase have no known place of birth,\nand 99% have no known ethnicity. In this paper, we propose a way\nto leverage existing Web-search\u2013based question-answering tech-\nnology to fill in the gaps in knowledge bases in a targeted way. In\nparticular, for each entity attribute, we learn the best set of queries\nto ask, such that the answer snippets returned by the search engine\nare most likely to contain the correct value for that attribute. For\nexample, if we want to find Frank Zappa\u2019s mother, we could ask the\nquery who is the mother of Frank Zappa. However, this is likely to\nreturn \u2018The Mothers of Invention\u2019, which was the name of his band.\nOur system learns that it should (in this case) add disambiguating\nterms, suchasZappa\u2019splaceofbirth, inordertomakeitmorelikely\nthat the search results contain snippets mentioning his mother. Our\nsystem also learns how many different queries to ask for each at-\ntribute, since in some cases, asking too many can hurt accuracy (by\nintroducing false positives). We discuss how to aggregate candidate\nanswers across multiple queries, ultimately returning probabilistic\npredictions for possible values for each attribute. Finally, we eval-\nuate our system and show that it is able to extract a large number of\nfacts with high confidence."}
{"Title": "A Time-based Collective Factorization for Topic Discovery\nand Monitoring in News", "Abstract": "Discovering and tracking topic shifts in news constitutes\na new challenge for applications nowadays. Topics evolve,\nemerge and fade, making it more difficult for the journal-\nist \u2013 or the press consumer \u2013 to decrypt the news. For in-\nstance, the current Syrian chemical crisis has been the start-\ning point of the UN Russian initiative and also the revival\nof the US France alliance. A topical mapping representing\nhow the topics evolve in time would be helpful to contex-\ntualize information. As far as we know, few topic tracking\nsystems can provide such temporal topic connections. In\nthis paper, we introduce a novel framework inspired from\nCollective Factorization for online topic discovery able to\nconnect topics between different time-slots. The framework\nlearns jointly the topics evolution and their time dependen-\ncies. It offers the user the ability to control, through one\nunique hyper-parameter, the tradeoff between the past ac-\ncumulated knowledge and the current observed data. We\nshow, on semi-synthetic datasets and on Yahoo News arti-\ncles, that our method is competitive with state-of-the-art\ntechniques while providing a simple way to monitor topics\nevolution (including emerging and disappearing topics)."}
{"Title": "The Dual-Sparse Topic Model: Mining Focused Topics and\nFocused Terms in Short Text", "Abstract": "Topic modeling has been proved to be an effective method\nfor exploratory text mining. It is a common assumption\nof most topic models that a document is generated from a\nmixture of topics. In real-world scenarios, individual docu-\nments usually concentrate on several salient topics instead\nof covering a wide variety of topics. A real topic also adopts\na narrow range of terms instead of a wide coverage of the\nvocabulary. Understanding this sparsity of information is\nespecially important for analyzing user-generated Web con-\ntent and social media, which are featured as extremely short\nposts and condensed discussions.\nIn this paper, we propose a dual-sparse topic model that\naddresses the sparsity in both the topic mixtures and the\nword usage. By applying a \u201cSpike and Slab\u201d prior to de-\ncouple the sparsity and smoothness of the document-topic\nand topic-word distributions, we allow individual documents\nto select a few focused topics and a topic to select focused\nterms, respectively. Experiments on different genres of large\ncorpora demonstrate that the dual-sparse topic model out-\nperforms both classical topic models and existing sparsity-\nenhanced topic models. This improvement is especially no-\ntable on collections of short documents."}
{"Title": "Acquisition of Open-Domain Classes via Intersective\nSemantics", "Abstract": "A weakly-supervised method acquires fine-grained class la-\nbels that do not occur verbatim in the input data or under-\nlying text collection. The method generates more specific\nclass labels (gold mining companies listed on the toronto\nstock exchange) that capture the semantics of the under-\nlying classes, out of pairs of input class labels (companies\nlisted on the toronto stock exchange, gold mining companies)\navailable for an instance (Golden Star Resources). When ap-\nplied to Wikipedia articles and their categories, the method\ngenerates new categories for existing articles, and expands\nexisting categories with additional articles."}
{"Title": "Automated Runtime Recovery\nfor QoS-based Service Composition", "Abstract": "Servicecompositionusesexistingservice-basedapplicationsascom-\nponents to achieve a business goal. The composite service operates\nin a highly dynamic environment; hence, it can fail at any time\ndue to the failure of component services. Service composition lan-\nguages such as BPEL provide a compensation mechanism to roll-\nback the error. But such a compensation mechanism has several\nissues. For instance, it cannot guarantee the functional properties\nof the composite service after compensation. In this work, we pro-\npose an automated approach based on a genetic algorithm to calcu-\nlate the recovery plan that could guarantee the satisfaction of func-\ntional properties of the composite service after recovery. Given a\ncomposite service with large state space, the proposed method does\nnot require exploring the full state space of the composite service;\ntherefore, it allows efficient selection of recovery plan. In addition,\nthe selection of recovery plans is based on their quality of service\n(QoS).AQoS-optimalrecoveryplanallowseffectiverecoveryfrom\nthe state of failure. Our approach has been evaluated on real-world\ncase studies, and has shown promising results."}
{"Title": "Similarity-based Web Browser Optimization", "Abstract": "The performance of web browsers has become a major bottleneck\nwhen dealing with complex webpages. Many calculation redun-\ndancies exist when processing similar webpages, thus it is possible\nto cache and reuse previously calculated intermediate results to\nimprove web browser performance significantly. In this paper,\nwe propose a similarity-based optimization approach to improve\nwebpage processing performance of web browsers. Through\ncaching and reusing of style properties calculated previously, we\nare able to eliminate the redundancies caused by processing similar\nwebpages from the same website. We propose a tree-structured\narchitecture to store style properties to facilitate efficient caching\nand reuse. Experiments on webpages of various websites show\nthat the proposed technique can speed up the webpage loading\nprocess by up to 68% and reduce the redundant style calculations\nby up to 77% for the first visit to a webpage with almost negligible\noverhead."}
{"Title": "Temporal QoS-Aware Web Service Recommendation via\nNon-negative Tensor Factorization", "Abstract": "With the rapid growth of Web Service in the past decade,\nthe issue of QoS-aware Web service recommendation is be-\ncoming more and more critical. Since the Web service QoS\ninformation collection work requires much time and effort,\nand is sometimes even impractical, the service QoS value is\nusually missing. There are some work to predict the miss-\ning QoS value using traditional collaborative filtering meth-\nods based on user-service static model. However, the QoS\nvalue is highly related to the invocation context (e.g., QoS\nvalue are various at different time). By considering the third\ndynamic context information, a Temporal QoS-Aware Web\nService Recommendation Framework is presented to predict\nmissing QoS value under various temporal context. Further,\nwe formalize this problem as a generalized tensor factoriza-\ntion model and propose a Non-negative Tensor Factorization\n(NTF) algorithm which is able to deal with the triadic rela-\ntions of user-service-time model. Extensive experiments are\nconducted based on our real-world Web service QoS dataset\ncollected on Planet-Lab, which is comprised of service invo-\ncation response-time and throughput value from 343 users\non 5,817 Web services at 32 time periods. The comprehen-\nsive experimental analysis shows that our approach a"}
{"Title": "Adscape: Harvesting and Analyzing Online Display Ads", "Abstract": "Over the past decade, advertising has emerged as the pri-\nmary source of revenue for many web sites and apps. In\nthis paper we report a first-of-its-kind study that seeks to\nbroadly understand the features, mechanisms and dynamics\nof display advertising on the web - i.e., the Adscape. Our\nstudy takes the perspective of users who are the targets of\ndisplay ads shown on web sites. We develop a scalable crawl-\ning capability that enables us to gather the details of dis-\nplay ads including creatives and landing pages. Our crawling\nstrategy is focused on maximizing the number of unique ads\nharvested. Of critical importance to our study is the recog-\nnition that a user\u2019s profile (i.e., browser profile and cookies)\ncan have a significant impact on which ads are shown. We\ndeploy our crawler over a variety of websites and profiles\nand this yields over 175K distinct display ads. We find that\nwhile targeting is widely used, there remain many instances\nin which delivered ads do not depend on user profile; further,\nads vary more over user profiles than over websites. We also\nassess the population of advertisers seen and identify over\n3.7K distinct entities from a variety of business segments.\nFinally, we find that when targeting is used, the specific\ntypes of ads delivered generally correspond with the details\nof user profiles, and also on users\u2019 patterns of visit."}
{"Title": "Statistical Inference in Two-Stage Online Controlled\nExperiments with Treatment Selection and Validation", "Abstract": "Online controlled experiments, also called A/B testing, have\nbeen established as the mantra for data-driven decision mak-\ning in many web-facing companies. A/B Testing support\ndecision making by directly comparing two variants at a\ntime. It can be used for comparison between (1) two can-\ndidate treatments and (2) a candidate treatment and an\nestablished control. In practice, one typically runs an ex-\nperiment with multiple treatments together with a control\nto make decision for both purposes simultaneously. This is\nknown to have two issues. First, having multiple treatments\nincreases false positives due to multiple comparison. Second,\nthe selection process causes an upward bias in estimated ef-\nfect size of the best observed treatment. To overcome these\ntwo issues, a two stage process is recommended, in which\nwe select the best treatment from the first screening stage\nand then run the same experiment with only the selected\nbest treatment and the control in the validation stage. Tra-\nditional application of this two-stage design often focus only\non results from the second stage. In this paper, we propose a\ngeneral methodology for combining the first screening stage\ndata together with validation stage data for more sensitive\nhypothesis testing and more accurate point estimation of\nthe treatment effect. Our method is widely applicable to\nexisting online controlled experimentation systems."}
{"Title": "An Experimental Evaluation of Bidders\u2019 Behavior in Ad\nAuctions", "Abstract": "We performed controlled experiments of human participants\nin a continuous sequence of ad auctions, similar to those\nused by Internet companies. The goal of the research was\nto understand users\u2019 strategies in making bids. We studied\nthe behavior under two auction types: (1) the Generalized\nSecond-Price (GSP) auction and (2) the Vickrey\u2013Clarke\u2013\nGroves (VCG) payment rule, and manipulated also the par-\nticipants\u2019 knowledge conditions: (1) explicitly given valua-\ntions and (2) payoff information from which valuations could\nbe deduced. We found several interesting behaviors, among\nthem are:\n\u2022 No convergence to equilibrium was detected; moreover\nthe frequency with which participants modified their\nbids increased with time.\n\u2022 We can detect explicit\u201cbetter-response\u201dbehavior rather\nthan just mixed bidding.\n\u2022 While bidders in GSP auctions do strategically shade\ntheir bids, they tend to bid higher than theoretically\npredicted by the standard VCG-like equilibrium of GSP.\n\u2022 Bidders who are not explicitly given their valuations\nbut can only deduce them from their gains behave a lit-\ntle less\u201cprecisely\u201dthan those with such explicit knowl-\nedge, but mostly during an initial learning phase.\n\u2022 VCG and GSP yield approximately the same (high)\nsocial welfare, but GSP tends to give higher revenue."}
{"Title": "Timeline Generation: Tracking individuals on Twitter", "Abstract": "In this paper, we preliminarily learn the problem of reconstructing\nusers\u2019 life history based on the their Twitter stream and proposed\nan unsupervised framework that create a chronological list for per-\nsonal important events (PIE) of individuals. By analyzing individ-\nual tweet collections, we find that what are suitable for inclusion\nin the personal timeline should be tweets talking about personal (as\nopposed to public) and time-specific (as opposed to time-general)\ntopics. To further extract these types of topics, we introduce a\nnon-parametric multi-level Dirichlet Process model to recognize\nfour types of tweets: personal time-specific (PersonTS), personal\ntime-general (PersonTG), public time-specific (PublicTS) and pub-\nlic time-general (PublicTG) topics, which, in turn, are used for fur-\nther personal event extraction and timeline generation. To the best\nof our knowledge, this is the first work focused on the generation of\ntimeline for individuals from Twitter data. For evaluation, we have\nbuilt gold standard timelines that contain PIE related events from\n20 ordinary twitter users and 20 celebrities. Experimental results\ndemonstrate that it is feasible to automatically extract chronologi-\ncal timelines for Twitter users from their tweet collection\n1 ."}
{"Title": "Modeling and Predicting the Growth and Death of\nMembership-based Websites", "Abstract": "Driven by outstanding success stories of Internet startups\nsuch as Facebook and The Hu?ngton Post, recent studies\nhave thoroughly described their growth. These highly visible\nonline success stories, however, overshadow an untold num-\nber of similar ventures that fail. The study of website popu-\nlarity is ultimately incomplete without general mechanisms\nthat can describe both successes and failures. In this work\nwe present six years of the daily number of users (DAU) of\ntwenty-two membership-based websites \u2013 encompassing on-\nline social networks, grassroots movements, online forums,\nand membership-only Internet stores \u2013 well balanced be-\ntween successes and failures. We then propose a combination\nof reaction-di\u21b5usion-decay processes whose resulting equa-\ntions seem not only to describe well the observed DAU time\nseries but also provide means to roughly predict their evo-\nlution. This model allows an approximate automatic DAU-\nbased classification of websites into self-sustainable v.s. un-\nsustainable and whether the startup growth is mostly driven\nby marketing & media campaigns or word-of-mouth adop-\ntions."}
{"Title": "Word Storms: Multiples of Word Clouds for\nVisual Comparison of Documents", "Abstract": "Word clouds are popular for visualizing documents, but are\nnot as useful for comparing documents, because identical\nwords are not presented consistently across different clouds.\nWe introduce the concept of word storms, a visualization\ntool for analyzing corpora of documents. A word storm is a\ngroup of word clouds, in which each cloud represents a single\ndocument, juxtaposed to allow the viewer to compare and\ncontrast the documents. We present a novel algorithm that\ncreates a coordinated word storm, in which words that ap-\npear in multiple documents are placed in the same location,\nusing the same color and orientation, across clouds. This\nensures that similar documents are represented by similar-\nlooking word clouds, making them easier to compare and\ncontrast visually. We evaluate the algorithm using an au-\ntomatic evaluation based on document classification, and a\nuser study. The results confirm that a coordinated word\nstorm allows for better visual comparison of documents."}
{"Title": "Exploring the Filter Bubble: The Effect of Using\nRecommender Systems on Content Diversity", "Abstract": "Eli Pariser coined the term \u2018filter bubble\u2019 to describe the po-\ntential for online personalization to effectively isolate people\nfrom a diversity of viewpoints or content. Online recom-\nmender systems - built on algorithms that attempt to pre-\ndict which items users will most enjoy consuming - are one\nfamily of technologies that potentially suffers from this ef-\nfect. Because recommender systems have become so preva-\nlent, it is important to investigate their impact on users\nin these terms. This paper examines the longitudinal im-\npacts of a collaborative filtering-based recommender system\non users. To the best of our knowledge, it is the first paper\nto measure the filter bubble effect in terms of content diver-\nsity at the individual level. We contribute a novel metric to\nmeasure content diversity based on information encoded in\nuser-generated tags, and we present a new set of methods\nto examine the temporal effect of recommender systems on\nthe user experience. We do find that recommender systems\nexpose users to a slightly narrowing set of items over time.\nHowever, we also see evidence that users who actually con-\nsume the items recommended to them experience lessened\nnarrowing effects and rate items more positively."}
{"Title": "Engaging with Massive Online Courses", "Abstract": "The Web has enabled one of the most visible recent developments\nineducation\u2014thedeploymentofmassiveopenonlinecourses. With\ntheir global reach and often staggering enrollments, MOOCs have\nthe potential to become a major new mechanism for learning. De-\nspite this early promise, however, MOOCs are still relatively unex-\nplored and poorly understood.\nIn a MOOC, each student\u2019s complete interaction with the course\nmaterials takes place on the Web, thus providing a record of learner\nactivity of unprecedented scale and resolution. In this work, we\nuse such trace data to develop a conceptual framework for under-\nstanding how users currently engage with MOOCs. We develop a\ntaxonomy of individual behavior, examine the different behavioral\npatterns of high- and low-achieving students, and investigate how\nforum participation relates to other parts of the course.\nWe also report on a large-scale deployment of badges as incen-\ntives for engagement in a MOOC, including randomized experi-\nments in which the presentation of badges was varied across sub-\npopulations. We find that making badges more salient produced\nincreases in forum engagement."}
{"Title": "User Satisfaction in Competitive Sponsored Search", "Abstract": "We present a model of competition between web search al-\ngorithms, and study the impact of such competition on user\nwelfare. In our model, search providers compete for cus-\ntomers by strategically selecting which search results to dis-\nplay in response to user queries. Customers, in turn, have\nprivate preferences over search results and will tend to use\nsearch engines that are more likely to display pages satisfy-\ning their demands.\nOur main question is whether competition between search\nengines increases the overall welfare of the users (i.e., the\nlikelihood that a user finds a page of interest). When search\nengines derive utility only from customers to whom they\nshow relevant results, we show that they differentiate their\nresults, and every equilibrium of the resulting game achieves\nat least half of the welfare that could be obtained by a social\nplanner. This bound also applies whenever the likelihood of\nselecting a given engine is a convex function of the proba-\nbility that a user\u2019s demand will be satisfied, which includes\nnatural Markovian models of user behavior.\nOn the other hand, when search engines derive utility from\nall customers (independent of search result relevance) and\nthe customer demand functions are not convex, there are\ninstances in which the (unique) equilibrium involves no dif-\nferentiation between engines and a high degree of random-\nness in search results. This can degrade social welfare by a\nfactor of \u2126( \u221a N) relative to the social optimum, where N is\nthe number of webpages. These bad equilibria persist even\nwhen search engines can extract only small (but non-zero)\nexpected revenue from dissatisfied users, and much higher\nrevenue from satisfied ones."}
{"Title": "Price Competition in Online Combinatorial Markets", "Abstract": "We consider a single buyer with a combinatorial preference\nthat would like to purchase related products and services\nfrom different vendors, where each vendor supplies exactly\none product. We study the general case where subsets of\nproducts can be substitutes as well as complementary and\nanalyze the game that is induced on the vendors, where a\nvendor\u2019s strategy is the price that he asks for his product.\nThis model generalizes both Bertrand competition (where\nvendors are perfect substitutes) and Nash bargaining (where\nthey are perfect complements), and captures a wide variety\nof scenarios that can appear in complex crowd sourcing or\nin automatic pricing of related products.\nWe study the equilibria of such games and show that a\npure efficient equilibrium always exists. In the case of sub-\nmodular buyer preferences we fully characterize the set of\npure Nash equilibria, essentially showing uniqueness. For\nthe even more restricted \u201csubstitutes\u201d buyer preferences we\nalso prove uniqueness over mixed equilibria. Finally we be-\ngin the exploration of natural generalizations of our setting\nsuch as when services have costs, when there are multiple\nbuyers or uncertainty about the the buyer\u2019s valuation, and\nwhen a single vendor supplies multiple products."}
{"Title": "Semantic Stability in Social Tagging Streams", "Abstract": "One potential disadvantage of social tagging systems is that\ndue to the lack of a centralized vocabulary, a crowd of users\nmay never manage to reach a consensus on the description\nof resources (e.g., books, users or songs) on the Web. Yet,\nprevious research has provided interesting evidence that the\ntag distributions of resources may become semantically sta-\nble over time as more and more users tag them. At the\nsame time, previous work has raised an array of new ques-\ntions such as: (i) How can we assess the semantic stability\nof social tagging systems in a robust and methodical way?\n(ii) Does semantic stabilization of tags vary across differ-\nent social tagging systems and ultimately, (iii) what are the\nfactors that can explain semantic stabilization in such sys-\ntems? In this work we tackle these questions by (i) present-\ning a novel and robust method which overcomes a number\nof limitations in existing methods, (ii) empirically investi-\ngating semantic stabilization processes in a wide range of\nsocial tagging systems with distinct domains and proper-\nties and (iii) detecting potential causes for semantic stabi-\nlization, specifically imitation behavior, shared background\nknowledge and intrinsic properties of natural language. Our\nresults show that tagging streams which are generated by a\ncombination of imitation dynamics and shared background\nknowledge exhibit faster and higher semantic stability than\ntagging streams which are generated via imitation dynamics\nor natural language phenomena alone."}
{"Title": "Test-driven Evaluation of Linked Data Quality", "Abstract": "Linked Open Data (LOD) comprises an unprecedented vol-\nume of structured data on the Web. However, these datasets\nare of varying quality ranging from extensively curated\ndatasets to crowdsourced or extracted data of often rela-\ntively low quality. We present a methodology for test-driven\nquality assessment of Linked Data, which is inspired by test-\ndriven software development. We argue that vocabularies,\nontologies and knowledge bases should be accompanied by\na number of test cases, which help to ensure a basic level of\nquality. We present a methodology for assessing the qual-\nity of linked data resources, based on a formalization of bad\nsmells and data quality problems. Our formalization em-\nploys SPARQL query templates, which are instantiated into\nconcrete quality test case queries. Based on an extensive sur-\nvey, we compile a comprehensive library of data quality test\ncase patterns. We perform automatic test case instantiation\nbased on schema constraints or semi-automatically enriched\nschemata and allow the user to generate specific test case in-\nstantiations that are applicable to a schema or dataset. We\nprovide an extensive evaluation of five LOD datasets, man-\nual test case instantiation for five schemas and automatic\ntest case instantiations for all available schemata registered\nwith Linked Open Vocabularies (LOV). One of the main ad-\nvantages of our approach is that domain specific semantics\ncan be encoded in the data quality test cases, thus being\nable to discover data quality problems beyond conventional\nquality heuristics."}
{"Title": "Don\u2019t Like RDF Reification? Making\nStatements about Statements Using Singleton Property", "Abstract": "Statements about RDF statements, or meta triples, provide\nadditional information about individual triples, such as the\nsource, the occurring time or place, or the certainty. In-\ntegrating such meta triples into semantic knowledge bases\nwould enable the querying and reasoning mechanisms to be\naware of provenance, time, location, or certainty of triples.\nHowever, an efficient RDF representation for such meta knowl-\nedge of triples remains challenging. The existing standard\nreification approach allows such meta knowledge of RDF\ntriples to be expressed using RDF by two steps. The first\nstep is representing the triple by a Statement instance which\nhas subject, predicate, and object indicated separately in\nthree different triples. The second step is creating assertions\nabout that instance as if it is a statement. While reification\nis simple and intuitive, this approach does not have formal\nsemantics and is not commonly used in practice as described\nin the RDF Primer.\nIn this paper, we propose a novel approach called Singleton\nProperty for representing statements about statements and\nprovide a formal semantics for it. We explain how this sin-\ngleton property approach fits well with the existing syntax\nand formal semantics of RDF, and the syntax of SPARQL\nquery language. We also demonstrate the use of singleton\nproperty in the representation and querying of meta knowl-\nedge in two examples of Semantic Web knowledge bases:\nYAGO2 and BKR. Our experiments on the BKR show that\nthe singleton property approach gives a decent performance\nin terms of number of triples, query length and query execu-\ntion time compared to existing approaches. This approach,\nwhich is also simple and intuitive, can be easily adopted for\nrepresenting and querying statements about statements in\nother knowledge bases."}
{"Title": "Comment-based Multi-View Clustering of Web 2.0 Items", "Abstract": "Clustering Web 2.0 items (i.e., web resources like videos, images)\ninto semantic groups benefits many applications, such as organiz-\ning items, generating meaningful tags and improving web search.\nInthispaper, wesystematicallyinvestigatehowuser-generatedcom-\nments can be used to improve the clustering of Web 2.0 items.\nIn our preliminary study of Last.fm, we find that the two data\nsources extracted from user comments \u2013 the textual comments and\nthe commenting users \u2013 provide complementary evidence to the\nitems\u2019 intrinsic features. These sources have varying levels of qual-\nity, but we importantly we find that incorporating all three sources\nimproves clustering. To accommodate such quality imbalance, we\ninvoke multi-view clustering, in which each data source represents\na view, aiming to best leverage the utility of different views.\nTo combine multiple views under a principled framework, we\npropose CoNMF (Co-regularized Non-negative Matrix Factoriza-\ntion), which extends NMF for multi-view clustering by jointly fac-\ntorizing the multiple matrices through co-regularization. Under our\nCoNMF framework, we devise two paradigms \u2013 pair-wise CoNMF\nand cluster-wise CoNMF \u2013 and propose iterative algorithms for\ntheir joint factorization. Experimental results on Last.fm and Yelp\ndatasets demonstrate the effectiveness of our solution. In Last.fm,\nCoNMF betters k-means with a statistically significant F 1 increase\nof 14%, while achieving comparable performance with the state-of-\nthe-art multi-view clustering method CoSC [24]. On a Yelp dataset,\nCoNMF outperforms the best baseline CoSC with a statistically\nsignificant performance gain of 7%."}
{"Title": "Finding Progression Stages in Time-evolving\nEvent Sequences", "Abstract": "Event sequences, such as patients\u2019 medical histories or users\u2019 se-\nquences of product reviews, trace how individuals progress over\ntime. Identifying common patterns, or progression stages, in such\nevent sequences is a challenging task because not every individual\nfollows the same evolutionary pattern, stages may have very differ-\nent lengths, and individuals may progress at different rates.\nIn this paper, we develop a model-based method for discover-\ning common progression stages in general event sequences. We\ndevelop a generative model in which each sequence belongs to a\nclass, and sequences from a given class pass through a common set\nof stages, where each sequence evolves at its own rate. We then de-\nvelop a scalable algorithm to infer classes of sequences, while also\nsegmenting each sequence into a set of stages. We evaluate our\nmethod on event sequences, ranging from patients\u2019 medical his-\ntories to online news and navigational traces from the Web. The\nevaluation shows that our methodology can predict future events in\na sequence, while also accurately inferring meaningful progression\nstages, and effectively grouping sequences based on common pro-\ngression patterns. More generally, our methodology allows us to\nreason about how event sequences progress over time, by discov-\nering patterns and categories of temporal evolution in large-scale\ndatasets of events."}
{"Title": "On Estimating the Average Degree", "Abstract": "Networks are characterized by nodes and edges. While there\nhas been a spate of recent work on estimating the number\nof nodes in a network, the edge-estimation question appears\nto be largely unaddressed. In this work we consider the\nproblem of estimating the average degree of a large network\nusing efficient random sampling, where the number of nodes\nis not known to the algorithm. We propose a new estima-\ntor for this problem that relies on access to node samples\nunder a prescribed distribution. Next, we show how to effi-\nciently realize this ideal estimator in a random walk setting.\nOur estimator has a natural and simple implementation us-\ning random walks; we bound its performance in terms of\nthe mixing time of the underlying graph. We then show\nthat our estimators are both provably and practically better\nthan many natural estimators for the problem. Our work\ncontrasts with existing theoretical work on estimating aver-\nage degree, which assume that a uniform random sample of\nnodes is available and the number of nodes is known."}
{"Title": "Who Proposed the Relationship? \u2014 Recovering the\nHidden Directions of Undirected Social Networks", "Abstract": "Together with the sign (positive or negative) and strength (strong or\nweak), the directionality is also an important property of social ties,\nthough usually ignored in undirected social networks for its invis-\nibility. However, we believe most social ties are natively directed,\nand the awareness of directionality can improve our understand-\ning about the network structures and further benefit social network\nanalysis and mining tasks. Thus it\u2019s appealing to study whether\nthere exist interesting patterns about directionality in social net-\nworks and whether we can learn the directions for undirected net-\nworks based on these patterns.\nIn this study, we engage in the investigation of directionality pat-\nterns on real-world directed social networks and summarize our\nfindings using four consistency hypotheses. Based on these hy-\npotheses, we propose ReDirect, an optimization framework which\nmakes it possible to infer the hidden directions of undirected social\nties based on the network topology only. This general framework\ncan incorporate various predictive models under specific scenarios.\nFurthermore, we show how to improve ReDirect by introducing\nsemi/self-supervision in the framework and how to construct the\nself-labeled training data using simple but effective heuristics. Ex-\nperimental results show that even without external information, our\napproach can recover the directions of networks effectively.\nMoreover, we\u2019requitesurprising tofindthat ReDirectcanbenefit\npredictive tasks remarkably, with a case study of link prediction. In\nexperiments the redirected networks inferred using ReDirect are\nproven much more informative than original undirected ones and\ncan improve the prediction performance significantly. It convinces\nus that ReDirect can be a beneficial general data preprocess tool\nfor various network analysis and mining tasks by uncovering the\nhidden directions of undirected social networks."}
{"Title": "User Profiling in an Ego Network: Co-profiling Attributes\nand Relationships", "Abstract": "User attributes, such as occupation, education, and loca-\ntion, are important for many applications. In this paper,\nwe study the problem of profiling user attributes in social\nnetwork. To capture the correlation between attributes and\nsocial connections, we present a new insight that social con-\nnections are discriminatively correlated with attributes via\na hidden factor \u2013 relationship type. For example, a user\u2019s\ncolleagues are more likely to share the same employer with\nhim than other friends. Based on the insight, we propose\nto co-profile users\u2019 attributes and relationship types of their\nconnections. To achieve co-profiling, we develop an efficient\nalgorithm based on an optimization framework. Our algo-\nrithm captures our insight effectively. It iteratively profiles\nattributes by propagation via certain types of connections,\nand profiles types of connections based on attributes and\nthe network structure. We conduct extensive experiments\nto evaluate our algorithm. The results show that our algo-\nrithm profiles various attributes accurately, which improves\nthe state-of-the-art methods by 12%."}
{"Title": "Attributed Graph Models:\nModeling Network Structure with Correlated Attributes", "Abstract": "Online social networks have become ubiquitous to today\u2019s\nsociety and the study of data from these networks has im-\nproved our understanding of the processes by which relation-\nships form. Research in statistical relational learning focuses\non methods to exploit correlations among the attributes of\nlinked nodes to predict user characteristics with greater ac-\ncuracy. Concurrently, research on generative graph models\nhas primarily focused on modeling network structure with-\nout attributes, producing several models that are able to\nreplicate structural characteristics of networks such as power\nlaw degree distributions or community structure. However,\nthere has been little work on how to generate networks with\nreal-world structural properties and correlated attributes.\nIn this work, we present the Attributed Graph Model\n(AGM) framework to jointly model network structure and\nvertex attributes. Our framework learns the attribute cor-\nrelations in the observed network and exploits a genera-\ntive graph model, such as the Kronecker Product Graph\nModel (KPGM) [11] and Chung Lu Graph Model (CL) [2],\nto compute structural edge probabilities. AGM then com-\nbines the attribute correlations with the structural proba-\nbilities to sample networks conditioned on attribute values,\nwhile keeping the expected edge probabilities and degrees\nof the input graph model. We outline an efficient method\nfor estimating the parameters of AGM, as well as a sam-\npling method based on Accept-Reject sampling to generate\nedges with correlated attributes. We demonstrate the effi-\nciency and accuracy of our AGM framework on two large\nreal-world networks, showing that AGM scales to networks\nwith hundreds of thousands of vertices, as well as having\nhigh attribute correlation."}
{"Title": "WikiWho: Precise and Efficient Attribution of Authorship\nof Revisioned Content", "Abstract": "Revisioned text content is present in numerous collaboration plat-\nforms on the Web, most notably Wikis. To track authorship of text\ntokensinsuchsystemshasmanypotentialapplications; theidentifi-\ncation of main authors for licensing reasons or tracing collaborative\nwriting patterns over time, to name some. In this context, two main\nchallenges arise. First, it is critical for such an authorship track-\ning system to be precise in its attributions, to be reliable for further\nprocessing. Second, it has to run efficiently even on very large\ndatasets, such as Wikipedia. As a solution, we propose a graph-\nbased model to represent revisioned content and an algorithm over\nthis model that tackles both issues effectively. We describe the op-\ntimal implementation and design choices when tuning it to a Wiki\nenvironment. We further present a gold standard of 240 tokens\nfrom English Wikipedia articles annotated with their origin. This\ngold standard was created manually and confirmed by multiple in-\ndependent users of a crowdsourcing platform. It is the first gold\nstandard of this kind and quality and our solution achieves an aver-\nage of 95% precision on this data set. We also perform a first-ever\nprecision evaluation of the state-of-the-art algorithm for the task,\nexceeding it by over 10% on average. Our approach outperforms\ntheexecutiontimeofthestate-of-the-artbyoneorderofmagnitude,\nas we demonstrate on a sample of over 240 English Wikipedia arti-\ncles. We argue that the increased size of an optional materialization\nof our results by about 10% compared to the baseline is a favorable\ntrade-off, given the large advantage in runtime performance."}
{"Title": "What Makes a Good Biography?", "Abstract": "With more than 22 million articles, the largest collaborative knowl-\nedge resource never sleeps, experiencing several article edits every\nsecond. Over one fifth of these articles describes individual peo-\nple, the majority of which are still alive. Such articles are, by their\nnature, prone to corruption and vandalism. Manual quality assur-\nance by experts can barely cope with this massive amount of data.\nCan it be effectively replaced by feedback from the crowd? Can we\nprovide meaningful support for quality assurance with automated\ntext processing techniques? Which properties of the articles should\nthen play a key role in the machine learning algorithms and why?\nIn this paper, we study the user-perceived quality of Wikipedia\narticles based on a novel Wikipedia user feedback dataset. In con-\ntrast to previous work on quality assessment which mostly relied\non judgements of active Wikipedia authors, we analyze ratings of\nordinary Wikipedia users along four quality dimensions (complete,\nwell written, trustworthy and objective). We first present an empi-\nrical analysis of the novel dataset with over 36 million Wikipedia\narticle ratings. We then select a subset of biographical articles and\nperform classification experiments to predict their quality ratings\nalong each of the dimensions, exploring multiple linguistic, surface\nand network properties of the rated articles. Additionally, we study\nthe classification performance and differences for the biographies\nof living and dead people as well as those for men and women. We\ndemonstrate the effectiveness of our approach by the F 1 scores of\n0.94,0.89,0.73, and0.73forthedimensionscomplete, wellwritten,\ntrustworthy, and objective. Based on the results, we believe that the\nquality assessment of big textual data can be effectively supported\nby current text classification and language processing tools."}
{"Title": "What Makes an Image Popular?", "Abstract": "Hundreds of thousands of photographs are uploaded to the\ninternet every minute through various social networking and\nphoto sharing platforms. While some images get millions\nof views, others are completely ignored. Even from the\nsame users, different photographs receive different number\nof views. This begs the question: What makes a photograph\npopular? Can we predict the number of views a photograph\nwill receive even before it is uploaded? These are some of the\nquestions we address in this work. We investigate two key\ncomponents of an image that affect its popularity, namely\nthe image content and social context. Using a dataset of\nabout 2.3 million images from Flickr, we demonstrate that\nwe can reliably predict the normalized view count of images\nwith a rank correlation of 0.81 using both image content and\nsocial cues. In this paper, we show the importance of image\ncues such as color, gradients, deep learning features and the\nset of objects present, as well as the importance of various\nsocial cues such as number of friends or number of photos\nuploaded that lead to high or low popularity of images."}
{"Title": "STFU NOOB! Predicting Crowdsourced Decisions\non Toxic Behavior in Online Games", "Abstract": "One problem facing players of competitive games is negative, or\ntoxic, behavior. League of Legends, the largest eSport game, uses\na crowdsourcing platform called the Tribunal to judge whether a\nreported toxic player should be punished or not. The Tribunal is a\ntwo stage system requiring reports from those players that directly\nobserve toxic behavior, and human experts that review aggregated\nreports. While this system has successfully dealt with the vague\nnature of toxic behavior by majority rules based on many votes, it\nnaturally requires tremendous cost, time, and human efforts.\nIn this paper, we propose a supervised learning approach for\npredicting crowdsourced decisions on toxic behavior with large-\nscale labeled data collections; over 10 million user reports involved\nin 1.46 million toxic players and corresponding crowdsourced de-\ncisions. Our result shows good performance in detecting over-\nwhelmingly majority cases and predicting crowdsourced decisions\non them. We demonstrate good portability of our classifier across\nregions. Finally, we estimate the practical implications of our ap-\nproach, potential cost savings and victim protection."}
{"Title": "Unveiling Group Characteristics in Online Social Games:\nA Socio-Economic Analysis", "Abstract": "UnderstandingthegroupcharacteristicsinMMORPGsisimportant\nin user behavior studies since people tend to gather together and\nform groups due to their inherent nature. In this paper, we analyze\nthe group activities of users in Aion, one of the largest MMORPGs,\nbased on the records of the activities of 94,497 users. In particular,\nwe focus on (i) how social interactions within a group differ from\nthe ones across groups, (ii) what makes a group rise, sustain, or\nfall, (iii) how group members join and leave a group, and (iv) what\nmakes a group end. We first find that structural patterns of social in-\nteractions within a group are more likely to be close-knit and recip-\nrocativethantheonesacrossgroups. Wealsoobservethatmembers\nin a rising group (i.e., the number of members increases) are more\ncohesive, and communicate with more evenly within the group than\nthe ones in other groups. Our analysis further reveals that if a group\nis not cohesive, not actively communicating, or not evenly commu-\nnicating among members, members of the group tend to leave."}
{"Title": "XXXtortion? Inferring Registration Intent in the .XXX TLD", "Abstract": "After a decade-long approval process, multiple rejections, and an\nindependent review, ICANN approved the xxx TLD for inclusion\nin the Domain Name System, to begin general availability on De-\ncember 6, 2011. Its sponsoring registry proposed it as an expansion\nof the name space, as well as a way to separate adult from child-\nappropriate content. Many independent groups, including trade-\nmark holders, political groups, and the adult entertainment indus-\ntry itself, were concerned that it would primarily generate value\nthrough defensive and speculative registrations, without actually\nserving a real need. This paper measures the validity of these con-\ncerns using data gathered from ICANN, whois, and Web requests.\nWe use this information to characterize each xxx domain and in-\nfer the registrant\u2019s most likely intent. We find that at most 3.8%\nof xxx domains host or redirect to potentially legitimate Web con-\ntent, with the rest generally serving either defensive or speculative\npurposes. Indeed, registrants spent roughly $13M up front to de-\nfend existing brands and trademarks within the xxx TLD, and an\nadditional $11M over the course of the first year. Additional ev-\nidence suggests that over 80% of annual domain registrations are\nfor purely defensive purposes and do not even resolve."}
{"Title": "The Bursty Dynamics of the Twitter Information Network", "Abstract": "In online social media systems users are not only posting, consum-\ning, and resharing content, but also creating new and destroying\nexisting connections in the underlying social network. While each\nof these two types of dynamics has individually been studied in the\npast, much less is known about the connection between the two.\nHow does user information posting and seeking behavior interact\nwith the evolution of the underlying social network structure?\nHere, we study ways in which network structure reacts to users\nposting and sharing content. We examine the complete dynamics\nof the Twitter information network, where users post and reshare\ninformation while they also create and destroy connections. We\nfind that the dynamics of network structure can be characterized by\nsteady rates of change, interrupted by sudden bursts. Information\ndiffusion in the form of cascades of post re-sharing often creates\nsuch sudden bursts of new connections, which significantly change\nusers\u2019 local network structure. These bursts transform users\u2019 net-\nworks of followers to become structurally more cohesive as well as\nmore homogenous in terms of follower interests. We also explore\nthe effect of the information content on the dynamics of the net-\nwork and find evidence that the appearance of new topics and real-\nworld events can lead to significant changes in edge creations and\ndeletions. Lastly, we develop a model that quantifies the dynam-\nics of the network and the occurrence of these bursts as a function\nof the information spreading through the network. The model can\nsuccessfully predict which information diffusion events will lead to\nbursts in network dynamics."}
{"Title": "Can Cascades be Predicted?", "Abstract": "On many social networking web sites such as Facebook and Twit-\nter, resharing or reposting functionality allows users to share oth-\ners\u2019 content with their own friends or followers. As content is\nreshared from user to user, large cascades of reshares can form.\nWhile a growing body of research has focused on analyzing and\ncharacterizing such cascades, a recent, parallel line of work has\nargued that the future trajectory of a cascade may be inherently un-\npredictable. In this work, we develop a framework for addressing\ncascade prediction problems. On a large sample of photo reshare\ncascades on Facebook, we find strong performance in predicting\nwhether a cascade will continue to grow in the future. We find that\nthe relative growth of a cascade becomes more predictable as we\nobserve more of its reshares, that temporal and structural features\nare key predictors of cascade size, and that initially, breadth, rather\nthan depth in a cascade is a better indicator of larger cascades. This\nprediction performance is robust in the sense that multiple distinct\nclasses of features all achieve similar performance. We also dis-\ncover that temporal features are predictive of a cascade\u2019s eventual\nshape. Observing independent cascades of the same content, we\nfind that while these cascades differ greatly in size, we are still able\nto predict which ends up the largest."}
{"Title": "Fast Topic Discovery From Web Search Streams", "Abstract": "Web search involves voluminous data streams that record millions\nof users\u2019 interactions with the search engine. Recently latent topics\nin web search data have been found to be critical for a wide range\nof search engine applications such as search personalization and\nsearch history warehousing. However, the existing methods usually\ndiscover latent topics from web search data in an offline and retro-\nspective fashion. Hence, they are increasingly ineffective in the\nface of the ever-increasing web search data that accumulate in the\nformat of online streams. In this paper, we propose a novel proba-\nbilistic topic model, the Web Search Stream Model (WSSM), which\nis delicately calibrated for handling two salient features of the web\nsearch data: it is in the format of streams and in massive volume.\nWe further propose an efficient parameter inference method, the\nStream Parameter Inference (SPI) to efficiently train WSSM with\nmassive web search streams. Based on a large-scale search engine\nquery log, we conduct extensive experiments to verify the effec-\ntiveness and efficiency of WSSM and SPI. We observe that WSSM\ntogether with SPI discovers latent topics from web search streams\nfaster than the state-of-the-art methods while retaining a compara-\nble topic modeling accuracy."}
{"Title": "A Hierarchical Dirichlet Model for Taxonomy Expansion for\nSearch Engines", "Abstract": "Emerging trends and products pose a challenge to mod-\nern search engines since they must adapt to the constantly\nchanging needs and interests of users. For example, verti-\ncal search engines, such as Amazon, eBay, Walmart, Yelp\nand Yahoo! Local, provide business category hierarchies for\npeople to navigate through millions of business listings. The\ncategory information also provides important ranking fea-\ntures that can be used to improve search experience. Howev-\ner, category hierarchies are often manually crafted by some\nhuman experts and they are far from complete. Manual-\nly constructed category hierarchies cannot handle the ever-\nchanging and sometimes long-tail user information needs.\nIn this paper, we study the problem of how to expand an\nexisting category hierarchy for a search/navigation system\nto accommodate the information needs of users more com-\nprehensively. We propose a general framework for this task,\nwhich has three steps: 1) detecting meaningful missing cate-\ngories; 2) modeling the category hierarchy using a hierarchi-\ncal Dirichlet model and predicting the optimal tree structure\naccording to the model; 3) reorganizing the corpus using the\ncomplete category structure, i.e., associating each webpage\nwith the relevant categories from the complete category hier-\narchy. Experimental results demonstrate that our proposed\nframework generates a high-quality category hierarchy and\nsignificantly boosts the retrieval performance."}
{"Title": "Recent and Robust Query Auto-Completion", "Abstract": "Query auto-completion (QAC) is a common interactive feature that\nassists users in formulating queries by providing completion sug-\ngestions as they type. In order for QAC to minimise the user\u2019s cog-\nnitive and physical effort, it must: (i) suggest the user\u2019s intended\nquery after minimal input keystrokes, and (ii) rank the user\u2019s in-\ntended query highly in completion suggestions. Typically, QAC\napproaches rank completion suggestions by their past popularity.\nAccordingly, QAC is usually very effective for previously seen\nand consistently popular queries. Users are increasingly turning\nto search engines to find out about unpredictable emerging and\nongoing events and phenomena, often using previously unseen or\nunpopular queries. Consequently, QAC must be both robust and\ntime-sensitive \u2013 that is, able to sufficiently rank both consistently\nand recently popular queries in completion suggestions. To address\nthis trade-off, we propose several practical completion suggestion\nranking approaches, including: (i) a sliding window of query pop-\nularity evidence from the past 2-28 days, (ii) the query popular-\nity distribution in the last N queries observed with a given prefix,\nand (iii) short-range query popularity prediction based on recently\nobserved trends. Using real-time simulation experiments, we ex-\ntensively investigated the parameters necessary to maximise QAC\neffectiveness for three openly available query log datasets with pre-\nfixes of 2-5 characters: MSN and AOL (both English), and So-\ngou 2008 (Chinese). Optimal parameters vary for each query log,\ncapturing the differing temporal dynamics and querying distribu-\ntions. Results demonstrate consistent and language-independent\nimprovements of up to 9.2% over a non-temporal QAC baseline\nfor all query logs with prefix lengths of 2-3 characters. This work\nis an important step towards more effective QAC approaches."}
{"Title": "Short-Text Representation using Diffusion Wavelets", "Abstract": "Usual text document representations such as tf-idf do not\nwork well in classification tasks for short-text documents and\nacross diverse data domains. Optimizing different represen-\ntations for different data domains is infeasible in a practical\nsetting on the Internet. Mining such representations from\nthe data in an unsupervised manner is desirable. In this\npaper, we study a representation based on the multi-scale\nharmonic analysis of term-term co-occurrence graph. This\nrepresentation is not only sparse, but also leads to the dis-\ncovery of semantically coherent topics in data. In our exper-\niments on user-generated short documents e.g., newsgroup\nmessages, user comments, and meta-data, we found this rep-\nresentation to outperform other representations across dif-\nferent choice of classifiers. Similar improvements were also\nobserved for data sets in Chinese and Portuguese languages."}
{"Title": "Trust Prediction Using Positive, Implicit, and Negative\nInformation", "Abstract": "We propose a novel method to predict accurately trust re-\nlationships of a target user even if he/she does not have\nmuch interaction information. The proposed method consid-\ners positive, implicit, and negative information of all users\nin a network based on belief propagation to predict trust\nrelationships of a target user."}
{"Title": "Detecting Suspicious Following Behavior in\nMultimillion-Node Social Networks", "Abstract": "In a multimillion-node network of who-follows-whom like\nTwitter, since a high count of followers leads to higher prof-\nits, users have the incentive to boost their in-degree. Can\nwe spot the suspicious following behavior, which may indi-\ncate zombie followers and suspicious followees? To answer\nthe above question, we propose CatchSync, which exploits\ntwo tell-tale signs of the suspicious behavior: (a) synchro-\nnized behavior: the zombie followers have extremely similar\nfollowing behavior pattern, because, say, they are generated\nby a script; and (b) abnormal behavior: their behavior pat-\ntern is very different from the majority. Our CatchSync\nintroduces novel measures to quantify both concepts and\ncatches the suspicious behavior. Moreover, we show it is\neffective in a real-world social network."}
{"Title": "Cognitive Resources-Aware Web Service Selection in\nMobile Computing Environments", "Abstract": "The proactive and spontaneous delivery of services for mobile\nusers on the move can lead to the depletion of users' mental\nresources, affecting the normal processes of their physical\nactivities. This is due to the competition for limited mental\nresources between the human-computer interactions required by\nservices and the users' physical activities. To deal with this\nproblem we propose a service selection method based on two\ntheories from cognitive psychology. This mechanism assesses the\ndegree of demand for mental resources by both the physical\nactivities and the services. Additionally, a service binding and\nscheduling algorithm ensures less cognitively-taxing mobile\nservice compositions."}
{"Title": "Learning from Unstructured Multimedia Data", "Abstract": "Information in today\u2019s world is highly heterogeneous and\nunstructured. Learning and inferring from such data is chal-\nlenging and is an active research topic. In this paper, we\npresent and investigate an approach to learning from het-\nerogeneous and unstructured multimedia data. Inspired by\napproaches in many fields including computer vision, we\ninvestigate a histogram based approach to represent mul-\ntimodal unstructured data. While existing works have pre-\ndominantly focused on histogram based approaches for uni-\nmodal data, we present a methodology to represent unstruc-\ntured multimodal data. We explain how to discover the pro-\ntotypical features or codewords over which these histograms\nare built. We present experimental results on classification\nand retrieval tasks performed on the histogram based repre-\nsentation."}
{"Title": "Hierarchical Interest Graph from Tweets", "Abstract": "Industry and researchers have identified numerous ways to\nmonetize microblogs for personalization and recommenda-\ntion. A common challenge across these different works is\nthe identification of user interests. Although techniques have\nbeen developed to address this challenge, a flexible approach\nthat spans multiple levels of granularity in user interests has\nnot been forthcoming. In this work, we focus on exploiting\nhierarchical semantics of concepts to infer richer user inter-\nests expressed as a Hierarchical Interest Graph. To create\nsuch graphs, we utilize users\u2019 tweets to first ground potential\nuser interests to structured background knowledge such as\nWikipedia Category Graph. We then adapt spreading acti-\nvation theory to assign user interest score to each category\nin the hierarchy. The Hierarchical Interest Graph not only\ncomprises of users\u2019 explicitly mentioned interests determined\nfrom Twitter, but also their implicit interest c"}
{"Title": "Cognitive Search Intents Hidden Behind Queries:\nA User Study on Query Formulations", "Abstract": "This study investigated query formulations by users with Cognitive\nSearch Intents (CSI), which are needs for the cognitive character-\nistics of documents to be retrieved, e.g. comprehensibility, subjec-\ntivity, and concreteness. We proposed an example-based method of\nspecifying search intents to observe unbiased query formulations.\nOur user study revealed that about half our subjects did not input\nany keywords representing CSIs, even though they were conscious\nof given CSIs."}
{"Title": "Identifying Spreaders of Malicious Behaviors\nin Online Games", "Abstract": "Massively multiplayer online role-playing games (MMORPGs)\nsimulate the real world and require highly complex user\ninteraction. Many human behaviors can be observed in\nMMORPGs, such as social interactions, economic behaviors, and\nmalicious behaviors. In this study, we primarily focus on\nmalicious behavior, especially cheating using game bots. Bots can\nbe diffused on the social network in an epidemic style. When bots\ndiffuse on the social network, a user\u2019s influence on the diffusion\nprocess varies owing to different characteristics and network\npositions. We aim to identify the influentials in the game world\nand investigate how they differ from normal users. Identifying the\ninfluentials in the diffusion of malicious behaviors will enable the\ngame company to act proactively and preventively towards\nmalicious users such as game bot users."}
{"Title": "Link Prediction Based on Generalized Cluster Information", "Abstract": "Understanding of which new interactions among data objects are\nlikely to occur in the future is crucial for a deeper understanding\nof network dynamics and evolution. This question is largely un-\nexplored except a local neighborhood perspective, partly owing to\nthe difficulty in finding major factors which heavily affect the link\nprediction problem. In this paper, we propose LPCSP, a novel link\nprediction method which exploits the generalized cluster informa-\ntion containing cluster relations and cluster evolution information.\nExperiments show that our proposed LPCSP is accurate, scalable,\nand useful for link prediction on real world graphs."}
{"Title": "Finding Informative Q&As on Twitter", "Abstract": "Question & Answer (Q&A) behaviors on social media have\nhuge potential as a rich source of information and knowl-\nedge online. However, little is known about how much di-\nversity there exists in the topics covered in such Q&As and\nwhether unstructured social media data can be made search-\nable. This paper seeks the feasibility of utilizing social me-\ndia data for developing a Q&A service by examining the\ntopic coverage in Twitter conversations. We propose a new\nframework to automatically extract informative Q&A con-\ntent using machine learning techniques."}
{"Title": "Macro-Level Information Transfer across Social Networks", "Abstract": "This study proposes a model-free approach to infer macro-\nlevel information flow across online social systems in terms of\nthe strength and directionality of influence among systems."}
{"Title": "A Computational Analysis of Agenda Setting", "Abstract": "Agenda setting theory explains how media affects its audi-\nence. While traditional media studies have done extensive\nresearch on agenda setting, there are important limitations\nin those studies, including using a small set of issues, running\ncostly surveys of public interest, and manually categorizing\nthe articles into positive and negative frames. In this paper,\nwe propose to tackle these limitations with a computational\napproach and a large dataset of online news. Overall, we\ndemonstrate how to carry out a large-scale computational\nresearch of agenda setting with online news data using ma-\nchine learning."}
{"Title": "Investigating Socio-cultural Behavior of Users Reflected in\nDifferent Social Channels on K-pop", "Abstract": "In this paper we investigated the socio-cultural behavior of users\nreflected in the two different social media channels, YouTube and\nTwitter. We conducted the comparative analysis of the networks\ngenerated from the two channels. The relationship we set for each\nnetwork is the relatedness on YouTube and the co-links on\nTwitter. From the results, we revealed that the social media\ninfluenced the distinct socio-cultural behaviors of their users.\nSpecifically,  Twitter  network  better  showed  the  actual\nconsumption of contents in the field of the k-pop culture than\nYouTube. From this study, we contributed to offer a novel\napproach for exploring the socio-cultural behavior of users on the\nsocial media."}
{"Title": "Semantically Enhanced Keyword Search for Smartphones", "Abstract": "To apply semantic search to smartphones, we propose an efficient\nsemantic search method based on a lightweight mobile ontology.\nThrough a prototype implementation of a semantic search engine\non an android smartphone, experimental results show that the\nproposed method provides more accurate search results and a\nbetter user experience compared to the conventional method."}
{"Title": "Motives for Mass Interactions in Online Sports Viewing", "Abstract": "Recent advances of social TV services allow sports fans to watch\nsports games at any place and to lively interact with other fans via\nonline chatting. Despite their popularity, however, so far little is\nknown about the key properties of such mass interactions in\nonline sports viewing. In this paper, we explore motives for mass\ninteractions in online sports viewing and investigate how the\nmotives are related to viewing and chatting behaviors, by studyin"}
{"Title": "The Market of Internet Sponsored Links in the Context of\nCompetition Law: Can Modeling Help?", "Abstract": "Internet search market of key words attracts much attention\nin conjunction with the legal proceedings against Google. It\nhas been recognized that legal argumentation alone may not\nbe sufficient to disentangle the complexity of the case. An\napproach that includs mathematical modeling is needed, to\ndistinguish the effects of the factors intrinsic to the market\nand the consequences of anticompetitive practices. This pa-\nper proposes a modeling framework based on explicit treat-\nment of users\u2019 switching between the search platforms in\nthe environment set by the platforms\u2019 strategic decisions,\nand demonstrates some of its applications."}
{"Title": "Photo Recall: Using the Internet to Label Your Photos", "Abstract": "We describe a system for searching your personal photos us-\ning an extremely wide range of text queries, including dates\nand holidays (Halloween), named and categorical places (Em-\npire State Building or park), events and occasions (Radio-\nhead concert or wedding), activities (skiing), object cate-\ngories (whales), attributes (outdoors), and object instances\n(Mona Lisa), and any combination of these \u2013 all with no\nmanual labeling required. We accomplish this by correlat-\ning information in your photos \u2013 the timestamps, GPS lo-\ncations, and image pixels \u2013 to information mined from the\nInternet. This includes matching dates to holidays listed on\nWikipedia, GPS coordinates to places listed on Wikimapia,\nplaces and dates to find named events using Google, visual\ncategories using classifiers either pre-trained on ImageNet or\ntrained on-the-fly using results from Google Image Search,\nand object instances using interest point-based matching,\nagain using results from Google Images. We tie all of these\ndisparate sources of information together in a unified way,\nallowing for fast and accurate searches using whatever infor-\nmation you remember about a photo."}
{"Title": "Learning To Predict Trending Queries: Classification \u2013\nBased", "Abstract": "Among the many tasks driven by very large scaled web\nsearch queries, it is an interesting task to predict how likely\nqueries about a topic become popular (a.k.a. trending or\nbuzzing) as the news in the near future, which is known as\n\u201cDetecting trending queries.\u201d This task is nontrivial since\nthe realization of buzzing trends of queries often requires\nsufficient statistics through users\u2019 activities. To address\nthis challenge, we propose a novel framework that predicts\nwhether queries become trending in the future. In princi-\nple, our system is built on the two learners. The first is\nto learn dynamics of time series for queries. The second,\nour decision maker, is to learn a binary classifier that de-\ntermines whether queries become trending. Our framework\nis extremely efficient to be built taking advantage of the\ngrid architecture that allows to deal with the large volume\nof data. In addition, it is flexible to continuously adapt as\ntrending patterns evolve. The experiments results show that\nour approach achieves high quality of accuracy (over 77.5%\ntrue positive rate) and yet detects much earlier (on average\n29 hours advanced) than that of the baseline system."}
{"Title": "Users\u2019 Behavioral Prediction for Phishing Detection", "Abstract": "This study explores the users\u2019 web browsing behaviors that\nconfront phishing situations for context-aware phishing detection.\nWe extract discriminative features of each clicked URL, i.e.,\ndomain name, bag-of-words, generic Top-Level Domains, IP\naddress, and port number, to develop a linear chain CRF model\nfor users\u2019 behavioral prediction. Large-scale experiments show\nthat our method achieves promising performance for predicting\nthe phishing threats of users\u2019 next accesses. Error analysis\nindicates that our model results in a favorably low false positive\nrate. In practice, our solution is complementary to the existing\nanti-phishing techniques for cost-effectively blocking phishing\nthreats from users\u2019 behavioral perspectives."}
{"Title": "Finding k-Highest Betweenness Centrality Vertices\nin Graphs", "Abstract": "The betweenness centrality is a measure for the relative par-\nticipation of the vertex in the shortest paths in the graph.\nIn many cases, we are interested in the k-highest between-\nness centrality vertices only rather than all the vertices in\na graph. In this paper, we study an efficient algorithm for\nfinding the exact k-highest betweenness centrality vertices."}
{"Title": "Towards Online Review Spam Detection", "Abstract": "User reviews play a crucial role in Web, since many decisions are\nmade based on them. However, review spam would misled the\nusers, which is extremely obnoxious. In this poster, we explore\nthe problem of online review spam detection. Firstly, we devise\nsix features to find the spam based on the review content and re-\nviewer behaviors. Secondly, we apply supervised methods and an\nunsupervised one for spotting the review spam as early as possible.\nFinally, we carry out intensive experiments on a real-world review\nset to verify the proposed methods."}
{"Title": "Efficient RDF Stream Reasoning with Graphics Processing\nUnits (GPUs)", "Abstract": "In this paper, we study the problem of stream reasoning and\npropose a reasoning approach over large amounts of RDF\ndata, which uses graphics processing units (GPU) to improve\nthe performance. First, we show how the problem of stream\nreasoning can be reduced to a temporal reasoning problem.\nThen, we describe a number of algorithms to perform stream\nreasoning with GPUs."}
{"Title": "Automatic Keywords Generation for Contextual\nAdvertising", "Abstract": "Contextual Advertising (CA) is an important area in the\nindustry of online advertising. Typically, CA algorithms re-\nturn a set of related ads based on some keywords extracted\nfrom the content of webpages. Therefore, extracting the best\nset of representative keywords from a given webpage is the\nkey to the success of CA. In this paper, we introduce a new\nkeywords generation approach that uses some novel NLP\nfeatures including POS and named-entities tagging. Unlike\nmost of the existing keyword extraction algorithms, our pro-\nposed framework is able to generate some related keywords\nwhich do not exist in the webpage. A monetization parame-\nter, predicted from historical search keyword performance, is\nalso used to rank potential keywords in order to balance the\nRPM (Revenue Per 1000 Matches) and relevance. Exper-\nimental results over a very large real-world data set shows\nthat the proposed approach can outperform the state-of-the-\nart system in both relevance and monetization metrics."}
{"Title": "Detecting Trending Topics Using Page Visitation Statistics", "Abstract": "Many applications including realtime recommenders and ad-\ntargeting systems have a need to identify trending concepts\nto prioritize the information presented to end-users. In this\npaper, we describe a novel approach that identifies trending\nconcepts using the hourly Wikipedia page visitation statis-\ntics freely available for download. We describe a MapReduce\nframework that analyzes the raw hourly visitation logs and\ngenerates a ranked list of trending concepts on a daily ba-\nsis. We validate this approach by extracting hourly lists of\ntrending news articles, mapping these articles to Wikipedia\nconcepts, and computing the similarity of the two lists ac-\ncording to several commonly used measures."}
{"Title": "Unsupervised Approach for Shallow Domain Ontology\nConstruction from Corpus", "Abstract": "In this work we propose an unsupervised approach to con-\nstruct a domain-specific ontology from corpus. It is essen-\ntial for Information Retrieval systems to identify important\ndomain concepts and relationships between them. We iden-\ntify important domain terms of which multi-words form an\nimportant component. Our approach identifies 40% of the\ndomain terms, compared to 22% identified by WordNet on\nmanually annotated smartphone data. We propose an ap-\nproach to construct a shallow ontology from discovered do-\nmain terms by identifying four domain relations namely,\nSynonyms (\u2018similar-to\u2019), Type-Of (\u2018is-a\u2019), Action-On (\u2018meth-\nods\u2019) and Feature-Of (\u2018attributes\u2019), where we achieve an F-\nScore of 49.14%, 65.5%, 65% and 80% respectively."}
{"Title": "SepaRating: An Approach to Reputation Computation\nBased on Rating Separation in e-Marketplace", "Abstract": "This paper proposes SepaRating, a novel mechanism that\nseparates a buyer\u2019s rating on a transaction into two kinds\nof scores: seller\u2019s score and item\u2019s score. SepaRating pro-\nvides the reputation of sellers correctly based on the seller\u2019s\nscore by repetitive separations, which helps potential buyers\nto find more reliable sellers. We verify the effectiveness of\nSepaRating via a series of experiments."}
{"Title": "Semantic Annotation for Dynamic Web Environment", "Abstract": "The semantic Web is a promising future Web environment. In or-\nder to realize the semantic Web, the semantic annotation should\nbe widely available. The studies for generating the semantic an-\nnotation do not provide a solution to the \u2018document evolution\u2019 re-\nquirement which is to maintain the consistency between semantic\nannotations and Web pages. In this paper, we propose an efficient\nsolution to the requirement, that is to separately generate the long-\nterm annotation and the short-term annotation. The experimental\nresults show that our approach outperforms an existing approach\nwhich is the most efficient among the automatic approaches based\non static Web pages."}
{"Title": "Learning Joint Representation for Community Question\nAnswering with Tri-modal DBM", "Abstract": "One of the main research tasks in Community question an-\nswering (CQA) is to find most relevant questions for a given\nnew query, thereby providing useful knowledge for the users.\nTraditionally used methods such as bag-of-words or latent\nsemantic models consider queries, questions and answers\nin a same feature space. However, the correlations among\nqueries, questions and answers imply that they lie in differ-\nent feature spaces. In light of these issues, we proposed a tri-\nmodal deep boltzmann machine (tri-DBM) to extract unified\nrepresentation for query, question and answer. Experiments\non Yahoo! Answers dataset reveal using these unified rep-\nresentation to train a classifier judging semantic matching\nlevel between query and question outperforms models using\nbag-of-words or LSA representation significantly."}
{"Title": "Efficient CPU-GPU Work Sharing for\nData-Parallel JavaScript Workloads", "Abstract": "Modern web browsers are required to execute many complex,\ncompute-intensive applications, mostly written in JavaScript.\nWith widespread adoption of heterogeneous processors, re-\ncent JavaScript-based data-parallel programming models,\nsuch as River Trail and WebCL, support multiple types of\nprocessing elements including CPUs and GPUs. However,\nsignificant performance gains are still left on the table since\nthe program kernel runs on only one compute device, typ-\nically selected at kernel invocation. This paper proposes a\nnew framework for efficient work sharing between CPU and\nGPU for data-parallel JavaScript workloads. The work shar-\ning scheduler partitions the input data into smaller chunks\nand dynamically dispatches them to both CPU and GPU\nfor concurrent execution. For four data-parallel programs,\nour framework improves performance by up to 65% with a\ngeometric mean speedup of 33% over GPU-only execution."}
{"Title": "User Profiles Based on Revisitation Times", "Abstract": "Our work is devoted to Web revisitation patterns of individ-\nual users. Everybody revisits Web pages, but their reasons\nfor doing so can differ. We analyzed Web interaction logs of\nmillions users to characterize how people revisit Web con-\ntent. We revealed that each user have its own distribution of\nrevisitation times. This distribution follows Power Law with\nsome exponent, which captures specific user peculiarities."}
{"Title": "Combining Geographical Information of Users and Content\nof Items for Accurate Rating Prediction", "Abstract": "Recommender systems have attracted attentions lately due\nto their wide and successful applications in online advertis-\ning. In this paper, we propose a bayesian generative model\nto describe the generative process of rating, which com-\nbines geographical information of users and content of items.\nThe generative model consists of two interacting LDA mod-\nels, where one LDA model for location-based user groups\n(user dimension) and the other for the topics of content of\nitems(item dimension). A Gibbs sampling algorithm is pro-\nposed for parameter estimation. Experiments have shown\nour proposed method outperforms baseline methods."}
{"Title": "RDF-X: A Language for Sanitizing RDF Graphs", "Abstract": "With the advent of Semantic Web and Resource Description\nFramework (RDF), the web is likely to witness an unprece-\ndented wealth of knowledge, resulting from seamless integra-\ntion of various data sources. Data integration is one of the\nkey features of RDF, however, absence of secure means for\nmanaging sensitive RDF data may prevent sharing of critical\ndata altogether or may cause serious damage. Towards this\nend we present a language for sanitizing RDF graphs, which\ncomprises a set of sanitization operations that transform a\ngraph by concealing the sensitive data. These operations are\nmodeled into a new SPARQL query form known as SANITIZE ,\nwhich can also be leveraged towards fine grained access con-\ntrol and building advanced anonymization features."}
{"Title": "Fast Maximum Clique Algorithms for Large Graphs", "Abstract": "We propose a fast, parallel maximum clique algorithm for\nlarge sparse graphs that is designed to exploit characteristics\nof social and information networks. Despite clique\u2019s status\nas an NP-hard problem with poor approximation guarantees,\nour method exhibits nearly linear runtime scaling over real-\nworld networks ranging from 1000 to 100 million nodes. In a\ntest on a social network with 1.8 billion edges, the algorithm\nfinds the largest clique in about 20 minutes. Key to the\nefficiency of our algorithm are an initial heuristic procedure\nthat finds a large clique quickly and a parallelized branch\nand bound strategy with aggressive pruning and ordering\ntechniques. We use the algorithm to compute the largest\ntemporal strong components of temporal contact networks."}
{"Title": "Implicit Feature Detection for Sentiment Analysis", "Abstract": "Implicit feature detection is a promising research direction\nthat has not seen much research yet. Based on previous\nwork, where co-occurrences between notional words and ex-\nplicit features are used to find implicit features, this research\ncritically reviews its underlying assumptions and proposes a\nrevised algorithm, that directly uses the co-occurrences be-\ntween implicit features and notional words. The revision is\nshown to perform better than the original method, but both\nmethods are shown to fail in a more realistic scenario."}
{"Title": "Summarizing Social Image Search Results", "Abstract": "Most existing social image search engines present search results as\na ranked list of images, which cannot be consumed by users in a\nnatural and intuitive manner. Here, we present a novel algorithm\nthat exploits both visual features and tags of the search results to\ngenerate high quality image search result summary. The summary\nnot only breaks the results into visually and semantically coherent\nclusters, but it also maximizes the coverage of the original search\nresults. We demonstrate the effectiveness of our method against\nstate-of-the-art image summarization and clustering algorithms."}
{"Title": "TOMOHA: TOpic MOdel-based HAshtag Recommendation\non Twitter", "Abstract": "On Twitter, hashtags are used to summarize topics of the tweet con-\ntent and to help to categorize and search tweets. However, hashtags\nare created in a free style and thus heterogeneous, increasing dif-\nficulty of their usage. We propose TOMOHA, a supervised TOpic\nMOdel-based solution for HAshtag recommendation on Twitter.\nWetreat hashtags as labels of topics, and developa supervised topic\nmodel to discover relationship among words, hashtags and topics\nof tweets. We also novelly add user following relationship into the\nmodel. We infer the probability that a hashtag will be contained\nin a new tweet, and recommend k most probable ones. We pro-\npose parallel computing and pruning techniques to speed up model\ntraining and recommendation process. Experiments show that our\nmethod can properly and efficiently recommend hashtags."}
{"Title": "Learning Semantic Representations Using Convolutional\nNeural Networks for Web Search", "Abstract": "This paper presents a series of new latent semantic models based\non a convolutional neural network (CNN) to learn low-\ndimensional semantic vectors for search queries and Web docu-\nments. By using the convolution-max pooling operation, local\ncontextual information at the word n-gram level is modeled first.\nThen, salient local features in a word sequence are combined to\nform a global feature vector. Finally, the high-level semantic in-\nformation of the word sequence is extracted to form a global vec-\ntor representation. The proposed models are trained on click-\nthrough data by maximizing the conditional likelihood of clicked\ndocuments given a query, using stochastic gradient ascent. The\nnew models are evaluated on a Web document ranking task using\na large-scale, real-world data set. Results show that our model\nsignificantly outperforms other semantic models, which were\nstate-of-the-art in retrieval performance prior to this work."}
{"Title": "Defending against User Identity Linkage Attack\nacross Multiple Online Social Networks", "Abstract": "We study the first countermeasure against user identity link-\nage attack across multiple online social networks (OSNs).\nOur goal is to keep as much as user\u2019s information in public\nand meanwhile prevent their identities from being linked on\ndifferent OSNs via k-anonymity. We develop a novel greedy\nalgorithm, incorporating an efficient manner to compute the\ngreedy function, and validate it in terms of both solution\nquality and robustness using real-world datasets."}
{"Title": "Beyond Modeling Private Actions: Predicting Social\nShares", "Abstract": "Westudytheproblemofpredictingsharingbehaviorfrome-commerce\nsites to friends on social networks via share widgets. The contex-\ntual variation in an action that is private (like rating a movie on\nNetflix), to one shared with friends online (like sharing an item on\nFacebook), to one that is completely public (like commenting on\na Youtube video) introduces behavioral differences that pose inter-\nesting challenges. In this paper, we show that users\u2019 interests man-\nifest in actions that spill across different types of channels such as\nsharing, browsing, and purchasing. This motivates leveraging all\nsuch signals available from the e-commerce platform. We show\nthat carefully incorporating signals from these interactions signifi-\ncantly improves share prediction accuracy."}
{"Title": "Face Recognition CAPTCHA Made Difficult", "Abstract": "A CAPTCHA is a Turing test to distinguish human users\nfrom automated scripts to defend against internet adversar-\nial attacks. As text-based CAPTCHAs (TBC) have become\nincreasingly difficult to solve, image-based CAPTCHAs, and\nparticularly face recognition CAPTCHAs (FRC), offer a chance\nto overcome TBC limitations. In this paper, we systemat-\nically design and implement a practical FRC, informed by\npsychological findings. We use gray-scale and binary images,\nwhich are computationally inexpensive to generate and de-\nploy. Furthermore, our FRC complies with CAPTCHA de-\nsign guidelines, thereby ensuring its robustness."}
{"Title": "Searching for Design Examples with Crowdsourcing", "Abstract": "Examples are very important in design, but existing tools\nfor design example search still do not cover many cases. For\ninstance, long tail queries containing subtle and subjective\ndesign concepts, like\u201ccalm and quiet\u201d,\u201celegant\u201d,\u201cdark back-\nground with a hint of color to make it less boring\u201d, are poorly\nsupported. This is mainly due to the inherent complexity\nof the task, which so far has been tackled only algorithmi-\ncally using general image search techniques. We propose\na powerful new approach based on crowdsourcing, which\ncomplements existing algorithmic approaches and addresses\ntheir shortcomings. Out of many explored crowdsourcing\nconfigurations we found that (1) a design need should be\nrepresented via several query images and (2) AMT crowd\nworkers should assess a query-specific relevance of a candi-\ndate example from a pre-built design collection. To test the\nutility of our approach, we compared it with Google Images\nin a query-by-example mode. Based on feedback from expert\ndesigners, the crowd selects more relevant design examples."}
{"Title": "Semantic Search Engine with an Intuitive User Interface", "Abstract": "It is crucial to enable regular users to explore RDF-compliant\ndata bases in an effective way, regardless their knowledge\nabout the SPARQL or the underlying ontology. Natural\nlanguage querying have been proposed to address this issue.\nHowever it has unavoidably lower accuracy, as compared\nto systems with graph-based querying interfaces, which, in\nturn, are usually still too difficult for regular users. This\npaper presents a search system of user interface that is more\nuser-friendly than widely known graph-based solutions."}
{"Title": "Translation Method of Contextual Information into Textual\nSpace of Advertisements", "Abstract": "Contextual advertising has a key problem to determine how\nto select the ads that are relevant to the page content and/or\nthe user information. We introduce a translation method\nthat learns a mapping of contextual information to the tex-\ntual features of ads by using past click data. This method is\neasy to implement and there is no need to modify an ordi-\nnary ad retrieval system because the contextual feature vec-\ntor is simply transformed into a term vector with the learned\nmatrix. We applied our approach with a real ad serving sys-\ntem and compared the online performance in A/B testing."}
{"Title": "Who will Trade with Whom?\nPredicting Buyer-Seller Interactions in Online Trading\nPlatforms through Social Networks", "Abstract": "Inthispaperwepresentthelatestresultsofarecentlystartedproject\nthat aims at studying the extent to which links between buyers and\nsellers, i.e. trading interactions in online trading platforms, can be\npredicted from external knowledge sources such as online social\nnetworks. To that end, we conducted a large-scale experiment on\ndata obtained from the virtual world Second Life. As our results re-\nveal, online social network data bears a significant potential (28%\nover the baseline) to predict links between buyers and sellers in\nonline trading platforms."}
{"Title": "Towards Awareness and Control in Choreographed User\nInterface Mashups", "Abstract": "Recent research in the field of user interface (UI) mashups\nhas focused on so-called choreographed compositions, where\ncommunication between components is not pre-defined by a\nmashup designer, but rather emerges from the components\u2019\nmessaging capabilities. Though the mashup development\nprocess gets simplified, such solutions bear several problems\nrelated to awareness and control of the emerging message\nflow. This paper presents an approach to systematically ex-\ntend choreographed mashups with visualization and tailor-\ning facilities. A first user study demonstrates that usability\nof the resulting solutions increases if proposed awareness and\ncontrol facilities are integrated."}
{"Title": "Ontology Population from Web Product Information", "Abstract": "With the vast amount of information available on the Web,\nthere is an increasing need to structure Web data in order to\nmake it accessible to both users and machines. E-commerce\nis one of the areas in which growing data congestion on the\nWeb has serious consequences. This paper proposes a frame-\nwork that is capable of populating a product ontology us-\ning tabular product information from Web shops. By for-\nmalizing product information in this way, better product\ncomparison or recommendation applications could be built.\nOur approach employs both lexical and syntactic matching\nfor mapping properties and instantiating values. The per-\nformed evaluation shows that instantiating consumer elec-\ntronics from Best Buy and Newegg.com results in an F 1\nscore of approximately 77%."}
{"Title": "Construction of Tag Ontological Graphs by Locally\nMinimizing Weighted Average Hops", "Abstract": "We present a data-driven approach for the construction of\nontological graphs on a set of image tags obtained from an-\nnotated image corpus. We treat each tag as a node in a\ngraph, and starting with a preliminary graph obtained us-\ning WordNet, we propose the graph construction as a re-\nfinement of the preliminary graph using corpus statistics.\nTowards this, we formulate an optimization problem which\nis solved using a local search based approach. To evaluate\nthe constructed ontological graphs, we propose a novel task\nwhich involves associating test images with tags while ob-\nserving partial set of associated tags."}
{"Title": "Answering Provenance-Aware Regular Path Queries on\nRDF Graphs Using an Automata-Based Algorithm", "Abstract": "This paper presents an automata-based algorithm for an-\nswering the provenance-aware regular path queries (RPQs)\nover RDF graphs on the Semantic Web. The provenance-\naware RPQs can explain why pairs of nodes in the classical\nsemantics appear in the result of an RPQ. We implement a\nparallel version of the automata-based algorithm using the\nPregel framework Giraph to efficiently evaluate provenance-\naware RPQs on large RDF graphs. The experimental results\nshow that our algorithms are effective and efficient to answer\nprovenance-aware RPQs on large real-world RDF graphs."}
{"Title": "3DOC: 3D Object CAPTCHA", "Abstract": "Current 2D CAPTCHA mechanisms can be easily defeated by\ncharacter recognition and segmentation attacks by automated\nmachines [1]. Recently, 3D CAPTCHA schemes [2] have been\nproposed to overcome the weaknesses of 2D CAPTCHA for a few\nwebsites. However, [4] demonstrates the offline pre-processing\ntechniques to break 3D CAPTCHA as well. In this work, we\npropose a novel 3D object based CAPTCHA scheme that projects\nthe CAPTCHA image over a 3D object. We develop the prototype\nand present the proof-of-concept of 3D object based CAPTCHA\nscheme to protect websites against automated attacks."}
{"Title": "Improving Query Suggestion through Noise Filtering and\nQuery Length Prediction", "Abstract": "Clustering-based methods are commonly used in Web search\nengines for query suggestion. Clustering is useful in reduc-\ning the sparseness of data. However, it also introduces noises\nand ignores the sequential information of query refinements\nin search sessions. In this paper, we propose to improve\ncluster based query suggestion from two perspectives: fil-\ntering out unrelated query candidates and predicting the\nrefinement direction. We observe two major refinements be-\nhaviors. One is to simplify the original query and the other\nis to specify it. Both could be modeled by predicting the\nlength (number of terms) of queries when candidates are be-\ning ranked. Two experimental results on the real query logs\nof a commercial search engine demonstrate the effectiveness\nof the proposed approaches."}
{"Title": "Detecting In-Situ Identity Fraud on Social Network\nServices: A Case Study on Facebook", "Abstract": "In this paper, we propose to use a continuous authentica-\ntion approach to detect the in-situ identity fraud incidents,\nwhich occur when the attackers use the same devices and IP\naddresses as the victims. Using Facebook as a case study, we\nshow that it is possible to detect such incidents by analyz-\ning SNS users\u2019 browsing behavior. Our experiment results\ndemonstrate that the approach can achieve reasonable ac-\ncuracy given a few minutes of observation time."}
{"Title": "Multi-Category Item Recommendation Using\nNeighborhood Associations in Trust Networks", "Abstract": "This paper proposes a novel recommendation method called\nRecDI. In the multi-category item recommendation domain,\nRecDI is designed to combine user ratings with information\ninvolving user\u2019s direct and indirect neighborhood associa-\ntions. Through relevant benchmarking experiments on two\nreal-world datasets, we show that RecDI achieves better per-\nformance than other traditional recommendation methods,\nwhich demonstrates the effectiveness of RecDI."}
{"Title": "Optimizing the Most Specific Concept Method\nfor Efficient Instance Checking", "Abstract": "Instancecheckingisconsideredacentraltoolfordataretrievalfrom\ndescription logic (DL) ontologies. In this paper, we propose a re-\nvised most specific concept (MSC) method for DL SHI, which\nconverts instance checking into subsumption problems. This re-\nvised method can generate small concepts that are specific-enough\nto answer a given query, and allow reasoning to explore only a sub-\nset of the ABox data to achieve efficiency. Experiments show effec-\ntiveness of our proposed method in terms of concept size reduction\nand the improvement in reasoning efficiency."}
{"Title": "Tag Propagation Based Recommendation across Diverse\nSocial Media", "Abstract": "Many real applications demand accurate cross-domain recommen-\ndation, e.g., recommending a Weibo (the largest Chinese Twitter)\nuser with the products in an e-commerce Web site. Since many so-\ncial media have rich tags on both items or users, tag-based profil-\ning became popular for recommendation. However, most previous\nrecommendation approaches have low effectiveness in handling s-\nparse data or matching tags from different social media. Address-\ning these problems, we first propose an optimized local tag propa-\ngationalgorithmtogeneratetagsforprofilingWeibousersandthen\nuse a Chinese knowledge graph accompanied by an improved ESA\n(explicit semantic analysis) for semantic matching of cross-domain\ntags. Empirical comparisons to the state-of-the-art approaches jus-\ntify the efficiency and effectiveness of our approaches."}
{"Title": "SoRank: Incorporating Social Information into Learning to\nRank Models for Recommendation", "Abstract": "Most existing learning to rank based recommendation meth-\nods only use user-item preferences to rank items, while ne-\nglecting social relations among users. In this paper, we pro-\npose a novel, effective and efficient model, SoRank, by inte-\ngrating social information among users into listwise ranking\nmodel to improve quality of ranked list of items. In ad-\ndition, with linear complexity to the number of observed\nratings, SoRank is able to scale to very large dataset. Ex-\nperimental results on publicly available dataset demonstrate\nthe effectiveness of SoRank."}
{"Title": "Election Trolling: Analyzing Sentiment in Tweets during\nPakistan Elections 2013", "Abstract": "The use of Twitter as a discussion platform for political is-\nsues has led researchers to study its role in predicting elec-\ntion outcomes. This work studies a much neglected aspect of\npolitics on Twitter namely \u201celection trolling\u201d whereby sup-\nporters of different political parties attack each other during\nelection campaigns. We also propose a novel strategy to de-\ntect terms that are usually not associated with sentiment\nbut are introduced by supporters of political parties to at-\ntack the opposing party. We demonstrate a lack of political\nmaturity as evidenced through high percentage of political\nattacks in a developing region such as Pakistan."}
{"Title": "Topic-STG: Extending the Session-based Temporal Graph\nApproach for Personalized Tweet Recommendation", "Abstract": "Micro-blogging is experiencing fantastic success in the world-\nwide. However, during its rapid development, it has encoun-\ntered the problem of information overload, which has trou-\nbled many users. In this paper, we mainly focus on the task\nof tweet recommendation to address this problem. We ex-\ntend the session-based temporal graph (STG) approach as\nTopic-STG for tweet recommendation which comprehensive-\nly considers three types of features in Twitter: the textual\ninformation, the time factor, and the users\u2019 behavior. The\nexperimental results conducted on a real dataset demon-\nstrate the effectiveness of our approach."}
{"Title": "Evolutionary Analysis on Online Social Networks using A\nSocial Evolutionary Game", "Abstract": "In this paper, we propose a social evolutionary game to in-\nvestigate the evolution of social networks. Through com-\nparison between simulation and empirical analysis on the\nsocial networks of Twitter and Sina Weibo, we validate the\neffectiveness of the proposed model and estimate the evolu-\ntionary phases of the two networks. We find that the users\nof Sina Weibo can withstand comparatively more costs than\nthe users of Twitter. Therefore, they can perform more\npositive behavior and consider more about their reputation\nthan Twitter users. Moreover, the evolutionary time of Sina\nWeibo to a stable state is longer than that of Twitter."}
{"Title": "App Mining: Finding the Real Value of Mobile Applications", "Abstract": "In this poster, we present a new model for estimating the\nactual value of mobile applications (Apps) to the users. The\nmodel assumes that users are implicitly evaluating the value\nof the apps in their smart phones when they choose to unin-\nstall some apps. Our proposed method thus makes use of\nthe install and uninstall log in a mobile app store to esti-\nmate the value of the apps. Experiments using data from\na popular mobile app store show that our model is better\nin predicting the future download trend of the apps as well\nas the future uninstallation rate of the apps. We believe\nsuch model will be very useful in generating more credible\nand appropriate mobile app recommendations to users, or\nin generating features for machine learning systems in more\ncomplex prediction tasks."}
{"Title": "Query Augmentation based Intent Matching in Retail\nVertical Ads", "Abstract": "Search advertising shows trends of vertical extension. Verti-\ncal ads typically offer better Return of Investment (ROI) to\nadvertisers as a result of better user engagement. However,\ncampaign and bids in vertical ads are not set at the keyword\nlevel. As a result, the matching between user query and ads\nsuffers low recall rate and the match quality is heavily im-\npacted by tail queries. In this paper, we propose a retail ads\nretrieval framework based on query rewrite using personal\nhistory data to improve ads recall rate. To insure ads qual-\nity, we also present a relevance model for matching rewritten\nqueries with user search intent, with a particular focus on\nrare queries. Extensive experiments are carried out on large-\nscale logs collected from the Bing search engine, and results\nshow our system achieves significant gains in ads retrieval\nrate without compromising ads quality. To our knowledge,\nthis work is the first attempt to leverage user behavioral data\nin ad matching and apply it to the vertical ads domain."}
{"Title": "An Upper Bound based Greedy Algorithm for Mining Top-k\nInfluential Nodes in Social Networks", "Abstract": "Influence maximization [4] is NP-hard under the Linear Thresh-\nold (LT) model, where a line of greedy algorithms have\nbeen proposed. The simple greedy algorithm [4] guaran-\ntees accuracy rate of 1 \u2212 1/e to the optimal solution; the\nadvanced greedy algorithm, e.g., the CELF algorithm [6],\nruns 700 times faster by exploiting the submodular prop-\nerty of the spread function. However, both models lack ef-\nficiency due to heavy Monte-Carlo simulations during esti-\nmating the spread function. To this end, in this paper we\nderive an upper bound for the spread function under the LT\nmodel. Furthermore, we propose an efficient UBLF algo-\nrithm by incorporating the bound into CELF. Experimental\nresults demonstrate that UBLF, compared with CELF, re-\nduces about 98.9% Monte-Carlo simulations and achieves at\nleast 5 times speed-raising when the size of seed set is small."}
{"Title": "Maximizing the Long-term Integral Influence in Social\nNetworks Under the Voter Model", "Abstract": "We address the problem of discovering the influential nodes\nin social networks under the voter model, which allows mul-\ntiple activations to the same node, by defining an integral\ninfluence maximization problem in a long term. We analyze\nthe problem formulation and present an exact solution to the\nmaximization problem. We also provide a sufficient condi-\ntion for the convergence of the integral influence. We exper-\nimentally compare the exact solution with other heuristic\nalgorithms in the aspects of quality and efficiency."}
{"Title": "Graph Structure in the Web \u2014 Revisited", "Abstract": "Knowledge about the general graph structure of the World Wide\nWeb is important for understanding the social mechanisms that\ngovern its growth, for designing ranking methods, for devising bet-\nter crawling algorithms, and for creating accurate models of its\nstructure. In this paper, we describe and analyse a large, pub-\nlicly accessible crawl of the web that was gathered by the Common\nCrawl Foundation in 2012 and that contains over 3.5 billion web\npages and 128.7 billion links. This crawl makes it possible to ob-\nserve the evolution of the underlying structure of the World Wide\nWebwithinthelast10years: weanalyseandcompare, amongother\nfeatures, degree distributions, connectivity, average distances, and\nthe structure of weakly/strongly connected components.\nOur analysis shows that, as evidenced by previous research [17],\nsome of the features previously observed by Broder et al. [10] are\nvery dependent on artefacts of the crawling process, whereas other\nappear to be more structural. We confirm the existence of a giant\nstrongly connected component; we however find, as observed by\nother researchers [12, 5, 3], very different proportions of nodes that\ncan reach or that can be reached from the giant component, sug-\ngesting that the \u201cbow-tie structure\u201d as described in [10] is strongly\ndependent on the crawling process, and to the best of our current\nknowledge is not a structural property of the web.\nMoreimportantly, statisticaltestingandvisualinspectionofsize-\nrank plots show that the distributions of indegree, outdegree and\nsizes of strongly connected components are not power laws, con-\ntrarily to what was previously reported for much smaller crawls,\nalthough they might be heavy-tailed. We also provide for the first\ntime accurate measurement of distance-based features, using re-\ncently introduced algorithms that scale to the size of our crawl [8]."}
{"Title": "The Semantic Evolution of Online Communities", "Abstract": "Despite their semantic-rich nature, online communities have,\nto date, largely been analysed through examining longitudi-\nnal changes in social networks, community uptake, or simple\nterm-usage and language adoption. As a result, the evolu-\ntion of communities on a semantic level, i.e. how concepts\nemerge, and how these concepts relate to previously dis-\ncussed concepts, has largely been ignored. In this paper we\npresent a graph-based exploration of the semantic evolution\nof online communities, thereby capturing dynamics of on-\nline communities on a conceptual level. We first examine\nhow semantic graphs (concept graphs and entity graphs) of\ncommunities evolve, and then characterise such evolution\nusing logistic population growth models. We demonstrate\nthe value of such models by analysing how sample commu-\nnities evolve and use our results to predict churn rates in\ncommunity forums."}
{"Title": "Inferring International and Internal Migration Patterns from\nTwitter Data ", "Abstract": "Data about migration flows are largely inconsistent across coun-\ntries, typically outdated, and often inexistent. Despite the impor-\ntance of migration as a driver of demographic change, there is lim-\nited availability of migration statistics. Generally, researchers rely\non census data to indirectly estimate flows. However, little can be\ninferred for specific years between censuses and for recent trends.\nThe increasing availability of geolocated data from online sources\nhas opened up new opportunities to track recent trends in migra-\ntion patterns and to improve our understanding of the relationships\nbetween internal and international migration. In this paper, we use\ngeolocated data for about 500,000 users of the social network web-\nsite \u201cTwitter\u201d. The data are for users in OECD countries during the\nperiod May 2011- April 2013. We evaluated, for the subsample of\nusers who have posted geolocated tweets regularly, the geographic\nmovements within and between countries for independent periods\nof four months, respectively. Since Twitter users are not repre-\nsentative of the OECD population, we cannot infer migration rates\nat a single point in time. However, we proposed a difference-in-\ndifferences approach to reduce selection bias when we infer trends\nin out-migration rates for single countries. Our results indicate that\nour approach is relevant to address two longstanding questions in\nthe migration literature. First, our methods can be used to predict\nturning points in migration trends, which are particularly relevant\nfor migration forecasting. Second, geolocated Twitter data can sub-\nstantially improve our understanding of the relationships between\ninternal and international migration. Our analysis relies uniquely\non publicly available data that could be potentially available in real\ntime and that could be used to monitor migration trends. The Web\nScience community is well-positioned to address, in future work,\na number of methodological and substantive questions that we dis-\ncuss in this article."}
{"Title": "Examining Wikipedia across Linguistic and Temporal\nBorders", "Abstract": "The Web has grown to be an integral part of modern society of-\nfering novel ways for humans to communicate, interact, and share\ninformation. New collaborative platforms are forming which are\nproviding individuals with new communities and knowledge bases\nand, at the same time, offering insights into human activity for\nresearchers, policy-makers and engineers. On a global scale, the\nrole of cultural and language barriers when studying such phe-\nnomenabecomesparticularlyrelevantandpresentssignificantchal-\nlenges: due to insufficient information, it is often hard to establish\nthe cultural or language groups in which individuals belong, while\nthere are technical difficulties in establishing the relevance and in\nanalysing resources in different languages. This paper presents a\nframework to the end of addressing those issues by leveraging data\non the use of Wikipedia. Resources available in different languages\nare explicitly correlated in Wikipedia along with time-stamped logs\nof access to its articles. This paper provides a framework to enable\ntemporal page views in Wikipedia to be associated with specific\ngeographic profiles. This framework is then used to examine the\nexchange of information between the English speaking and Chi-\nnese speaking localities and reports initial findings on the role of\nlanguage and culture in diffusion in this context."}
{"Title": "Taking Brazil\u2019s Pulse: Tracking Growing Urban Economies\nfrom Online Attention", "Abstract": "Urban resources are allocated according to socio-economic\nindicators, and rapid urbanization in developing countries\ncalls for updating those indicators in a timely fashion. The\nprohibitive costs of census data collection make that very\ndifficult. To avoid allocating resources upon outdated indi-\ncators, one could partly update or complement them using\ndigital data. It has been shown that it is possible to use\nsocial media in developed countries (mainly UK and USA)\nfor such a purpose. Here we show that this is the case for\nBrazil too. We analyze a random sample of a microblogging\nservice popular in that country and accurately predict the\nGDPs of 45 Brazilian cities. To make these predictions, we\nexploit the sociological concept of glocality, which says that\neconomically successful cities tend to be involved in interac-\ntions that are both local and global at the same time. We\nindeed show that a city\u2019s glocality, measured with social me-\ndia data, effectively signals the city\u2019s economic well-being."}
{"Title": "Analysis of Local Online Review Systems as Digital\nWord-of-Mouth", "Abstract": "Using a large dataset of Yelp\u2019s online reviews for local busi-\nnesses, we investigate how Word-of-Mouth research can in-\nform the design of local online review systems and how these\nsystems\u2019 data can extend our understanding of digital WOM\nin a local context. In this paper, we analyze how visual cues\ncurrently present in Yelp map to WOM concepts. We also\nshow that these concepts are highly related to the perceived\nusefulness of the local reviews, which is aligned with prior\nWOM literature. Additionally, we found that local exper-\ntise, measured at the level of the neighborhood, strongly\ncorrelates with the perceived usefulness of reviews. Our\nfindings augment the understanding of local online WOM\nand have design implications for local review systems."}
{"Title": "Long Time No See:\nThe Probability of Reusing Tags as a Function of\nFrequency and Recency", "Abstract": "In this paper, we introduce a tag recommendation algorithm that\nmimics the way humans draw on items in their long-term mem-\nory. This approach uses the frequency and recency of previous tag\nassignments to estimate the probability of reusing a particular tag.\nUsing three real-world folksonomies gathered from bookmarks in\nBibSonomy, CiteULike and Flickr, we show how incorporating a\ntime-dependent component outperforms conventional \u201cmost pop-\nular tags\u201d approaches and another existing and very effective but\nless theory-driven, time-dependent recommendation mechanism.\nBy combining our approach with a simple resource-specific fre-\nquency analysis, our algorithm outperforms other well-established\nalgorithms, such as FolkRank, Pairwise Interaction Tensor Fac-\ntorization and Collaborative Filtering. We conclude that our ap-\nproach provides an accurate and computationally efficient model\nof a user\u2019s temporal tagging behavior. We demonstrate how effec-\ntive principles of information retrieval can be designed and imple-\nmented if human memory processes are taken into account."}
{"Title": "User Churn in Focused Question Answering Sites:\nCharacterizations and Prediction", "Abstract": "Given a user on a Q&A site, how can we tell whether s/he is\nengaged with the site or is rather likely to leave? What are\nthe most evidential factors that relate to users churning?\nQuestion and Answer (Q&A) sites form excellent repos-\nitories of collective knowledge. To make these sites self-\nsustainable and long-lasting, it is crucial to ensure that new\nusers as well as the site veterans who provide most of the\nanswers keep engaged with the site. As such, quantifying\nthe engagement of users and preventing churn in Q&A sites\nare vital to improve the lifespan of these sites.\nWe study a large data collection from stackoverflow.com\nto identify significant factors that correlate with newcomer\nuser churn in the early stage and those that relate to veterans\nleaving in the later stage. We consider the problem under\ntwo settings: given (i) the first k posts, or (ii) first T days\nof activity of a user, we aim to identify evidential features\nto automatically classify users so as to spot those who are\nabout to leave. We find that in both cases, the time gap\nbetween subsequent posts is the most significant indicator of\ndiminishing interest of users, besides other indicative factors\nlike answering speed, reputation of those who answer their\nquestions, and number of answers received by the user."}
{"Title": "The Web Observatory Extension: Facilitating Web Science\nCollaboration through Semantic Markup", "Abstract": "The multi-disciplinary nature of Web Science and the large size\nand diversity of data collected and studied by its practitioners has\ninspired a new type of Web resource known as the Web\nObservatory. Web observatories are platforms that enable\nresearchers to collect, analyze and share data about the Web and\nto share tools for Web research. At the Boston Web Observatory\nWorkshop 2013 [3], a semantic model for describing Web\nObservatories was drafted and an extension to the schema.org\nmicrodata vocabulary collection was proposed. This paper details\nour implementation of the proposed extension, and how we have\napplied it to the Web Observatory Portal created by the Tetherless\nWorld Constellation at Rensselaer Polytechnic Institute (TWC\nRPI). We recognize this effort to be the \u201cfirst-step\u201d in the\nconstruction, evaluation and validation of the Web observatory\nmodel and not the final recommendation. Our hope is that this\nextension recommendation and our initial implementation sparks\nadditional discussion among the Web Science community of on\nwhether such direction enables Web Observatory curators to\nbetter expose and explain their individual Web Observatories to\nothers, thereby enabling better collaboration between researchers\nacross the Web Science community"}
{"Title": "Modeling Patient Engagement in Peer-to-Peer Healthcare", "Abstract": "Patients now turn to other patients online for health information\nand advice in a phenomenon known as peer-to-peer healthcare.\nThis paper describes a model of patients\u2019 peer-to-peer\nengagement, based upon qualitative studies of three patient or\ncarer groups searching for online information and advice from\ntheir health peers. We describe a three-phase process through\nwhich patients engage with peer experience (PEx). In phase I\n(gating) patients determine the suitability and trustworthiness of\nthe material they encounter; in phase II (engagement) they search\nout information, support and/or advice from others with similar or\nrelevant experience; and in phase III (evaluation) they make\njudgments about the costs and benefits of engaging with particular\nwebsites in the longer term. This model provides a useful\nframework for understanding web based interactions in different\npatient groups."}
{"Title": "A Study of the Online Profile of Enterprise Users in\nProfessional Social Networks", "Abstract": "Understanding the impact of corporate information publicly\ndistributed on the Web is becoming more and more crucial.\nIn this paper we report the result of a study that involved\n130 IBM employees: we explored the correctness and extent\nof organisational information that can be observed from the\nonline profiles of a company\u2019s employees. Our work con-\ntributes new insights to the study of social networks by\nshowing that, even by considering a small fraction of the\navailable online data, it is possible to discover accurate infor-\nmation about an organisation, its structure, and the factors\nthat characterise the social reach of their employees."}
{"Title": "Information Network or Social Network?\nThe Structure of the Twitter Follow Graph", "Abstract": "In this paper, we provide a characterization of the topologi-\ncal features of the Twitter follow graph, analyzing properties\nsuch as degree distributions, connected components, short-\nest path lengths, clustering coefficients, and degree assorta-\ntivity. For each of these properties, we compare and con-\ntrast with available data from other social networks. These\nanalyses provide a set of authoritative statistics that the\ncommunity can reference. In addition, we use these data\nto investigate an often-posed question: Is Twitter a social\nnetwork or an information network? The \u201cfollow\u201d relation-\nship in Twitter is primarily about information consumption,\nyet many follows are built on social ties. Not surprisingly,\nwe find that the Twitter follow graph exhibits structural\ncharacteristics of both an information network and a social\nnetwork. Going beyond descriptive characterizations, we hy-\npothesize that from an individual user\u2019s perspective, Twitter\nstarts off more like an information network, but evolves to\nbehave more like a social network. We provide preliminary\nevidence that may serve as a formal model of how a hybrid\nnetwork like Twitter evolves."}
{"Title": "Mining Triadic Closure Patterns in Social Networks", "Abstract": "A closed triad is a group of three people who are connected with\neach other. It is the most basic unit for studying group phenomena\nin social networks. In this paper, we study how closed triads are\nformed in dynamic networks. More specifically, given three per-\nsons, what are the fundamental factors that trigger the formation\nof triadic closure? There are various factors that may influence the\nformation of a relationship between persons. Can we design a uni-\nfied model to predict the formation of triadic closure? Employing\na large microblogging network as the source in our study, we for-\nmally define the problem and conduct a systematic investigation.\nThe study uncovers how user demographics and network topology\ninfluence the process of triadic closure. We also present a proba-\nbilistic graphical model to predict whether three persons will form\na closed triad in dynamic networks. The experimental results on\nthe microblogging data demonstrate the efficiency of our proposed\nmodel for the prediction of triadic closure formation."}
{"Title": "Big Smog Meets Web Science: Smog Disaster Analysis\nbased on Social Media and Device Data on the Web", "Abstract": "Nowadays, people are increasingly concerned about smog\ndisaster and the caused health hazard. However, the cur-\nrent methods for big smog analysis are usually based on the\ntraditional lagging data sources or merely adopt physical en-\nvironment observations, which limit the methods\u2019 accuracy\nand usability. The discipline of Web Science, the research\nfields of which include web of people and web of devices, pro-\nvides real time web data as well as novel web data analysis\napproaches. In this paper, both social web data and device\nweb data are proposed for smog disaster analysis. Firstly,\nwe utilize social web data to define and calculate Individual\nPublic Health Indexes (IPHIs) for smog caused health haz-\nard quantification. Secondly, we integrate social web data\nand device web data to build standard health hazard rating\nreference and train smog-health models for health hazard\nprediction. Finally, we apply the rating reference and mod-\nels to online and location-sensitive smog disaster monitoring,\nwhich can better guide people\u2019s behaviour and government\u2019s\nstrategy design for disaster mitigation."}
{"Title": "Indexing and Analyzing Wikipedia\u2019s Current Events Portal,\nthe Daily News Summaries by the Crowd", "Abstract": "Wikipedia\u2019sCurrentEventsPortal(WCEP)isaspecialpartofWikipedia\nthat focuses on daily summaries of news events. The WikiTimes\nproject provides structured access to WCEP by extracting and in-\ndexing all its daily news events. In this paper we study this part of\nWikipediaandtakeacloserlookintoitscontentandthecommunity\nbehind it. First, we provide descriptive analysis of the collected\nnews events. Second, we compare between the news summaries\ncreated by the WCEP crowd and the ones created by professional\njournalists on the same topics. Finally, we analyze the revision\nlogs of news events over the past 7 years in order to characterize\nthe WCEP crowd and their activities. The results show that WCEP\nhas reached a stable state in terms of the volume of contributions\nas well as the size of its crowd, which makes it an important source\nof news summaries for the public and the research community."}
{"Title": "Evolution of Reddit: From the Front Page of the Internet to\na Self-referential Community?", "Abstract": "In the past few years, Reddit \u2013 a community-driven platform for\nsubmitting, commenting and rating links and text posts \u2013 has grown\nexponentially, from a small community of users into one of the\nlargest online communities on the Web. To the best of our knowl-\nedge, this work represents the most comprehensive longitudinal\nstudy of Reddit\u2019s evolution to date, studying both (i) how user sub-\nmissions have evolved over time and (ii) how the community\u2019s allo-\ncation of attention and its perception of submissions have changed\nover 5 years based on an analysis of almost 60 million submis-\nsions. Our work reveals an ever-increasing diversification of topics\naccompanied by a simultaneous concentration towards a few se-\nlected domains both in terms of posted submissions as well as per-\nception and attention. By and large, our investigations suggest that\nReddit has transformed itself from a dedicated gateway to the Web\nto an increasingly self-referential community that focuses on and\nreinforces its own user-generated image- and textual content over\nexternal sources."}
{"Title": "Songrium: A Music Browsing Assistance Service with\nInteractive Visualization and Exploration of\na Web of Music", "Abstract": "This paper describes a music browsing assistance service,\nSongrium (http://songrium.jp), which increases user en-\njoyment when listening to songs and allows visualization and\nexploration of a\u201cWeb of Music\u201d. We define a Web of Music\nin this paper to be a network of \u201cweb-native music\u201d, which\nwe define in turn to be music that is published, shared, and\nremixed (has derivative works created) entirely on the web.\nSongrium was developed as an attempt to realize a Web\nof Music, by showing relations between both original songs\nand derivative works and offering an enriched listening ex-\nperience. Songrium has analyzed over 600,000 music video\nclips on the most popular Japanese video-sharing service,\nNiconico, which contains original songs of web-native music\nand their derivative works such as covers and dance arrange-\nments. Analysis of over 100,000 original songs reveals that\nover 500,000 derivative works were generated and have con-\ntributed to enrich the Web of Music."}
{"Title": "Exploring the User-Generated Content (UGC) Uploading\nBehavior on YouTube", "Abstract": "YouTube is the world\u2019s largest video sharing platform where\nboth professional and non-professional users participate in\ncreating, uploading, and viewing content. In this work, we\nanalyze content in the music category created by the non-\nprofessionals, which we refer to as user-generated content\n(UGCs). Non-professional users frequently upload content\n(UGCs) that are parodies, remakes, or covers of the music\nvideos uploaded by professionals, namely the official record\nlabels. Along with the success of official music videos on\nYouTube, we find the increased participation of users in cre-\nating the UGCs related to the music videos. In this study, we\ncharacterize the UGC uploading behavior in terms of what,\nwhere, and when. Furthermore, we measure the relationship\nbetween the popularity of the original content and creation\nof the related UGCs. We find that the UGC uploading be-\nhavior is different depending on the types of the UGC and\nacross different genres of music videos. We also find that\nUGC sharing is a highly global activity; popular UGCs are\ncreated from all over the world despite the fact that the pop-\nular music videos originate from a very limited number of\nlocations. Our findings imply that utilizing the information\non re-created UGCs is important in order to understand and\nto predict the popularity of the original content."}
{"Title": "Hippocampus: Answering Memory Queries\nusing Transactive Search", "Abstract": "Memory queries denote queries where the user is trying to\nrecall from his/her past personal experiences. Neither Web\nsearch nor structured queries can effectively answer this type\nof queries, even when supported by Human Computation so-\nlutions. In this paper, we propose a new approach to answer\nmemory queries that we call Transactive Search: The user-\nrequested memory is reconstructed from a group of people by\nexchanging pieces of personal memories in order to reassem-\nble the overall memory, which is stored in a distributed fash-\nion among members of the group. We experimentally com-\npare our proposed approach against a set of advanced search\ntechniques including the use of Machine Learning methods\nover the Web of Data, online Social Networks, and Human\nComputation techniques. Experimental results show that\nTransactive Search significantly outperforms the effective-\nness of existing search approaches for memory queries."}
{"Title": "Reachable Subwebs for Traversal-Based Query Execution", "Abstract": "Traversal-basedapproachestoexecutequeriesoverdataontheWeb\nhave recently been studied. These approaches make use of up-to-\ndate data from initially unknown data sources and, thus, enable ap-\nplications to tap the full potential of the Web. While existing work\nfocuses primarily on implementation techniques, a principled anal-\nysis of subwebs that are reachable by such approaches is missing.\nSuch an analysis may help to gain new insight into the problem of\noptimizing the response time of traversal-based query engines. Fur-\nthermore, a better understanding of characteristics of such subwebs\nmay also inform approaches to benchmark these engines.\nThis paper provides such an analysis. In particular, we identify\ntypical graph-based properties of query-specific reachable subwebs\nand quantify their diversity. Furthermore, we investigate whether\nvertex scoring methods (e.g., PageRank) are able to predict query-\nrelevance of data sources when applied to such subwebs."}
{"Title": "Bots vs. Wikipedians, Anons vs. Logged-Ins", "Abstract": "Wikipedia is a global crowdsourced encyclopedia that at\ntime of writing is available in 287 languages. Wikidata is\na likewise global crowdsourced knowledge base that pro-\nvides shared facts to be used by Wikipedias. In the con-\ntext of this research, we have developed an application and\nan underlying Application Programming Interface (API) ca-\npable of monitoring realtime edit activity of all language\nversions of Wikipedia and Wikidata. This application al-\nlows us to easily analyze edits in order to answer questions\nsuch as \u201cBots vs. Wikipedians, who edits more?\u201d, \u201cWhich is\nthe most anonymously edited Wikipedia?\u201d, or\u201cWho are the\nbots and what do they edit?\u201d. To the best of our knowledge,\nthis is the first time such an analysis could be done in real-\ntime for Wikidata and for really all Wikipedias\u2014large and\nsmall. Our application is available publicly online at the\nURL http://wikipedia-edits.herokuapp.com/ , its code\nhas been open-sourced under the Apache 2.0 license."}
{"Title": "Quantising Contribution Effort in Online Communities", "Abstract": "We describe the Joint Effort-Topic (JET) model and the Author Joint\nEffort-Topic ( \u03b1 JET) model that estimate the effort required for users\nto contribute on different topics. We propose to learn word-level\neffort taking into account term preference over time and use it to\nset the priors of our models. Since there is no gold standard which\ncan be easily built, we evaluate them by measuring their abilities\nto validate expected behaviours such as correlations between user\ncontributions and the associated effort."}
{"Title": "Spotting Misbehaviors in Location-based Social\nNetworks using Tensors", "Abstract": "The proliferation of mobile devices that are capable of esti-\nmating their position, has lead to the emergence of a new\nclass of social networks, namely location-based social net-\nworks (LBSNs for short). The main interaction between\nusers in an LBSN is location sharing. While the latter can be\nrealized through continuous tracking of a user\u2019s whereabouts\nfrom the service provider, the majority of LBSNs allow users\nto voluntarily share their location, through check-ins. LBSNs\nprovide incentives to users to perform check-ins. However,\nthese incentives can also lead to people faking their location,\nthus, generating false information. In this work, we propose\nthe use of tensor decomposition for spotting anomalies in\nthe check-in behavior of users. To the best of our knowl-\nedge, this is the first attempt to model this problem using\ntensor analysis."}
{"Title": "Spatial and Temporal Patterns of\nOnline Food Preferences", "Abstract": "Since food is one of the central elements of all human be-\nings, a high interest exists in exploring temporal and spatial\nfood and dietary patterns of humans. Predominantly, data\nfor such investigations stem from consumer panels which\ncontinuously capture food consumption patterns from indi-\nviduals and households. In this work we leverage data from\na large online recipe platform which is frequently used in\nthe German speaking regions in Europe and explore (i) the\nassociation between geographic proximity and shared food\npreferences and (ii) to what extent temporal information\nhelps to predict the food preferences of users. Our results\nreveal that online food preferences of geographically closer\nregions are more similar than those of distant ones and show\nthat specific types of ingredients are more popular on specific\ndays of the week. The observed patterns can successfully be\nmapped to known real-world patterns which suggests that\nexisting methods for the investigation of dietary and food\npatterns (e.g., consumer panels) may benefit from incorpo-\nrating the vast amount of data generated by users browsing\nrecipes on the Web."}
{"Title": "When is it Biased? Assessing the Representativeness of\nTwitter\u2019s Streaming API", "Abstract": "Twitter shares a free 1% sample of its tweets\nthrough the\u201cStreaming API\u201d. Recently, research\nhas pointed to evidence of bias in this source.\nThe methodologies proposed in previous work\nrely on the restrictive and expensive Firehose to\nfind the bias in the Streaming API data. We\ntackle the problem of finding sample bias without\ncostly and restrictive Firehose data. We propose\na solution that focuses on using an open data\nsource to find bias in the Streaming API."}
{"Title": "Haters Gonna Hate:\nJob-Related Offenses in Twitter", "Abstract": "In this paper, we aim at finding out which users are likely to\npublicly demonstrate frustration towards their jobs on the\nmicroblogging platform Twitter - we will call these users\nhaters 1 . We show that the profiles of haters have specific\ncharacteristics in terms of vocabulary and connections. The\nimplications of these findings may be used for the develop-\nment of an early alert system that can help users to think\ntwice before they post potentially self-harming content."}
{"Title": "Characterizing High-impact Features for Content Retention\nin Social Web Applications", "Abstract": "One of the core challenges of automatically creating Social\nWeb summaries is to decide which posts to remember, i.e., to\nconsider for summary inclusion and which to forget. Keeping\neverything would overwhelm the user and would also neglect\nthe often intentionally ephemeral nature of Social Web posts.\nIn this paper, we analyze high-impact features that charac-\nterize memorable posts as a first step for this selection pro-\ncess. Our work is based on a user evaluation for discovering\nhuman expectations towards content retention."}
{"Title": "Topic-Based Place Semantics Discovered\nfrom Microblogging Text Messages", "Abstract": "Location-based social network services (LBSNS) such as\nFoursquare are getting the highlight with the extensive spread of\nGPS-enabled mobile devices, and a large body of research has\nbeen conducted to devise methods for understanding and\nclustering places. However, in previous studies, the predefined set\nof semantic categories of places play a critical role in both\ndiscovery and evaluation of the results, despite its limited ability\nto represent the dynamics of the places. We explore beyond the\npredefined semantic categories of the places and discover topic-\nbased place semantics through the use of Latent Dirichlet\nAllocation, by extracting topics from the text which people post\non site. We also show the proposed method allows for\nunderstanding the temporal dynamics of the place semantics. The\nfinding of this study is intended for, but not limited to, context\naware services and place recommendation systems."}
{"Title": "Has Much Potential but Biased:\nExploring the Scholarly Landscape in Twitter", "Abstract": "We explore how research papers are shared in Twitter to un-\nderstand its potential and limitation of the current practice\nthat measures or predicts the scientific impact of research\npapers from the web. We track 54 second-level domains of-\nfering the top 100 journals listed in Google Scholar and col-\nlect 403,165 tweets sharing 75,677 unique research papers\nby 142,743 users over the course of 135 days. Our findings\nshow the great potential of Twitter as a platform for paper\nsharing, but at the same time, indicate the limitations of\nmeasuring scientific impact through the lens of social media\nmainly due to the highly skewed and limited attention to\nfew number of top journals."}
{"Title": "Crowd vs. Experts: Nichesourcing for Knowledge\nIntensive Tasks in Cultural Heritage", "Abstract": "The results of our exploratory study provide new insights\nto crowdsourcing knowledge intensive tasks. We designed\nand performed an annotation task on a print collection of\nthe Rijksmuseum Amsterdam, involving experts and crowd\nworkers in the domain-specific description of depicted flow-\ners. We created a testbed to collect annotations from flower\nexperts and crowd workers and analyzed these in regard\nto user agreement. The findings show promising results,\ndemonstrating how, for given categories, nichesourcing can\nprovide useful annotations by connecting crowdsourcing to\ndomain expertise."}
{"Title": "Learning to Rank for Joy", "Abstract": "User-generated content is a growing source of valuable infor-\nmation and its analysis can lead to a better understanding\nof the users needs and trends. In this paper, we leverage\nuser feedback about YouTube videos for the task of affec-\ntive video ranking. To this end, we follow a learning to rank\napproach, which allows us to compare the performance of\ndifferent sets of features when the ranking task goes beyond\nmere relevance and requires an affective understanding of\nthe videos. Our results show that, while basic video fea-\ntures, such as title and tags, lead to effective rankings in\nan affective-less setup, they do not perform as good when\ndealing with an affective ranking task."}
{"Title": "WebAlive: a New Paradigm for Bringing Things to Life on\nthe Web", "Abstract": "In this poster, we propose WebAlive, a new paradigm that\nbrings any Thing in the realm of reality to \u2018life\u2019 on the Web\nby creating an entangled virtual existence for it. For exam-\nple, consider a physical object like the lamp on your desk.\nAs a human, you are aware of its existence. You can see it,\ntouch it, feel it, interact with it (turn it on, off, etc.) and in-\nvestigate its attributes (color, material, etc.). You can even\nalter or trash it. Now, imagine that the lamp had a virtual\ncounterpart on the Web such that the two of them were en-\ntangled with each other. A software agent on the Web can\nnow become aware of its existence, it can perform actions on\nit, learn about its attributes and even (with the right capa-\nbility) alter it just how it was accessible to you. Referring to\nthe most broadest definition of the word Thing, we can ex-\ntend this entanglement concept beyond physical things like\ndevices, objects, people, etc. to all intelligible things like\nentities, processes, concepts, ideas, etc. By doing so, we can\nopen a new realm of possibilities where we could have digi-\ntal access to the world around us. This poster provides an\nintroduction to the paradigm and its realization."}
{"Title": "Predicting Ideological Friends and Foes in Twitter\nConflicts", "Abstract": "The rise in popularity of Twitter in recent years has in parallel led\nto an increase in online controversies. To monitor and control such\nconflicts early on, we design and evaluate a language-agnostic clas-\nsifier to tell pairs of ideological friends from foes. We build the\nclassifier using features from four different aspects: user-based,\ninteraction-based, relationship-based and conflict-based. By exper-\nimenting with three large data sets containing diverse conflicts, we\ndemonstrate the effectiveness of language-agnostic classification of\nideological relation, achieving satisfactory results across all three\ndata sets. Such a classifier potentially enables studies of diverse\nconflicts on Twitter on a large scale."}
{"Title": "Testsuite and Harness for Browser Based Editing", "Abstract": "The Web is still awkward when it comes to online editing.\nIt is time to fix that, and make it easier for developers to\ncreate smarter browser based editing tools. This short pa-\nper presents work on a test framework for a cross browser\nopen source library for browser based editing. The aim is\nto encourage a proliferation of different kinds of browser\nbased editing for a wide range of purposes. The library steps\naround the considerable variations in the details of browser\nsupport for designMode and contentEditable, which have\nacted as a brake on realizing the full potential for browser\nbased editing."}
{"Title": "CrossLanguageSpotter: A Library for Detecting Relations\nin Polyglot Frameworks", "Abstract": "Nowadays, most of the web frameworks are developed us-\ning different programming languages, both for server and\nclient side programmes. The typical scenario includes a\ngeneral purpose language (e.g. Ruby, Python, Java) used\ntogether with different specialized languages: HTML, CSS,\nJavascript and SQL. All the artifacts are connected via dif-\nferent types of relations, most of which depend on the adopted\nframework. These cross-language relations are normally not\ncaptured by tools which require the developer to learn and\nto remember those associations in order to understand and\nmaintain the application. This paper describes a library for\ndetecting cross-language relations in polyglot frameworks.\nThe library has been developed to be modular and to be eas-\nily integrated in existing IDEs. The library is publicly avail-\nable at http://github.com/CrossLanguageProject/crosslanguagespotter ."}
{"Title": "A Voice-Controlled Web Browser to Navigate Hierarchical\nHidden Menus of Web Pages in a Smart-TV Environment", "Abstract": "This paper proposes a new voice web browser that can be\noperated in smart TV environments. Previous voice web browsers\nhad the limitation of being run under limited conditions; for\nexample, a list of the specific contents of a page was outputted by\nvoice, or the user entered a search term by voice. In our method\nproposed in this paper, all the hierarchical menu areas on a web\npage are recognized and controlled with voice keywords so that\npage navigation according to a menu can be conveniently done in\na voice supported web browser. Although many studies have been\nconducted on web page menu recognition, most of them provide\ninsufficient information to recognize the hierarchical menu\nstructure. In other words, most web pages in recent browsers\nshowed submenus only as a result of a specific user interaction,\nsince these previous studies had no way of recognizing or\ncontrolling the submenus. Therefore, in the web browser proposed\nin this study, a hierarchical menu structure, which is inserted\ndynamically via user interaction, is recognized and selected by\nvoice, thus making it possible to maneuver on the web page.\nFurthermore, the core code of the browser is implemented in\nJavaScript, so it can be flexibly used not only for a web browser\non Smart TVs, but also as functional extensions of existing web\nbrowsers in a PC environment"}
{"Title": "ROSeAnn: Taming Online Semantic Annotators", "Abstract": "Named entity extractors are a popular means for enriching docu-\nments with semantic annotations. Both the overlap and the increas-\ning diversity in the capabilities and in the vocabularies of the an-\nnotators motivate the need for managing and integrating semantic\nannotations in a coherent and uniform fashion.\nROS E A NN is a framework for the management and the recon-\nciliation of semantic annotations. It provides end-users and pro-\ngrammers with a unified view over the results of multiple online\nand standalone annotators, linking them to an integrated ontology\nof their vocabularies, and supporting a variety of document formats\nsuch as: plain text, live Web pages, and PDF documents. Although\nROS E A NN provides two pre-defined algorithms for conflict reso-\nlution \u2013 one supervised, appropriate when representative training\ndata is available, and one unsupervised \u2013 it also allows application\ndevelopers to define their own integration techniques, as well as\nextending the pool of annotators as new ones become available."}
{"Title": "Developing Web of Data Applications from the Browser", "Abstract": "WikiNEXT is a wiki engine 100% written in JavaScript that relies\non recent APIs and frameworks. It has been designed to author web\napplications directly in a web browser, which can exploit the web of\ndata. It combines the functionalities of a semantic wiki with those of\na Web-based IDE (Integrated Development Environment) in order\nto develop web applications in addition to writing classic\ndocuments. It gives developers a rich internal API (Application\nProgramming Interface) and provides several functionalities to\nexploit the web of data. Our approach uses templates, a special type\nof wiki pages that represent the semantic data model. Templates\ngenerate wiki pages with semantic annotations that are stored as\nquadruplets in a triple store engine. To query this semantic data, we\nprovide a SPARQL endpoint. Screencasts are available on YouTube\n(look for WikiNEXT)."}
{"Title": "When Machines Dominate Humans:\nThe Challenges of Mining and Consuming Machine-\ngenerated Web Mail", "Abstract": "In spite of personal communications moving more and more\ntowards social and mobile, especially with younger generations,\nemail traffic continues to grow. This growth is mostly attributed\nto (non-spam) machine-generated email, which, against common\nperception, is often extremely valuable. Indeed, together with\nmonthly newsletters that can easily be ignored, inboxes contain\nflight itineraries, booking confirmations, receipts or invoices that\nare critical to many users. In this talk, I will discuss the new\nnature of consumer email, which is dominated by machine-\ngenerated messages of highly heterogeneous forms and value. I\nwill show how the change has not been fully recognized yet by\nmy most email clients (as an example, why should there still be a\nreply option associated with a message coming from a \u201cdo-not-\nreply@\u201d address?). I will introduce some approaches for large-\nscale mail mining specifically tailored to machine-generated\nemail. I will conclude by discussing possible applications and\nresearch directions."}
{"Title": "Analyzing Behavioral Data for Improving Search\nExperience", "Abstract": "Yandex is one of the largest internet companies in Europe,\noperating Russia\u2019s most popular search engine, generating\n62% of all search traffic in Russia, what means processing\nabout 220 million queries from about 22 million users daily.\nClearly, the amount and the variety of user behavioral data\nwhich we can monitor at search engines is rapidly increasing.\nStill, we do not always recognize its potential to help us solve\nthe most challenging search problems and do not immedi-\nately know the ways to deal with it most effectively both for\nsearch quality evaluation and for its improvement. My talk\nwill focus on various practical challenges arising from the\nneed to \u201cgrok\u201d search engine users and do something useful\nwith the data they most generously, though almost uncon-\nsciously share with us. I will also present some answers to\nthat by overviewing our latest research on user model based\nretrieval quality evaluation, implicit feedback mining and\npersonalization."}
{"Title": "YouTube Monetization: Creating User-Centric\nExperiences Using Large Scale Data", "Abstract": "Over the last 4 years, YouTube has grown from a viral video\nsharing site to a platform that fuels a win-win ecosystem for video\ncontent creators, advertisers and users. A key driving force behind\nthis successful transformation is building out products/platforms\nthat focus and optimize for the user. In this talk, we focus on some\nillustrative user-centric efforts such as user-controlled skippable ads\n(TrueView Instream) and dynamic ad loads (data driven system that\nbalances user impact with advertising opportunities). Leveraging\nvery large amounts of real-time activity data is paramount to\nsuccessfully building and deploying such user-centric models. We\nconclude the talk with challenges and opportunities in this important\narea of real-time user analysis and large data modeling."}
{"Title": "Identifying Social Roles in reddit Using Network Structure", "Abstract": "As social networks and the user-generated content that pop-\nulates them continue to grow in prevalence, size, and in-\nfluence, understanding how users interact and produce this\ncontent becomes increasingly important. Insight into these\ncommunity dynamics could prove valuable for measuring\ncontent trust, providing role-based group recommendations,\nor evaluating group stability and growth. To this end, we ex-\nplore user posting behavior on reddit, a large social network-\ning site comprised of many sub-communities in which a user\nmay participate simultaneously. We demonstrate that the\nwell-known\u201canswer-person\u201drole is present in the reddit com-\nmunity, provide an exposition on an automated method for\nidentifying this role based solely on user interactions (forego-\ning expensive content analysis), and show that users rarely\nexhibit significant participation in more than one communi-\nties."}
{"Title": "Mining Cross-Domain Rating Datasets from Structured\nData on Twitter", "Abstract": "While rating data is essential for all recommender systems\nresearch, there are only a few public rating datasets avail-\nable, most of them years old and limited to the movie do-\nmain. With this work, we aim to end the lack of rating data\nby illustrating how vast amounts of ratings can be unam-\nbiguously collected from Twitter. We validate our approach\nby mining ratings from four major online websites focusing\non movies, books, music and video clips. In a short mining\nperiod of 2 weeks, close to 3 million ratings were collected.\nSince some users turned up in more than one dataset, we\nbelieve this work to be amongst the first to provide a true\ncross-domain rating dataset."}
{"Title": "Predicting Crowd Behavior with Big Public Data", "Abstract": "With public information becoming widely accessible and\nshared on today\u2019s web, greater insights are possible into\ncrowd actions by citizens and non-state actors such as large\nprotests and cyber activism. We present efforts to predict\nthe occurrence, specific timeframe, and location of such ac-\ntions before they occur based on public data collected from\nover 300,000 open content web sources in 7 languages, from\nall over the world, ranging from mainstream news to gov-\nernment publications to blogs and social media. Using natu-\nral language processing, event information is extracted from\ncontent such as type of event, what entities are involved and\nin what role, sentiment and tone, and the occurrence time\nrange of the event discussed. Statements made on Twitter\nabout a future date from the time of posting prove partic-\nularly indicative. We consider in particular the case of the\n2013 Egyptian coup d\u2019\u00b4 etat. The study validates and quanti-\nfies the common intuition that data on social media (beyond\nmainstream news sources) are able to predict major events."}
{"Title": "On the Evolution of Social Groups\nDuring Coffee Breaks", "Abstract": "This paper focuses on the analysis of group evolution events\nin networks of face-to-face proximity. First, we analyze sta-\ntistical properties of group evolution, e.g., individual activ-\nity and typical group sizes. Furthermore, we define a set\nof specific group evolution events. We analyze these using\nreal-world data collected at the LWA 2010 conference using\nthe Conferator system, and discuss patterns according to\ndifferent phases of the conference."}
{"Title": "On the Predictability of Recurring Links\nin Networks of Face-to-Face Proximity", "Abstract": "This paper focuses on the predictability of recurring links:\nThese links are generated repeatedly in a network for dif-\nferent forms of social ties, e.g., by face-to-face interactions\nin offline social networks. In particular, we analyse the\npredictability of recurring links in networks of face-to-face\nproximity using several path-based measures, and compare\nthese to network-proximity measures based on the nodes\u2019\nneighbourhood. Furthermore, we show that the current tie\nstrength is a good predictor for this link prediction task. In\naddition we show that the removal of weak ties improves\nthe predictability for most of the considered network prox-\nimity measures. For our analysis we utilize three real-world\ndatasets collected at different scientific conferences using the\nConferator (http://www.conferator.org) system."}
{"Title": "Inferring Twitter User Locations With 10km Accuracy", "Abstract": "Geographic locations of users form an important axis in\npublic polls and localized advertising, but are not available\nby default. The number of users who make their locations\npublic or use GPS tagging is relatively small, compared to\nthe huge number of users in online social networking ser-\nvices and social media platforms. In this work we propose\na new framework to infer a user\u2019s main location of activi-\nties in Twitter using their textual contents. Our approach\nis based on a probabilistic generative model that filters lo-\ncal words, employs data binning for scalability, and applies\na map projection technique for performance. For Korean\nTwitter users, we report that 60% of users are identified\nwithin 10 km of their locations, a significant improvement\nover existing approaches."}
{"Title": "On the Ground Validation of Online Diagnosis with Twitter\nand Medical Records", "Abstract": "Social media has been considered as a data source for track-\ning disease. However, most analyses are based on models\nthat prioritize strong correlation with population-level dis-\nease rates over determining whether or not specific individ-\nual users are actually sick. Taking a different approach, we\ndevelop a novel system for social-media based disease detec-\ntion at the individual level using a sample of professionally\ndiagnosed individuals. Specifically, we develop a system for\nmaking an accurate influenza diagnosis based on an individ-\nual\u2019s publicly available Twitter data. We find that about\nhalf (17/35 = 48.57%) of the users in our sample that were\nsick explicitly discuss their disease on Twitter. By develop-\ning a meta classifier that combines text analysis, anomaly\ndetection, and social network analysis, we are able to diag-\nnose an individual with greater than 99% accuracy even if\nshe does not discuss her health."}
{"Title": "Participatory Disease Detection through Digital\nVolunteerism: How the DoctorMe Application Aims to\nCapture Data for Faster Disease Detection in Thailand", "Abstract": "This paper reports the work in progress of incorporating a\nparticipatory disease detection mechanism into the existing web-\nand mobile device application DoctorMe in Thailand. As\nSoutheast Asia has a high likelihood of hosting potential\noutbreaks of epidemics it is crucial to enable citizens to\ncollectively contribute to improved public health through\ncrowdsourced data, which is currently lacking. This paper focuses\nforemost on the localised approach, utilizing elements such as\ngamification, digital volunteerism and personalised health\nrecommendations  for  participating  users.  DoctorMe\u2019s\nparticipatory disease detection approach aims to tap into the\naccelerating technological landscape in Thailand and to improve\npersonal health and provide valuable data for institutional analysis\nthat may prevent or decrease the impact of infectious disease\noutbreaks."}
{"Title": "Integration and Visualization Public Health Dashboard:\nThe medi+board Pilot Project", "Abstract": "Traditional public health surveillance systems would benefit from\nintegration with knowledge created by new situation-aware realtime\nsignals from social media, online searches, mobile/sensor networks and\ncitizens\u2019 participatory surveillance systems. However, the challenge of\nthreat validation, cross-verification and information integration for risk\nassessment has so far been largely untackled.\nIn this paper, we propose a new system, medi+board, monitoring\nepidemic intelligence sources and traditional case-based surveillance to\nbetter automate early warning, cross-validation of signals for outbreak\ndetection and visualization of results on an interactive dashboard. This\nenables public health professionals to see all essential information at a\nglance. Modular and configurable to any \u2018event\u2019 defined by public\nhealth experts, medi+board scans multiple data sources, detects\nchanging patterns and uses a configurable analysis module for signal\ndetection to identify a threat. These can be validated by an analysis\nmodule and correlated with other sources to assess the reliability of the\nevent classified as the reliability coefficient which is a real number\nbetween zero and one. Events are reported and visualized on the\nmedi+board dashboard which integrates all information sources and can\nbe navigated by a timescale widget.\nSimulation with three datasets from the swine flu 2009 pandemic (HPA\nsurveillance, Google news, Twitter) demonstrates the potential of\nmedi+board to automate data processing and visualization to assist\npublic health experts in decision making on control and response\nmeasures."}
{"Title": "System for Surveillance and Investigation of Disease\nOutbreaks", "Abstract": "Information technology contributes greatly in improving peo-\nple\u2019s health. Through our interaction with different com-\nmunication channels such as social media, telephone calls or\npurchasing over-the-counter medicines, we emit signals and\nleave trails of information related to our health. Informa-\ntion that can be used to understand our health situation.\nSome of these communication channels are more structured,\nfiltered and suited for evaluation, as a result they require\nless demanding filteration and analysis than others. One\nsuch channel is the Swedish National Telephone Health Ser-\nvice 1177, where professional healthcare personnel assist and\ngive advice to callers. The main aim of this work is to detect\npoint source outbreaks. For this purpose a project called\nEvent-based Surveillance System (ESS) was initiated to de-\nvelope a system for surveillance and detection using the for-\nmer mentioned information source. The system is currently\nrunning and is used to notify local authorities whenever a\ndeviation in the telephone traffic pattern is recorded"}
{"Title": "One Health Informatics", "Abstract": "Zoonoses are a class of infectious diseases causing growing\nconcern of health authorities worldwide. Human and economic\ncosts of zoonoses are substantial, especially in low-resource\ncountries. New zoonoses emerge as a consequence of ecological,\ndemographic,  cultural,  social  and  behavioral  factors.\nMeanwhile, global antimicrobial resistance increases. This\npublic health threat demands for a new approach to which the\nconcept of \u2018One Health\u2019 is emblematical. It emphasizes the\ninterconnectedness of human, animal and environmental health.\nTo protect and improve public health it is imperative that\ntransdisciplinary collaboration and communication takes place\nbetween the human and the veterinary domain. This strategy is\nnow widely endorsed by international, regional and national\nhealth  policy  and  academic  bodies.  Nonetheless  the\ncontributions of both the social sciences and the new data\nsciences need more appreciation. Evidence is available that the\nmethods and concepts they provide can budge \u2018One Health\u2019."}
{"Title": "Volunteer-powered Automatic Classification of Social\nMedia Messages for Public Health in AIDR", "Abstract": "Microblogging platforms such as Twitter have become a\nvaluable resource for disease surveillance and monitoring.\nAutomatic classification can be used to detect disease-related\nmessages and to sort them into meaningful categories. In\nthis paper, we show how the AIDR (Artificial Intelligence\nfor Disaster Response) platform can be used to harvest and\nperform analysis of tweets in real-time using supervised ma-\nchine learning techniques. AIDR is a volunteer-powered on-\nline social media content classification platform that auto-\nmatically learns from a set of human-annotated examples\nto classify tweets into user-defined categories. In addition,\nit automatically increases classification accuracy as new ex-\namples become available. AIDR can be operated through a\nweb interface without the need to deal with the complexity\nof the machine learning methods used."}
{"Title": "Understanding Twitter Influence in the Health Domain: A\nsocial-psychological contribution", "Abstract": "Twitter can be a powerful tool for the dissemination and\ndiscussion of public health information but how can we best\ndescribe its influence? In this paper we draw on social-\npsychological  concepts  such  as  social  norms,  social\nrepresentations, emotions and rhetoric to explain how influence\nworks both in terms of the spread of information and also its\npersonal impact. Using tweets drawn from a range of health\nissues, we show that social psychological theory can be used in\nthe qualitative analysis of Twitter data to further our\nunderstanding of how health behaviours can be affected by social\nmedia discourse."}
{"Title": "Measuring and Maximizing Group Closeness Centrality\nover Disk-Resident Graphs", "Abstract": "As an important metric in graphs, group closeness centrality\nmeasures how close a group of vertices is to all other vertices\nin a graph, and it is used in numerous graph applications\nsuch as measuring the dominance and influence of a node\ngroup over the graph. However, when a large-scale graph\ncontains hundreds of millions of nodes/edges which cannot\nreside entirely in computer\u2019s main memory, measuring and\nmaximizing group closeness become challenging tasks. In\nthis paper, we present a systematic solution for efficiently\ncalculating and maximizing the group closeness for disk-\nresident graphs. Our solution first leverages a\u201cprobabilistic\ncounting method\u201dto efficiently estimate the group closeness\nwith high accuracy, rather than exhaustively computing it\nin an exact fashion. In addition, we design an I/O-efficient\ngreedy algorithm to find a node group that maximizes group\ncloseness. Our proposed algorithm significantly reduces the\nnumber of random accesses to disk, thereby dramatically im-\nproving computational efficiency. Experiments on real-world\nbig graphs demonstrate the efficacy of our approach."}
{"Title": "Efficient Network Generation Under General Preferential\nAttachment", "Abstract": "Preferential attachment (PA) models of network structure\nare widely used due to their explanatory power and con-\nceptual simplicity. PA models are able to account for the\nscale-free degree distributions observed in many real-world\nlarge networks through the remarkably simple mechanism\nof sequentially introducing nodes that attach preferentially\nto high-degree nodes. The ability to efficiently generate in-\nstances from PA models is a key asset in understanding both\nthe models themselves and the real networks that they repre-\nsent. Surprisingly, little attention has been paid to the prob-\nlem of efficient instance generation. In this paper, we show\nthat the complexity of generating network instances from a\nPA model depends on the preference function of the model,\nprovide efficient data structures that work under any prefer-\nence function, and present empirical results from an imple-\nmentation based on these data structures. We demonstrate\nthat, by indexing growing networks with a simple augmented\nheap, we can implement a network generator which scales\nmany orders of magnitude beyond existing capabilities (10 6\n\u2013 10 8 nodes). We show the utility of an efficient and general\nPA network generator by investigating the consequences of\nvarying the preference functions of an existing model. We\nalso provide \u201cquicknet\u201d, a freely-available open-source im-\nplementation of the methods described in this work."}
{"Title": "Finding Influential Neighbors to Maximize Information\nDiffusion in Twitter", "Abstract": "The problem of spreading information is a topic of consid-\nerable recent interest, but the traditional influence maxi-\nmization problem is inadequate for a typical viral marketer\nwho cannot access the entire network topology. To fix this\nflawed assumption that the marketer can control any arbi-\ntrary k nodes in a network, we have developed a decentral-\nized version of the influential maximization problem by influ-\nencing k neighbors rather than arbitrary users in the entire\nnetwork. We present several reasonable neighbor selection\nschemes and evaluate their performance with a real dataset\ncollected from Twitter. Unlike previous studies using net-\nwork topology alone or synthetic parameters, we use real\npropagation rate for each node calculated from the Twitter\nmessages during the 2010 UK election campaign. Our ex-\nperimental results show that information can be efficiently\npropagated in online social networks using neighbors with a\nhigh propagation rate rather than those with a high number\nof neighbors."}
{"Title": "Kindred Domains: Detecting and Clustering Botnet\nDomains Using DNS Traffic", "Abstract": "In this paper we focus on detecting and clustering distinct group-\nings of domain names that are queried by numerous sets of infected\nmachines. We propose to analyze domain name system (DNS) traf-\nfic, such as Non-Existent Domain (NXDomain) queries, at several\npremier Top Level Domain (TLD) authoritative name servers to\nidentify strongly connected cliques of malware related domains.\nWe illustrate typical malware DNS lookup patterns when observed\non a global scale and utilize this insight to engineer a system ca-\npable of detecting and accurately clustering malware domains to a\nparticular variant or malware family without the need for obtaining\na malware sample. Finally, the experimental results of our system\nwill provide a unique perspective on the current state of globally\ndistributed malware, particularly the ones that use DNS."}
{"Title": "Network Analysis of University Courses", "Abstract": "Crucial courses have a high impact on students progress at\nuniversities and ultimately on graduation rates. Detecting\nsuch courses should therefore be a major focus of decision\nmakers at universities. Based on complex network analysis\nand graph theory, this paper proposes a new framework to\nnot only detect such courses, but also quantify their cru-\nciality. The experimental results conducted using data from\nthe University of New Mexico (UNM) show that the distri-\nbution of course cruciality follows a power law distribution.\nThe results also show that the ten most crucial courses at\nUNM are all in mathematics. Applications of the proposed\nframework are extended to study the complexity of curric-\nula within colleges, which leads to a consideration of the\ncreation of optimal curricula. Optimal curricula along with\nthe earned letter grades of the courses are further exploited\nto analyze the student progress. This work is important as\nit presents a robust framework to ensure the ease of flow\nof students through curricula with the goal of improving a\nuniversity\u2019s graduation rate."}
{"Title": "Classifying Latent Infection States\nin Complex Networks", "Abstract": "In this work, we develop techniques to identify the latent\ninfected nodes in the presence of missing infection time-and-\nstate data. Based on the likely epidemic paths predicted by\nthe simple susceptible-infected epidemic model, we propose\na measure (Infection Betweenness Centrality) for uncovering\nunknown infection states. Our experimental results using\nmachine learning algorithms show that Infection Between-\nness Centrality is the most effective feature for identifying\nlatent infected nodes."}
{"Title": "Temporal Capacity Graphs for Time-Varying Mobile\nNetworks", "Abstract": "With the rapid emergence of applications in mobile net-\nworks, understanding and characterizing their properties be-\ncomes extremely important. In this paper, from the funda-\nmental model of time-varying graphs, we introduce Tempo-\nral Capacity Graphs (TCG), which characterizes the maxi-\nmum amount of the data that can be transmitted between\nany two nodes within any time, and consequently reveals\nthe transmission capacity of the whole network. By ap-\nplying TCG to several realistic mobile networks, we analyze\ntheir unique properties. Moreover, using TCG, we reveal the\nfundamental relationships and tradeoffs between the mobile\nnetwork settings and system performance."}
{"Title": "Complex Network Comparison Using Random Walks", "Abstract": "In this paper, we proposed a network comparison method\nbased on the mathematical theory of diffusion over mani-\nfolds using random walks over graphs. We show that our\nmethod not only distinguishes between graphs with differ-\nent degree distributions, but also different graphs with the\nsame degree distributions. We compare the undirected pow-\ner law graphs generated by Barabasi-Albert model and di-\nrected power law graphs generated by Krapivsky\u2019s model to\nthe random graphs generated by Erdos-Renyi model. We\nalso compare power law graphs generated by four different\ngenerative models with the same degree distribution."}
{"Title": "Mal-Netminer: Malware Classification based on Social\nNetwork Analysis of Call Graph", "Abstract": "In this work, we aim to classify malware using automatic classi-\nfiers by employing graph metrics commonly used in social net-\nwork analysis. First, we make a malicious system call dictionary\nthat consists of system calls found in malware. To analyze the gen-\neral structural information of malware and measure the influence of\nsystem calls found in malware, we adopt social network analysis.\nThus, we use social network metrics such as the degree distribu-\ntion, degree centrality, and average distance, which are implicitly\nequivalent to distinct behavioral characteristics. Our experiments\ndemonstrate that the proposed system performs well in classifying\nmalware families within each malware class with accuracy greater\nthan 98%. As exploiting the social network properties of system\ncalls found in malware, our proposed method can not only classify\nthe malware with fewer features than previous methods adopting\ngraph features but also enables us to build a quick and simple de-\ntection system against malware."}
{"Title": "Designing a High-Performance Mobile Cloud Web Browser", "Abstract": "A mobile cloud web browser is a web browser that enables\nmobile devices with constrained resources to support com-\nplex web pages by performing most of resource demanding\noperations on a cloud web server. In this paper, we present\na design of a mobile web cloud browser with efficient data\nstructure."}
{"Title": "Propagation Phenomena in Large Social Networks", "Abstract": "Social media and blogging services have become extremely popu-\nlar. Every day hundreds of millions of users share conversations on\nrandom thoughts, emotional expressions, political news, and social\nissues. Usersinteractbyfollowingeachother\u2019supdatesandpassing\nalong interesting pieces of information to their friends. Information\ntherefore can diffuse widely and quickly through social links. In-\nformation propagation in networks like Twitter is unique in that\ntraditional media sources and word-of-mouth propagation coex-\nist. The availability of digitally-logged propagation events in social\nmedia help us better understand how user influence, tie strength, re-\npeated exposures, conventions, and various other factors come into\nplay in the way people generate and consume information in the\nmodern society.\nIn this talk, I will present several findings on how bad news [9],\nrumors [8], prominent events [11], conventions [6,7], tags [1,4],\nbehaviors [12], and moods [10] propagate in social media based on\na large amount of data collected from networks like Twitter, Flickr,\nFacebook, and Blogosphere. I will talk about the different roles\nof user types [2] and content types [5] in propagations as well as\nways to measure their influence [3]. Among various findings, I\nwill demonstrate that indegree of a user, a well-known measure of\npopularity, alone can reveal little about the influence."}
{"Title": "Challenges of Computational Verification in Social\nMultimedia", "Abstract": "Fake or misleading multimedia content and its distribution\nthrough social networks such as Twitter constitutes an in-\ncreasingly important and challenging problem, especially in\nthe context of emergencies and critical situations. In this\npaper, the aim is to explore the challenges involved in apply-\ning a computational verification framework to automatically\nclassify tweets with unreliable media content as fake or real.\nWe created a data corpus of tweets around big events focus-\ning on the ones linking to images (fake or real) of which the\nreliability could be verified by independent online sources.\nExtracting content and user features for each tweet, we ex-\nplored the fake prediction accuracy performance using each\nset of features separately and in combination. We consid-\nered three approaches for evaluating the performance of the\nclassifier, ranging from the use of standard cross-validation,\nto independent groups of tweets and to cross-event training.\nThe obtained results included a 81% for tweet features and\n75% for user ones in the case of cross-validation. When us-\ning different events for training and testing, the accuracy is\nmuch lower (up to %58) demonstrating that the generaliza-\ntion of the predictor is a very challenging issue."}
{"Title": "Alethiometer: a Framework for Assessing Trustworthiness\nand Content Validity in Social Media", "Abstract": "There are both positive and negative aspects in the use of so-\ncial media in news and information dissemination. To deal\nwith the negative aspects, such as the spread of rumours\nand fake news, the flow of information should implicitly\nbe filtered and marked to specific criteria such as credibil-\nity, trustworthiness, reputation, popularity, influence, and\nauthenticity. This paper proposes an approach that can\nenhance trustworthiness and content validity in the pres-\nence of information overload. We introduce Alethiometer,\na framework for assessing truthfulness in social media that\ncan be used by professional and general news users alike.\nWe present different measures that delve into the detailed\nanalysis of the content, the contributors of the content and\nthe underlying context. We further propose an approach for\nderiving a single metric that considers, in a unified manner,\nthe quality of a contributor and of the content provided by\nthat contributor. Finally, we present some preliminary sta-\ntistical results from the examination of a set of 10 million\ntwitter users, that provide useful insights on the character-\nistics of social media data."}
{"Title": "Trends of News Diffusion in Social Media\nbased on Crowd Phenomena", "Abstract": "Information spreads across social media, bringing hetero-\ngeneous social networks interconnected and diffusion pat-\nterns varied in different topics of information. Studying\nsuch cross-population diffusion in various context helps us\nunderstand trends of information diffusion in a more accu-\nrate and consistent way. In this study, we focus on real-\nworld news diffusion across online social systems such as\nmainstream news (News), social networking sites (SNS), and\nblogs (Blog), and we analyze behavioral patterns of the sys-\ntems in terms of activity, reactivity, and heterogeneity. We\nfound that News is the most active, SNS is the most reactive,\nand Blog is the most persistent, which governs time-evolving\nheterogeneity of these systems. Finally, we interpret the\ndiscovered crowd phenomena from various angles using our\nprevious model-free and model-driven approaches, showing\nthat the strength and directionality of influence reflect the\nbehavioral patterns of the systems in news diffusion."}
{"Title": "Describing and Contextualizing Events in TV News Show", "Abstract": "Describing multimedia content in general and TV programs\nin particular is a hard problem. Relying on subtitles to\nextract named entities that can be used to index fragments\nof a program is a common method. However, this approach\nis limited to what is being said in a program and written in a\nsubtitle, therefore lacking a broader context. Furthermore,\nthis type of index is restricted to a flat list of entities. In this\npaper, we combine the power of non-structured documents\nwith structured data coming from DBpedia to generate a\nmuch richer, context aware metadata of a TV program. We\ndemonstrate that we can harvest a rich context by expanding\nan initial set of named entities detected in a TV fragment.\nWe evaluate our approach on a TV news show."}
{"Title": "News from the Crowd: Grassroots and Collaborative\nJournalism in the Digital Age", "Abstract": "Information content provided by members of the general public\n(mostly online) is playing an ever-increasing role in the detection,\nproduction and distribution of news. This paper investigates the\nconcepts of (1) grassroots journalism and (2) collaborative\njournalism. It looks into similarities and differences and shows\nhow both phenomena have been influenced by the emergence of\nthe Internet and digital technologies. Then, the consequences for\njournalism in general will be analysed. Ultimately, strategies to\nmeet the new challenges are suggested in order to maintain the\nquality and reliability of news coverage in the future."}
{"Title": "Exploring Social Activeness and Dynamic Interest in\nCommunity-based Recommender System", "Abstract": "Community-based recommender systems have attracted much\nresearch attention. Forming communities allows us to re-\nduce data sparsity and focus on discovering the latent char-\nacteristics of communities instead of individuals. Previous\nwork focused on how to detect the community using vari-\nous algorithms. However, they failed to consider users\u2019 so-\ncial attributes, such as social activeness and dynamic inter-\nest, which have strong correlations to users\u2019 preference and\nchoice. Intuitively, people have different social activeness\nin a social network. Ratings from users with high active-\nness are more likely to be trustworthy. Temporal dynamic\nof interest is also significant to user\u2019s preference. In this pa-\nper, we propose a novel community-based framework. We\nfirst employ PLSA-based model incorporating social active-\nness and dynamic interest to discover communities. Then\nthe state-of-the-art matrix factorization method is applied\non each of the communities. The experiment results on two\nreal world datasets validate the effectiveness of our method\nfor improving recommendation performance."}
{"Title": "Mining User Trails in Critiquing Based Recommenders", "Abstract": "Critiquing based recommenders are very commonly used to\nhelp users navigate through the product space to find the\nrequired product by tweaking/critiquing one or more fea-\ntures. By critiquing a product, the user gives an informa-\ntive feedback(i.e, which feature needs to be modified) about\nwhy they rejected a product and preferred the other one.\nAs a user interacts with such a system, trails are left be-\nhind. We propose ways of leveraging these trails to induce\npreference models of items which can be used to estimate\nthe relative utilities of products which can be used in rank-\ning the recommendations presented to the user. The idea is\nto effectively complement knowledge of explicit user inter-\nactions in traditional social recommenders with knowledge\nimplicitly obtained from trails."}
{"Title": "Folksonomy Based Socially-Aware Recommendation of\nScholarly Papers for Conference Participants", "Abstract": "Due to the significant proliferation of scholarly papers in both\nconferences and journals, recommending relevant papers to\nresearchers for academic learning has become a substantial\nproblem. Conferences, in comparison to journals have an aspect\nof social learning, which allows personal familiarization through\nvarious interactions among researchers. In this paper, we improve\nthe social awareness of participants of smart conferences by\nproposing  an  innovative  folksonomy-based  paper\nrecommendation  algorithm,  namely,  Socially-Aware\nRecommendation of Scholarly Papers (SARSP). Our proposed\nalgorithm recommends scholarly papers, issued by Active\nParticipants (APs), to other Group Profile participants at the same\nsmart conference based on similarity of their research interests.\nFurthermore, through computation of social ties, SARSP\ngenerates effective recommendations of scholarly papers to\nparticipants who have strong social ties with an AP. Through a\nrelevant real-world dataset, we evaluate our proposed algorithm.\nOur experimental results verify that SARSP has encouraging\nimprovements over other existing methods."}
{"Title": "Online Dating Recommendations: Matching Markets and\nLearning Preferences", "Abstract": "Recommendation systems for online dating have recently at-\ntracted much attention from the research community. In this\npaper we propose a two-side matching framework for online\ndating recommendations and design an Latent Dirichlet Al-\nlocation (LDA) model to learn the user preferences from the\nobserved user messaging behavior and user profile features.\nExperimental results using data from a large online dating\nwebsite shows that two-sided matching improves the rate of\nsuccessful matches by as much as 45%. Finally, using simu-\nlated matching we show that the LDA model can correctly\ncapture user preferences."}
{"Title": "A New Correlation-based Information Diffusion Prediction", "Abstract": "For predicting the diffusion process of information, we intro-\nduce and analyze a new correlation between the information\nadoptions of users sharing a friend in online social networks.\nBased on the correlation, we propose a probabilistic model to\nestimate the probability of a user\u2019s adoption using the naive\nBayes classifier. Next, we build a recommendation method\nusing the probabilistic model. Finally, we demonstrate the\neffectiveness of the proposed method with the data from\nFlickr and Movielens which are well-known web services.\nFor all cases in the experiments, the proposed method is\nmore accurate than comparison methods."}
{"Title": "Are Influential Writers More Objective?\nAn Analysis of Emotionality in Review Comments", "Abstract": "People increasingly rely on other consumers\u2019 opinion to make\nonline purchase decisions. Amazon alone provides access to\nmillions of reviews, risking to cause information overload to\nan average user. Recent research has thus aimed at under-\nstanding and identifying reviews that are considered helpful.\nMost of such works analyzed the structure and connectivity\nof social networks to identify influential users. We believe\nthat insight about influence can be gained from analyzing\nthe affective content of the text as well as affect intensity.\nWe employ text mining to extract the emotionality of 68,049\nhotel reviews in order to investigate how those influencers\nbehave, especially their choice of words. We analyze whether\ntexts with words and phrases indicative of a writer\u2019s emo-\ntions, moods, and attitudes are more likely to trigger a gen-\nuine interest compared to more neutral texts. Our initial\nhypothesis was that influential writers are more likely to re-\nfrain themselves from expressing their sentiments in order\nto achieve a more perceived objectivity. But contrary to\nthis initial assumption, our study shows that they use more\naffective words, both in terms of emotion variety and inten-\nsity. This work describes the first step towards building a\nhelpfulness prediction algorithm using emotion lexicons."}
{"Title": "Tensor-based Item Recommendation using Probabilistic\nRanking in Social Tagging Systems", "Abstract": "A common problem with the use of tensor modeling in generating\nquality recommendations for large datasets is scalability. In this\npaper, we propose the Tensor-based Recommendation using\nProbabilistic Ranking method that generates the reconstructed\ntensor using block-striped parallel matrix multiplication and then\nprobabilistically calculates the preferences of user to rank the\nrecommended items. Empirical analysis on two real-world da-\ntasets shows that the proposed method is scalable for large tensor\ndatasets and is able to outperform the benchmarking methods in\nterms of accuracy."}
{"Title": "Random Walks in Recommender Systems:\nExact Computation and Simulations", "Abstract": "A recommender system uses information about known as-\nsociations between users and items to compute for a given\nuser an ordered recommendation list of items which this user\nmight be interested in acquiring. We consider ordering rules\nbased on various parameters of random walks on the graph\nrepresenting associations between users and items. We ex-\nperimentally compare the quality of recommendations and\nthe required computational resources of two approaches: (i)\ncalculate the exact values of the relevant random walk pa-\nrameters using matrix algebra; (ii) estimate these values by\nsimulating random walks. In our experiments we include\nmethods proposed by Fouss et al. [7, 8] and Gori and Pucci\n[10], method P 3 , which is based on the distribution of the\nrandom walk after three steps, and method P 3\n\u03b1 , which gener-\nalises P 3 . We show that the simple method P 3 can outper-\nform previous methods and method P 3\n\u03b1 can offer further im-\nprovements. We show that the time- and memory-efficiency\nof direct simulation of random walks allows application of\nthese methods to large datasets. We use in our experiments\nthe three MovieLens datasets."}
{"Title": "Towards a Scalable Social Recommender Engine for\nOnline Marketplaces: The Case of Apache Solr", "Abstract": "Recent research has unveiled the importance of online social net-\nworks for improving the quality of recommenders in several do-\nmains, what has encouraged the research community to investigate\nways to better exploit the social information for recommendations.\nHowever, there is a lack of work that offers details of frameworks\nthat allow an easy integration of social data with traditional recom-\nmendation algorithms in order to yield a straight-forward and scal-\nable implementation of new and existing systems. Furthermore, it\nis rare to find details of performance evaluations of recommender\nsystems such as hardware and software specifications or bench-\nmarking results of server loading tests.\nInthispaperweintendtobridgethisgapbypresentingthedetails\nof a social recommender engine for online marketplaces built upon\nthe well-known search engine Apache Solr. We describe our archi-\ntecture and also share implementation details to facilitate the re-use\nof our approach by people implementing recommender systems. In\naddition, we evaluate our framework from two perspectives: (a)\nrecommendation algorithms and data sources, and (b) system per-\nformance under server stress tests. Using a dataset from the Sec-\nondLife virtual world that has both trading and social interactions,\nwe contribute to research in social recommenders by showing how\ncertain social features allow to improve recommendations in online\nmarketplaces. On the platform implementation side, our evaluation\nresults can serve as a baseline to people searching for performance\nreferences in terms of scalability, model training and testing trade-\noffs, real-time server performance and the impact of model updates\nin a production system."}
{"Title": "Analyzing Temporal Characteristics of Check-in data", "Abstract": "There is a surge in the use of location activity in social me-\ndia, in particular to broadcast the change of physical where-\nabouts. We are interested in analyzing the temporal char-\nacteristics of check-ins data from the user\u2019s perspective and\nalso at the aggregate level for detecting patterns. In this\npaper we conduct a large study using check-in data from\nFacebook to analyze different temporal characteristics in\nfour venue categories (restaurants, movies, shopping, and\nget-away). We present the results of such study and out-\nline application areas where the conjunction of location and\ntemporal-aware data can help in new search scenarios."}
{"Title": "TempoWordNet for Sentence Time Tagging", "Abstract": "In this paper, we propose to build a temporal ontology,\nwhich may contribute to the success of time-related appli-\ncations. Temporal classifiers are learned from a set of time-\nsensitive synsets and then applied to the whole WordNet to\ngive rise to TempoWordNet. So, each synset is augmented\nwith its intrinsic temporal value. To evaluate TempoWord-\nNet, we use a semantic vector space representation for sen-\ntence temporal classification, which shows that improve-\nments may be achieved with the time-augmented knowledge\nbase against a bag-of-ngrams representation."}
{"Title": "Extracting and Aggregating Temporal Events from Text", "Abstract": "Finding reliable information about a given event from large\nand dynamic text collections is a topic of great interest. For\ninstance, rescue teams and insurance companies are inter-\nested in concise facts about damages after disasters, which\ncan be found in web blogs, newspaper articles, social net-\nworks etc. However, finding, extracting, and condensing\nspecific facts is a highly complex undertaking: It requires\nidentifying appropriate textual sources, recognizing relevant\nfacts within the sources, and aggregating extracted facts into\na condensed answer despite inconsistencies, uncertainty, and\nchanges over time. In this paper, we present a three-step\nframework providing techniques and solutions for each of\nthese problems. We tested the feasibility of extracting time-\nassociated event facts using our framework in a comprehen-\nsive case study: gathering data on particular earthquakes\nfrom web data sources. Our results show that it is, under\ncertain circumstances, possible to automatically obtain reli-\nable and timely data on natural disasters from the web."}
{"Title": "NTCIR Temporalia: A Test Collection for Temporal\nInformation Access Research", "Abstract": "Time is one of the key constructs of information quality.\nFollowing an upsurge of research in temporal aspects of\ninformation search, it has become clear that the community needs\nstandardized evaluation benchmark for fostering research in\nTemporal Information Access. This paper introduces Temporalia\n(Temporal Information Access), a new pilot task run at NTCIR-11\nto create re-usable datasets for those who are interested in\ntemporal aspects of search technologies, and discusses its task\ndesign in detail."}
{"Title": "Infrastructure for Supporting Exploration\nand Discovery in Web Archives", "Abstract": "Web archiving initiatives around the world capture ephem-\neral web content to preserve our collective digital memory.\nHowever, unlocking the potential of web archives requires\ntools that support exploration and discovery of captured\ncontent. These tools need to be scalable and responsive,\nand to this end we believe that modern\u201cbig data\u201dinfrastruc-\nture can provide a solid foundation. We present Warcbase,\nan open-source platform for managing web archives built\non the distributed datastore HBase. Our system provides a\nflexible data model for storing and managing raw content as\nwell as metadata and extracted knowledge. Tight integra-\ntion with Hadoop provides powerful tools for analytics and\ndata processing. Relying on HBase for storage infrastruc-\nture simplifies the development of scalable and responsive\napplications. We describe a service that provides tempo-\nral browsing and an interactive visualization based on topic\nmodels that allows users to explore archived content."}
{"Title": "Wikipedia as a Time Machine", "Abstract": "Wikipedia encyclopaedia projects, which consist of vast collections\nof user-edited articles covering a wide range of topics, are among\nsome of the most popular websites on internet. With so many users\nworking collaboratively, mainstream events are often very quickly\nreflected by both authors editing content and users reading arti-\ncles. With temporal signals such as changing article content, page\nviewing activity and the link graph readily available, Wikipedia has\ngained attention in recent years as a source of temporal event infor-\nmation. This paper serves as an overview of the characteristics and\npast work which support Wikipedia (English, in this case) for time-\naware information retrieval research. Furthermore, we discuss the\nmain content and meta-data temporal signals available along with\nillustrative analysis. We briefly discuss the source and nature of\neach signal, and any issues that may complicate extraction and use.\nTo encourage further temporal research based on Wikipedia, we\nhave released all the distilled datasets referred to in this paper."}
{"Title": "The 4 th Temporal Web Analytics Workshop (TempWeb\u201914)", "Abstract": "In this paper we give an overview on the 4 th Temporal Web\nAnalytics Workshop (TempWeb). The goal of TempWeb is to\nprovide a venue for researchers of all domains (IE/IR, Web\nmining, etc.) where the temporal dimension opens up an entirely\nnew range of challenges and possibilities. The workshop\u2019s\nambition is to help shaping a community of interest on the\nresearch challenges and possibilities resulting from the\nintroduction of the time dimension in web analysis. Having a\ndedicated workshop will help, we believe, to take a rich and cross-\ndomain approach to this new research challenge with a strong\nfocus on the temporal dimension. For the fourth time, TempWeb\nhas been organized in conjunction with the International World\nWide Web (WWW) conference, being held on April 8, 2014 in\nSeoul, Korea."}
{"Title": "Personal APIs as an Enabler for Designing and\nImplementing People as Social Machines", "Abstract": "In this paper, we extend the initial classification scheme for\nSocial Machines (SM) by including Personal APIs as a new\nSM-related topic of research inquiry. Personal APIs basi-\ncally refer to the use of Open Application Programming In-\nterfaces (Open APIs) to programmatically access informa-\ntion about a person (e.g., personal basic info, health-related\nstatistics, busy data) and/or trigger his/her human capabil-\nities in a standardized way. Here, we provide an overview of\nsome existing Personal APIs and show how this approach can\nbe used to enable the design and implementation of people\nas individual SMs on the Web. A proof-of-concept system\nthat demonstrates these ideas is also outlined in this paper."}
{"Title": "A New Architecture Description Language\nfor Social Machines", "Abstract": "The term \u2018Social Machine\u2019 (SM) has been commonly used as a\nsynonym for what is known as the programmable web or web 3.0.\nSome definitions of a Social Machine have already been provided\nand they basically support the notion of relationships between\ndistributed entities. The type of relationship molds which services\nwould be provided or required by each machine, and under certain\ncomplex constraints. In order to deal with the complexity of this\nemerging web, we present a language that can describe networks\nof Social Machines, named SMADL \u2013 the Social Machine\nArchitecture Description Language. In few words, SMADL is as a\nrelationship-driven language which can be used to describe the\ninteractions between any number of machines in a multitude of\nways, as a means to represent real machines interacting in the real\nweb, such as, Twitter running on top of Amazon AWS or mash-\nups built upon Google Maps, and obviously, as a means to\nrepresent interactions with other social machines too."}
{"Title": "LSCitter: Building Social Machines by Augmenting\nExisting Social Networks with Interaction Models", "Abstract": "We present LSCitter, an implemented framework for sup-\nporting human interaction on social networks with formal\nmodels of interaction, designed as a generic tool for creating\nsocial machines on existing infrastructure. Interaction mod-\nels can be used to choreograph distributed systems, pro-\nviding points of coordination and communication between\nmultiple interacting actors. While existing social networks\nspecify how interactions happen\u2014who messages go to and\nwhen, the effects of carrying out actions\u2014these are typically\nimplicit, opaque and non user-editable. Treating interaction\nmodels as first class objects allows the creation of electronic\ninstitutions, on which users can then choose the kinds of in-\nteraction they wish to engage in, with protocols which are\nexplicit, visible and modifiable. However, there is typically\na cost to users to engage with these institutions. In this pa-\nper we introduce the notion of \u201cshadow institutions\u201d, where\nactions on existing social networks are mapped onto formal\ninteraction protocols, allowing participants access to compu-\ntational intelligence in a seamless, zero-cost manner to carry\nout computation and store information."}
{"Title": "Community Structure for Efficient Information Flow in\n\u2018ToS;DR\u2019, a Social Machine for Parsing Legalese", "Abstract": "This paper presents a case study of \u2018Terms-of-Service; Didn\u2019t\nRead\u2019, a social machine to curate, parse, and rate website\nterms and privacy policies. We examine the relationships\nbetween its human contributors and machine counterparts\nto determine community structure and information flow."}
{"Title": "The Berners-Lee Hypothesis: Power laws and Group\nStructure in Flickr", "Abstract": "An intriguing hypothesis, suggested by Tim Berners-Lee,\nis that the structure of online groups should conform to a\npower law distribution. We believe this is likely a conse-\nquence of the Dunbar Number, which is a supposed limit to\nthe number of persistent social contacts a user can have in a\ngroup. As preliminary results, we show that the number of\ncontacts of a typical Flickr user, the number of groups a user\nbelongs to, and the size of Flickr groups all follow power law\ndistributions. Furthermore, we find some unexpected differ-\nences in the internal structure of public and private Flickr\ngroups. For further research, we further operationalize the\nBerners-Lee hypothesis to suppose that users with a group\nmembership distribution that follows a power law will pro-\nduce more content for social Web systems."}
{"Title": "Community-based Crowdsourcing", "Abstract": "This paper is focused on community-based crowdsourcing\napplications, i.e. the ability of spawning crowdsourcing tasks\nupon multiple communities of performers, thus leveraging\nthe peculiar characteristics and capabilities of the commu-\nnity members. We show that dynamic adaptation of crowd-\nsourcing campaigns to community behaviour is particularly\nrelevant. We demonstrate that this approach can be very\neffective for obtaining answers from communities, with very\ndifferent size, precision, delay and cost, by exploiting the\nsocial networking relations and the features of the crowd-\nsourcing task. We show the approach at work within the\nCrowdSearcher platform, which allows configuring and dy-\nnamically adapting crowdsourcing campaigns tailored to dif-\nferent communities. We report on an experiment demon-\nstrating the effectiveness of the approach."}
{"Title": "Constructed Identity and Social Machines: A Case Study\nin Creative Media Production", "Abstract": "Current discussions of social machines rightly emphasise a\nhuman\u2019s role as a crucial part of a system rather than a user\nof a system. The human \u2018parts\u2019 are typically considered in\nterms of their aggregate outcomes and collective behaviours,\nbut human participants are rarely all equal, even within a\nsmall system. We argue that due to the complex nature of\nonline identity, understanding participants in a more granu-\nlar way is crucial for social machine observation and design.\nWe present the results of a study of the personas portrayed\nby participants in a social machine that produces creative\nmedia content, and discover that inconsistent or misleading\nrepresentations of individuals do not necessarily undermine\nthe system in which they are participating. We describe a\npreliminary framework for making sense of human partici-\npants in social machines, and the ongoing work that develops\nthis further."}
{"Title": "Government as a Social Machine in an Ecosystem", "Abstract": "The Web is becoming increasingly pervasive throughout all\naspects of human activity. As citizens and organisations adopt\nWeb technologies, so governments are beginning to respond by\nthemselves utilising the electronic space. Much of this has been\nreactive, and there is very little understanding of the impact that\nWeb technologies are having on government systems and\nprocesses, let alone a proactive approach to designing systems that\ncan ensure a positive and beneficial societal impact. The\necosystem which encompasses governments, citizens and\ncommunities is both evolving and adaptive, and the only way to\nexamine and understand the development of Web-enabled\ngovernment, and its possible implications, is to consider\ngovernment itself as a \u201csocial machine\u201d within a social machine\necosystem. In this light, there are significant opportunities and\nchallenges for government that this paper identifies."}
{"Title": "Introducing the \u2126-machine", "Abstract": "In this paper, we propose the \u2126-machine model for social\nmachines. By introducing a cluster of \"oracles\" to a traditional\nTuring machine, the \u2126-machine is capable of describing the\ninteraction between human participants and mechanical machines.\nWe also give two examples of social machines, collective\nintelligence and rumor spreading, and demonstrate how the\ngeneral \u2126-machine model could be used to simulate their\ncomputations."}
{"Title": "Working out the plot:\nthe role of Stories in Social Machines", "Abstract": "Although Social Machines do not have yet a formalized def-\ninition, some efforts have been made to characterize them\nfrom a \u201cmachinery\u201dpoint of view. In this paper, we present\na methodology by which we attempt to reveal the sociality\nof Social Machines; to do so, we adopt the analogy of sto-\nries. By assimilating a Social Machine to a story, we can\nidentify the stories within and about that machine and how\nthis storytelling perspective might reveal the sociality of So-\ncial Machines. After illustrating this storytelling approach\nwith a few examples, we then propose three axes of inquiry\nto evaluate the health of a social machine: (1) assessment\nof the sociality of a Social Machine through evaluation of its\nstorytelling potential and realization; (2) assessment of the\nsustainability of a Social Machine through evaluation of its\nreactivity and interactivity; and (3) assessment of emergence\nthrough evaluation of the collaboration between authors and\nof the distributed/mixed nature of authority."}
{"Title": "7 Billion Home Telescopes: Observing Social Machines\nthrough Personal Data Stores", "Abstract": "Web Observatories aim to develop techniques and meth-\nods to allow researchers to interrogate and answer questions\nabout society through the multitudes of digital traces people\nnow create. In this paper, we propose that a possible path\ntowards surmounting the inevitable obstacle of personal pri-\nvacy towards such a goal, is to keep data with individuals,\nunder their own control, while enabling them to participate\nin Web Observatory-style analyses in situ. We discuss the\nkinds of applications such a global, distributed, linked net-\nwork of Personal Web Observatories might have, a few of\nthe many challenges that must be resolved towards realis-\ning such an architecture in practice, and finally, our work\ntowards a fundamental reference building block of such a\nnetwork."}
{"Title": "Seed Selection for Domain-Specific Search", "Abstract": "The last two decades have witnessed an exponential rise in\nweb content from a plethora of domains, which has neces-\nsitated the use of domain-specific search engines. Diversity\nof crawled content is one of the crucial aspects of a domain-\nspecific search engine. To a large extent, diversity is gov-\nerned by the initial set of seed URLs. Most of the existing\napproaches rely on manual effort for seed selection. In this\nwork we automate this process using URLs posted on Twit-\nter. We propose an algorithm to get a set of diverse seed\nURLs from a Twitter URL graph. We compare the perfor-\nmance of our approach against the baseline zero similarity\nseed selection method and find that our approach beats the\nbaseline by a significant margin."}
{"Title": "Pragmatic Hypermedia:\nCreating a Generic, Self-Inflating API Client\nfor Production Use", "Abstract": "Hypermedia API design is a method of creating APIs using hyper-\nlinks to represent and publish an API\u2019s functionality. Hypermedia-\nbased APIs bring theoretical advantages over many other designs,\nincluding the possibility of self-updating, generic API client soft-\nware. Such hypermedia API clients only lately have come to exist,\nand the existing hypermedia client space did not compare favorably\nto custom API client libraries, requiring somewhat tedious manual\naccess to HTTP resources. Nonetheless, the limitations in creating\na compelling hypermedia client were few.\nThis paper describes the design and implementation of Hyper-\nResource [19], a fully generic, production-ready Ruby client library\nfor hypermedia APIs. The project leverages the inherent practicality\nof hypermedia design, demonstrates its immediate usefulness in\ncreating self-generating API clients, enumerates several abstractions\nand strategies that help in creating hypermedia APIs and clients, and\npromotes hypermedia API design as the easiest option available to\nan API programmer."}
{"Title": "REST to JavaScript for Better Client-side Development", "Abstract": "In today\u2019s Web-centric era, embedded systems become mashup vari-\nous web services via RESTful web services. RESTful web services\nuse REST APIs that describe actions as resource state transfers via\nstandard HTTP methods such as GET, PUT, POST, and DELETE.\nWhile RESTful web services are lightweight and executable on any\nplatforms that support HTTP methods, writing programs composed\nof only such primitive methods is not a familiar concept to develop-\ners. Therefore, no single design strategy for (fully) RESTful APIs\nworks for arbitrary domains, and current REST APIs are system\ndependent, incomplete, and likely to change. To help sever-side\ndevelopment of REST APIs, several domain-specific languages such\nas WADL, WSDL 2.0, and RDF provide automatic tools to generate\nREST APIs. However, client-side developers who often do not\nknow the web services domain and do not understand RESTful web\nservices suffer from the lack of any development help.\nIn this paper, we present a new approach to build JavaScript APIs\nthat are more accessible to client-side developers than REST APIs.\nWe show a case study of our approach that uses JavaScript APIs\nand their wrapper implementation instead of REST APIs, and we\ndescribe the efficiency in the client-side development."}
{"Title": "Atomic Distributed Transactions: a RESTful Design", "Abstract": "The REST architectural style supports the reliable interaction of\nclients with a single server. However, no guarantees can be made\nfor more complex interactions which require to atomically trans-\nfer state among resources distributed across multiple servers. In\nthis paper we describe a lightweight design for transactional com-\nposition of RESTful services. The approach \u2013 based on the Try-\nCancel/Conf rm (TCC) pattern \u2013 does not require any extension to\ntheHTTPprotocol. Thedesignassumesthatresources aredesigned\nto comply with the TCC pattern and ensures that the resources in-\nvolved in the transaction are not aware of it. It delegates the respon-\nsability of achieving the atomicity of the transaction to a coordina-\ntor which exposes a RESTful API."}
{"Title": "Seven Challenges for RESTful Transaction Models", "Abstract": "The REpresentational State Transfer (REST) architectural style de-\nscribes the design principles that made the World Wide Web scalable\nand the same principles can be applied in enterprise context to do\nloosely coupled and scalable application integration. In recent years,\nRESTful services are gaining traction in the industry and are com-\nmonly used as a simpler alternative to SOAP Web Services.\nHowever, one of the main drawbacks of RESTful services is\nthe lack of standard mechanisms to support advanced quality-of-\nservice requirements that are common to enterprises. Transaction\nprocessing is one of the essential features of enterprise information\nsystems and several transaction models have been proposed in the\npast years to fulfill the gap of transaction processing in RESTful\nservices. The goal of this paper is to analyze the state-of-the-art\nRESTful transaction models and identify the current challenges."}
{"Title": "Publish Data as Time consistent Web API with Provenance", "Abstract": "Many organisations publish their data through a Web API . This\nstimulates use by Web applications, enabling reuse and enrichments.\nRecently, resource-oriented API s are increasing in popularity be-\ncause of their scalability. However, for organisations subject to data\narchiving, creating such an API raises certain issues. Often, datasets\nare stored in different files and different formats. Therefore, tracking\nrevisions is a challenging task and the API has to be custom built.\nMoreover, standard API s only provide access to the current state of\na resource. This creates time-based inconsistencies when they are\ncombined. In this paper, we introduce an end-to-end solution for\npublishing a dataset as a time-based versioned REST API , with mini-\nmal input of the publisher. Furthermore, it publishes the provenance\nof each created resource. We propose a technology stack composed\nof prior work, which versions datasets, generates provenance, cre-\nates an API and adds Memento Datetime negotiation."}
{"Title": "The W3C Web Cryptography API: Motivation and Overview", "Abstract": "The W3C Web Cryptography API is the standard API for access-\ning cryptographic primitives in Javascript-based environments. We\ndescribe the motivations behind the creation of the W3C Web Cryp-\ntography API and give a high-level overview with motivating use-\ncases while addressing objections."}
{"Title": "A RESTful API\nfor Controlling Dynamic Streaming Topologies", "Abstract": "Streaming applications have become more and more dynamic and\nheterogeneous thanks to new technologies which enable platforms\nlike microcontrollers and Web browsers to be able to host part of a\nstreaming topology. A dynamic heterogeneous streaming applica-\ntion should support load balancing and fault tolerance while being\ncapable of adapting and rearranging topologies to user needs at\nruntime. In this paper we present a REST API to control dynamic\nheterogeneous streaming applications. By means of resources, their\nuniform interface and hypermedia we show how it is possible to\nmonitor, change and adapt the deployment configuration of a stream-\ning topology at runtime."}
{"Title": "The COMPOSE API for the Internet of Things", "Abstract": "The COMPOSE project aims to provide an open Marketplace for\nthe Internet of Things as well as the necessary platform to support\nit. A necessary component of COMPOSE is an API that allows\nthings, COMPOSE users and the platform to communicate. The\nCOMPOSE API allows for things to push data to the platform,\nthe platform to initiate asynchronous actions on the things, and\nCOMPOSE users to retrieve and process data from the things. In this\npaper we present the design and implementation of the COMPOSE\nAPI, as well as a detailed description of the main key requirements\nthat the API must satisfy. The API documentation and the source\ncode for the platform are available at [9]"}
{"Title": "An Approach for Composing RESTful Linked Services on\nthe Web", "Abstract": "In this paper, we present an approach to compose linked services\non the Web based on the principles of linked data and REST. Our\ncontribution is a unified method for discovering both the interaction\npossibilities a service offers and the available semantic links to\nother services. Our composition engine is implemented as a generic\nclient that allows exploring a service API and interacting with other\nservices to answer user\u2019s goal. We rely on a typical scenario in\norder to illustrate the benefits of our composition approach. We\nimplemented a prototype to demonstrate the applicability of our\nproposal, experiment and discuss the results obtained."}
{"Title": "Exploring Intelligence of Web Communities", "Abstract": "Web Intelligence is a multidisciplinary area dealing with utilizing\ndata and services over the Web, to create new data and services\nusing Information and Communication Technologies (ICT) and\nIntelligent techniques. The link to Networking and Web\nCommunities (WCs) is apparent: the Web is a set of nodes,\nproviding and consuming data and services; the permanent or\ntemporary ties and exchanges in-between these nodes build the\nvirtual communities; and the ICT and intelligent techniques\ninfluence the modeling and the processes, and it automates (or\nsemi-automate) communication and cooperation. In this paper, we\nwill explore one aspect of (Web) intelligence pertinent to the Web\nCommunities. The \u201cintelligent\u201d features may emerge in a Web\ncommunity from interactions and knowledge-transmissions\nbetween the community members. We will also introduce the\nWI&C\u201914 workshop\u2019s goal and structure."}
{"Title": "Towards Web Intelligence through the\nCrowdsourcing of Semantics", "Abstract": "A key success factor for the Web as a whole was and is\nits participatory nature. We discuss strategies for engaging\nhuman-intelligence to make the Web more semantic."}
{"Title": "Population Dynamics in Open Source Communities: An\nEcological Approach Applied to Github", "Abstract": "Open Source Software (OSS) has gained high amount of pop-\nularity during the last few years. It is becoming used by\npublic and private institutions, even companies release por-\ntions of their code to obtain feedback from the community\nof voluntary developers. As OSS is based on the voluntary\ncontributions of developers, the number of participants rep-\nresents one of the key elements that impact the quality of\nthe software. In order to understand how the the population\nof contributors evolve over time, we propose a methodology\nthat adapts Lotka-Volterra-based biological models used for\ndescribing host-parasite interactions. Experiments based on\ndata from the Github collaborative platform showed that\nthe proposed approach performs effectively in terms of pro-\nviding an estimation of the population of developers for each\nproject over time."}
{"Title": "History-Guided Conversational Recommendation", "Abstract": "Product recommendation is an important aspect of many\ne-commerce systems. It provides an effective way to help\nusers navigate complex product spaces. In this paper, we\nfocus on critiquing-based recommenders. We present a new\ncritiquing-based approach, History-Guided Recommendation\n(HGR), which is capable of using the recommendation pairs\n(item and critique) or critiques only so far in the current rec-\nommendation session to predict the most likely product rec-\nommendations and therefore short-cut the sometimes pro-\ntracted recommendation sessions in standard critiquing ap-\nproaches. The HGR approach shows a significant improve-\nment in the interaction between the user and the recom-\nmender. It also enables successfully accepted recommenda-\ntions to be made much earlier in the session."}
{"Title": "Strengthening Collaborative Data Analysis and Decision\nMaking in Web Communities", "Abstract": "Generally speaking, modern research becomes increasingly\ninterdisciplinary and collaborative in nature. Researchers need to\ncollaborate and make decisions by meaningfully assembling,\nmining and analyzing available large-scale volumes of complex\nmulti-faceted data residing in different sources. At the same time,\nthey need to efficiently and effectively exploit services available\nover the Web. Arguing that dealing with data-intensive and\ncognitively complex settings is not a technical problem alone, this\npaper presents a novel collaboration support platform for Web\ncommunities. The proposed solution adopts a hybrid approach\nthat builds on the synergy between machine and human\nintelligence to facilitate the underlying sense-making and decision\nmaking processes. User experience shows that the platform\nenables stakeholders to make better, more informed and quicker\ndecisions. The functionalities of the proposed platform are\ndescribed through a real-world case from a biomedical research\ncommunity."}
{"Title": "A Semantic Web of Know-How:\nLinked Data for Community-Centric Tasks", "Abstract": "This paper proposes a novel framework for representing com-\nmunity \u2018know-how\u2019 on the Semantic Web. Procedural knowl-\nedge generated by web communities typically takes the form\nof natural language instructions or videos and is largely un-\nstructured. The absence of semantic structure impedes the\ndeployment of many useful applications, in particular the\nability to discover and integrate know-how automatically.\nWe discuss the characteristics of community know-how and\nargue that existing knowledge representation frameworks fail\nto represent it adequately. We present a novel framework for\nrepresenting the semantic structure of community know-how\nand demonstrate the feasibility of our approach by provid-\ning a concrete implementation which includes a method for\nautomatically acquiring procedural knowledge for real-world\ntasks."}
{"Title": "How Placing Limitations on the Size of Personal Networks\nChanges the Structural Properties of Complex Networks", "Abstract": "People-to-people interactions in the real world and in virtual envi-\nronments (e.g., Facebook) can be represented through complex\nnetworks. Changes of the structural properties of these complex\nnetworks are caused through a variety of dynamic processes.\nWhile accepting the fact that variability in individual patterns of\nbehavior (i.e., establishment of random or FOAF-type potential\nlinks) in social environments might lead to an increase or decrease\nin the structural properties of a complex network, in this paper, we\nfocus on another factor that may contribute to such changes,\nnamely the size of personal networks. Any personal network\ncomes with the cost of maintaining individual connections. De-\nspite the fact that technology has shrunk our world, there is also a\nlimit to how many close friends one can keep and count on. It is a\nrelatively small number. In this paper, we develop a multi-agent\nbased model to capture, compare, and explain the structural\nchanges within a growing social network (e.g., expanding the\nsocial relations beyond one's social circles). We aim to show that,\nin addition to various dynamic processes of human interactions,\nlimitations on the size of personal networks can also lead to\nchanges in the structural properties of networks (i.e., the average\nshortest-path length). Our simulation result shows that the famous\nsmall world theory of interconnectivity holds true or even can be\nshrunk, if people manage to utilize all their existing connections\nto reach other parties. In addition to this, it can clearly be ob-\nserved that the network\u2019s average path length has a significantly\nsmaller value, if the size of personal networks is set to larger\nvalues in our network growth model. Therefore, limitations on the\nsize of personal networks in network growth models lead to an\nincrease in the network\u2019s average path length."}
{"Title": "The Design of a Live Social Observatory System", "Abstract": "With the emergence of social networks and their potential impact\non society, many research groups and originations are collecting\nhuge amount of social media data from various sites to serve\ndifferent applications. These systems offer insights on different\nfacets of society at different moments of time. Collectively they\nare known as social observatory systems. This paper describes the\narchitecture and implementation of a live social observatory\nsystem named \u2018NExT-Live\u2019. It aims to analyze the live online\nsocial media data streams to mine social senses, phenomena,\ninfluences and geographical trends dynamically. It incorporates\nan efficient and robust set of crawlers to continually crawl online\nsocial interactions on various social network sites. The data\ncrawled are stored and processed in a distributed Hadoop\narchitecture. It then performs the analysis on these social media\nstreams jointly to generate analytics at different levels. In\nparticular, it generates high-level analytics about the sense of\ndifferent target entitles, including People, Locations, Topics and\nOrganizations. NExT-Live offers a live observatory platform that\nenables people to know the happenings of the place in order to\nlead better life."}
{"Title": "Observing the Web by Understanding the Past:\nArchival Internet Research", "Abstract": "This paper discusses the challenges and opportunities for\nusing archival Internet data in order to observe a host of\nsocial science phenomena. Specifically, this paper\nintroduces HistoryTracker, a new tool for accessing and\nextracting archived data from the Internet Archive, the\nlargest repository of archived Web data in existence. The\nHistoryTracker tool serves to create a Web observatory that\nallows scholars to study the history of the Web.\nHistoryTracker takes advantages of Hadoop processing\ncapacity, and allows researchers to extract large swaths of\narchived data into a link list format that can be easily\ntransferred to a number of other analytical tools. A brief\nillustration of the use of HistoryTracker is presented\ndemonstrating the use of the tool. Finally, a number of\ncontinuing research challenges are discussed, and future\nresearch opportunities are outlined."}
{"Title": "Fluctuation and Burst Response in Social Media", "Abstract": "A salient dynamic property of social media is bursting be-\nhavior. In this paper, we study bursting behavior in re-\nlation to the structure of fluctuation, known as fluctuation-\nresponse relation, to reveal the origin of bursts. More specif-\nically, we study the temporal relation between a preceding\nbaseline fluctuation and the successive burst response us-\ning a frequency time series of 3,000 keywords on Twitter.\nWe find three types of keyword time series in terms of the\nfluctuation-response relation. For the first type of keyword,\nthe baseline fluctuation has a positive correlation with the\nburst size; as the preceding fluctuation increases, the burst\nsize increases. These bursts are caused endogenously as a\nresult of word-of-mouth interactions in a social network; the\nkeyword is sensitive only to the internal context of the sys-\ntem. For the second type, there is a critical threshold in\nthe fluctuation value up to which a positive correlation is\nobserved. Beyond this value, the size of the bursts becomes\nindependent from the fluctuation size. Our analysis shows\nthat this critical threshold emerges because the bursts in\nthe time series are endogenous and exogenous. This type\nof keyword is sensitive to internal and external stimuli. The\nthird type is mainly bursts caused by exogenous bursts. This\ntype of keyword is mostly sensitive only to external stimuli.\nThese results are useful for characterizing how excitable a\nkeyword is on Twitter and could be used, for example, for\nmarketing purposes."}
{"Title": "Humour Reactions in Crisis: A Proximal analysis of\nChinese posts on Sina Weibo in Reaction to the Salt Panic\nof March 2011", "Abstract": "This paper presents an analysis of humour use in Sina Weibo in\nreaction to the Chinese salt panic, which occurred as a result of\nthe Fukushima disaster in March 2011. Basing the investigation\non the humour Proximal Distancing Theory (PDT), and utilising a\ndataset from Sina Weibo in 2011, an examination of humour\nreactions is performed to identify the proximal spread of\nhumourous Weibo posts in relation to the consequent salt panic in\nChina. As a result of this method, we present a novel\nmethodology for understanding humour reactions in social media,\nand provide recommendations on how such a method could be\napplied to a variety of other social media, crises, cultural and\nspatial settings."}
{"Title": "Zooniverse: Observing the World\u2019s Largest Citizen\nScience Platform", "Abstract": "This paper introduces the Zooniverse citizen science project\nand software framework, outlining its structure from an ob-\nservatory perspective: both as an observable web-based sys-\ntem in itself, and as an example of a platform iteratively\ndeveloped according to real-world deployment and used at\nscale. We include details of the technical architecture of Zo-\noniverse, including the mechanisms for data gathering across\nthe Zooniverse operation, access, and analysis. We consider\nthe lessons that can be drawn from the experience of design-\ning and running Zooniverse, and how this might inform de-\nvelopment of other web observatories."}
{"Title": "Visualising Data in Web Observatories: A Proposal\nfor Visual Analytics Development & Evaluation", "Abstract": "Web Observatories use innovative analytic processes to gather\ninsights from observed data and use the Web as a platform for\npublishing interactive data visualisations. Recordable events\nassociated with interactivity on the Web provide an opportunity to\nopenly evaluate the utility of these artefacts, assessing fitness for\npurpose and observing their use. The three principles presented in\nthis paper propose a community evaluation approach to\ninnovation in visual analytics and visualisation for Web\nObservatories through code sharing, the capturing of semantically\nenriched interaction data and by openly stating the intended goals\nof all visualisation work. The potential of this approach is\nexampled with a set of front-end tools suitable for adoption by the\nmajority of Web Observatories as a means of visualising data on\nthe Web as part the shared, open, and community-driven\ndevelopmental process. The paper outlines the method for\ncapturing user interaction data as a series of semantic events,\nwhich can be used to identify improvements in both the structure\nand functionality of visualisations. Such refinements in user\nbehaviour are proposed as part of a new methodology that\nintroduces Economics as an evaluation tool for visual analytics."}
{"Title": "Legal and Ethical Considerations: Step 1b in Building a\nHealth Web Observatory", "Abstract": "This paper explores the impact of health information\ntechnologies, including the Web, on society and advocates for the\ndevelopment of a Health Web Observatory (HWO) to collect,\nstore and analyze new sources of health information. The paper\nbegins with a high-level literature review from across domains to\ndemonstrate the need for a multi-disciplinary pursuit when\nbuilding web observatories. For as researchers in the social\nsciences and legal domains have highlighted, data carries\nassumptions of power, identity, governance, etc., which should\nnot be overlooked. The paper then recommends example legal and\nethical questions to consider when building any health web\nobservatory. The goal is to insert social and regulatory concerns\nmuch earlier into the WO methodology."}
{"Title": "Towards a Taxonomy for Web Observatories", "Abstract": "In this paper, we propose an initial structure to support a\ntaxonomy for Web Observatories (WO). The work is based on a\nsmall sample of cases drawn from the work of the Web Science\nTrust and the Web Science Institute and reflects aspects of\nacademic, business and government Observatories. Whilst this is\nearly work it is hoped, by drawing broad brushstrokes at the edges\nof different types of Observatory, that future work based on a\nmore systematic review will refine this model and hence refine\nour understanding of the nature of Observatories. We also seek\nhere to enhance a faceted classification scheme (which is thought\nto be weak in the area of visualisation) through the use of\nsimplified concept maps."}
{"Title": "Addictive Links: Engaging Students through Adaptive\nNavigation Support and Open Social Student Modeling", "Abstract": "Empirical studies of adaptive annotation in the educational\ncontext have demonstrated that it can help students to acquire\nknowledge  faster,  improve  learning  outcomes,  reduce\nnavigational overhead, and encourage non-sequential navigation\n[1]. Over the last 8 years we have explored a lesser known effect\nof adaptive annotation \u2013 its ability to significantly increase\nstudent engagement in working with non-mandatory educational\ncontent. In the presence of adaptive link annotation, students tend\nto access significantly more learning content; they stay with it\nlonger, return to it more often and explore a wider variety of\nlearning resources. This talk will present an overview of our\nexploration of the addictive links effect in many course-long\nstudies, which we ran in several domains (C, SQL and Java\nprogramming), for several types of learning content (quizzes,\nproblems, interactive examples). The first part of the talk will\nreview our exploration of a more traditional knowledge-based\npersonalization approach [2; 3] and the second part will focus on\nmore recent studies of social navigation and open social student\nmodeling [4; 5]."}
{"Title": "Building Engagement for MOOC Students\nIntroducing Support for Time Management on Online Learning Platf", "Abstract": "The main objectives of massive open online courses (MOOC)\nare to foster knowledge through free high quality learning\nmaterials procurement; to create new knowledge through di-\nverse users\u2019 interactions with the providing platform; and to\nempower research on learning. However, MOOC providers\nare also businesses (either profit or not-for-profit). They\nare still in the early stages of their development, but sooner\nor later, in order to secure their existence and assure their\nlongterm growth, they will have to adapt a business model\nand monetize the services they provide. Nevertheless, de-\nspite their popularity MOOCs are characterized by a very\nhigh drop-out rate (about 90% [33, 22, 26]), which may\nturn out to be a problem regardless of the adapted busi-\nness model. Hence, MOOC providers can either assume the\nscale benefits to be sufficiently high to ignore the problem\nof low MOOC completion rate or tackle this problem.\nIn this paper we explore the problem of the high drop-\nout rate in massive open online courses. First, we identify\nits main cause by conducting an online survey, namely bad\ntime organization. Secondly, we provide suggestions to re-\nduce the rate. Specifically, we argue that MOOC platforms\nshould not only provide their users with high quality educa-\ntional materials and interaction facilities. But they should\nalso support and assist the users in their quest for knowl-\nedge. Thus, MOOC platforms should provide tools helping\nthem optimize their time usage and subsequently develop\nmetacognitive skills indispensable in proper time manage-\nment of learning processes."}
{"Title": "A Web-based Degree Program in Open Source Education\n\u2013  A Case Study", "Abstract": "In this paper, we describe the details of an interactive online\nweb-based degree program in the area of Computer Science with\nspecialization in Free/Open Source Software (FOSS) that has\nbeen successfully running for two years in a leading technological\nuniversity in India. The subjects taught as well as the tools and\nplatforms used in delivering the course are exclusively FOSS and\nput together by the university team, as described here. We also\ndescribe the details of the program, its goals and purpose, the\nmanner of its implementation, the learnings we have had and the\nchallenges being faced in going forward."}
{"Title": "Tutoring From the Desktop: Facilitating Learning Through\nGoogle+ Hangouts", "Abstract": "Many studies have demonstrated the effectiveness of tutor-\ning as a teaching strategy. Though much attention has re-\ncently been focussed on using the web to extend the reach of\nthe university classroom to high achieving students, compar-\natively less attention has been paid to the potential of the\nweb to bring personalized tutoring to at-risk students. In\nthis paper, we describe Tutoring From the Desktop, a pro-\ngram in which high school students in California use Google\nPlus Hangouts to tutor students in Kohlapur, India. We\nshow how a simple structured program can be used to over-\ncome the barriers of time-zones, accents and much more."}
{"Title": "Crowdcrawling Approach for Community Based\nPlagiarism Detection Service", "Abstract": "In the era of exponentially growing web and exploding online\neducation the problem of digital plagiarism has become one of the\nmost burning ones in many areas. Efficient internet plagiarism\ndetection tools should have a capacity similar to that of\nconventional web search engines. This requirement makes\ncommercial plagiarism detection services expensive and therefore\nless accessible to smaller education institutions. This work-in-\nprogress paper proposes the concept of crowdcrawling as a tool to\ndistribute the most laborious part of the web search among\ncommunity servers thus providing scalability and sustainability to\nthe community driven plagiarism detection. It outlines roles for\ncommunity members depending on the resources they are willing\nto contribute to the service."}
{"Title": "Language Technologies for Enhancement of\nTeaching and Learning in Writing", "Abstract": "Writing is a vital issue for education as well as a fundamen-\ntal skill in teaching and learning. With the development of\ninformation technologies, more and more professional writ-\ning tools emerge. As each of them mostly concentrates on\naddressing a specific issue, people need a one-stop platform,\nwhich could integrate multiply functions. In addition, with\nthe supported concept of e-learning ecosystem for future ed-\nucation, a comprehensive platform will be more promising.\nTherefore, we introduce VeriGuide Platform, which provides\na professional writing toolbox to promote the enhancement\nof teaching and learning in writing. It contains six vertical\ncomponents, which could be split into two groups. The first\ngroup, Editing Assistance, facilitates students write papers\nand point out grammar and spelling errors. While the sec-\nond group, Text Analysis, offers document analysis results,\nwhich enables students to achieve further writing improve-\nment with explanatory feedbacks. Furthermore, we could do\neducation data analytics to enhance the efficiency of teach-\ning and learning. Specifically, the Editing Assistance con-\ntains well-organized writing and formatting, and grammar\nand spelling checking, while readability assessment, similar-\nity detection, citation analysis, and sentiment analysis are\nincluded in the Text Analysis."}
{"Title": "Collective Copyright", "Abstract": "The way people create and share content has radically\nchanged in the past few years thanks to the advent of social\nnetworks, web communities, blogs, wikis, and other online\ncollaborative media. Such online social data are continu-\nously growing in a way that makes it difficult to efficiently\naggregate them, since they are the expression of a multitude\nof single content creators that most of the times show only\na small percentage of originality. The act of \u2018sharing\u2019 is still\ntied to a pre-Internet fashion that sees it as a step following\n(and never preceding) content creation, as enforced by the\nrules of publishing and copyright. In the Internet era, the\npieces of the puzzle of a valuable work might be scattered\nthroughout the whole Web. In order to hinder the obsolete\ncreate-then-share trend that is killing creativity and useful-\nness of the Web, we propose a new copyright framework,\nwhich allows content to be shared while being created, in a\nway that this can gain increasing value as it becomes part\nof an increasingly richer puzzle."}
{"Title": "Identifying Fraudulently Promoted Online Videos", "Abstract": "Fraudulent product promotion online, including online videos, is\non the rise. In order to understand and defend against this ill, we\nengage in the fraudulent video economy for a popular video sharing\nwebsite, YouTube, and collect a sample of over 3,300 fraudulently\npromoted videos and 500 bot profiles that promote them. We then\ncharacterizefraudulentvideosandprofilesandtrainsupervisedma-\nchine learning classifiers that can successfully differentiate fraudu-\nlent videos and profiles from legitimate ones."}
{"Title": "Incredible: Is (Almost) All Web Content Trustworthy?\nAnalysis of Psychological Factors Related to Website\nCredibility Evaluation", "Abstract": "This paper describes the results of a study conducted in\nFebruary 2013 on Amazon Mechanical Turk aimed at\nidentifying various determinants of credibility evaluations.\n2046 adult participants evaluated credibility of websites with\ndiversified trustworthiness reference index. We concentrated\non psychological factors that lead to the characteristic positive\nbias observed in many working social feedback systems on the\nInternet. We have used International Personality Item Pool\n(IPIP) and measured the following traits: trust, conformity,\nrisk taking, need for cognition and intellect. Results suggest\nthat trustworthiness and risk taking are factors clearly\ndifferentiating people with respect to tendency to overestimate,\nunderestimate and judge accordingly websites\u2019 credibility.\nIntuitively people characterized by high general trust tend to\nbe more generous in their credibility evaluations. On the other\nhand, people who are more willing to take risk, tend to be\nmore critical of the Internet content. The latter indicates that\nhigh credibility evaluations are being treated as a default\noption, and lower ratings require special conditions. Other,\nmore detailed psychological patterns related to websites\u2019\ncredibility evaluations are described in full paper."}
{"Title": "Quality Evaluation of Social Tags\naccording to Web Resource Types", "Abstract": "In the social tagging system, users annotate different web\nresources according to their need of future information\norganization and retrieval, and users also annotate resources with\ndifferent types of tags, such as objective tag, subjective tag, self-\norganized tag and so on. Because every web resource has its own\ncharacteristics, the tag types of each web resource are different.\nAccording to the web resource, the quality of each tag type is\ndifferent. We should depend on resource types to evaluate the\nquality of tag types, in order to provide efficient tag\nrecommendation service and design better user tagging interfaces.\nIn this paper, we firstly selected five web resources, namely the\nblog, book, image, music and video, to explore the tag types when\nannotating different resources. Then we chose specific resource\nand tags to explore the quality of each tag type according to these\nfive web resources and study the relationship between tag type\nand quality. The conclusion is that the quality of tag types for\ndifferent web resources is different."}
{"Title": "Learning Conflict Resolution Strategies for\nCross-Language Wikipedia Data Fusion", "Abstract": "In order to efficiently use the ever growing amounts of struc-\ntured data on the web, methods and tools for quality-aware\ndata integration should be devised. In this paper we propose\nan approach to automatically learn the conflict resolution\nstrategies, which is a crucial step in large-scale data integra-\ntion. The approach is implemented as an extension of the\nSieve data quality assessment and fusion framework. We ap-\nply and evaluate our approach on the use case of fusing data\nfrom 10 language editions of DBpedia, a large-scale struc-\ntured knowledge base extracted from Wikipedia. We also\npropose a method for extracting rich provenance metadata\nfor each DBpedia fact, which is later used in data fusion."}
{"Title": "Predicting Webpage Credibility using Linguistic Features", "Abstract": "The article focuses on predicting trustworthiness from tex-\ntual content of webpages. The recent work Olteanu et al.\nproposes a number of features (linguistic and social) to ap-\nply machine learning methods to recognize trust levels. We\ndemonstrate that this approach can be substantially im-\nproved in two ways: by applying machine learning methods\nto vectors computed using psychosocial and psycholinguistic\nfeatures and in a high-dimensional bag-of-words paradigm of\nword occurrences. Following [13], we test the methods in two\nclassification settings, as a 2-class and 3-class scenario, and\nin a regression setting. In the 3-class scenario, the features\ncompiled by [13] achieve weighted precision of 0.63, while the\nmethods proposed in our paper raise it to 0.66 and 0.70. We\nalso examine coefficients of the models in order to discover\nwords associated with low and high trust."}
{"Title": "Active Learning with Partially Featured Data", "Abstract": "In this paper, we propose a new active learning algorithm\nin which the learner chooses the samples to be queried from\nthe unlabeled data points whose attributes are only par-\ntially observed. In addition, we propose a cost-driven de-\ncision framework where the learner chooses to query either\nthe labels or the missing attributes. This problem state-\nment addresses a common constraint when building large\ndatasets and applying active learning techniques on them,\nwhere some of the attributes (including the labels) are sig-\nnificantly harder or more costly to acquire per data point.\nWe take a novel approach to this problem, first by building\nan imputation model that maps from the partially featured\ndata to the fully featured dimension, and then performing\nactive learning on the projected input space combined with\nthe estimated confidence of inference. We discuss that our\napproach is flexible and can work with graph mining tasks\nas well as conventional semi-supervised learning problems.\nThe results suggest that the proposed algorithm facilitates\nmore cost-efficient annotation than the baselines."}
{"Title": "From Graphs to Tables the Design of Scalable Systems for\nGraph Analytics", "Abstract": "From social networks to language modeling, the growing\nscale and importance of graph data has driven the develop-\nment of numerous new graph-parallel systems (e.g., Giraph\nand GraphLab). By restricting the types of computation\nthat can be expressed and by introducing new techniques to\npartition and distribute graphs, these systems can efficiently\nexecute sophisticated graph algorithms orders of magnitude\nfaster than more general data-parallel systems.\nHowever, the same restrictions that enable graph-parallel\nsystems to achieve substantial performance gains also limit\ntheir ability to express many of the important stages in a\ntypical graph-analytics pipeline. Moreover, while graph-\nparallel systems are optimized for iterative diffusion algo-\nrithms like PageRank they are not well suited for more basic\ntasks like constructing the graph, modifying its structure, or\nexpressing computation that spans multiple graphs. While\nexisting systems address specific stages of a typical graph-\nanalytics pipeline, they do not address the entire pipeline,\nforcing the user to deal with multiple systems, complex and\nbrittle file interfaces, and inefficient data-movement and du-\nplication.\nTo fill the need for a holistic approach to graph-analytics\nwe introduce GraphX, which unifies graph-parallel and data-\nparallel computation under a single API and system. GraphX\nrecasts advances in graph-processing in the context of re-\nlational algebra and distributed join optimization enabling\nmore general data-parallel systems to process graphs effi-\nciently. We evaluate the GraphX system on several real-\nworld tasks and show that its end-to-end performance can\nexceed that of specialized systems.\nThis talk describes recent work at the UC Berkeley AM-\nPLab in collaboration with Reynold S. Xin, Ankur Dave,\nDaniel Crankshaw, Michael J. Franklin, and Ion Stoica."}
{"Title": "A Fast Approximation for Influence Maximization in Large\nSocial Networks", "Abstract": "This paper deals with a novel research work about a new\nefficient approximation algorithm for influence maximiza-\ntion, which was introduced to maximize the benefit of viral\nmarketing. For efficiency, we devise two ways of exploiting\nthe 2-hop influence spread which is the influence spread on\nnodes within 2-hops away from nodes in a seed set. Firstly,\nwe propose a new greedy method for the influence maximiza-\ntion problem using the 2-hop influence spread. Secondly, to\nspeed up the new greedy method, we devise an effective way\nof removing unnecessary nodes for influence maximization\nbased on optimal seed\u2019s local influence heuristics. In our ex-\nperiments, we evaluate our method with real-life datasets,\nand compare it with recent existing methods. From exper-\nimental results, the proposed method is at least an order\nof magnitude faster than the existing methods in all cases\nwhile achieving similar accuracy."}
{"Title": "Detecting Community Structure for Undirected Big Graphs\nBased on Random Walks", "Abstract": "Community detection is a common problem in various types of\nbig graphs. It is meaningful to understand the functions and\ndynamics of networks. The challenges of detecting community\nfor big graphs include high computational cost, no prior\ninformation, etc.. In this work, we analyze the process of random\nwalking in graphs, and find out that the weight of an edge gotten\nby processing the vertices visited by the walker could be an\nindicator to measure the closeness of vertex connection. Based\non this idea, we propose a community detection algorithm for\nundirected big graphs which consists of three steps, including\nrandom walking using a single walker, weight calculating for\nedges and community detecting. Our algorithm is running in\nO(n 2 ) without prior information. Experimental results show that\nour algorithm is capable of detecting the community structure and\nthe overlapping parts of graphs in real-world effectively, and\nhandling the challenges of community detection in big graph era."}
{"Title": "Processing Scientific Mesh Queries in Graph Databases", "Abstract": "In this work-in-progress paper, we model scientific meshes\nas a multi-graph in Neo4j graph database using the graph\nproperty model. We conduct experiments to measure the\nperformance of the graph database solution in processing\nmesh queries and compare it with GrAL mesh library and\nPostgreSQL database on synthetic and real mesh datasets.\nThe experiments show that the databases outperform the\nmesh library. However, each of the databases perform bet-\nter on specific query type, i.e, the graph database shows the\nbest performance on global path-intensive queries and the\nrelational database on local and field queries. Based on the\nexperiments, we propose a mediator architecture for pro-\ncessing mesh queries by using the three database systems."}
{"Title": "A Visual Workflow to Explore the Web of Data for Scholars", "Abstract": "As the Web evolves in an integrated and interlinked knowl-\nedge space thanks to the growing amount of published Linked\nOpen Data, the need to find solutions that enable the schol-\nars to discover, explore and analyse the underlying research\ndata emerges. Scholars, typically non-expert technology\nusers, lack of in-depth understanding of the underlying se-\nmantic technology which limits their ability to interpret and\nquery the data. We present a visual workflow to connect\nscholars and scientific resources on the Web of Data. We\nallow scholars to move from exploratory analysis in aca-\ndemic social networks to exposing relations between these\nresources. We allow them to reveal experts in a particu-\nlar field and discover relations in and beyond their research\ncommunities. This paper aims to evaluate the potential of\nsuch a visual workflow to be used by non-expert users to\ninteract with the semantically enriched data and familiarize\nwith the underlying dataset."}
{"Title": "Modeling Collaboration in Academia:\nA Game Theoretic Approach", "Abstract": "In this work, we aim to understand the mechanisms driving aca-\ndemic collaboration. We begin by building a model for how re-\nsearchers split their effort between multiple papers, and how col-\nlaboration affects the number of citations a paper receives, sup-\nported by observations from a large real-world publication and ci-\ntation dataset, which we call the h-Reinvestment model. Using tools\nfrom the field of Game Theory, we study researchers\u2019 collaborative\nbehavior over time under this model, with the premise that each re-\nsearcher wants to maximize his or her academic success. We find\nanalyticallythatthereisastrongincentivetocollaborateratherthan\nwork in isolation, and that studying collaborative behavior through\na game-theoretic lens is a promising approach to help us better un-\nderstand the nature and dynamics of academic collaboration."}
{"Title": "Can Web Presence Predict Academic Performance? \u2013 The\nCase of E\u00f6tv\u00f6s University", "Abstract": "This paper reports the preliminary results of a project that\naims at incorporating the analysis of the web presence (con-\ntent) of research institutions into the scientometric analy-\nsis of these research institutions. The problem is to under-\nstand and predict the dynamics of academic activity and\nresource allocation using web presence. The present paper\napproaches this problem in two parts. First we develop a\ncrawler and an archive of the web contents obtained from\nacademic institutions, and present an early analysis of the\nrecords. Second, we use (currently off-line records to ana-\nlyze the dynamics of resource allocation. Combination of\nthe two parts is an ambition of ongoing work.\nThe motivation in this study is twofold. First, we strongly\nbelieve that independent archiving, indexing and searching\nof (past) web content is an important task, even with regards\nto academic web presence. We are particularly interested in\nstudying the dynamics of the \u201donline scientific discourse\u201d,\nbased on the assumption that the changing traces of web\npresence is an important factor that documents the intensity\nof activity. Second, we maintain that the trend-analysis of\nscientific activity represents a hitherto unused potential. We\nillustrate this by a pilot where, using \u2019offline\u2019 longitudinal\ndatasets, we study whether past (i.e. cumulative) success\ncan predict current (and future) activity in academia. Or,\nin short: do institutions invest and publish in areas where\nthey have been successful? Answer to this question is, we\nbelieve, important to understanding and predicting research\npolicies and their changes."}
{"Title": "Trust and Hybrid Reasoning for Ontological Knowledge\nBases", "Abstract": "Projects such as Libra and Cimple have built systems to capture\nknowledge in a research community and to respond to semantic\nqueries. However, they lack the support for a knowledge base that\ncan evolve over time while responding to queries requiring\nreasoning. We consider a semantic web that covers linked data\nabout science research that are being harvested from the Web and\nare supplemented and edited by community members. We use\nontologies to incorporate semantics to detect conflicts and resolve\ninconsistencies, and to infer new relations or proof statements\nwith a reasoning engine. We consider a semantic web subject to\nchanges in the knowledge base, the underlying ontology or the\nrule set that governs the reasoning. In this paper we explore the\nidea of trust where each change to the knowledge base is analyzed\nas to what subset of the knowledge base can still be trusted. We\npresent algorithms that adapt the reasoner such that, when proving\na goal, it does a simple retrieval when it encounters trusted items\nand backward chaining over untrusted items. We provide an\nevaluation of our proposed modifications that show that our\nalgorithm is conservative and that it provides significant gains in\nperformance for certain queries."}
{"Title": "Query Complex Graph Patterns: Tools and Applications", "Abstract": "In his world-widely renowned book [6], Nobel laureate Herbert Si-\nmon pointed out that it is more the complexity of the environment,\nthan the complexity of the individual persons, that determines the\ncomplex behavior of humans. The emergence of online social net-\nwork sites and web 2.0 applications provides a new connected envi-\nronment/context, where people generate, share and search massive\nhuman knowledge; and interact and collaborate with each other to\ncollectively perform some complex tasks.\nIn this talk, we focus on how to make sense of the collaboration\ndata in the context of graphs/networks. To be specific, we will in-\ntroduce a suite of tools for querying complex patterns from such\ngraphs. Exemplar questions we aim to answer include (a) what\nmakes a team more successful than others, and how to find the best\nreplacement if one of its team members becomes unavailable? [2,\n4]; (b) how to find a group of authors from databases, data mining\nand bioinformatics and they collaborate with each other in a star-\nshape? [8, 5]; (c) given a set of querying authors of interest, how\nto find somebody who initiates the research field these querying\nauthors belong to, and how to summarize and visualize the query-\ning authors? [3, 7, 1]; (d) how to incorporate users\u2019 preference into\nthese complex queries [9, 10]. We will also introduce the compu-\ntational challenges behind these querying tools and how to remedy\nthem."}
{"Title": "An Article Level Metric in the Context of Research\nCommunity", "Abstract": "With the rapid increase of research papers, article-level metrics are\nof growing importance for helping researchers select papers. Clas-\nsical metrics have a significant drawback of just using single factor,\nwhich limits the effectiveness of assessing papers in different peri-\nods after publication. Moreover, with the development of web 2.0,\nsome new factors are introduced to assess papers. So, a novel arti-\nclelevel metricinthecontext of research community (ALM_RC) is\nproposed. It integrates the impact of different factors comprehen-\nsively, because different factors have different time features and\ncan complement each other in different periods after publication.\nIn addition, as a research community is based on certain research\ndirections, it is a relatively stable environment with related jour-\nnals and scholars contributing their efforts to development of this\nresearch field. So in the context of research community, it is con-\nsistent, practical and reasonable to calculate the impact of the jour-\nnals and scholars under relatively fair criteria. Experimental results\nshow the novel metric is effective and robust in assessing papers."}
{"Title": "Aligning Web Collaboration Tools\nwith Research Data for Scholars", "Abstract": "Resources for research are not always easy to explore, and\nrarely come with strong support for identifying, linking and\nselecting those that can be of interest to scholars. In this\nwork we introduce a model that uses state-of-the-art seman-\ntic technologies to interlink structured research data and\ndata from Web collaboration tools, social media and Linked\nOpen Data. We use this model to build a platform that\nconnects scholars, using their profiles as a starting point to\nexplore novel and relevant content for their research. Schol-\nars can easily adapt to evolving trends by synchronizing new\nsocial media accounts or collaboration tools and integrate\nthen with new datasets. We evaluate our approach by a\nscenario of personalized exploration of research repositories\nwhere we analyze real world scholar profiles and compare\nthem to a reference profile."}
{"Title": "ACRec: A Co-authorship based Random Walk Model for\nAcademic Collaboration Recommendation", "Abstract": "Recent academic procedures have depicted that work involv-\ning scientific research tends to be more prolific through col-\nlaboration and cooperation among researchers and research\ngroups. On the other hand, discovering new collaborators\nwho are smart enough to conduct joint-research work is ac-\ncompanied with both difficulties and opportunities. One\nnotable difficulty as well as opportunity is the big schol-\narly data. In this paper, we satisfy the demand of collab-\noration recommendation through co-authorship in an aca-\ndemic network. We propose a random walk model using\nthree academic metrics as basics for recommending new col-\nlaborations. Each metric is studied through mutual paper\nco-authoring information and serves to compute the link im-\nportance such that a random walker is more likely to visit\nthe valuable nodes. Our experiments on DBLP dataset show\nthat our approach can improve the precision, recall rate\nand coverage rate of recommendation, compared with other\nstate-of-the-art approaches."}
{"Title": "Relatedness Measures between Conferences in Computer\nScience \u2013 a Preliminary Study Based on DBLP", "Abstract": "A large percentage of the research in computer science is\npublished in conferences and workshops. We propose three\nmethods which compute a\u201crelatedness score\u201dfor conferences\nrelative to a pivot conference, usually a top rated confer-\nence. We experiment with the DBLP bibliography to show\nthat our relatedness ranking can be used to help understand\nthe basis of conference reputation ratings, determine what\nconferences are related to an area and the classification of\nconferences into areas."}
{"Title": "Indicators and Functionalities of Exploitation of Academic\nStaff CV using Semantic Web Technologies", "Abstract": "We have transformed five years of curriculum data of our\nacademic staff from relational databases to a semantic model.\nThanks to semantic queries, capabilities of NoSQL models,\ninference reasoners and data mining techniques we obtain\nknowledge that it improves the personal management of cur-\nriculum data, the quality and efficiency of exploitation tasks,\nand the transparency, dissemination and collaboration with\ncitizens. The huge catalogue of CV data remains an under-\nutilized resource. Private companies such as editorials have\nrobust services based only on publications but academic in-\nstitutions have the option of integrating other databases re-\nlated with their staff to obtain more indicators. We analyse\nthe transformation of data, highlighting the mapping pro-\ncess of authors, and we present two ways of exploitation\nusing semantic queries and complex networks. Thus, in-\nstitutions, researchers and citizens will have a quality data\ncatalogue for diverse studies."}
{"Title": "People Like Us: Mining Scholarly Data for Comparable\nResearchers", "Abstract": "We present the problem of finding comparable researchers for any\ngiven researcher. This problem has many motivations. Firstly,\nknow thyself. The answers of where we stand among research\ncommunity and who we are most alike may not be easily found\nby existing evaluations of ones\u2019 research mainly based on citation\ncounts. Secondly, there are many situations where one needs to\nfind comparable researchers e.g., for reviewing peers, constructing\nprogramming committees or compiling teams for grants. It is often\ndone through an ad hoc and informal basis.\nUtilizing the large scale scholarly data accessible on the web,\nwe address the problem of automatically finding comparable re-\nsearchers. We propose a standard to quantify the quality of re-\nsearch output, via the quality of publishing venues. We represent\na researcher as a sequence of her publication records, and develop\na framework of comparison of researchers by sequence matching.\nSeveral variations of comparisons are considered including match-\ning by quality of publication venue and research topics, and per-\nforming prefix matching. We evaluate our methods on a large cor-\npus and demonstrate the effectiveness of our methods through ex-\namples. In the end, we identify several promising directions for\nfurther work."}
{"Title": "Road Traffic Prediction by Incorporating Online\nInformation", "Abstract": "Road traffic conditions are typically affected by events such\nas extreme weather or sport games. With the advance of\nWeb, events and weather conditions can be readily retrieved\nin real-time. In this paper, we propose a traffic condition\nprediction system incorporating both online and offline in-\nformation. RFID-based system has been deployed for mon-\nitoring road traffic. By incorporating data from both road\ntraffic monitoring system and online information, we pro-\npose a hierarchical Bayesian network to predict road traffic\ncondition. Using historical data, we establish a hierarchical\nBayesian network to characterize the relationships among\nevents and road traffic conditions. To evaluate the model,\nwe use the traffic data collected in Western Massachusetts\nas well as online information about events and weather. Our\nproposed prediction achieves an accuracy of 93% overall."}
{"Title": "Can Social Media Help Us Reason about Mental Health?", "Abstract": "Millions of people each year suffer from depression, which makes\nmental illness one of the most serious and widespread health\nchallenges in our society today. There is therefore a need for\neffective policies, interventions, and prevention strategies that\nenable early detection and diagnosis of mental health concerns in\npopulations. This talk reports some findings on the potential of\nleveraging social media postings as a new type of lens in\nunderstanding mental illness in individuals and populations.\nInformation gleaned from social media bears potential to\ncomplement traditional survey techniques in its ability to provide\nfiner grained measurements of behavior over time while radically\nexpanding population sample sizes. The talk highlights how this\nresearch direction may be useful in developing tools for identifying\nthe onset of depressive disorders, for use by healthcare agencies; or\non behalf of individuals, enabling those suffering from mental\nillness to be more proactive about their mental health."}
{"Title": "Do You Know the Speaker? An Online Experiment with\nAuthority Messages on Event Websites", "Abstract": "With the widespread adoption of the Web, many compa-\nnies and organizations have established websites that pro-\nvide information and support online transactions (e.g., buy-\ning products or viewing content). Unfortunately, users have\nlimited attention to spare for interacting with online sites.\nHence, it is of utmost importance to design sites that attract\nuser attention and effectively guide users to the product or\ncontent items they like. Thus, we propose a novel and scal-\nable experimentation approach to evaluate the effectiveness\nof online site designs. Our case study focuses on the effects\nof an authority message on visitors\u2019 browsing behavior on\nworkshop and seminar online announcement sites. An au-\nthority message emphasizes a particular prominent speaker\nand his/her achievements. Through dividing users into con-\ntrol and treatment groups and carefully tracking their online\nactivities, we observe that the authority message influences\nthe way users interact with page elements on the website\nand increases their interests in the authority speakers."}
{"Title": "A Behavior Observation Tool (BOT)\nfor Mobile Device Network Connection Logs", "Abstract": "With the advances of sensory, satellite and mobile commu-\nnication technologies in recent decades, locational data be-\ncome widely available. A lot of work has been developed\nto find useful information from these data, and various ap-\nproaches have been proposed. In this work, we aim to use\none specific type of locational data \u2014 network connection\nlogs of mobile devices, which is widely available and easily\naccessible to telecom companies, to identify and extract ac-\ntive areas of users. This is a challenging topic due to the\nexistence of inaccurate location and fluctuating log time in-\ntervals of this kind of data. In order to observe user be-\nhavior from this kind of data set, we propose a new al-\ngorithm, namely Behavior Observation Tool (BOT), which\nuses Convex Hull Algorithm with sliding time windows to\nmodel the user\u2019s movement, and thus knowledge about the\nuser\u2019s lifestyle and habits can extracted from the mobile de-\nvice network logs."}
{"Title": "Inferring Offline Hierarchical Ties from Online Social\nNetworks", "Abstract": "Social networks can represent many different types of rela-\ntionships between actors, some explicit and some implicit.\nFor example, email communications between users may be\nrepresented explicitly in a network, while managerial rela-\ntionships may not. In this paper we focus on analyzing ex-\nplicit interactions among actors in order to detect hierarchi-\ncal social relationships that may be implicit. We start by\nemploying three well-known ranking-based methods, PageR-\nank, Degree Centrality, and Rooted-PageRank (RPR) to in-\nfer such implicit relationships from interactions between ac-\ntors. Then we propose two novel approaches which take into\naccount the time-dimension of interactions in the process of\ndetecting hierarchical ties. We experiment on two datasets,\nthe Enron email dataset to infer manager-subordinate re-\nlationships from email exchanges, and a scientific publica-\ntion co-authorship dataset to detect PhD advisor-advisee\nrelationships from paper co-authorships. Our experiments\nshow that time-based methods perform considerably better\nthan ranking-based methods. In the Enron dataset, they\ndetect 48% of manager-subordinate ties versus 32% found\nby Rooted-PageRank. Similarly, in co-author dataset, they\ndetect 62% of advisor-advisee ties compared to only 39% by\nRooted-PageRank."}
{"Title": "Connecting Dream Networks Across Cultures", "Abstract": "Many species dream, yet there remain many open research\nquestions in the study of dreams. The symbolism of dreams\nand their interpretation is present in cultures throughout\nhistory. Analysis of online data sources for dream interpreta-\ntion using network science leads to understanding symbolism\nin dreams and their associated meaning. In this study, we\nintroduce dream interpretation networks for English, Chi-\nnese and Arabic that represent different cultures from var-\nious parts of the world. We analyze communities in these\nnetworks, finding that symbols within a community are se-\nmantically related. The central nodes in communities give\ninsight about cultures and symbols in dreams. The commu-\nnity structure of different networks highlights cultural sim-\nilarities and differences. Interconnections between different\nnetworks are also identified by translating symbols from dif-\nferent languages into English. Structural correlations across\nnetworks point out relationships between cultures. Simi-\nlarities between network communities are also investigated\nby analysis of sentiment in symbol interpretations. We find\nthat interpretations within a community tend to have similar\nsentiment. Furthermore, we cluster communities based on\ntheir sentiment, yielding three main categories of positive,\nnegative, and neutral dream symbols."}
{"Title": "Evaluation of Information Extraction Techniques to Label\nExtracted Data from e-Commerce Web Pages", "Abstract": "Automatically determining and assigning shared and mean-\ningful text labels to data extracted from an e-Commerce web\npage is a challenging problem. An e-Commerce web page\ncan display a list of data records, each of which can contain\na combination of data items (e.g. product name and price)\nand explicit labels, which describe some of these data items.\nRecent advances in extraction techniques have made it\nmuch easier to precisely extract individual data items and\nlabels from a web page, however, there are two open prob-\nlems: 1. assigning an explicit label to a data item, and\n2. determining labels for the remaining data items. Fur-\nthermore, improvements in the availability and coverage of\nvocabularies, especially in the context of e-Commerce web\nsites, means that we now have access to a bank of relevant,\nmeaningful and shared labels which can be assigned to ex-\ntracted data items.\nHowever, there is a need for a technique which will take\nas input a set of extracted data items and assign automati-\ncally to them the most relevant and meaningful labels from\na shared vocabulary. We observe that the Information Ex-\ntraction (IE) community has developed a great number of\ntechniques which solve problems similar to our own. In this\nwork-in-progress paper we propose our intention to theoret-\nically and experimentally evaluate different IE techniques to\nascertain which is most suitable to solve this problem."}
{"Title": "Iterative Algorithm for Inferring Entity Types from\nEnumerative Descriptions", "Abstract": "Entity type matching has many real world applications, especially\nin entity clustering, de-duplication and efficient query processing.\nCurrent methods to extract entities from text usually disregard\nregularities in the order of entities appearing in the text. In this\npaper, we focus on enumerative descriptions which enlist entity\nnames in a certain hidden order, often occurring in web documents\nas listings and tables. We propose an algorithm to discover entity\ntypes from enumerative descriptions, where a type hierarchy is\nknown but enumerating orders are hidden and heterogeneous, and\npartial entity-type mappings are given as seed instances. Our\nalgorithm is iterative: We extract skeletons from syntactic patterns,\nthen train a hidden Markov model to find an optimum enumerating\norder from seed instances and skeletons, to find a best-fit entity-\ntype assignment."}
{"Title": "Query Interfaces Understanding by Statistical Parsing", "Abstract": "Users submit queries to an online database via its query interface.\nQuery interface parsing, which is important for many applications,\nunderstands the query capabilities of a query interface. Since most\nquery interfaces are organized hierarchically, we present a novel\nquery interface parsing method, StatParser (Statistical Parser), to\nautomatically extract the hierarchical query capabilities of query\ninterfaces. StatParser automatically learns from a set of parsed\nquery interfaces and parses new query interfaces. StatParser starts\nfrom a small grammar and enhances the grammar with a set of\nprobabilities learned from parsed query interfaces under the\nmaximum-entropy principle. Given a new query interface, the\nprobability-enhanced-grammar identifies the parse tree with the\nlargest global probability to be the query capabilities of the query\ninterface. Experimental results show that StatParser very\naccurately extracts the query capabilities and can effectively\novercome the problems of existing query interface parsers."}
{"Title": "Extraction and Integration of web sources with Humans\nand Domain Knowledge", "Abstract": "The extraction and integration of data from many web sources\nin different domains is an open issue. Two promising solu-\ntions take on this challenge: top down approaches rely on\na domain knowledge that is manually crafted by an expert\nto guide the process and bottom up approaches try to infer\nthe schema from many web sources to make sense of the\nextracted data. The first solutions scale over the number\nof web sources, but for settings with different domains, an\nexpert has to manually craft an ontology for each domain.\nThe second solutions do not require a domain expert, but\nhigh quality is achieved only with a lot of human interactions\nboth in the extraction and integration steps.\nWe introduce a framework that takes the best from both\napproaches. The framework addresses synergically both ex-\ntraction and integration of data from web sources. No do-\nmain expert is required, it exploits data from a seed knowl-\nedge base to enhance the automatic extraction and integra-\ntion (top down). Human workers from crowdsourcing plat-\nforms are engaged to improve the quality and the coverage\nof the extracted data. The framework adopts techniques to\nautomatically extract both the schema and the data from\nmultiple web sources (bottom up). The extracted informa-\ntion is then used to bootstrap the seed knowledge base, re-\nducing in this way the human effort for future tasks."}
{"Title": "Integrating Product Data from Websites Offering Microdata\nMarkup", "Abstract": "Large numbers of websites have started to markup their con-\ntent using standards such as Microdata, Microformats, and\nRDFa. The marked-up content elements comprise descrip-\ntions of people, organizations, places, events, products, rat-\nings, and reviews. This development has accelerated in last\nyears as major search engines such as Google, Bing and Ya-\nhoo! use the markup to improve their search results. Em-\nbedding semantic markup facilitates identifying content el-\nements on webpages. However, the markup is mostly not\nas fine-grained as desirable for applications that aim to in-\ntegrate data from large numbers of websites. This paper\ndiscusses the challenges that arise in the task of integrating\ndescriptions of electronic products from several thousand e-\nshops that offer Microdata markup. We present a solution\nfor each step of the data integration process including Mi-\ncrodata extraction, product classification, product feature\nextraction, identity resolution, and data fusion. We evalu-\nate our processing pipeline using 1.9 million product offers\nfrom 9240 e-shops which we extracted from the Common\nCrawl 2012, a large public Web corpus."}
{"Title": "Entity Linking with a Unified Semantic Representation", "Abstract": "Entity Linking (EL) consists in linking mentions in a docu-\nment to their referent entities in a Knowledge Base. Current\napproaches fall into two main categories: local approaches, in\nwhich mentions are linked independently of each other, and\nglobal approaches, in which all mentions are linked collec-\ntively. Local approaches often ignore the semantic related-\nness of entities, and while global approaches incorporate the\nsemantic relatedness, they tend to focus only on directly con-\nnected entities, ignoring indirect connections which might\nbe useful. We present a global EL approach that unifies the\nrepresentation of the semantics of entities and documents\u2013\nthe probability distribution of entities being visited during\na random walk on an entity graph\u2013that accounts for direct\nand indirect connections. An experimental evaluation shows\nthat our method outperforms five state-of-the-art EL sys-\ntems and two very strong baselines."}
{"Title": "Online Analysis of Information Diffusion in Twitter", "Abstract": "The advent of social media has facilitated the study of information\ndiffusion, user interaction and user influence over social networks.\nThe research on analyzing information spreading focuses mostly\non modeling, while analyses of real-life data have been limited\nto small, carefully cleaned datasets that are analyzed in an offline\nfashion. In this paper, we present an approach for online analysis of\ninformation diffusion in Twitter. We reconstruct so-called informa-\ntion cascades that model how information is being propagated from\nuser to user from the stream of messages and the social graph. The\nresults show that such an inference is feasible even on noisy, large-\nscale, rapidly produced data. We provide insights into the impact\nof incomplete data and the effect of different influence models on\nthe cascades. The observed cascades show a significant amount of\nvariety in scale and structure."}
{"Title": "The Multi agent based Information Diffusion Model for\nFalse RumorDiffusion Analysis", "Abstract": "Twitter is a famous social networking service and has re-\nceived attention recently. Twitter user have increased rapidly,\nand many users exchange information. When the East Japan\ngreat earthquake disaster occurred on March 11, 2011, many\npeople could obtain important information from social net-\nworking service. Although twitter also played the important\nrole a false rumor diffusion was pointed out. So, in this talk,\nI would like to focus on the false rumor diffusion phenomena,\nand introduce about our multi agent information diffusion\nmodel based on SIR model. And I would like to discuss\nabout more rapid correction-tweet diffusion methodology."}
{"Title": "Towards Large-Scale Graph Stream Processing Platform", "Abstract": "In recent years, real-time data mining for large-scale time-\nevolving graphs is becoming a hot research topic. Most of the\nprior arts target relatively static graphs and also process them in\nstore-and-process batch processing model. In this paper we\npropose a method of applying on-the-fly and incremental graph\nstream computing model to such dynamic graph analysis. To\nprocess large-scale graph streams on a cluster of nodes\ndynamically in a scalable fashion, we propose an incremental\nlarge-scale graph processing model called \u201cIncremental GIM-V\n(Generalized Iterative Matrix-Vector Multiplication)\u201d. We also\ndesign and implement UNICORN, a system that adopts the\nproposed incremental processing model on top of IBM InfoSphere\nStreams. Our performance evaluation demonstrates that our\nmethod achieves up to 48% speedup on PageRank with Scale 16\nLog-normal Graph (vertexes=65,536, edges=8,364,525) with 4\nnodes, 3023% speedup on Random walk with Restart with\nKronecker  Graph  with  Scale  18  (vertexes=262,144,\nedges=8,388,608) with 4 nodes against original GIM-V."}
{"Title": "Towards Scalable X10 Based Link Prediction for Large\nScale Social Networks", "Abstract": "The use of social application such as Twitter or FaceBook\nbecomes popular in recent years. In particular, Twitter in-\ncreases the number of the users rapidly from 2009 as the\nplace that users can tweet anything in 140 characters.\nIn the area of social network analysis, the user network\nof Twitter is frequently analyzed. Haewoon, et.al.,[4] ana-\nlyzed the Twitter user network from various point of view in\n2009, and they show that the Twitter user network has some\ndifferent feature from conventional social networks. Bong-\nwon, et.al., also made a collection of 74 millions tweets in\n2010, and investigated the influence that \u201dretweet\u201d gives for\ndiffusion of the information. Such analysis not only reveal\nthe unique characteristics of Twitter user network, but also\nmake some networking service such as finding users who are\nsimilar to someone, or the recommendation of commodities\nby using tweet information.\nThere are some analysis such as clustering which needs en-\ntire data of the network. However, since social networks are\nincreasing day by day, it becomes impossible to obtain the\nentire network by crawling. As a solution of this problem,\nthere is the network analysis called link prediction. This\nenables to predict true network from a given part of the\nnetwork. If we use link prediction, we can recover the entire\nnetwork from the network data which we already obtained,\nand apply some analysis such as clustering to predicted net-\nwork, then we may get the approximate result of the analysis\nfor the entire network.\nIn our research, we implemented one of the link prediction\nalgorithm named Link Propagation in X10, which is a par-\nallel programming language. And evaluated its scalability\nand precision with Twitter user network data."}
{"Title": "A Novel Link Prediction Approach for Scale-free Networks", "Abstract": "The link prediction problem is to predict the existence of a\nlink between every node pair in the network based on the\npast observed networks arising in many practical applica-\ntions such as recommender systems, information retrieval,\nand the marketing analysis of social networks. Here, we\npropose a new mathematical programming approach for pre-\ndicting a future network utilizing the node degree distribu-\ntion identified from historical observation of the past net-\nworks. We develop an integer programming problem for\nthe link prediction problem, where the objective is to max-\nimize the sum of link scores (probabilities) while respecting\nthe node degree distribution of the networks. The perfor-\nmance of the proposed framework is tested on the real-life\nFacebook networks. The computational results show that\nthe proposed approach can considerably improve the perfor-\nmance of previously published link prediction methods."}
{"Title": "Pruned Labeling Algorithms: Fast, Exact, Dynamic, Simple\nand General Indexing Scheme for Shortest-Path Queries", "Abstract": "Shortest-paths and distances are two of the most fundamental\nnotions for pairs of nodes on a network, and thus they play an\nimportant role in a wide range of applications such as network\nanalysis and network-aware search. In this talk, I will introduce\nour indexing method for efficiently answering shortest-paths,\nreferred to as pruned landmark labeling (SIGMOD\u201913). In spite of\nits simplicity, it significantly outperforms previous indexing\nmethods in both scalability and query time. Moreover,\ninterestingly, it turned out that the algorithm automatically\nexploits the common structures of real networks. We also briefly\nmention its variants: pruned path labeling (CIKM\u201913), pruned\nhighway labeling (ALENEX\u201914) and historical  pruned\nlandmark labeling (WWW\u201914)."}
